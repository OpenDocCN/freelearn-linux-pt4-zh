- en: Chapter 2.  NSX Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will have a high-level discussion on NSX architecture.
    To properly install and configure NSX, we should understand the core components
    that are involved in the NSX solution. By the end of this chapter, we will have
    a good understanding of the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Network planes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX core components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying controller roles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The controller cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VXLAN architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing network planes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a traditional switch/router, routing and packet forwarding is ideally done
    on the same device. What does this mean? Let''s take a classic example of configuring
    a router. We might configure SSH for managing the router and later configure routing
    protocols to exchange the routes with its neighbors. All these common tasks are
    done specifically on the same hardware device. So, in a nutshell, each and every
    router will take a forwarding decision based on the configuration of routers.
    The power of software-defined networking is decoupling the forwarding and control
    plane functionality to a centralized device called a controller and the end result
    is the controller maintaining the forwarding information and taking decisions
    rather than going via hop by hop in the traditional way. As shown in the following
    figure, the three functional planes of a network are the management plane, the
    control plane, and the data plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing network planes](img/image_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The three functional planes of a network are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Management plane**: The management plane is a straightforward concept: a
    slice of software through which we will make changes and configure network devices,
    and protocols such as SSH and SNMP are used to access and monitor them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Control plane**: The classic example for the control plane is learning routes
    and making decisions based on routing algorithms. However, control plane functionality
    is not limited to learning routes. The control plane also helps in pairing with
    vendor-specific devices, and secures control plane access such as SSH and Telnet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data plane**: Data plane traffic is traditionally performed in dedicated
    hardware devices by consuming a little bit of compute resources. Primarily, the
    data plane is focused on data forwarding tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I know most of us will be wondering why we are discussing network planes here.
    Network planes are DNA in the NSX world and we have all three planes which make
    the network virtualization layer.
  prefs: []
  type: TYPE_NORMAL
- en: NSX vSphere components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NSX uses the management plane, control plane, and data plane models. The components
    are represented diagrammatically in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NSX vSphere components](img/image_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The management plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The management plane contains the NSX Manager and vCenter Server. It is important
    to know that each NSX Manager should be registered with only one vCenter Server.
    The NSX Manager provides a management UI and API for NSX. We will be discussing
    NSX Manager and vCenter Server integration during NSX Manager installation and
    configuration modules. Once the integration is done, NSX Manager can be managed
    from a vSphere web client, which acts as a single pane of glass for configuring
    and securing the vSphere infrastructure. Immediate benefit is network administrators
    no longer need to switch between multiple management consoles. All network services
    can be configured and monitored from a single interface.
  prefs: []
  type: TYPE_NORMAL
- en: The control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The control plane primarily consists of NSX Controllers and the control VM,
    which allows us to perform distributed routing. The control plane also allows
    multicast-free VXLAN networks, which was a limitation in earlier vCloud networking
    and security versions. Controllers maintain ARP, VTEP (VXLAN tunnel endpoint),
    and MAC table. The NSX logical router control virtual machine and VMware NSX Controller
    are virtual machines that are deployed by VMware NSX Manager. The **User World
    Agent** (**UWA**) is composed of the ntcpad and vsfwd daemons on the ESXi host.
    Communication related to NSX between the NSX Manager instance or the NSX Controller
    instances and the ESXi host happen through the UWA. NSX Controller clusters are
    deployed in ODD number fashion and the maximum number of supported controllers
    is three. Since every controller in a control cluster is active at the same time,
    it ensures that the control plane is intact even if there is a controller failure.
    Controllers talk to each other to be in sync through a secured SSL channel. Controllers
    use a slicing technology to divide the workload among other controllers. Have
    a look at the following figure, which is a three-node controller cluster, in which
    slicing technology is dividing the workload across the controllers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The control plane](img/B03244_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is important to understand that there are two types of applications running
    on each of these controllers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VXLAN**: Enables extension of a Layer-2 IP subnet anywhere in the fabric,
    irrespective of the physical network design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logical router**: Routing between IP subnets can be done in a logical space
    without traffic touching the physical router. This routing is performed directly
    in the hypervisor kernel with minimal CPU/memory overhead. This functionality
    provides an optimal data path for routing traffic within the virtual infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The functionality of these applications is to learn and populate controller
    tables, and also distribute learned routes to underlying ESXi hosts. Lastly, the
    control plane and data plane configuration will be intact even during the failure
    of a management plane-this is the real power of software-defined networking.
  prefs: []
  type: TYPE_NORMAL
- en: Three-node controller clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a large-scale distributed system with *n* number of servers, it is extremely
    difficult to ensure that one specific server can perform a write operation to
    a database or that only one server is the master that processes all writes. The
    fundamental problem is we do not have a simple way through which process execution
    can be done. How do we resolve this? All we need is to promote one server as master,
    and have some consensus with other servers. Paxos is a distributed consensus protocol
    published in 1989\. The algorithm also ensures we have a leader election whenever
    there is a server failure. Paxos distinguishes the roles of proposer, acceptor,
    and learner, where a process (server/node) can play one or more roles simultaneously.
    The following are a few vendors who are using the Paxos algorithm extensively
    for the same reason:'
  prefs: []
  type: TYPE_NORMAL
- en: VMware NSX Controller uses a Paxos-based algorithm within an NSX Controller
    cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Web Services uses the Paxos algorithm extensively to power its platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nutanix implements the Paxos algorithm to ensure strict consistency is maintained
    in cassandara (for storing cluster metadata)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mesos uses the Paxos algorithm for its replicated log coordination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google uses the Paxos algorithm for providing the Chubby lock service for loosely
    coupled distributed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Windows fabric used by many of the Azure services makes use of the Paxos
    algorithm for replication between nodes in a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Controllers are deployed in a three-node clustered fashion to ensure we
    are getting the highest level of resiliency since the controllers are running
    a fault-tolerant, distributed consensus algorithm called **Paxos**.
  prefs: []
  type: TYPE_NORMAL
- en: Controller roles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The NSX Controller provides the control plane functions for routing and logical
    switching functions. Each controller node is running a set of roles that defines
    the type of task the controller node can run. There are total of five roles running
    in a controller node; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switch manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directory server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While each of these roles needs a different master, it is important to understand
    that the leader is the responsible controller for allocating the tasks to other
    controllers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the responsibilities of various roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Controller roles](img/B03244_02_04-1024x783.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we know, three node controllers form a control cluster. We will have a look
    at the role election per controller. Each role has a master controller node and
    only if a master controller node for a given role fails there would be a cluster-wide
    election for a new master role. This is one of the prime reasons a three-node
    cluster is a must in an enterprise environment, to avoid any split-brain situation
    which might eventually end up with data inconsistencies and the whole purpose
    of the control plane would be defeated. In the following figure, we have a three-node
    control cluster running and each controller is running a master role:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Controller 1**: Directory server master role running'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller 2**: Persistence server master role running'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller 3**: API and switch manager master role running'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Controller roles](img/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The data plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NSX logical switches, ESXi hypervisor, distributed switches, and NSX edge devices
    are all data plane components. Once the management plane is up and running, we
    can deploy control plane and data plane software and components. Behind the scenes,
    these three **VMware Installation Bundles** (**VIB**) get pushed to the underlying
    ESXi hypervisor:'
  prefs: []
  type: TYPE_NORMAL
- en: VXLAN VIB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed routing VIB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distributed firewall VIB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Up to now, we have discussed the management, control, and data plane components
    in the NSX world; in the upcoming modules, we will have a closer look at the installation
    part and design specification for each layer.
  prefs: []
  type: TYPE_NORMAL
- en: Overlay networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A virtual network which is built on top of a physical network is called an
    overlay network. Does that sound familiar by any chance? Most enterprise environments
    would have used VPN technology for securing private or public networks, which
    is an IP-over-IP overlay technology. However, it is very important to understand
    that not all overlay networks are built on top of IP networks. The real question
    is, why do we require an overlay network ? As we can see from the following figure,
    we have two data centers and each of these data centers is following a spine-leaf
    topology. Flexible workload placement and virtual machine mobility, along with
    strong multitenancy for each tenant, are common asks in a virtualized infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Overlay networks](img/B03244_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: VXLAN, MPLS, NVGRE, VPN, and OTV are some of the classic examples of network-based
    overlays. Let's go back to the roots of server virtualization. Server virtualization
    virtualizes a physical server and allows multiple virtual machines to run on top
    of it with its own compute and storage resources. Now, each virtual machine might
    be having one or more than one MAC address, which eventually increases the MAC
    table in the network. Added to that, VM mobility forces the broadcast domains
    to grow. The traditional way of segmenting a network would be with the help of
    VLAN. As per the 802.1q standard, a VLAN tag is a 12-bit space providing a maximum
    of 4,096 VLANS. This is not a feasible solution in current multi tenant cloud
    environments wherein multiple machines reside on the same server and network isolation
    is required and workloads keep spiking, which demands a few VLANs to be provisioned
    for future growth and VLAN sprawl continues to happen based on workload mobility.
    Overlay networks alleviate this problem by providing Layer 2 connectivity independent
    of physical networks. We all know new technologies solve a lot of problems; however,
    there will always be challenges associated with them. Guess how difficult it will
    be for a network administrator to troubleshoot an overlay network! Trust me, when
    the mapping between the overlay network and physical network is crystal clear,
    it is extremely easy to perform troubleshooting. An NSX-VXLAN-based overlay network
    is a host-based overlay network which uses a UDP-based VXLAN encapsulation.
  prefs: []
  type: TYPE_NORMAL
- en: The VLAN packet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before trying to understand VXLAN, let''s go back to the fundamentals of a
    VLAN packet. How does  tagging work in a VLAN packet? It''s very simple concept:
    4 bytes are inserted into the Ethernet header field (IEEE), which are a combination
    of a 2-byte **Tag Protocol Identifier** (**TPID**) and 2 bytes of **Tag Control
    Information** (**TCI**). The priority field is a 3-bit field that allows information
    priority to be encoded in the overall frame, 0 being the lowest priority and 8
    the highest value. CFI is typically a bit used for compatibility between Ethernet
    and token ring networks and if the value is 0, those are Ethernet switches. Last
    but not the least, we have the VLAN field - VID:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The VLAN packet](img/B03244_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The act of creating a VLAN on a switch involves defining a set of switch ports,
    and end devices get connected to these ports. They all become part of that VLAN
    domain which eventually stops a broadcast not to be forwarded to another set of
    VLANs. I know whatever we have discussed so far is something which we will have
    heard in every networking class. This is just a repetition to ensure we never
    forget it. Now we can move on and discuss VXLAN.
  prefs: []
  type: TYPE_NORMAL
- en: A VXLAN overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VXLAN is a technology developed by vendors such as Arista, VMware, Cisco, and
    Broadcom. Each of these VXLAN networks is called a logical switch (virtual wires
    in vCloud network security solution) and they are identified by a 24-bit segment-ID.
    In this way, customers can create up to 16 million VXLAN networks. **Virtual Tunnel
    End Points** (**VTEPs**) are the endpoints that encapsulate and de-encapsulate
    the VXLAN frames. Let''s understand a few key terminologies in VXLAN; and we will
    discuss VXLAN frames after that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VXLAN VIB**: VXLAN VIB or VMkernel modules are pushed to an underlying hypervisor
    during ESXi host preparation from NSX Manager.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vmknic adapter**: Virtual adapter is responsible for sending ARP, DHCP, and
    multicast join messages. Yes, there would be an IP assigned (static/dynamic) to
    vmknic from the VTEP IP pool, which is one of the prerequisites for VXLAN configuration.
    NSX supports multiple VXLAN vmknics per host for uplink load balancing features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VXLAN port group**: VXLAN port group is preconfigured during host preparation
    and it includes components and features such as NIC teaming policy, VLAN, and
    other NIC details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VTEP proxy**: VTEP proxy is a VTEP that forwards VXLAN traffic to its local
    segment from another VTEP in a remote segment. NSX uses three modes of VXLAN:
    unicast, multicast, and hybrid. In unicast mode, VXLAN VTEP proxy is called UTEP
    and in hybrid mode, it is called MTEP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VNI**: **VXLAN Network Identifier** (**VNI**) or segment ID is similar to
    VLAN-bit field; however, VNI is a 24-bit address that gets added to a VXLAN frame.
    This is one of the most critical elements in VXLAN frames, since it uniquely identifies
    the VXLAN network just as a VLAN ID identifies a VLAN network. VNI numbers start
    with 5000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transport zones**: Transport zones are basically a cluster or a group of
    clusters which define a VXLAN boundary or domain. Transport zones can be local
    or universal for multi VC deployment based on NSX design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VXLAN frame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the main components of the VXLAN frame along with the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outer Ethernet header (L2 header)**: The destination MAC in the outer Ethernet
    header can be a next hop router MAC or destination VTEP MAC addresses and the
    source outer MAC address will be the source VTEP MAC address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outer IP header (L3 header)**: Respective VTEP source and destination IP
    will be populated in the outer IP header.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outer UDP header (L4 header)**: The outer UDP header is a combination of
    source port and destination port. IANA has assigned the value `4789` for UDP;
    however, the VMware NSX default UDP port is `8472.` So it is important to allow
    port `8472` in physical/virtual firewall devices for VXLAN traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VXLAN header**: This is an 8-byte field which will have the VXLAN flag value,
    segment-ID, and reserved field:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The VXLAN frame](img/image_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since we have discussed the VXLAN packet format, let's move on and check the
    inner Ethernet frame.
  prefs: []
  type: TYPE_NORMAL
- en: The inner Ethernet frame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the main components of the inner Ethernet frame along with
    the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frame Check Sequence (FCS)**: FCS is a field at the end of the frame which
    is used to store the **cyclic redundancy check** (**CRC**) information. CRC is
    an algorithm which will run each time a frame is built, based on the data in the
    frame. When a receiving host receives the frame and runs the CRC, the answer should
    be the same. If not, the frame is discarded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ethernet payload**: The length of a frame is very important, whether it is
    maximum or minimum frame size. The Ethernet payload field is a length field delimiting
    the length of the packet.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inner source MAC address**: The inner source MAC address will be the MAC
    address of the virtual machine which is connected to the VXLAN network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outer destination MAC address**: The outer destination MAC address will be
    the MAC address of the destination virtual machine. Pause for a moment: what if
    a virtual machine doesn''t know the destination virtual machine MAC? There is
    no rocket science here. All it does is a traditional broadcast, `ff:ff:ff:ff:ff:ff`
    (destination MAC), which is the broadcast address, and addresses every network
    adapter in the network and later this complete L2 frame will get encapsulated
    with the VXLAN frame before leaving the hypervisor host.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The life of a VXLAN packet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we need to understand how a sequence of packets from a source machine reaches
    the destination in a unicast, multicast, or broadcast domain, all we do is a simple
    packet walk. In the following example, if **VM-A** is communicating with **VM-B**
    for the first time in a VXLAN domain, how is the encapsulation and de-encapsulation
    process happening?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The life of a VXLAN packet](img/image_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now go through the steps one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: VM A-192.168.1.1, MAC- A on ESXi host A generates a broadcast frame (Layer 2
    broadcast frame)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: VTEP A on host A encapsulates the broadcast frame into UDP header with the destination
    IP as multicast/unicast IP based on VXLAN replication modes (Layer 2 header gets
    encapsulated with VXLAN header)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We will certainly have a discussion on VXLAN replication modes in [Chapter 4](ch04.html
    "Chapter 4. NSX Virtual Networks and Logical Router"), *NSX Virtual Networks and
    Logical Routing*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The physical network delivers the packet to the host, because it was part of
    either the multicast group or unicast IP that we have defined during VXLAN replication
    modes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: VTEP B on ESXi host B will look at the VXLAN header (24-bit) and if matches
    with the VTEP table entries, it removes the VXLAN encapsulation header and delivers
    the Layer 2 packet to the virtual machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding four steps mentioned are a simple packet walk on how virtual machines
    communicate in a VXLAN network. However, I haven't explained about ARP suppression
    and VTEP table learning because I want to explain that during NSX virtual networks
    and logical router.
  prefs: []
  type: TYPE_NORMAL
- en: Think for a minute and check in which scenario there won't be any encapsulation
    and de-encapsulation even though virtual machines are connected to a VXLAN network.
    No brainteasers here. If both the virtual machines are residing on same ESXi host
    and same VXLAN network, all it does is traditional Layer 2 learning and there
    is no encapsulation or de-encapsulation. These are very important points to note
    since it would ease a lot of troubleshooting issues when virtual machine to virtual
    machine communication is not happening due to VXLAN/physical network issues. I'm
    sure we have done such troubleshooting in vSphere environments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began the chapter with a brief introduction of NSX core components and looked
    at the management, control, and data plane components. We then discussed the NSX
    Manager and the NSX Controller clusters, which was followed by a VXLAN architecture
    overview discussion where we looked at the VLAN and VXLAN packet followed by a
    simple packet walk. Now we are familiar with the core components and their functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss NSX Manager installation and configuration.
  prefs: []
  type: TYPE_NORMAL
