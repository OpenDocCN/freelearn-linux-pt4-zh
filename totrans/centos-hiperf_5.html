<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;5.&#xA0;Monitoring the Cluster Health" id="173721-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Monitoring the Cluster Health</h1></div></div></div><p class="calibre8">In <a class="calibre1" title="Chapter 2. Installing Cluster Services and Configuring Network Components" href="part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0">Chapter 2</a>, <span class="strong"><em class="calibre9">Installing Cluster Services and Configuring Network Components</em></span>, we mentioned that becoming familiar with PCS and its myriad options would be helpful along the path that might lead us to the installation of a full operational high availability cluster. Although during the previous chapters we confirmed how true that statement was, here we will make further use of PCS to monitor the performance and availability of our cluster in order to identify and prevent possible bottlenecks and troubleshoot any issue that may arise.</p></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Monitoring the Cluster Health" id="173721-1d2c6d19b9d242db82da724021b51ea0">
<div class="book" title="Cluster services and performance"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec35" class="calibre1"/>Cluster services and performance</h1></div></div></div><p class="calibre8">Although every system administrator must be well acquainted with the widely used Linux commands, such<a id="id230" class="calibre1"/> as <code class="email">top</code> and <code class="email">ps</code>, to quickly report a snapshot of running daemons<a id="id231" class="calibre1"/> and other processes in each node, you must also learn to rely on the new utilities provided by CentOS 7 to start our node monitoring, which we have introduced in previous chapters. But even more importantly, we will also use PCS-based commands to gain further insight into our cluster and its resources.</p></div></div>
<div class="book" title="Monitoring the node status"><div class="book" id="181NK2-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec36" class="calibre1"/>Monitoring the node status</h1></div></div></div><p class="calibre8">As you can guess, perhaps the first thing that you always need to check is the status of each node—whether they are online or offline. Otherwise, there is little point in proceeding<a id="id232" class="calibre1"/> with further availability and performance analysis.</p><p class="calibre8">If you have a network <a id="id233" class="calibre1"/>management system (such as <span class="strong"><strong class="calibre2">Zabbix</strong></span> or <span class="strong"><strong class="calibre2">Nagios</strong></span>) server, you <a id="id234" class="calibre1"/>can easily monitor the status of your cluster members and receive alerts when they are unreachable. If not, you must come up with a supplementary solution of your own (which may not be as effective or errorproof) that you can use to detect when a node has gone offline.</p><p class="calibre8">One such solution is a simple bash script (we will name it <code class="email">pingreport.sh</code>, save it inside <code class="email">/root/scripts</code>, and make it executable with <code class="email">chmod +x /root/scripts/pingreport.sh</code>) which will periodically ping your nodes from another host and report via an e-mail to the system administrator if one of them is offline in order for you to take appropriate action. The following shell script does just that for nodes with IP addresses <code class="email">192.168.0.2</code> and <code class="email">192.168.0.3</code> (you can add as many nodes in the <code class="email">NODES</code> variable, which will be used in the following for loop, but remember to separate them with a blank space). If both nodes are pingable, the report will be empty and no e-mails will be sent.</p><p class="calibre8">In order to<a id="id235" class="calibre1"/> take advantage of the following script, you will need to have an e-mail solution in place in order to send out alerts. In this case, we use the mail tool called mailx, which is available after installing a package (<code class="email">yum install mailx</code>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">#!/bin/bash</strong></span>

<span class="strong"><strong class="calibre2"># Directory where the ping script is located</strong></span>
<span class="strong"><strong class="calibre2">DIR=/root/scripts</strong></span>

<span class="strong"><strong class="calibre2"># Hostname or IP of remote host (to send alerts to)</strong></span>
<span class="strong"><strong class="calibre2">REMOTEHOST="192.168.0.5"</strong></span>

<span class="strong"><strong class="calibre2"># Name of report file</strong></span>
<span class="strong"><strong class="calibre2">PING_REPORT="ping_report.txt"</strong></span>

<span class="strong"><strong class="calibre2"># Make sure the current file is empty</strong></span>
<span class="strong"><strong class="calibre2">cat /dev/null &gt; $DIR/$PING_REPORT</strong></span>

<span class="strong"><strong class="calibre2">#Current date to be used in the ping script</strong></span>
<span class="strong"><strong class="calibre2">CURRENT_DATE=$(date +'%Y-%m-%d %H:%M')</strong></span>

<span class="strong"><strong class="calibre2"># Node list</strong></span>
<span class="strong"><strong class="calibre2">NODES="node01 node02"</strong></span>

<span class="strong"><strong class="calibre2"># Loop through the list of nodes</strong></span>
<span class="strong"><strong class="calibre2">for node in $NODES</strong></span>
<span class="strong"><strong class="calibre2">     do</strong></span>
<span class="strong"><strong class="calibre2">     LOST_PACKETS=$(ping -c 4 $node | grep -i unreachable | wc -l)</strong></span>
<span class="strong"><strong class="calibre2">     if [ $LOST_PACKETS -ne "0" ]</strong></span>
<span class="strong"><strong class="calibre2">             then</strong></span>
<span class="strong"><strong class="calibre2">             echo "$"LOST_PACKETS packets were missed while pinging $node at $CURRENT_DATE" &gt;&gt; $DIR/$PING_REPORT</strong></span>
<span class="strong"><strong class="calibre2">     fi</strong></span>
<span class="strong"><strong class="calibre2">done</strong></span>

<span class="strong"><strong class="calibre2"># Mail the report unless it's' empty</strong></span>
<span class="strong"><strong class="calibre2">if [ -s "$"DIR/$PING_REPORT" ]</strong></span>
<span class="strong"><strong class="calibre2">     then</strong></span>
<span class="strong"><strong class="calibre2">     mail root@$REMOTEHOST -s "Ping report" -a $DIR/$PING_REPORT</strong></span>
<span class="strong"><strong class="calibre2">fi</strong></span></pre></div><p class="calibre8">Even though the<a id="id236" class="calibre1"/> preceding script is enough to determine whether a node is pingable or not, you can tweak that script as you like, and then add it to cron in order for it to run automatically on the desired frequency. For example, the following cron job will execute the script every five minutes, regardless of the day:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">*/5 * * * * /root/scripts/pingreport.sh</strong></span></pre></div><p class="calibre8">If you want to run the script manually, you can do so as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">/root/scripts/pingreport.sh</strong></span></pre></div><p class="calibre8">The following example indicates that both <code class="email">192.168.0.2</code> and <code class="email">192.168.0.3</code> were not pingable when the script was last run. Note that for simplicity, the script was executed from <code class="email">node01</code>, a cluster member; however, under normal circumstances, you will want to use a separate host for this:</p><div class="mediaobject"><img src="../images/00053.jpeg" alt="Monitoring the node status" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We will resume working with the script later in this chapter and extend its functionalities.</p><p class="calibre8">Now, it is time to dig a little deeper and view the status of the nodes configured in <code class="email">corosync</code>/<code class="email">pacemaker</code> with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status nodes pacemaker | corosync | both</strong></span></pre></div><p class="calibre8">In the<a id="id237" class="calibre1"/> preceding command, a vertical bar is used to indicate mutually exclusive arguments.</p><p class="calibre8">In the following screenshot, you can see how <code class="email">pcs status nodes both</code> returns the status of both <code class="email">pacemaker</code> and <code class="email">corosync</code> on both nodes:</p><div class="mediaobject"><img src="../images/00054.jpeg" alt="Monitoring the node status" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre8">Although you can check the cluster's overall status with <code class="email">pcs status</code>, as we have mentioned earlier, <code class="email">pcs status nodes both</code> will give you the fine-grained node status information. You can stop one (or both) of the services on either node and run this same command to verify. This is equivalent to using <code class="email">systemctl is-active pacemaker | corosync</code> on each node.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Monitoring the resources"><div class="book" id="190862-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec37" class="calibre1"/>Monitoring the resources</h1></div></div></div><p class="calibre8">As we have explained in the previous chapters, a cluster resource is a highly available service that is<a id="id238" class="calibre1"/> made available through at least one of the nodes. Among the resources that we configured up until this point, we can mention the virtual IP, the replicated storage device, the web server, and the database server. You can refer to <a class="calibre1" title="Chapter 4. Real-world Implementations of Clustering" href="part0030_split_000.html#SJGS1-1d2c6d19b9d242db82da724021b51ea0">Chapter 4</a>, <span class="strong"><em class="calibre9">Real-world Implementations of Clustering</em></span>, where we added constraints that indicated how (in what order) and where (in which node) the cluster resources should be started.</p><p class="calibre8">Either <code class="email">pcs status</code> or <code class="email">pcs resource show</code>, the preferred alternative, will list the names and status of all currently configured resources.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre8">If you specify a resource using its ID (that is, <code class="email">pcs resource show virtual_ip</code>), you will see the options for the configured resource. On the other hand, if <code class="email">--full</code> is specified (<code class="email">pcs resource show --full</code>), all configured resource options will be displayed instead.</p></div><p class="calibre8">If a resource is started on the wrong node (for example, if it depends on a service that is currently active on another node), you will get an informative message when you attempt to use it. For<a id="id239" class="calibre1"/> example, the following screenshot shows that <code class="email">dbserver</code> is started on <code class="email">node02</code>, whereas its associated underlying storage device (<code class="email">db_fs</code>) has been started on <code class="email">node01</code>. You will recall from earlier chapters that this is part of the output of <code class="email">pcs status</code>:</p><div class="mediaobject"><img src="../images/00055.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For this reason, if you attempt to log on to the database server using the virtual IP address (which is the common link to the cluster resources), you will get the error message indicated in the following screenshot telling you that you can't connect to the MariaDB instance:</p><div class="mediaobject"><img src="../images/00056.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Let's see what happens (as shown in the next screenshot) when we move the <code class="email">dbserver</code> resource to <code class="email">node01</code> and enable it manually so that it starts right away. The following constraint is intended to cause <code class="email">dbserver</code> to prefer <code class="email">node01</code> so that it always runs on <code class="email">node01</code> whenever such a node is available:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs constraint location dbserver prefers node01=INFINITY</strong></span>
<span class="strong"><strong class="calibre2">pcs resource restart dbserver</strong></span></pre></div><div class="mediaobject"><img src="../images/00057.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre8">If you need to remove a constraint, find out its id with <code class="email">pcs constraint --full</code> and locate the associated resource. Then, delete it with <code class="email">pcs constraint remove constraint_id</code>, where <code class="email">constraint_id</code> is the identification as returned by the first command. You can also manually remove resources from one node to another with <code class="email">pcs resource move &lt;resource_id&gt; &lt;node_name&gt;</code>, but be aware that the current constraints may or may not allow you to successfully complete the operation.</p></div><p class="calibre8">Now we<a id="id240" class="calibre1"/> can access the database server resource as expected, as shown here:</p><div class="mediaobject"><img src="../images/00058.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Once in a while, you may encounter some errors during or after a failover procedure or during boot—you name it. These messages are visible in the output of <code class="email">pcs status</code>, as in the excerpt shown here:</p><div class="mediaobject"><img src="../images/00059.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Before we proceed further, perhaps you will ask yourself: What if I want to save all available information about cluster problems to properly analyze and troubleshoot offline? If you are expecting PCS to have a tool to help you with that, you are right. Put a date and time following the <code class="email">--from</code> and <code class="email">--to</code> options and replace <code class="email">dest</code> with a filename (a specific example is provided in the following command as well):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster report [--from "YYYY-M-D H:M:S" [--to "YYYY-M-D" H:M:S"]]" dest</strong></span></pre></div><p class="calibre8">This will <a id="id241" class="calibre1"/>create a tarball containing every piece of information that is needed when reporting cluster problems. If <code class="email">--from</code> and <code class="email">--to</code> are not used, the report will include the data of the last 24 hours.</p><p class="calibre8">In the screenshot that will follow, we have omitted the <code class="email">--from</code> and <code class="email">--to</code> flags for brevity, and we can see yet another reason why setting up key-based authentication via <code class="email">ssh</code> during <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span> was not a mere suggestion—you have to report cluster information from both nodes.</p><p class="calibre8">In our case, we will execute the following command to obtain a tarball named <code class="email">YYYY-MM-DD-report.tar.gz</code> in the current working directory. Note that the date part in the filename is for identification purposes only:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster report $(date +%Y-%m-%d)-report</strong></span></pre></div><div class="mediaobject"><img src="../images/00060.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Once the tarball with the report files has been created, you can untar and examine it. You will notice that it contains the files and directories seen in the following image. Before proceeding further, you may want to take a look at some of them, shown as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">tar xzf $(date +%Y-%m-%d)-report.tar.gz</strong></span>
<span class="strong"><strong class="calibre2">cd $(date +%Y-%m-%d)-report</strong></span></pre></div><div class="mediaobject"><img src="../images/00061.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, of course<a id="id242" class="calibre1"/> you want to purge records of the past failed actions that have been resolved. For this reason, PCS allows you to instruct the cluster to forget the operation history of a resource (or all of them), reset the fail count, and redetect the current states:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource cleanup &lt;resource_id&gt;</strong></span></pre></div><p class="calibre8">Note that if <code class="email">resource_id</code> is not specified, then all resources/STONITH devices will be cleaned up.</p><p class="calibre8">Finally, while we are still talking about monitoring cluster resources, we might as well ask ourselves: Is there a way we can backup the current cluster configuration files and restore them later if needed, and can we easily go back to a previous configuration? The answer to both questions is yes—let's see how.</p><p class="calibre8">In order to back up the cluster configuration files, you will use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs config backup &lt;filename&gt;</strong></span></pre></div><p class="calibre8">Here, <code class="email">&lt;filename&gt;</code> is a file identification of your choice to which PCS will append the <code class="email">tar.bz2</code> extension after creating the tarball.</p><p class="calibre8">Consider the following example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs config backup cluster_config_$(date +%Y-%m-%d)</strong></span></pre></div><p class="calibre8">This will result in the tarball backup with the contents shown in the following screenshot For our convenience, let us create a subdirectory named <code class="email">cluster_config</code> inside our current working directory. We will use this newly created subdirectory to extract the contents of the report tarball:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mkdir cluster_config</strong></span>
<span class="strong"><strong class="calibre2">tar xzf cluster_config_$(date +%Y-%m-%d).tar.bz2 -C cluster_config</strong></span>
<span class="strong"><strong class="calibre2">ls -R cluster_config</strong></span></pre></div><div class="mediaobject"><img src="../images/00062.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre8">If you have followed the installation process step by step, as outlined in this book, bzip2 will most likely not be available. You will need to install it with <code class="email">yum update &amp;&amp; yum install bzip2</code> in order to untar the cluster configuration tarball.</p></div><p class="calibre8">Restoring the <a id="id243" class="calibre1"/>configuration is just as easy (you will need to stop the node and then start it again after the restoration process is completed), use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs config restore [--local] &lt;filename&gt;</strong></span></pre></div><p class="calibre8">This command will restore the backed-up cluster configuration files on all nodes using the backup as source. If you only need to restore the files on the current node, use the <code class="email">--local flag</code>. Note that filename must be the <code class="email">.tar.bz2</code> file (not the extracted files).</p><p class="calibre8">You can also go back to a certain point in time, as far as cluster configuration is concerned, using <code class="email">pcs config checkpoint</code> with its associated options. With no options, <code class="email">pcs config checkpoint</code> will list all available configuration checkpoints, as shown here:</p><div class="mediaobject"><img src="../images/00063.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <code class="email">pcs config checkpoint view &lt;checkpoint_number&gt;</code> command displays to standard <a id="id244" class="calibre1"/>output the specified configuration checkpoint details, as shown in the next screenshot. Consider the following example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs config checkpoint view 1</strong></span></pre></div><div class="mediaobject"><img src="../images/00064.jpeg" alt="Monitoring the resources" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <code class="email">pcs config checkpoint restore &lt;checkpoint_number&gt;</code> command restores cluster configuration to a specified checkpoint, which is why it's a great idea to check the details of the desired checkpoint before restoring.</p></div>

<div class="book" title="Monitoring the resources">
<div class="book" title="When a resource refuses to start"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec19" class="calibre1"/>When a resource refuses to start</h2></div></div></div><p class="calibre8">Under normal circumstances, cluster resources will be managed automatically without much intervention from the system administrator. However, there will be times when something may<a id="id245" class="calibre1"/> prevent a resource from starting properly, and it will be necessary to take immediate action.</p><p class="calibre8">As the man page for PCS states,</p><div class="blockquote"><blockquote class="blockquote1"><p class="calibre16"><span class="strong"><em class="calibre9">Starting resources on a cluster is (almost) always done by pacemaker and not directly from PCS. If your resource isn't starting, it's usually due to either a misconfiguration of the resource (which you debug in the system log), or constraints preventing the resource from starting or the resource being disabled. You can use <code class="email">pcs resource debug-start</code> to test resource configuration, but it should not normally be used to start resources in a cluster.</em></span></p></blockquote></div><p class="calibre8">Having said that, when <code class="email">pacemaker</code> cannot, for some reason, properly start a resource, execute the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource debug-start &lt;resource id&gt; [--full]</strong></span></pre></div><p class="calibre8">This will force the specified resource to start on the current node, ignoring the cluster recommendations. The result will be printed to the screen (use the <code class="email">--full</code> flag to obtain more detailed output) and will provide helpful information to assist you in troubleshooting the resource and the cluster operation.</p><p class="calibre8">In the following screenshot, the output of <code class="email">pcs resource debug-start virtual_ip --full</code> is truncated for the sake of brevity:</p><div class="mediaobject"><img src="../images/00065.jpeg" alt="When a resource refuses to start" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">From this<a id="id246" class="calibre1"/> example, you can begin to glimpse how useful this command can be as it provides you with very detailed information, step by step, of the resource operation. For example, if the <code class="email">dbserver</code> resource refuses to start and returns errors even after repeatedly having cleaned it up, run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource debug-start dbserver --full | less</strong></span></pre></div><p class="calibre8">With this, you will be able to view—with great detail—the steps that are usually performed by the cluster when trying to bring up such a resource. If this process fails at some point, you will be provided with a description of what went wrong and when, and then you will be better able to fix it.</p></div></div>

<div class="book" title="Monitoring the resources">
<div class="book" title="Checking the availability of core components"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec20" class="calibre1"/>Checking the availability of core components</h2></div></div></div><p class="calibre8">Before <a id="id247" class="calibre1"/>wrapping up, let's go back to the first example (checking the online status of each node) and extend it so that we can also monitor the core components of the cluster framework, that is, <code class="email">pacemaker</code>, <code class="email">corosync</code>, and <code class="email">pcsd</code>, as outlined earlier in <a class="calibre1" title="Chapter 2. Installing Cluster Services and Configuring Network Components" href="part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0">Chapter 2</a>, <span class="strong"><em class="calibre9">Installing Cluster Services and Configuring Network Components</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre8">In order to ensure a successful connection via <code class="email">ssh</code> from a node to itself, you will need to copy its key to <code class="email">authorized_keys</code> Thus, to enable passwordless user login for user root, run the following command on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys</strong></span></pre></div></div><p class="calibre8">In the best case scenario, during a graceful failover, you will want to be notified whenever one (or more) of those services is stopped. Adding a few lines to the<a id="id248" class="calibre1"/> script will also check for the status of the corresponding daemons and alert you if they're down:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">#!/bin/bash</strong></span>

<span class="strong"><strong class="calibre2"># Directory where the ping script is located</strong></span>
<span class="strong"><strong class="calibre2">DIR=/root/scripts</strong></span>

<span class="strong"><strong class="calibre2"># Hostname or IP of remote host (to send alerts to)</strong></span>
<span class="strong"><strong class="calibre2">REMOTEHOST="192.168.0.5"</strong></span>
<span class="strong"><strong class="calibre2"># Name of report file</strong></span>
<span class="strong"><strong class="calibre2">PING_REPORT="ping_report.txt"</strong></span>

<span class="strong"><strong class="calibre2"># Make sure the current file is empty</strong></span>
<span class="strong"><strong class="calibre2">cat /dev/null &gt; $DIR/$PING_REPORT</strong></span>

<span class="strong"><strong class="calibre2">#Current date to be used in the ping script</strong></span>
<span class="strong"><strong class="calibre2">CURRENT_DATE=$(date +'%Y-%m-%d %H:%M')</strong></span>
<span class="strong"><strong class="calibre2"># Node list</strong></span>
<span class="strong"><strong class="calibre2">NODES="node01 node02"</strong></span>

<span class="strong"><strong class="calibre2"># Outer loop: check each node</strong></span>
<span class="strong"><strong class="calibre2">for node in $NODES</strong></span>
<span class="strong"><strong class="calibre2">       do</strong></span>
<span class="strong"><strong class="calibre2">       LOST_PACKETS=$(ping -c 4 $node | grep -i unreachable | wc -l)</strong></span>
<span class="strong"><strong class="calibre2">       if [ $LOST_PACKETS -ne "0" ]</strong></span>
<span class="strong"><strong class="calibre2">               then</strong></span>
<span class="strong"><strong class="calibre2">               echo "$"LOST_PACKETS packets were missed while pinging $node at $CURRENT_DATE" &gt;&gt; $DIR/$PING_REPORT</strong></span>
<span class="strong"><strong class="calibre2">       fi</strong></span>
<span class="strong"><strong class="calibre2"># Inner loop: check all cluster core components in each node</strong></span>
<span class="strong"><strong class="calibre2">       for service in corosync pacemaker pcsd</strong></span>
<span class="strong"><strong class="calibre2">       do</strong></span>
<span class="strong"><strong class="calibre2">       IS_ACTIVE=$(ssh -qn $node systemctl is-active $service)</strong></span>
<span class="strong"><strong class="calibre2">       if [ $IS_ACTIVE != "active" ]</strong></span>
<span class="strong"><strong class="calibre2">               then</strong></span>
<span class="strong"><strong class="calibre2">               echo "$"service is NOT active on $node. Please check ASAP." &gt;&gt; $DIR/$PING_REPORT</strong></span>
<span class="strong"><strong class="calibre2">       fi</strong></span>
<span class="strong"><strong class="calibre2">       done</strong></span>
<span class="strong"><strong class="calibre2">done</strong></span>

<span class="strong"><strong class="calibre2"># Mail the report unless it's' empty</strong></span>
<span class="strong"><strong class="calibre2">if [ -s "$"DIR/$PING_REPORT" ]</strong></span>
<span class="strong"><strong class="calibre2">       then </strong></span>
<span class="strong"><strong class="calibre2">       mail -s "Ping report" root@localhost &lt; $DIR/$PING_REPORT</strong></span>
<span class="strong"><strong class="calibre2">fi</strong></span></pre></div><p class="calibre8">As a<a id="id249" class="calibre1"/> simple test, stop the cluster (<code class="email">pcs cluster stop</code>) on <code class="email">node02</code> (<code class="email">192.168.0.3</code>), run the script from the monitoring host or from any node, and check your mail inbox to verify that it is working correctly. In the following screenshot, you can see an example of what it should look like:</p><div class="mediaobject"><img src="../images/00066.jpeg" alt="Checking the availability of core components" class="calibre10"/></div><p class="calibre11"> </p></div></div>
<div class="book" title="Summary" id="19UOO1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec38" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we have explained how to monitor, troubleshoot, and fix common cluster problems and needs. Not all of these will be undesired or unexpected as a sudden system crash. There will be times when you need to bring down the cluster and the resources it is running for some planned maintenance or during a power outage before your <span class="strong"><strong class="calibre2">uninterruptible power supply</strong></span> (<span class="strong"><strong class="calibre2">UPS</strong></span>) runs out.</p><p class="calibre8">Because prevention is your best ally in these circumstances, ensure that you routinely monitor the health of your cluster. Follow the procedures outlined in this chapter so that you don't run into any surprises when real emergencies come up. Specifically, under either real or simulated cases, ensure that you back up the cluster configuration, stop the cluster on both nodes separately, and then and only then, halt the node.</p></div></body></html>