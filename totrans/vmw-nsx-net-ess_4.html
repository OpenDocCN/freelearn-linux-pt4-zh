<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;NSX Virtual Networks and Logical Router"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. NSX Virtual Networks and Logical Router</h1></div></div></div><p>Scalability and flexibility are impressive features of Network Virtualization. On any IP network, any Switches/Routers, any physical network design NSX works flawlessly. With the emerging interest in overlay networks, there are different encapsulation technologies currently in the market. VXLAN, NVGRE, LISP are few in that list. In this chapter we will discuss on logical networks and how NSX simplifies datacenter routing and switching. Following topics will be the key points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">NSX logical switches</li><li class="listitem" style="list-style-type: disc">NSX virtual network creation - multicast, unicast, and hybrid replication modes</li><li class="listitem" style="list-style-type: disc">NSX virtual network best practices and deployment considerations</li><li class="listitem" style="list-style-type: disc">NSX logical router</li><li class="listitem" style="list-style-type: disc">NSX logical routing and bridging best practices</li></ul></div><div class="section" title="NSX logical switches"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"/>NSX logical switches</h1></div></div></div><p>Using VMware NSX virtual networks, we can create a logical network on top of any IP network. We have already discussed VXLAN fundamentals and the host installation process in the previous chapters. Now that we have a good understanding of the basics, it's time to move on with virtual network creation. Before we begin exploring, it is important to understand that each logical network is a separate broadcast domain.</p><div class="section" title="Logical network prerequisites"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec21"/>Logical network prerequisites</h2></div></div></div><p>Firstly, let's have a look at the prerequisites for logical network creation:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Host preparation</li><li class="listitem" style="list-style-type: disc">Segment ID (VNI) pool</li><li class="listitem" style="list-style-type: disc">Global transport zone</li></ul></div><div class="section" title="Host preparation"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec5"/>Host preparation</h3></div></div></div><p>We have already discussed in detail how the underlying ESXi host is prepared from NSX Manager in 
<a class="link" href="ch03.html" title="Chapter 3. NSX Manager Installation and Configuration">Chapter 3</a>
, <span class="emphasis"><em>NSX Manager Installation and Configuration</em></span>. Here, it is time to recollect that knowledge.</p><p>The hypervisor kernel modules enable ESXi hosts to support VXLAN, the logical switch, the distributed router, and the distributed firewall.</p></div><div class="section" title="Segment ID (VNI) pool"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec6"/>Segment ID (VNI) pool</h3></div></div></div><p>As we know, the VXLAN network identifier is a 24-bit address that gets added to the VXLAN frame, which allows us to isolate each VXLAN network from another VXLAN network.</p><div class="section" title="Steps to configure the VNI pool"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec1"/>Steps to configure the VNI pool</h4></div></div></div><p>Here are the steps to configure the VNI pool:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">On the <span class="strong"><strong>Logical Network Preparation</strong></span> tab, click the <span class="strong"><strong>Segment ID</strong></span> button.</li><li class="listitem">Click <span class="strong"><strong>Edit</strong></span> to open the <span class="strong"><strong>Edit Segment IDs and Multicast Address Allocation</strong></span> dialog box and configure the given options.</li><li class="listitem">Click <span class="strong"><strong>OK</strong></span> as shown in the following screenshot:</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" alt="Steps to configure the VNI pool"/></div><p>
</p><p>In this example, we are not using multicast networks; rather, we want to leverage unicast networks. One of the classic examples of multicast VXLAN networks would be when a customer has existing VXLAN networks (created when he was using vCloud network security) and the management software was upgraded to NSX. For such a scenario, people stick with multicast mode VXLAN networks in NSX. Also, please note that we don't even need a controller in that case. Do not use 239.0.0.0/24 or 239.128.0.0/24 as the multicast address range, because these networks are used for local subnet control, meaning that the physical switches flood all traffic that uses these addresses. The complete list is documented at 
<a class="ulink" href="https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01">https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01</a>
.</p></div></div><div class="section" title="Transport zone"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec7"/>Transport zone</h3></div></div></div><p>A transport zone is a boundary for a VNI. All clusters in the same transport zone share the same VNI. A transport zone can contain multiple clusters and a cluster can be part of multiple transport zones or, in other words, a host can be part of multiple transport zones. In the following screenshot, we have three clusters: <span class="strong"><strong>Cluster A</strong></span>, <span class="strong"><strong>Cluster B</strong></span>, and <span class="strong"><strong>Cluster C</strong></span>. <span class="strong"><strong>Cluster A</strong></span> and <span class="strong"><strong>Cluster B</strong></span> are part of <span class="strong"><strong>Transport Zone A</strong></span> and <span class="strong"><strong>Cluster C</strong></span> is part of <span class="strong"><strong>Transport Zone B</strong></span>.</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" alt="Transport zone"/></div><p>
</p><div class="section" title="Configuring a global transport zone"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec2"/>Configuring a global transport zone</h4></div></div></div><p>The following steps will help you to configure the transport zone:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">On the <span class="strong"><strong>Logical Network Preparation</strong></span> tab, click <span class="strong"><strong>Transport Zones</strong></span>.</li><li class="listitem">Click the green plus sign to open the <span class="strong"><strong>New Transport Zone</strong></span> dialog box and configure the following options and click <span class="strong"><strong>OK</strong></span>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Enter the transport zone name in the <span class="strong"><strong>Name</strong></span> text box.</li><li class="listitem" style="list-style-type: disc">In <span class="strong"><strong>Control Plane Mode</strong></span>, select the <span class="strong"><strong>Unicast</strong></span>, <span class="strong"><strong>Multicast</strong></span>, or <span class="strong"><strong>Hybrid</strong></span> button.</li><li class="listitem" style="list-style-type: disc">In <span class="strong"><strong>Select clusters to add</strong></span>, select the check box for each of the vSphere clusters listed. Also, have a look at the distributed switch selection highlighted in red. We are following one of the NSX-DVS design best practices. Both the compute clusters are running on <span class="strong"><strong>Compute_VDS</strong></span> and the management cluster is on <span class="strong"><strong>Mgmt_Edge_VDS</strong></span>.</li></ul></div><p>
</p></li><li class="listitem">Once it is updated, verify that the transport zone appears in the transport zone list, with a control plane mode set to unicast, multicast, or hybrid based on the preceding selection. Refer to the following figure:</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/image_04_004.jpg" alt="Configuring a global transport zone"/></div><p>
</p><p>We have now completed all the prerequisites for NSX logical networks. In this example, we have the following virtual machines already created in vSphere without any network connectivity. We will go ahead and create four logical networks and will connect to virtual machines and later we will test connectivity:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Two web servers</strong></span>: web-sv-01a and web-sv-02a</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>One DB server</strong></span>: DB-sv-01a</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Application server</strong></span>: app-sv-01a</li></ul></div></div></div></div><div class="section" title="Creating logical switches"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec22"/>Creating logical switches</h2></div></div></div><p>In an old-fashioned vSphere environment, ideally, virtual machines will be connected to a preconfigured vSphere PortGroup with or without VLAN tagging. But now we are in the NSX world and let's leverage an NSX logical switch for virtual machine connectivity. In the left navigation pane, select <span class="strong"><strong>Logical Switches</strong></span> and in the center pane, click the green plus sign to open the <span class="strong"><strong>New Logical Switch</strong></span> dialog box. Perform the following actions to configure the logical switch:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Enter <code class="literal">App-Tier</code> in the <span class="strong"><strong>Name</strong></span> text box.</li><li class="listitem">Verify that the <span class="strong"><strong>Transport Zone</strong></span> selection is <span class="strong"><strong>Transport</strong></span>.</li><li class="listitem">Verify that the <span class="strong"><strong>Control Plane Mode</strong></span> selection is <span class="strong"><strong>Unicast</strong></span>.</li><li class="listitem">Click <span class="strong"><strong>OK</strong></span>:</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/image_04_005.jpg" alt="Creating logical switches"/></div><p>
</p><p>The following are the two options shown on the <span class="strong"><strong>New Logical Switch</strong></span> screen:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>
<span class="strong"><strong>Enable IP Discovery</strong></span>
</strong></span>: Another great feature, which minimizes ARP flooding in a VXLAN network. When a virtual machine sends an ARP packet, the switch security module - which is nothing but a dvfilter module attached to VNIC - will query the NSX Controller to know if they have the MAC entry for that destination IP. Everyone knows that we have two chances in that case:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The controller has the MAC entry</li><li class="listitem" style="list-style-type: disc">The controller doesn't have the MAC entry</li><li class="listitem" style="list-style-type: disc">In the first condition, since the controller has the MAC entry, it will respond with the MAC address and, that way, ARP traffic is reduced. In the second condition, the controller responds with no MAC reply and the ARP will flood in the normal way.</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>
<span class="strong"><strong>Enable MAC Learning</strong></span>
</strong></span>: When we enable MAC learning a VLAN/MAC pair table is maintained on each vNIC which is used by dvfilter data. This table will be intact whenever VM migrates from one host to another host with the help of dvfilter data.</li></ul></div><p>Wait for the update to complete and confirm app-network appears with the status set to <span class="strong"><strong>Normal</strong></span>. Repeat Steps 1-4 and create three more logical switches, and name them <span class="strong"><strong>Web-Tier</strong></span>, <span class="strong"><strong>DB-Tier</strong></span>, and <span class="strong"><strong>Transit</strong></span> networks. The successful creation of logical switches will show us the same results when populated under logical switches, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_007.jpg" alt="Creating logical switches"/></div><p>
</p><p>With the preceding steps, we see four port groups created in the vSphere networking option with their respective VNI-ID:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>5000</strong></span>: <span class="strong"><strong>Transit</strong></span> network</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>5001</strong></span>: <span class="strong"><strong>Web_Tier</strong></span> network</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>5002</strong></span>: <span class="strong"><strong>App_Tier</strong></span> network</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>5003</strong></span>: <span class="strong"><strong>DB_Tier</strong></span> network</li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_06.jpg" alt="Creating logical switches"/></div><p>
</p></div></div></div>
<div class="section" title="Understanding replication modes"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Understanding replication modes</h1></div></div></div><p>Let's discuss replication modes in more detail and later we will connect the logical switches to virtual machines. With the addition of an NSX Controller, the requirement for multicast protocol support on physical networks is completely removed for VXLAN. There are three replication modes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>
<span class="strong"><strong>Multicast</strong></span>
</strong></span>: When multicast replication mode is chosen for a given logical switch, NSX relies on the Layer 2 and Layer 3 multicast capability of the data center physical network to ensure VXLAN encapsulated multi-destination traffic is sent to all the VTEPs. This mode is recommended only when you are upgrading from older VXLAN deployments (vCloud network security). It requires PIM/IGMP on a physical network.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>
<span class="strong"><strong>Unicast</strong></span>
</strong></span>: The control plane is handled by the NSX Controller. All unicast traffic leverages head end replication. No multicast IP address or special network configuration is required. In unicast mode, the ESXi hosts in the NSX domain are divided into separate VTEP segments based on the IP subnet their VTEP interfaces belong to. There will be a UTEP selection for each segment to play the role of <span class="strong"><strong>Unicast Tunnel End Point</strong></span> (<span class="strong"><strong>UTEP</strong></span>). The UTEP is responsible for replicating multi-destination traffic received from the ESXi hypervisor hosting the VM sourcing the traffic and belonging to a different VTEP segment from all the ESXi hosts part of its segment.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>
<span class="strong"><strong>Hybrid</strong></span>
</strong></span>: The optimized unicast mode. Offloads local traffic replication to a physical network (L2 multicast). This requires IGMP snooping on the first-hop switch, but does not require PIM. The first-hop switch handles traffic replication for the subnet.</li></ul></div><p>We will consider the following screenshot as a network topology and will explain all three modes of replication, which will give us a precise picture of how replication works:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_009.jpg" alt="Understanding replication modes"/></div><p>
</p><p>Firstly, let's understand the configuration. The preceding screenshot shows us the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are two transport zones in this set-up, <span class="strong"><strong>
<span class="strong"><strong>Transport Zone A</strong></span>
</strong></span> and <span class="strong"><strong>
<span class="strong"><strong>Transport Zone B</strong></span>
</strong></span></li><li class="listitem" style="list-style-type: disc">A<span class="strong"><strong>
<span class="strong"><strong>Distributed Virtual Switch</strong></span> (<span class="strong"><strong>DVS</strong></span>)</strong></span> is part of both the transport zones</li><li class="listitem" style="list-style-type: disc">All the virtual machines are connected to one common VXLAN network - <span class="strong"><strong>
<span class="strong"><strong>VXLAN 5001</strong></span>
</strong></span></li></ul></div><p>We will walk through all the modes one by one and also discuss their design decisions in the following sections.</p><div class="section" title="Unicast mode packet walk"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec23"/>Unicast mode packet walk</h2></div></div></div><p>Let's discuss a unicast mode VXLAN packet walk:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VM-A</strong></span> generated <span class="strong"><strong>Broadcast, Unknown Unicast, Multicast</strong></span> (<span class="strong"><strong>BUM</strong></span>) traffic, which is typically Layer 2 traffic.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ESXi-A</strong></span> will do a local VTEP lookup and will learn that the packet needs to be locally replicated (same subnet), in this case <span class="strong"><strong>ESXi-B</strong></span>.</li><li class="listitem" style="list-style-type: disc">In addition to that, the packet will also get remotely replicated. Since we have four hosts in a remote subnet (ESXi E, F, G, and H), to which host will it send the packet? This is one key differentiator between multicast mode VXLAN and unicast. In unicast mode, the packet will be sent to a proxy module called <span class="strong"><strong>UTEP</strong></span> with a replicate-locally bit set. Why is it like that? The answer is very simple: since there is a replicate-locally bit set, UTEP will replicate the packet locally to one of the ESXi hosts that are part of same subnet. In this example, we have two subnets; based on the network topology, the process will be same for every subnet.</li></ul></div><div class="section" title="Design decisions for unicast mode VXLAN"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec8"/>Design decisions for unicast mode VXLAN</h3></div></div></div><p>There is not much to be discussed about unicast mode VXLAN design. However, it is important to know the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We could simply stick with traditional IP network design and just ensure that the MTU is increased to 1,600.</li><li class="listitem" style="list-style-type: disc">Let's go back and read the VXLAN packet walk in unicast mode; what does it do in a nutshell? It replicates the packet locally and sends one copy to the remote subnet and again replicates it locally. Who does that replication? The ESXi host does all this intelligent work and of course, based on how big the environment is or how often we have <span class="strong"><strong>BUM</strong></span> traffic, it will create a slight overhead on the hypervisor. So I would suggest unicast mode as the best way to start using VXLAN; however, it is not a great candidate for large environments.</li><li class="listitem" style="list-style-type: disc">In environments where customers have multicast limitation, unicast mode VXLAN is best.</li></ul></div></div></div><div class="section" title="Multicast mode packet walk"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec24"/>Multicast mode packet walk</h2></div></div></div><p>Whenever I used to explain about multicast VXLAN networks, it made me recollect the VMware<span class="strong"><strong> vCloud Networking and Security</strong></span> (<span class="strong"><strong>vCNS</strong></span>) solution days. Multicast mode VXLAN was the starting stage of VXLAN implementation in both virtualized vSphere environments and cloud environments running on vCloud director software. The solution was very powerful; however, physical network prerequisites were one of the difficult factors for all architects because it really defeats the purpose of saying NSX can be run on any IP network. The bitter truth is that IP networking demands some requirements for the technology to work flawlessly. With that said, let's start with a packet walk:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VM-A</strong></span> generates <span class="strong"><strong>BUM</strong></span> traffic.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The ESXi A</strong></span> host encapsulates the packet with a VXLAN header (5001). Time to start guessing who it will send the packet to. Will it simply be broadcast? The Layer 2 frame is a broadcast frame encapsulated with a VXLAN header; however, the host would be sending it to one of the multicast groups. How will we ensure the multicast reaches only <span class="strong"><strong>ESXi B</strong></span>, <span class="strong"><strong>E</strong></span>, <span class="strong"><strong>F</strong></span>, <span class="strong"><strong>G</strong></span>, and <span class="strong"><strong>H</strong></span> since we have a virtual machine running on the same VXLAN network? This is where a physical network requirement is a must. We need IGMP snoop for that; if not, that would be treated as an unknown multicast packet.</li><li class="listitem" style="list-style-type: disc">The<span class="strong"><strong> router</strong></span> will perform an L3 multicast and will send it to a Layer 2 switch and the switch will again check the multicast group and will send it to the right host. Eventually, the VM running on the destination host will receive the packet after getting de-encapsulated by the VTEP.</li></ul></div><div class="section" title="Design decisions for multicast mode VXLAN"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec9"/>Design decisions for multicast mode VXLAN</h3></div></div></div><p>As I mentioned, we certainly need to take care of physical network prerequirements in multicast mode VXLAN:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>IGMP snoop</strong></span> and <span class="strong"><strong>IP multicasting</strong></span> are required in the switch and router throughout the network.</li><li class="listitem" style="list-style-type: disc">Ideally, one VXLAN segment to one multicast group is the recommended way to provide optimal multicast forwarding, which also demands an increase in multicast groups if we have large segments. Something which I have seen in cloud environments, wherein VXLAN networks are created on-the-fly and the cloud provider ensures enough multicast IP is available for 1:1 mapping; that way, a packet forwarded to one tenant won't be seen by other tenants.</li></ul></div></div></div><div class="section" title="Hybrid mode packet walk"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec25"/>Hybrid mode packet walk</h2></div></div></div><p>Hybrid mode VXLAN is recommended for most large environments, primarily because of the simplicity and limited configuration changes that are demanded in the network. Let's have a look at a hybrid mode VXLAN packet walk:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Virtual Machine A</strong></span> generates <span class="strong"><strong>BUM</strong></span> traffic.</li><li class="listitem"><span class="strong"><strong>ESXi A</strong></span> host encapsulates the <span class="strong"><strong>L2 header</strong></span> with <span class="strong"><strong>VXLAN header 5001</strong></span> and will send it to a physical switch.</li><li class="listitem">In this case, an encapsulated L2 header will be send it to a multicast group which is defined on the physical switch. I hope it makes more sense now. The physical switch will deliver the packet to destination ESXi host part of that multicast group, in this case, ESXi B, C, D. In addition to that, ESXi A will send a <code class="literal">Locally_Replicate_BIT</code> set packet to a remote subnet. This packet will be received by a proxy module called <span class="strong"><strong>Multicast Tunnel End Point</strong></span> (<span class="strong"><strong>MTEP</strong></span>). Again, it is a straightforward answer, since there is a replicate locally bit set, MTEP (ESXi host) will replicate the packet locally to one of the ESXi host that are part of same subnet.</li><li class="listitem">MTEP will again send the packet to a physical switch and the physical switch will deliver the packet to all the host part of same multicast group.</li></ol></div><div class="section" title="Design decisions for hybrid mode VXLAN"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec10"/>Design decisions for hybrid mode VXLAN</h3></div></div></div><p>Hybrid mode VXLAN being one of most widely used replication modes, I believe all of us will be interested to know the key design decisions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>IGMP snoop</strong></span> is required to be configured on a physical switch throughout the VXLAN network.</li><li class="listitem" style="list-style-type: disc">IP multicast is not required in the physical router throughout the network. I'm not sure if I'm safe enough to say that, because replication modes can be selected per logical switch, which means we can deploy a logical switch in unicast, multicast, or hybrid mode. What if we are deploying logical switch A in multicast and logical switch B in hybrid mode in the same VXLAN domain? It demands IP multicasting in physical networking. But again, we don't need IP multicasting explicitly for hybrid mode VXLAN networking.</li></ul></div><p>It is strongly recommended to define an <span class="strong"><strong>IGMP Querier</strong></span> for each VLAN to ensure successful L2 multicast delivery and avoid non-deterministic behavior. In order for IGMP, and thus IGMP snooping, to function, a multicast router must exist on the network and generate IGMP queries. The tables created for snooping (holding the member ports for each multicast group) are associated with the querier. I believe we have a strong foundation in VXLAN and its replication modes; let's move on to the connectivity of logical switches and virtual machines.</p></div><div class="section" title="Connecting virtual machines to logical switches"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec11"/>Connecting virtual machines to logical switches</h3></div></div></div><p>Since we have already created logical switches, let's go ahead and connect logical switches to the following virtual machines:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Two web servers</strong></span>: web-sv-01a and web-sv-02a</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>One DB server</strong></span>: DB-sv-01a</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Application server</strong></span>: app-sv-01a</li></ul></div><p>Let us see how to connect the logical switches:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Click the <span class="strong"><strong>vSphere Web Client</strong></span> home icon.</li><li class="listitem">On the <span class="strong"><strong>vSphere Web Client</strong></span> home tab, click <span class="strong"><strong>Inventories</strong></span> | <span class="strong"><strong>Networking &amp; Security</strong></span>.</li><li class="listitem">In the left navigation pane, select <span class="strong"><strong>Logical Switches</strong></span>. In the center pane, select the <span class="strong"><strong>App-Tier </strong></span>logical switch.</li><li class="listitem">Click the <span class="strong"><strong>Add Virtual Machines</strong></span> icon, or select <span class="strong"><strong>Add VM</strong></span> from the <span class="strong"><strong>Actions</strong></span> drop-down menu.</li><li class="listitem">In the <span class="strong"><strong>App-Tier</strong></span>,<span class="strong"><strong> </strong></span>add virtual machines dialog box.</li><li class="listitem">In the filter list, select the <span class="strong"><strong>app-01a</strong></span> check boxes.</li><li class="listitem">Click <span class="strong"><strong>Next</strong></span>. In the <span class="strong"><strong>Select VNICs</strong></span> list, select the <span class="strong"><strong>Network Adapter 1 (VM Network)</strong></span> check box.</li><li class="listitem">Click <span class="strong"><strong>Next</strong></span> as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/B03244_04_08-1024x516.jpg" alt="Connecting virtual machines to logical switches"/></div><p>
</p></li><li class="listitem">Click <span class="strong"><strong>Finish</strong></span>.</li><li class="listitem">Repeat Steps 1-7 and connect both the web servers (web-sv-01a, web-sv-02a) and DB server.</li></ol></div></div></div></div>
<div class="section" title="Testing connectivity"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Testing connectivity</h1></div></div></div><p>As we know that our three-tier application, web, app, and DB, is connected to logical switches, let's do some basic testing to confirm their connectivity:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Firstly, go ahead and power on those machines.</li><li class="listitem">Click the <span class="strong"><strong>vSphere Web Client</strong></span> home icon.</li><li class="listitem">On the <span class="strong"><strong>vSphere Web Client</strong></span> home tab, click the <span class="strong"><strong>Inventories</strong></span> | <span class="strong"><strong>VMs and Templates</strong></span> icon.</li><li class="listitem">Expand the VMs and templates inventory tree and power on each of the following virtual machines found in the discovered virtual machine folder:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">web-sv-01a</li><li class="listitem" style="list-style-type: disc">web-sv-02a</li><li class="listitem" style="list-style-type: disc">app-sv-01a</li><li class="listitem" style="list-style-type: disc">db-sv-01a</li></ul></div><p>
</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_011.jpg" alt="Testing connectivity"/></div><p>
</p></li><li class="listitem">To power on a virtual machine, select the virtual machine in the inventory, then select <span class="strong"><strong>Power On</strong></span> from the <span class="strong"><strong>Actions</strong></span> drop-down menu.</li><li class="listitem">Once the machines are powered on, we will go ahead and record their IP address:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">web-01a</code>: <code class="literal">172.16.10.11</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">web-02a</code>: <code class="literal">172.16.10.12</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">app-01a</code>: <code class="literal">172.16.20.11</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DB-01a</code>: <code class="literal">172.16.30.11</code></li></ul></div><p>
</p></li></ol></div><p>We now do a simple ping test between <code class="literal">web-01a</code> and <code class="literal">app-01a</code> as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_012.jpg" alt="Testing connectivity"/></div><p>
</p><p>Why do we have 100% packet loss when we ping <code class="literal">web-01a (172.16.10.11)</code> and <code class="literal">app-01a(172.16.20.11)</code>? Will a logical switch perform a Layer 3 routing? Definitely not. The traditional way of performing routing for such networks would be through a physical router, which is nothing but going all the way out of the rack, gets it routed to the right destination. Let's not do that legacy routing in this case, we would leverage NSX logical router capability.</p></div>
<div class="section" title="The Distributed Logical Router"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>The Distributed Logical Router</h1></div></div></div><p>The whole purpose of routing is to process the packets between two different IP networks. Let's discuss the fundamentals of routing before getting into logical routers. Every router will build a routing table, which will have information about <span class="strong"><strong>destination network</strong></span>, <span class="strong"><strong>next hop router</strong></span>, <span class="strong"><strong>metrics, and administrative distance</strong></span>. There are two methods of building a routing table:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Static routing</strong></span>: Static routing is manually created and updated by a network administrator. Based on the network topology, we will be in need of configuring a static route on each and every router for end-to-end network connectivity. Even though this gives full control over the routing, it would be an extremely tedious job to configure routes on a large network.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dynamic routing</strong></span>: Dynamic routing is created and updated by a routing protocol running on a router; <span class="strong"><strong>Routing Information Protocol</strong></span> (<span class="strong"><strong>RIP</strong></span>) and <span class="strong"><strong>Open Shortest Path First</strong></span> (<span class="strong"><strong>OSPF</strong></span>) are some examples. Dynamic routing protocols are intelligent enough to choose a better path whenever there is a change in routing infrastructure.</li></ul></div><p>The VMware NSX Distributed Logical Router supports static routes, OSPF, ISIS, and BGP routing protocols. It is important to know that dynamic routing protocols are supported only on external interface (uplink) of the <span class="strong"><strong>Distributed Logical Router</strong></span> (<span class="strong"><strong>DLR</strong></span>). The DLR allows an ESXi hypervisor to locally do routing intelligence, through which we can optimize East-West data plane traffic.</p><div class="section" title="Deploying a Distributed Logical Router"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>Deploying a Distributed Logical Router</h2></div></div></div><p>A DLR is a virtual appliance which has the control plane intelligence and it relies on NSX Controllers to push the routing updates to the ESXi kernel modules.</p><div class="section" title="Procedure for deploying a logical router"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec12"/>Procedure for deploying a logical router</h3></div></div></div><p>Let's walk through the step-by-step configuration of a Distributed Logical Router:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the vSphere web client, navigate to <span class="strong"><strong>Home</strong></span> | <span class="strong"><strong>Networking &amp; Security</strong></span> | <span class="strong"><strong>NSX Edges</strong></span>.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note6"/>Note</h3><p>Select the appropriate NSX Manager on which to make your changes. If you are creating a universal logical router, you must select the primary NSX Manager. We will be discussing about Primary/Secondary NSX manager concepts in 
<a class="link" href="ch07.html" title="Chapter 7. NSX Cross vCenter">Chapter 7</a>
, <span class="emphasis"><em>NSX Cross vCenter</em></span>.</p></div></div></li><li class="listitem">Select the type of router you wish to add; in this case, we would add <span class="strong"><strong>logical router</strong></span>.</li><li class="listitem">Select <span class="strong"><strong>Logical (Distributed) Router</strong></span> to add a logical router local to the selected NSX Manager.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note7"/>Note</h3><p>Since we haven't discussed the cross-vCenter NSX environment, we won't leverage a universal logical distributed router in this chapter.</p></div></div></li><li class="listitem">Type a name for the device. This name appears in your <span class="strong"><strong>vCenter inventory</strong></span>. The name should be unique across all logical routers within a single tenant. Optionally, you can also enter a hostname. This name appears in the CLI. If you do not specify the hostname, the edge ID, which gets created automatically, is displayed in the CLI.</li><li class="listitem">The <span class="strong"><strong>Deploy Edge Appliance</strong></span> option is selected by default. An edge appliance (also called a logical router virtual appliance) is required for dynamic routing and the logical router appliance's firewall, which applies to logical router pings, SSH access, and dynamic routing traffic. You can deselect the <span class="strong"><strong>Deploy Edge Appliance</strong></span> option if you require only static routes, and do not want to deploy an edge appliance. You cannot add an edge appliance to the logical router after the logical router has been created.</li><li class="listitem">The <span class="strong"><strong>Enable High Availability</strong></span> option is not selected by default. Select the <span class="strong"><strong>Enable High Availability</strong></span> check box to enable and configure high availability. High Availability is required if you are planning to do dynamic routing. I want everyone to think from a cloud provider perspective: if your tenant is requesting the High Availability feature, how do you satisfy that requirement? NSX Edge replicates the configuration of the primary appliance for the standby appliance and ensures that the two HA NSX Edge virtual machines are not on the same ESXi host even after you use DRS and vMotion.

 Two virtual machines are deployed on vCenter in the same resource pool and data store as the appliance you configured. Local link IPs are assigned to HA virtual machines in the NSX Edge HA so that they can communicate with each other. But remember that instead of one control VM, we have two control VMs running now so definitely it will consume twice the compute resource.<p>The following screenshot shows NSX DLR-VM deployment:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_013.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p></li><li class="listitem">Type and retype a password for the logical router. The password must be 12-255 characters and must contain the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">At least one uppercase letter</li><li class="listitem" style="list-style-type: disc">At least one lowercase letter</li><li class="listitem" style="list-style-type: disc">At least one number</li><li class="listitem" style="list-style-type: disc">At least one special character</li></ul></div><p>
</p></li><li class="listitem">Enable <span class="strong"><strong>SSH</strong></span> and set the log level (optional). By default, <span class="strong"><strong>SSH</strong></span> is disabled. If you do not enable SSH, you can still access the logical router by opening the virtual appliance console. Enabling SSH here causes the SSH process to run on the logical router virtual appliance, but you will also need to adjust the logical router firewall configuration manually to allow SSH access to the logical router's protocol address. The protocol address is configured when you configure dynamic routing on the logical router. By default, the log level is emergency.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note8"/>Note</h3><p>On logical routers, only IPv4 addressing is supported.</p></div></div></li><li class="listitem">Configure the interfaces. Under Configure interfaces, add four <span class="strong"><strong>logical interfaces</strong></span> (<span class="strong"><strong>LIFs</strong></span>) to the logical router:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Uplink connected to <span class="strong"><strong>Transit-Network-01</strong></span> logical switch with an IP of 192.168.10.2/29</li><li class="listitem" style="list-style-type: disc">Internal connected to <span class="strong"><strong>Web-Tier-01 Logical Switch</strong></span> with IP 172.16.10.1/24</li><li class="listitem" style="list-style-type: disc">Internal connected to <span class="strong"><strong>App-Tier-01 Logical Switch</strong></span> with IP 172.16.20.1/24</li><li class="listitem" style="list-style-type: disc">Internal connected to <span class="strong"><strong>DB-Tier-01 Logical Switch</strong></span> with IP 172.16.30.1/24</li></ul></div><p>
</p><p>The following screenshot depicts the <span class="strong"><strong>Add Interface</strong></span> screen:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_014.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p></li><li class="listitem">Configure interfaces of this NSX Edge: Internal interfaces are for connections to logical switches that allow VM-to-VM (East-West) communication. Internal interfaces are created on the logical router virtual appliance and we call them LIF. Uplink interfaces are for North-South communication. A logical router uplink interface can be connected to an NSX Edge services gateway, third-party router VM, or a VLAN-backed dvPortgroup to make the logical router connection to a physical router directly. You must have at least one uplink interface for dynamic routing to work. Uplink interfaces are created as vNICs on the logical router virtual appliance.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note9"/>Note</h3><p>We can add, remove, and modify interfaces after a logical router is deployed.</p></div></div></li></ol></div><p>The following screenshot depicts the DLR configuration that we have performed so far:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_015.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>Now that we have successfully deployed a DLR and configured with logical interfaces, we would expect the DLR to perform basic routing functionality for web, app, and DB machines to communicate with each other, which was not possible earlier.</p><p>The following screenshot depicts the three-tier application architecture without routing:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_016.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>Let's go ahead and perform a quick ping test between <code class="literal">web-01a (172.16.10.11)</code> and <code class="literal">app (172.16.20.11)</code>. As we can see from the following screenshot, web servers and application servers are able to communicate each other since we have a Distributer Logical Router, which does the routing in this case. The first ping result is before adding the Distributed Logical Router:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_017.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>So far, we have discussed the <span class="strong"><strong>Distributed Logical Router</strong></span> (<span class="strong"><strong>DLR</strong></span>), which allows ESXi hypervisor to locally do routing intelligence through which we can optimize East-West data plane traffic. But I know we are very keen to view the DLR routing table in an ESXi host. Let's focus on the following screenshot to know the network topology.</p><p>The following screenshot depicts the three-tier application architecture with DLR connection:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_16.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>The following questions might come to our mind:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How many networks do we have?<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">172.16.10.0/24</li><li class="listitem" style="list-style-type: disc">172.16.20.0/24</li><li class="listitem" style="list-style-type: disc">172.16.30.0/24</li><li class="listitem" style="list-style-type: disc">192.168.10.0/29</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc">Are the networks directly connected to the router?<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Yes, they are connected to the router.</li></ul></div><p>
</p></li></ul></div><p>We will go ahead and SSH to one of the ESXi hosts to check the logical router instance, MAC, ARP, and routing tables, which will certainly give granular-level details:</p><pre class="programlisting">
<span class="strong"><strong>Net-vdr -I -l</strong></span>
</pre><p>The preceding command will display the logical router instance as shown in the following screenshot. You can see the following parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">VDR Name</code> is <code class="literal">default+edge-19</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Number of Lifs</code> is <code class="literal">4</code></li><li class="listitem" style="list-style-type: disc">Remember we connected four logical networks to the distributed router? Hence the count is <code class="literal">4</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Number of Routes</code> is <code class="literal">4</code></li></ul></div><p>Since we have connected four logical networks, the router is aware of those directly connected networks:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_17.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>The following command will verify the network routes discovered by DLR:</p><pre class="programlisting">
<span class="strong"><strong>net-vdr --route -l VDR Name</strong></span>
</pre><p>For example:</p><pre class="programlisting">
<span class="strong"><strong>net-vdr --route -l default+edge-19</strong></span>
</pre><p>The logical router routing table is pushed by the NSX Controller to the ESXi host and it will be consistent across all the ESXi hosts. You will see the following output:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_18.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>Now log in to the controller CLI to view the logical router state information:</p><pre class="programlisting">
<span class="strong"><strong>nvp-controller <span class="strong"><strong># show control-cluster logical-routers instance all (List all LR instances)</strong></span>
</strong></span>
</pre><p>You will see the following output:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_19.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p><p>The other command is:</p><pre class="programlisting">
<span class="strong"><strong>nvp-controller <span class="strong"><strong># show control-cluster logical-routers interface-summary 1460487509</strong></span>
</strong></span>
</pre><p>All four logical switches (VXLAN 5000, 5001, 5002, and 5003, which we have connected to the logical router) are displaying in the following output with their respective interface IP, which would be the default gateway for web, app, and DB machines. Again, the idea here is to showcase the power of NSX CLI commands, which give granular-level information and are extremely useful when troubleshooting:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_20.jpg" alt="Procedure for deploying a logical router"/></div><p>
</p></div><div class="section" title="Understanding logical interfaces"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec13"/>Understanding logical interfaces</h3></div></div></div><p>I'm pretty sure we now have a good understanding of how distributed routing works in the NSX environment. Again, NSX DLR is not limited between VXLAN networks; we can certainly leverage the routing functionality between VXLAN and VLAN networks. Let's discuss logical interfaces in more detail:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the Distributed Logical Router connects to a vSphere distributed switch port group, the interface is called a <span class="strong"><strong>VLAN LIF</strong></span>. VLAN LIFs make use of <span class="strong"><strong>Designated Instance</strong></span> (<span class="strong"><strong>DI</strong></span>) for resolving ARP queries. The NSX Controller randomly selects one of the ESXi hosts as the designated instance to ease the ARP traffic so that any ARP traffic for that subnet will be handled by one of the ESXi hosts and every other ESXi host is also aware of where DI is running.</li><li class="listitem" style="list-style-type: disc">If the Distributed Logical Router connects to a logical switch, the interface is called a <span class="strong"><strong>VXLAN LIF</strong></span>.</li><li class="listitem" style="list-style-type: disc">A LIF can either be an uplink or internal.</li><li class="listitem" style="list-style-type: disc">Multiple LIFs can be configured on one Distributed Logical Router instance.</li><li class="listitem" style="list-style-type: disc">An ARP table is maintained for each LIF.</li><li class="listitem" style="list-style-type: disc">Each LIF has assigned an IP address representing the default IP gateway for the logical network it connects to and a vMAC address. The IP address is unique for each LIF, whereas the same virtual MAC is assigned to all the defined LIFs.</li><li class="listitem" style="list-style-type: disc">We can configure up to 999 interfaces, with a maximum of eight uplinks.</li><li class="listitem" style="list-style-type: disc">The routing table can be populated in multiple ways:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Directly connected</li><li class="listitem" style="list-style-type: disc">Static routes</li><li class="listitem" style="list-style-type: disc">OSPF</li><li class="listitem" style="list-style-type: disc">BGP</li><li class="listitem" style="list-style-type: disc">Route redistribution</li></ul></div><p>
</p></li></ul></div><p>We will discuss dynamic routing and route redistribution during 
<a class="link" href="ch05.html" title="Chapter 5.  NSX Edge Services">Chapter 5</a>
, <span class="emphasis"><em>NSX Edge Services</em></span>, which will give us a clear view on how tenants access public networks (North-South connectivity).</p></div><div class="section" title="Logical router deployment considerations"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec14"/>Logical router deployment considerations</h3></div></div></div><p>
<span class="strong"><strong>Distributed Logical Router</strong></span> (<span class="strong"><strong>DLR</strong></span>) deployment is highly critical in NSX environments. Let's check a few critical decision factors:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ensure that controllers are up-and-running before deploying a logical router.</li><li class="listitem" style="list-style-type: disc">Don't deploy a logical router during controller deployment. This is not limited to DLR deployments; it applies for all NSX features.</li><li class="listitem" style="list-style-type: disc">If a logical router is to be connected to VLAN dvPortgroups, ensure that all hypervisor hosts with a logical router appliance installed can reach each other on UDP port 6999 for logical router VLAN-based ARP proxy to work.</li><li class="listitem" style="list-style-type: disc">Logical router interfaces should not be created on two different distributed port groups (dvPortgroups) with the same VLAN ID if the two networks are in the same vSphere distributed switch.</li><li class="listitem" style="list-style-type: disc">Starting with VMware NSX for vSphere 6.2, the L2 bridging feature can now participate in distributed logical routing. The VXLAN network to which the bridge instance is connected will be used to connect the routing instance and the bridge instance. This was unsupported in earlier releases.</li><li class="listitem" style="list-style-type: disc">DLR interfaces don't support trunking; however, each DLR interface can be connected to NSX Edge sub-interfaces which support trunking. But we are limited with leveraging IP-Sec, L2-VPN, BGP (dynamic routing), DHCP, and DNAT features on sub-interfaces.</li><li class="listitem" style="list-style-type: disc">1,000 logical interfaces are supported on DLR.</li><li class="listitem" style="list-style-type: disc">DLR doesn't support <span class="strong"><strong>virtual routing and forwarding</strong></span> (<span class="strong"><strong>VRF</strong></span>). For true network multitenancy, we need to deploy a unique DLR which can be connected to the same or different NSX Edges.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Equal Cost Multi Path</strong></span> (<span class="strong"><strong>ECMP</strong></span>) is supported in DLR; however, state full firewalls are not supported, primarily because of asymmetric routing behavior.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>We will discuss ECMP and asymmetric routing in the next chapter.</p></div></div></li><li class="listitem" style="list-style-type: disc">The DLR control VM should not deployed in the compute cluster. If the host fails, both the data plane and control plane will be impacted at the same time if the control VM is also residing on same ESXi host. So the right place to deploy the DLR control VM will be either on the management cluster or if we have a separate vSphere Edge cluster, that is the best option.</li></ul></div></div></div></div>
<div class="section" title="Layer 2 bridges"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Layer 2 bridges</h1></div></div></div><p>Logical network to physical network access might be required due to multiple reasons in a NSX environment:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">During <span class="strong"><strong>Physical to Virtual</strong></span> (<span class="strong"><strong>P2V</strong></span>) migrations where changing IP addresses is not an option</li><li class="listitem" style="list-style-type: disc">Extending virtual services in the logical switch to external devices</li><li class="listitem" style="list-style-type: disc">Extending physical network services to virtual machines in logical switches</li><li class="listitem" style="list-style-type: disc">Accessing existing physical network and security resources</li></ul></div><p>Since Layer 2 bridging is a NSX Edge Distributed Logical Router functionality, the L2 bridge runs on the same host on which the edge logical router control virtual machine is running. Bridging is entirely done at kernel level, as it was for Distributed Logical Routing. A special dvPort type called a <span class="strong"><strong>sink port</strong></span> is used to steer packets to the bridge. In the following screenshot, we have a VXLAN environment wherein virtual machines in VXLAN network 5006 need to communicate with a physical site, which is in VLAN-100:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_21-1024x617.jpg" alt="Layer 2 bridges"/><div class="caption"><p>NSX Layer 2 bridging</p></div></div><p>
</p><div class="section" title="Deploying an L2 bridge"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Deploying an L2 bridge</h2></div></div></div><p>Let's have a look at the deployment of a Layer 2 bridge:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to the vSphere Web Client.</li><li class="listitem">Click <span class="strong"><strong>Networking &amp; Security</strong></span> and then click <span class="strong"><strong>NSX Edges</strong></span>.</li><li class="listitem">Double-click a logical router.</li><li class="listitem">Click <span class="strong"><strong>Manage</strong></span> and then click <span class="strong"><strong>Bridging</strong></span>.</li><li class="listitem">Click the add icon.</li><li class="listitem">Type a name for the bridge.</li><li class="listitem">Select the logical switch that you want to create a bridge for.</li><li class="listitem">Select the distributed virtual port group to which you want to bridge the logical switch.</li><li class="listitem">Click <span class="strong"><strong>OK</strong></span>.</li></ol></div><p>In the following example, we are bridging logical switch <span class="strong"><strong>bRANCH</strong></span> with <span class="strong"><strong>Mgmt_Edge_VDS</strong></span> port group which is VLAN enabled:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_04_22.jpg" alt="Deploying an L2 bridge"/></div><p>
</p></div><div class="section" title="Design considerations for the L2 bridge"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec28"/>Design considerations for the L2 bridge</h2></div></div></div><p>Like any other NSX components, L2 bridging is an equally important design decision factor. The following are the key points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Bridging VLAN-ID 0 is not supported.</li><li class="listitem" style="list-style-type: disc">Multiple bridges are supported per logical router; however, we cannot have more than one bridge instance active per VXLAN-VLAN pair.</li><li class="listitem" style="list-style-type: disc">Bridging cannot be used for VLAN-VLAN connection.</li><li class="listitem" style="list-style-type: disc">Bridging is not a data center interconnect technology.</li><li class="listitem" style="list-style-type: disc">Starting from NSX 6.2, DLR interfaces can be connected to a VXLAN network that is bridged with a VLAN network. Earlier versions don't have this feature.</li><li class="listitem" style="list-style-type: disc">Don't mix a DLR and a next hop router (NSX Edge) on the same host; host failure will have a direct impact on both the devices.</li><li class="listitem" style="list-style-type: disc">Even though Layer 2 bridging is a great feature, remember that all ARP resolutions are done explicitly by a bridge instance module running on the same host wherein we have deployed the logical router. For the same reason, running too many bridge instances and that too if they all are on the same host in addition to that if we have UTEPs and MTEPs running on the same host there would be certainly a performance impact. So try to run bridge instances on separate hosts as much as we can or, in another words, distribute logical router deployment across the management cluster.</li></ul></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Summary</h1></div></div></div><p>We started this chapter with an introduction to the NSX Distributed Logical Router and discussed VXLAN replication modes and a few packet walks. Later, we covered a few key design decisions while deploying a DLR. We also discussed the Layer 2 bridging feature of DLRs and we moved on to important design decisions that need to be noted while leveraging bridging functionality.</p><p>There are exciting times ahead as we discuss more and more features and their functionalities.</p><p>In the next chapter, we will discuss NSX Edge routing and we will establish connectivity with the DLR with a dynamic routing protocol.</p></div></body></html>