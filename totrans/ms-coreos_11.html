<html><head></head><body>
<p id="filepos766574" class="calibre_"><span class="calibre1"><span class="bold">Chapter 11. CoreOS and Containers – Production Considerations</span></span></p><p class="calibre_8">There is a big difference between running applications and containers in development versus production environments. Production environments pose a special set of challenges. The challenges mainly lie in scalability, high availability, security, and automation. CoreOS and Docker have solved significant challenges in taking applications from development to production. In this chapter, we will cover the production considerations for microservice infrastructure, including deployment, automation, and security.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">CoreOS cluster design considerations</li><li value="2" class="calibre_13">Distributed infrastructure design consideration – Service discovery, deployment patterns, PaaS, and stateful and stateless Containers</li><li value="3" class="calibre_13">Security considerations</li><li value="4" class="calibre_13">Deployment and automation – CI/CD approaches and using Ansible for automation</li><li value="5" class="calibre_13">CoreOS and the Docker roadmap</li><li value="6" class="calibre_13">Microservice infrastructure – platform choices and solution providers</li></ul><div class="mbp_pagebreak" id="calibre_pb_232"/>


<p id="filepos768171" class="calibre_14"><span class="calibre1"><span class="bold">CoreOS cluster design considerations</span></span></p><p class="calibre_8">The<a/> cluster size <a/>and update strategy are important design<a/> considerations for a CoreOS cluster.</p><div class="mbp_pagebreak" id="calibre_pb_233"/>


<p id="filepos768472" class="calibre_9"><span class="calibre3"><span class="bold">The update strategy</span></span></p><p class="calibre_8">The CoreOS automatic update feature keeps the nodes in the cluster secure and up-to-date. CoreOS provides <a/>you with various update mechanisms to control updates, and the user can select an approach based on their needs. We covered details of update strategies in <a href="index_split_075.html#filepos216260">Chapter 3</a>, <span class="italic">CoreOS AutoUpdate</span>. Some customers prefer doing the update only in the maintenance window and CoreOS gives control to do this.</p><div class="mbp_pagebreak" id="calibre_pb_234"/>


<p id="filepos769079" class="calibre_9"><span class="calibre3"><span class="bold">Cluster considerations</span></span></p><p class="calibre_8">The following are some considerations that need to be taken into account when choosing the CoreOS<a/> cluster. We have covered these individual topics in earlier chapters.</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Cluster size: A bigger cluster size provides better redundancy but updates take a little longer.</li><a/><li value="2" class="calibre_13">Cluster architecture: We need to choose the architecture based on whether the cluster is used for development or production. For a production cluster, the preferable scheme is to have a small master cluster to run critical services such as Etcd and Fleet and have worker nodes point to the master cluster. Worker nodes should be used only to run application Containers.</li><a/><li value="3" class="calibre_13">Etcd heartbeat and timeout tuning: These parameter values need to be tuned depending on whether the cluster is local or geographically distributed.</li><a/><li value="4" class="calibre_13">Node backup and restore: Nodes can go bad. It is necessary to take periodic backups.</li><a/><li value="5" class="calibre_13">Adding and removing nodes in the cluster: CoreOS provides mechanisms to add and remove nodes in the Etcd cluster dynamically without data loss. This can be used to grow the cluster size organically.</li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_235"/>


<p id="filepos770682" class="calibre_"><span class="calibre1"><span class="bold">Distributed infrastructure design considerations</span></span></p><p class="calibre_8">In this section, we will<a/> cover some miscellaneous infrastructure design considerations that were not covered in earlier chapters.</p><div class="mbp_pagebreak" id="calibre_pb_236"/>


<p id="filepos771016" class="calibre_9"><span class="calibre3"><span class="bold">Service discovery</span></span></p><p class="calibre_8">Microservices are dynamic and Service discovery refers to how microservices can find each other dynamically. Service discovery has three components:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It discovers services automatically as they come up and accesses a service by the service name using DNS</li><a/><li value="2" class="calibre_13">It maintains a shared database of services along with their access details that can be accessed from multiple hosts</li><li value="3" class="calibre_13">It accesses services using a load balancer and handles service failures automatically</li></ul><p class="calibre_8">Service discovery is automatically taken care of when using a Container orchestration system such as Kubernetes. For smaller deployments, when there is no Orchestration system, we can do this manually using standalone tools.</p><p class="calibre_8">We covered Service discovery in <a href="index_split_093.html#filepos245712">Chapter 4</a>, <span class="italic">CoreOS Core Services – Etcd, Systemd, Fleet</span> in the <span class="italic">Service discovery</span> section using the Sidekick service and Etcd. This approach did not provide DNS lookup. The following approach is another way of doing service discovery with integrated DNS.</p><p id="filepos772480" class="calibre_9"><span class="calibre3"><span class="bold">Service discovery using Registrator and Consul</span></span></p><p class="calibre_8">Consul (<a href="https://consul.io/">https://consul.io/</a>) and the<a/> Gliderlabs registrator (<a href="https://github.com/gliderlabs/docker-consul/tree/consul-0.4">https://github.com/gliderlabs/docker-consul/tree/consul-0.4</a>) in combination provide <a/>automatic service<a/> discovery and a service<a/> database.</p><p class="calibre_8">The<a/> following figure shows<a/> the model:</p><p class="calibre_9"><img src="images/00218.jpg" class="calibre_412"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>points show you how this works:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Consul provides service discovery, shared key-value storage, DNS-based service lookup and service health monitoring</li><li value="2" class="calibre_13">Gliderlabs registrator monitors the Docker socket for service creation and informs Consul about registration</li><a/><li value="3" class="calibre_13">As DNS is integrated with Consul, services can be accessed by the service name</li></ul><p class="calibre_8">The following are<a/> the steps necessary to try this out in a Ubuntu Linux machine:</p><p class="calibre_8">Set the Docker daemon to<a/> use the Docker bridge IP as one of the DNS lookup servers.</p><p class="calibre_8">Add this line to <tt class="calibre2">/etc/default/docker</tt>:</p><p class="calibre_8"><tt class="calibre2">DOCKER_OPTS="--dns 172.18.0.1 --dns 8.8.8.8 --dns-search service.consul"</tt></p><p class="calibre_8">Restart the Docker daemon:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo service docker restart</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the Consul server:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker run -d -p 8400:8400 -p 8500:8500 -p 172.18.0.1:53:8600/udp -h node1 gliderlabs/consul-server -server –bootstrap</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The preceding command exposes port <tt class="calibre2">8400</tt> for rpc, <tt class="calibre2">8500</tt> for UI, and <tt class="calibre2">8600</tt> for DNS. We mapped DNS to the Docker bridge IP address (<tt class="calibre2">172.18.0.1</tt>), and this allows us to access service names directly from inside Containers. In the preceding step, we made the Docker bridge IP as one of the DNS lookup servers.</p><p class="calibre_8">Start the Gliderlabs registrator:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker run -d \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">    --name=registrator \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">    --net=host \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">    --volume=/var/run/docker.sock:/tmp/docker.sock \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">    gliderlabs/registrator:latest \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">      consul://localhost:8500</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">In the preceding<a/> command, we also specified the location of Consul so<a/> that the registrator can register services to Consul.</p><p class="calibre_8">Now, let's start<a/> a few Containers:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker run -d -P --name=nginx nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --name mysql -e MYSQL_ROOT_PASSWORD=mysql -d mysql</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --name wordpress --link mysql:mysql -d -P wordpress</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --name wordpress1 --link mysql:mysql -d -P wordpress</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Following output shows the running <a/>Containers:</p><p class="calibre_9"><img src="images/00415.jpg" class="calibre_413"/></p><p class="calibre_8">
</p><p class="calibre_8">We can look at the Consul UI to check whether the Services are registered. The Consul, NGINX, and WordPress Containers are seen in the following output along with their IP addresses and port numbers:</p><p class="calibre_9"><img src="images/00473.jpg" class="calibre_414"/></p><p class="calibre_8">
</p><p class="calibre_8">We can check whether the service lookup by the DNS name is working by accessing the service across<a/> Containers. The following output shows that<a/> the NGINX container is able to access the <a/>WordPress container by the service<a/> name, <tt class="calibre2">wordpress</tt>:</p><p class="calibre_9"><img src="images/00299.jpg" class="calibre_415"/></p><p class="calibre_8">
</p><p id="filepos777568" class="calibre_9"><span class="calibre3"><span class="bold">Dynamic load balancing</span></span></p><p class="calibre_8">As part of Service discovery, a load balancer should be able to automatically find out active services and load balance among the active instances of the service. For example, when three instances of a web service are started, and if one of the instances dies, the load balancer should automatically<a/> be able to remove the<a/> inactive instance from the load balance list.</p><p class="calibre_8">I found the following two approaches to be useful to achieve this.</p><p id="filepos778201" class="calibre_9"><span class="bold">Load balancing with confd and nginx</span></p><p class="calibre_8">In the approach at <a href="https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos">https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos</a>, the following is a list<a/> of the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">The Sidekick service registers service details with <tt class="calibre2">etcd</tt>.</li><a/><li value="2" class="calibre_13">Confd listens for etcd changes and updates <tt class="calibre2">nginx.conf</tt>.</li><a/><li value="3" class="calibre_13">The Nginx load balancer does the load balancing based on entries in <tt class="calibre2">nginx.conf</tt>.</li></ol><p class="calibre_8">The following diagram illustrates the load balancing with <span class="bold">ETCD</span>, <span class="bold">HA DISCOVER</span>, and <span class="bold">HAProxy</span>:</p><p class="calibre_9"><img src="images/00402.jpg" class="calibre_416"/></p><p class="calibre_8">
</p><p id="filepos779429" class="calibre_9"><span class="bold">Load balancing with HAdiscover and HAproxy</span></p><p class="calibre_8">In the <a/>approach at <a href="http://adetante.github.io/articles/service-discovery-haproxy/">http://adetante.github.io/articles/service-discovery-haproxy/</a>, the following is a list of the steps:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The registrator registers the service details with <tt class="calibre2">etcd</tt></li><a/><li value="2" class="calibre_13">HAdiscover listens for changes to etcd and updates <tt class="calibre2">haproxy.conf</tt></li><a/><li value="3" class="calibre_13">HAproxy does the load balancing based on the HAproxy configuration</li></ul><p class="calibre_8">The following diagram illustrates the load balancing with ETCD, HA DISCOVER, and NGINX:</p><p class="calibre_9"><img src="images/00024.jpg" class="calibre_417"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_237"/>


<p id="filepos780539" class="calibre_9"><span class="calibre3"><span class="bold">Deployment patterns</span></span></p><p class="calibre_8">We covered the<a/> advantages of microservices in the first chapter. Designing a microservice-based application is similar to object-oriented programming, where the Container image can be compared to a class and Containers can be compared to objects. There are many design patterns in object-oriented <a/>programming that specify how to split a monolithic application into classes and how classes can work together with other classes. Some of the object-oriented design principles also apply to microservices.</p><p class="calibre_8">In <a href="index_split_181.html#filepos585742">Chapter 8</a>, <span class="italic">Container Orchestration</span>, we covered Kubernetes Pods and how closely related containers can be grouped together in a single Pod. Design patterns such as Sidecar, ambassador, and adapter are pretty widely used to create Pods. Even though these design patterns are mentioned in the context of the Kubernetes pod, these can also be used in a non-Kubernetes-based system as well.</p><p class="calibre_8">This link (<a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html</a>) talks about <a/>common Kubernetes composite patterns.</p><p class="calibre_8">The following are more details on common Kubernetes composite patterns.</p><p id="filepos782059" class="calibre_9"><span class="calibre3"><span class="bold">The Sidecar pattern</span></span></p><p class="calibre_8">In the Sidecar<a/> pattern, there are two dependent Containers accomplishing a single task.</p><p class="calibre_8">In the following diagram, the<a/> health check container monitors the web container and updates the results in a shared storage, such as <span class="bold">ETCD</span>, which can <a/>be used by the load balancer:</p><p class="calibre_9"><img src="images/00032.jpg" class="calibre_418"/></p><p class="calibre_8">
</p><p class="calibre_8">In the following diagram, the Git sync container updates data volume from the Git server, which is used by the web container to update the web page:</p><p class="calibre_9"><img src="images/00037.jpg" class="calibre_419"/></p><p class="calibre_8">
</p><p class="calibre_8">In the following diagram, the Web container updates the log volume that is read by the logging container to update the central log server:</p><p class="calibre_9"><img src="images/00385.jpg" class="calibre_420"/></p><p class="calibre_8">
</p><p id="filepos783394" class="calibre_9"><span class="calibre3"><span class="bold">The Ambassador pattern</span></span></p><p class="calibre_8">The Ambassador pattern is used when there is a need to access different types of services from a client container and it is not efficient to modify the client container for each type of service. A proxy <a/>container will take care of accessing different types of service and the client container needs to talk only to the proxy container. For example, the redis proxy takes care of talking to a single redis master scenario or a<a/> scenario with a redis master and multiple redis slaves without the redis client being aware of the type of the redis service.</p><p class="calibre_8">The following diagram shows the redis client with the redis ambassador accessing the redis service:</p><p class="calibre_9"><img src="images/00047.jpg" class="calibre_421"/></p><p class="calibre_8">
</p><p id="filepos784405" class="calibre_9"><span class="calibre3"><span class="bold">The Adapter pattern</span></span></p><p class="calibre_8">The Adapter pattern is the inverse<a/> of the Ambassador pattern. An <a/>example of the Adapter pattern is a service container exposing a common interface independent of the application residing in the service. For example, a monitoring or logging application wants a common interface to gather inputs irrespective of the application type. An adapter container takes care of converting the data to a standard format expected by the monitoring or logging application.</p><p class="calibre_8">The following example <a/>shows a monitoring/logging <a/>application accessing two different container applications, each with their own adapters:</p><p class="calibre_9"><img src="images/00322.jpg" class="calibre_422"/></p><p class="calibre_8">
</p><p id="filepos785373" class="calibre_9"><span class="calibre3"><span class="bold">Rolling updates with the Canary pattern</span></span></p><p class="calibre_8">This is an upgrade approach<a/> used when an application runs as a Container across a cluster of servers behind a load balancer. In this approach, the application upgrade is done on a few servers, and based on the preliminary feedback from customers, the upgrade can either be continued or reverted.</p><p class="calibre_8">Kubernetes supports a<a/> rolling upgrade with the Canary pattern. In the following example, we will demonstrate the Canary pattern with Kubernetes running on a CoreOS cluster in AWS. Here, we will upgrade <tt class="calibre2">hello1-controller</tt> with three replicas of the <tt class="calibre2">hello:v1</tt> container to <tt class="calibre2">hello2-controller</tt>, which also has three replicas of the <tt class="calibre2">hello:v2</tt> container.</p><p class="calibre_8">For this example, we need a three-node Kubernetes CoreOS cluster. Installation instructions can be found in <a href="index_split_181.html#filepos585742">Chapter 8</a>, <span class="italic">Container Orchestration</span>.</p><p class="calibre_8">The following is a<a/> three-node cluster with one master and two<a/> worker nodes:</p><p class="calibre_9"><img src="images/00230.jpg" class="calibre_346"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the replication controller, <tt class="calibre2">hello1-controller.json</tt>, with the <tt class="calibre2">hello1</tt> container image and three replicas:</p><p class="calibre_8"><tt class="calibre2">apiVersion: v1<br class="calibre4"/>kind: ReplicationController<br class="calibre4"/>metadata:<br class="calibre4"/>  name: hello1<br class="calibre4"/>  labels:<br class="calibre4"/>    name: hello<br class="calibre4"/>spec:<br class="calibre4"/>  replicas: 3<br class="calibre4"/>  selector:<br class="calibre4"/>    name: hello<br class="calibre4"/>    version: v1<br class="calibre4"/>  template:<br class="calibre4"/>    metadata:<br class="calibre4"/>      labels:<br class="calibre4"/>        name: hello<br class="calibre4"/>        version: v1<br class="calibre4"/>    spec:<br class="calibre4"/>      containers:<br class="calibre4"/>      - name: hello<br class="calibre4"/>        image: quay.io/kelseyhightower/hello:1.0.0<br class="calibre4"/>        ports:<br class="calibre4"/>        - containerPort: 80</tt></p><p class="calibre_8">The following is the <tt class="calibre2">hello-s.json</tt> service using the <tt class="calibre2">hello1</tt> replication controller:</p><p class="calibre_8"><tt class="calibre2">apiVersion: v1<br class="calibre4"/>kind: Service<br class="calibre4"/>metadata:<br class="calibre4"/>  name: hello<br class="calibre4"/>  labels:<br class="calibre4"/>    name: hello<br class="calibre4"/>spec:<br class="calibre4"/>  # if your cluster supports it, uncomment the following to automatically create<br class="calibre4"/>  # an external load-balanced IP for the hello service.<br class="calibre4"/>  type: NodePort<br class="calibre4"/>  ports:<br class="calibre4"/>    # the port that this service should serve on<br class="calibre4"/>    - port: 80<br class="calibre4"/>  selector:<br class="calibre4"/>    name: hello</tt></p><p class="calibre_8">Let's start the<a/> replication controller and service:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">kubectl create -f hello1-controller.json </span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create -f hello-s.json</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the<a/> running services and pods:</p><p class="calibre_9"><img src="images/00271.jpg" class="calibre_423"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's create a new replication controller and perform a Canary pattern rolling upgrade. The following is the new replication <tt class="calibre2">controller</tt>, <tt class="calibre2">hello2-controller.json</tt>, using the <tt class="calibre2">hello:2.0.0</tt> container image:</p><p class="calibre_8"><tt class="calibre2">apiVersion: v1<br class="calibre4"/>kind: ReplicationController<br class="calibre4"/>metadata:<br class="calibre4"/>  name: hello2<br class="calibre4"/>  labels:<br class="calibre4"/>    name: hello<br class="calibre4"/>spec:<br class="calibre4"/>  replicas: 3<br class="calibre4"/>  selector:<br class="calibre4"/>    name: hello<br class="calibre4"/>    version: v2<br class="calibre4"/>  template:<br class="calibre4"/>    metadata:<br class="calibre4"/>      labels:<br class="calibre4"/>        name: hello<br class="calibre4"/>        version: v2<br class="calibre4"/>    spec:<br class="calibre4"/>      containers:<br class="calibre4"/>      - name: hello<br class="calibre4"/>        image: quay.io/kelseyhightower/hello:2.0.0<br class="calibre4"/>        ports:<br class="calibre4"/>        - containerPort: 80</tt></p><p class="calibre_8">The following command does the rolling upgrade to <tt class="calibre2">hello2</tt>:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">kubectl rolling-update hello1 --update-period=10s -f hello2-controller.json</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The <tt class="calibre2">update-period</tt> parameter specifies the time interval between the upgrade of each pod.</p><p class="calibre_8">The following output <a/>shows you how each pod gets upgraded<a/> from <tt class="calibre2">hello1</tt> to <tt class="calibre2">hello2</tt>. At the end, the <tt class="calibre2">hello1</tt> replication controller is deleted:</p><p class="calibre_9"><img src="images/00307.jpg" class="calibre_424"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the running replication controllers now. As we can see in the following output, <tt class="calibre2">hello2</tt> RC is running and <tt class="calibre2">hello1</tt> RC has been deleted:</p><p class="calibre_9"><img src="images/00335.jpg" class="calibre_346"/></p><p class="calibre_8">
</p><p class="calibre_8">Kubernetes<a/> also supports the rollback option. In case a problem is detected as part of a rolling upgrade, the<a/> rolling upgrade can be stopped and rollback can be done using the <tt class="calibre2">--rollback</tt> option.</p><div class="mbp_pagebreak" id="calibre_pb_238"/>


<p id="filepos791369" class="calibre_9"><span class="calibre3"><span class="bold">Containers and PaaS</span></span></p><p class="calibre_8">Traditionally, the Services<a/> architecture has three types:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">IaaS</span> (<span class="bold">Infrastructure as a service</span>)</li><a/><li value="2" class="calibre_13"><span class="bold">PaaS</span> (<span class="bold">Platform as a service</span>)</li><li value="3" class="calibre_13"><span class="bold">SaaS</span> (<span class="bold">Software as a service</span>)</li><a/></ul><p class="calibre_8">With the advent of <a/>Docker, the PaaS layer has become a little difficult to define. PaaS vendors have used Containers as their underlying technology from the beginning. In fact, Docker came from the Dotcloud Company, which was providing a PaaS service.</p><p class="calibre_8">The following figure describes the new PaaS models and how they tie in to the traditional PaaS models and IaaS:</p><p class="calibre_9"><img src="images/00384.jpg" class="calibre_425"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>are some notes on the preceding diagram as<a/> well as how newer PaaS models are being developed:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">PaaS is typically used to simplify application deployment, which allows application developers to just develop the application, and PaaS provides necessary infrastructure services such as HA, scalability, and networking. PaaS is typically used for web applications.</li><li value="2" class="calibre_13">PaaS is typically deployed internally as Containers though users of PaaS need not be aware of this.</li><li value="3" class="calibre_13">Even though PaaS makes deploying applications faster, flexibility gets lost with PaaS.</li><li value="4" class="calibre_13">Examples of traditional PaaS systems are AWS Elastic beanstalk, Google GAE, Openshift, and Cloudfoundry.</li><li value="5" class="calibre_13">There is a new class of Micro-PaaS, where every service runs as a Docker Container, and this gives a little more flexibility than traditional PaaS. Examples are Deis, Flynn, and Tutum. Tutum was recently acquired by Docker.</li><a/><li value="6" class="calibre_13">With Docker containers, Container orchestration systems such as Kubernetes, and Container OSes such as CoreOS, it becomes easier for customers to build a PaaS system by themselves, which gives them maximum flexibility. Both Amazon and Google have launched Container services where users can run their Containers. Users have the option to build Container services on top of their own infrastructure as well.</li></ul><div class="mbp_pagebreak" id="calibre_pb_239"/>


<p id="filepos794353" class="calibre_14"><span class="calibre3"><span class="bold">Stateful and Stateless Containers</span></span></p><p class="calibre_8">Stateless containers are<a/> typically web applications such as NGINX, Node.js, and others. These follow the<a/> 12-factor application development (<a href="http://12factor.net/">http://12factor.net/</a>) methodology. These containers can be horizontally scaled. Stateful containers are used to store data like databases as data volumes in the host machine. Examples of stateful containers are Redis, MySQL, and MongoDB. We <a/>covered options for Container data persistence in <a href="index_split_147.html#filepos425088">Chapter 6</a>, <span class="italic">CoreOS Storage Management</span>. When stateful containers are migrated, it is necessary to migrate the data associated with the stateful containers. The following options are available to migrate stateful containers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Using tools such as Flocker, which takes care of the volume and data migration when a Container moves across hosts</li><li value="2" class="calibre_13">Using a cluster-file system or NFS so that the same data volume can be seen across multiple hosts</li></ul><p class="calibre_8">If implementing stateful containers is difficult, the other option for storage is to keep databases separate from application containers and run them on special systems.</p><div class="mbp_pagebreak" id="calibre_pb_240"/>


<p id="filepos795859" class="calibre_"><span class="calibre1"><span class="bold">Security</span></span></p><p class="calibre_8">The following are some<a/> approaches to secure the CoreOS cluster.</p><div class="mbp_pagebreak" id="calibre_pb_241"/>


<p id="filepos796087" class="calibre_9"><span class="calibre3"><span class="bold">Secure the external daemons</span></span></p><p class="calibre_8">Services such as Etcd, Fleet, and Docker can be reached externally. We can secure the client and server side using TLS and client and server certificates. We covered some of these details in earlier <a/>chapters when<a/> individual services were covered. If we are using Container orchestration such as Kubernetes, we need to make sure that the Kubernetes API server is using the TLS mechanism.</p><div class="mbp_pagebreak" id="calibre_pb_242"/>


<p id="filepos796664" class="calibre_9"><span class="calibre3"><span class="bold">SELinux</span></span></p><p class="calibre_8">SELinux<a/> is a Linux kernel<a/> feature that allows Container isolation even in case of a kernel bug that can cause the hacker to escape the Container namespace. SELinux integration is available from CoreOS 808.0 release. CoreOS disables SELinux by <a/>default. It can be enabled using the procedure at <a href="https://coreos.com/os/docs/latest/selinux.html">https://coreos.com/os/docs/latest/selinux.html</a>. There are some limitations like not being able to run SELinux with the btrfs filesystem and with Containers sharing volumes.</p><div class="mbp_pagebreak" id="calibre_pb_243"/>


<p id="filepos797368" class="calibre_9"><span class="calibre3"><span class="bold">Container image signing</span></span></p><p class="calibre_8">Docker supports Container signing using the Docker content trust. Rkt supports image signing using GPG. Using<a/> these approaches, we can validate that Containers running on CoreOS come from reliable sources and the Container image is not tampered in the middle. Container image signing was covered in detail in <a href="index_split_164.html#filepos509414">Chapter 7</a>, <span class="italic">Container Integration with CoreOS – Docker and Rkt</span>.</p><div class="mbp_pagebreak" id="calibre_pb_244"/>


<p id="filepos797939" class="calibre_"><span class="calibre1"><span class="bold">Deployment and automation</span></span></p><p class="calibre_8">Containers make it<a/> easy to package and ship software and guarantee that the same Container can work in development as well as production environments. Combining Containers with good deployment and automation techniques will aid in faster software<a/> deployment.</p><div class="mbp_pagebreak" id="calibre_pb_245"/>


<p id="filepos798386" class="calibre_9"><span class="calibre3"><span class="bold">Continuous Integration and Continuous Delivery</span></span></p><p class="calibre_8">The <a/>traditional approach <a/>of releasing software has the following problems:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Software release cycles were spaced apart, which caused new features taking a longer time to reach the customers</li><li value="2" class="calibre_13">Majority of the processes from the development stage to production were manual</li><li value="3" class="calibre_13">Considering the different deployment scenarios, it was difficult to guarantee that software worked in all environments and configurations</li></ul><p class="calibre_8">Containers have tried to mitigate some of these problems. Using microservices and the Container approach, it is guaranteed that the application will behave similarly in the development and production stages. Process automation and appropriate testing are still necessary for a Container-based environment.</p><p class="calibre_8"><span class="bold">Continuous Integration</span> (<span class="bold">CI</span>) refers to<a/> the process of making an executable or Container image automatically after a developer has done the Unit testing and code commit..</p><p class="calibre_8"><span class="bold">Continuous Delivery</span> (<span class="bold">CD</span>) refers<a/> to the process of taking the developer's built image, setting up the staging environment to test the image, and deploying it successfully for production.</p><p class="calibre_8">The following figure illustrates the steps for CI/CD:</p><p class="calibre_9"><img src="images/00430.jpg" class="calibre_426"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some notes on the preceding diagram:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The first row in the preceding diagram captures the steps for CI and the second row is the steps for CD.</li><li value="2" class="calibre_13">The CI process starts when developers commit the code after their basic UT. Typically, GitHub or Bitbucket is used as an image repository.</li><li value="3" class="calibre_13">There are hooks provided from the image repository to build system to automatically trigger the build after committing. The build system could be something such as Jenkins, which integrates with different code repositories. For Container images, <tt class="calibre2">Dockerfile</tt> and <tt class="calibre2">docker build</tt> will be used to build the Container image.</li><li value="4" class="calibre_13">Automatic UT suites can be kicked in if necessary before the image is committed to the image repository.</li><a/><li value="5" class="calibre_13">The build itself needs to be done inside Containers in order to eliminate dependency on the host system.</li><a/><li value="6" class="calibre_13">An image repository could be something such as the Docker hub or Docker trusted registry for Containers. CoreOS support the Quay repository for Container images.</li><li value="7" class="calibre_13">Once the image is pushed to a repository, the start of CD is automatically triggered.</li><li value="8" class="calibre_13">The staging environment needs to be set up with different Containers, storage, and other non-container software if necessary.</li><li value="9" class="calibre_13">QA tests are done in the staging environment. It is necessary that the staging environment be as close as possible to production.</li><li value="10" class="calibre_13">Once the QA tests are successful, images are deployed in production, such as AWS or GCE. If it's a PaaS application, it can be deployed to Cloudfoundry, among others.</li><li value="11" class="calibre_13">There are companies that provide integrated CI/CD solutions, such as Codeship, CircleCI, Shippable, and others. Docker has released an enterprise product called <span class="bold">Universal Control Plane</span> (<span class="bold">UCP</span>), which targets the CD part. Jenkins has Docker plugins to build images in Containers and also provides integration with the Docker hub.</li><a/><li value="12" class="calibre_13">There are different deployment patterns to do the upgrade. We covered the Canary deployment pattern in an earlier section.</li></ul><div class="mbp_pagebreak" id="calibre_pb_246"/>


<p id="filepos803098" class="calibre_14"><span class="calibre3"><span class="bold">Ansible integration with CoreOS and Docker</span></span></p><p class="calibre_8">Ansible is a <a/>configuration<a/> management and automation tool. Ansible is<a/> a very popular DevOps tool and <a/>serves similar purposes as Puppet or Chef. Ansible<a/> has a unique feature that there is no need to install an agent on the device side and this makes it very popular. There is active work ongoing to integrate Ansible with CoreOS and Docker. The following are some integration possibilities:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Manage the CoreOS system with Ansible. As CoreOS does not come with Python installed and the fact that packages cannot be installed directly, there are some workarounds necessary to get Ansible to manage a CoreOS system.</li><li value="2" class="calibre_13">Ansible has a Docker module that simplifies Container management such as starting and stopping containers and controlling Container properties.</li><a/><li value="3" class="calibre_13">The Docker installation can be automated with Ansible. Other than automating the Docker installation, Ansible can also manage other host infrastructure such as logging, storage, and networking.</li><a/><li value="4" class="calibre_13">Ansible can be used to build Docker images instead of using Dockerfile. There is a <tt class="calibre2">docker_image</tt> module (<a href="http://docs.ansible.com/ansible/docker_image_module.html">http://docs.ansible.com/ansible/docker_image_module.html</a>), but it is advised not to use it as its idempotent nature causes the Docker image to not be built in certain cases, which is a problem.</li><a/></ul><p id="filepos804923" class="calibre_14"><span class="calibre3"><span class="bold">Using Ansible to manage CoreOS</span></span></p><p class="calibre_8">I followed the<a/> procedure at <a href="https://coreos.com/blog/managing-coreos-with-ansible/">https://coreos.com/blog/managing-coreos-with-ansible/</a> to manage CoreOS with Ansible. As there is no package manager in CoreOS, Python cannot be installed directly. An approach at <a href="https://github.com/defunctzombie/ansible-coreos-bootstrap">https://github.com/defunctzombie/ansible-coreos-bootstrap</a> that is being <a/>used is to install PyPy, which is a<a/> minimal Python interpreter in CoreOS in the user directory and get Ansible to use this. The following example prepares the CoreOS node to be managed by Ansible and starts Etcd and Fleet service in the node using Ansible.</p><p class="calibre_8">The following<a/> are the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Install Ansible in the host machine. In my case, I am running Ansible 1.9 version in my Ubuntu 14.04 machine.</li><li value="2" class="calibre_13">Create a CoreOS cluster.</li><li value="3" class="calibre_13">Run the CoreOS bootstrap role to install the Python interpreter in CoreOS and update the system PATH to use it. Ansible roles create an abstraction over playbooks for specific tasks.</li><li value="4" class="calibre_13">Run Ansible playbooks to start CoreOS services. Playbook is an Ansible task list.</li></ol><p class="calibre_8">Set up a CoreOS cluster:</p><p class="calibre_8">The following commands set up the CoreOS cluster. In this case, a single-node <a/>cluster is created:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">git clone https://github.com/defunctzombie/coreos-ansible-example.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">cd coreos-ansible-example</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">vagrant up</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Set up passwordless SSH access:</p><p class="calibre_8">Use the following command to set up passwordless SSH access. Ansible needs passwordless SSH access.</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">./bin/generate_ssh_config</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Run the CoreOS bootstrap role:</p><p class="calibre_8">The following command sets up a CoreOS node with Python using the <a/>Ansible role, <tt class="calibre2">defunctzombie.coreos-bootstrap</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible-galaxy install defunctzombie.coreos-bootstrap -p ./roles</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible-playbook -i inventory/vagrant bootstrap.yml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">I created the following playbook to start CoreOS services, Etcd2, and Fleet:</p><p class="calibre_8"><tt class="calibre2">//Coreos_services.yml:<br class="calibre4"/>- name: CoreOS services<br class="calibre4"/>  hosts: web<br class="calibre4"/>  tasks:<br class="calibre4"/>    - name: Start etcd2<br class="calibre4"/>      service: name=etcd2.service state=started<br class="calibre4"/>      sudo: true<br class="calibre4"/>      sudo_user: root<br class="calibre4"/><br class="calibre4"/>    - name: Start fleet<br class="calibre4"/>      service: name=fleet.service state=started<br class="calibre4"/>      sudo: true<br class="calibre4"/>      sudo_user: root</tt></p><p class="calibre_8">In the preceding playbook, we have used the Ansible <tt class="calibre2">service</tt> module. Ansible modules are functions to do specific tasks. Ansible ships with a number of default modules and users can extend or write their own modules.</p><p class="calibre_8">The following is the output when I started the playbook for the first time. The inventory file contains details of the single CoreOS node:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible-playbook –i inventory/vagrant coreos-services.yml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00125.jpg" class="calibre_427"/></p><p class="calibre_8">
</p><p class="calibre_8">.</p><p class="calibre_8">The following is<a/> the output when I ran the same playbook one more time:</p><p class="calibre_9"><img src="images/00133.jpg" class="calibre_428"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see, services <a/>don't get restarted as they have already started and the <tt class="calibre2">changed</tt> variable is not set.</p><p class="calibre_8">The following output shows the running Etcd2 and Fleet services in the CoreOS node:</p><p class="calibre_9"><img src="images/00143.jpg" class="calibre_429"/></p><p class="calibre_8">
</p><p id="filepos810053" class="calibre_9"><span class="calibre3"><span class="bold">Using Ansible to manage Docker Containers</span></span></p><p class="calibre_8">Ansible provides you with a <a/>Docker module (<a href="http://docs.ansible.com/ansible/docker_module.html">http://docs.ansible.com/ansible/docker_module.html</a>) to manage Docker Containers. The Docker<a/> module can manage the<a/> Container life cycle, which includes the starting and stopping of Containers. As Ansible modules are idempotent, we can use this functionality to pull Docker images only if necessary and restart Containers only if the base image has changed.</p><p class="calibre_8">The following is a playbook that is executed on the same CoreOS node where we had run the CoreOS services playbook in the previous section. This will install the Docker module in the remote host and start the NGINX Container and a WordPress service having the WordPress and MySQL Containers:</p><p class="calibre_8"><tt class="calibre2">//Coreos_containers.yml:<br class="calibre4"/>- name: CoreOS Container<br class="calibre4"/>  hosts: web<br class="calibre4"/>  tasks:<br class="calibre4"/>    - name: Install docker-py<br class="calibre4"/>      pip: name=docker-py version=1.1.0<br class="calibre4"/><br class="calibre4"/>    - name: pull container<br class="calibre4"/>      raw: docker pull nginx<br class="calibre4"/><br class="calibre4"/>    - name: launch nginx container<br class="calibre4"/>      docker:<br class="calibre4"/>        image: "nginx"<br class="calibre4"/>        name: "example-nginx"<br class="calibre4"/>        ports: "8080:80"<br class="calibre4"/>        net: bridge<br class="calibre4"/>        state: reloaded<br class="calibre4"/><br class="calibre4"/>    - name: launch mysql container<br class="calibre4"/>      docker:<br class="calibre4"/>        image: mysql<br class="calibre4"/>        name: mysql<br class="calibre4"/>        pull: always<br class="calibre4"/>        net: bridge<br class="calibre4"/>        state: reloaded<br class="calibre4"/>        env:<br class="calibre4"/>            MYSQL_ROOT_PASSWORD: mysql<br class="calibre4"/><br class="calibre4"/>    - name: launch wordpress container<br class="calibre4"/>      docker:<br class="calibre4"/>        image: wordpress<br class="calibre4"/>        name: wordpress<br class="calibre4"/>        pull: always<br class="calibre4"/>        ports: 8000:80<br class="calibre4"/>        net: bridge<br class="calibre4"/>        state: reloaded<br class="calibre4"/>        links:<br class="calibre4"/>        - "mysql:mysql"</tt></p><p class="calibre_8">The following shows the<a/> output when the playbook is<a/> started for the first time:</p><p class="calibre_9"><img src="images/00152.jpg" class="calibre_430"/></p><p class="calibre_8">
</p><p class="calibre_8">The following screenshot shows the output when the same playbook is run again. As we can see, the <tt class="calibre2">changed</tt> flag is not set as all the Containers are running and there is no configuration change necessary:</p><p class="calibre_9"><img src="images/00162.jpg" class="calibre_431"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows<a/> the running Containers<a/> in the CoreOS node:</p><p class="calibre_9"><img src="images/00172.jpg" class="calibre_318"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: The <tt class="calibre2">reloaded</tt> flag in the Ansible Docker module should restart containers only if the base image is changed or configuration flags have changed. I hit a bug where <a/>Containers were restarted always. The link here (<a href="https://github.com/ansible/ansible-modules-core/issues/1251">https://github.com/ansible/ansible-modules-core/issues/1251</a>) describes this bug. Its workaround is to specify the <tt class="calibre2">net</tt> parameter as I have done in the preceding playbook.</p><p class="calibre_8">The <tt class="calibre2">reloaded</tt> and <tt class="calibre2">pull</tt> flags are available from Ansible 1.9.</p><p id="filepos814193" class="calibre_9"><span class="calibre3"><span class="bold">Ansible as a Container</span></span></p><p class="calibre_8">Public Container images <a/>with Ansible preinstalled are available. This link, <a href="https://hub.docker.com/r/ansible/ubuntu14.04-ansible/">https://hub.docker.com/r/ansible/ubuntu14.04-ansible/</a>, is an example Container image with Ansible preinstalled. The following output shows the Ansible version in the<a/> running Container:</p><p class="calibre_9"><img src="images/00181.jpg" class="calibre_163"/></p><p class="calibre_8">
</p><p id="filepos814836" class="calibre_9"><span class="calibre3"><span class="bold">Using Ansible to install Docker</span></span></p><p class="calibre_8">Ansible has this concept of Roles that gives a good abstraction to share a list of playbooks that accomplish a single task. Ansible Roles are available to install Docker on the Linux host. Ansible<a/> Roles are maintained in a central repository called Ansible Galaxy, which can be shared across users. Ansible Galaxy is similar to<a/> the Docker hub for Ansible roles.</p><p class="calibre_8">The following are the steps necessary:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Install the Ansible role locally from Ansible Galaxy.</li><li value="2" class="calibre_13">Create a playbook with this role and run it.</li></ol><p class="calibre_8">I used this Galaxy role (<a href="https://github.com/jamesdbloom/ansible-install-docker">https://github.com/jamesdbloom/ansible-install-docker</a>) to install Docker on<a/> my Ubuntu node. There are a few other roles in Galaxy accomplishing the same task.</p><p class="calibre_8">Use the following command to install the role:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible-galaxy install jamesdbloom.install-docker -p ./roles</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Create the <tt class="calibre2">install_docker1.yml</tt> playbook with the role:</p><p class="calibre_8"><tt class="calibre2">- name: install docker<br class="calibre4"/>  hosts: ubuntu<br class="calibre4"/>  gather_facts: True<br class="calibre4"/>  sudo: true<br class="calibre4"/>  roles:<br class="calibre4"/>    - jamesdbloom.install-docker</tt></p><p class="calibre_8">Run the playbook as follows:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible-playbook -i inventory/vagrant install_docker1.yml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is my inventory file:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">## inventory file for vagrant machines</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ubuntu-01 ansible_ssh_host=172.13.8.101</span></tt><tt class="calibre2"><br class="calibre4"/><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">[ubuntu]</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ubuntu-01</span></tt><tt class="calibre2"><br class="calibre4"/><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">[ubuntu:vars]</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ansible_ssh_user=vagrant</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The<a/> following output <a/>shows the playbook output:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Ansible-playbook –i inventory/vagrant install_docker1.yml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00190.jpg" class="calibre_432"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows Docker installed on my Ubuntu host using the preceding playbook:</p><p class="calibre_9"><img src="images/00321.jpg" class="calibre_98"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: I faced an<a/> issue with restarting the Docker service. I was able to solve it using the procedure at <a href="https://github.com/ansible/ansible-modules-core/issues/1170">https://github.com/ansible/ansible-modules-core/issues/1170</a>, where the init file has to be removed. I faced this issue with Ansible 1.9.1; however, this is fixed in later<a/> Ansible <a/>versions.</p><div class="mbp_pagebreak" id="calibre_pb_247"/>


<p id="filepos818585" class="calibre_"><span class="calibre1"><span class="bold">The CoreOS roadmap</span></span></p><div class="mbp_pagebreak" id="calibre_pb_248"/>


<p id="filepos818708" class="calibre_9"><span class="calibre3"><span class="bold">Ignition</span></span></p><p class="calibre_8">The Ignition (<a href="https://github.com/coreos/ignition">https://github.com/coreos/ignition</a>) project is being developed to setup initial <a/>CoreoS filesystem and it overcomes some of the issues with <tt class="calibre2">coreos-cloudinit</tt>. The <tt class="calibre2">coreos-cloudinit</tt> program is used to set up an initial CoreOS system<a/> configuration. The following are some known issues with <tt class="calibre2">coreos-cloudinit</tt>:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It is difficult to feed in dynamic environment variables. This makes it difficult to run CoreOS in Openstack environments and other environments where it is difficult to determine the IP address. This link, <a href="https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4">https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4</a>, describes the case where IP addresses don't get set because of which <tt class="calibre2">cloud-config</tt> services fail in Openstack.</li><a/><li value="2" class="calibre_13">The <tt class="calibre2">cloud-config</tt> service is processed serially and we cannot specify dependencies.</li></ul><p class="calibre_8">Ignition is run once <a/>on initial system bring-up and it writes the necessary files like service files and configuration data. On the first boot, Ignition reads the configuration from a specific location that's specified in the bootloader.</p><p class="calibre_8">Systemd, as part of a running provider metadata service file, will create <tt class="calibre2">coreos-metadata.target</tt>, which will contain necessary environment variables that service files can use. Service files will specify this target file as a dependency and systemd will take care of this dependency.</p><p class="calibre_8">The following is a sample <tt class="calibre2">etcd2.service</tt> file, which specifies <tt class="calibre2">coreos-metadata.service</tt> as<a/> a dependency. The <tt class="calibre2">/run/metadata/coreos</tt> environment file will contain <tt class="calibre2">COREOS_IPV4_PUBLIC</tt>, and this will be generated by <tt class="calibre2">coreos-metadata.service</tt>:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Requires=coreos-metadata.service<br class="calibre4"/>After=coreos-metadata.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>EnvironmentFile=/run/metadata/coreos<br class="calibre4"/>ExecStart=<br class="calibre4"/>ExecStart=/usr/bin/etcd2 \<br class="calibre4"/>    --advertise-client-urls=http://${COREOS_IPV4_PUBLIC}:2379 \<br class="calibre4"/>    --initial-advertise-peer-urls=http://${COREOS_IPV4_LOCAL}:2380 \<br class="calibre4"/>    --listen-client-urls=http://0.0.0.0:2379 \<br class="calibre4"/>    --listen-peer-urls=http://${COREOS_IPV4_LOCAL}:2380 \<br class="calibre4"/>    --initial-cluster=${ETCD_NAME}=http://${COREOS_IPV4_LOCAL}:2380</tt></p><p class="calibre_8">Ignition will be<a/> backward-compatible with cloudinit. Ignition has not yet been officially released.</p><div class="mbp_pagebreak" id="calibre_pb_249"/>


<p id="filepos821752" class="calibre_9"><span class="calibre3"><span class="bold">DEX</span></span></p><p class="calibre_8">DEX is an<a/> open source project started by CoreOS for identity management, including authentication <a/>and authorization. The<a/> following are some properties of DEX:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">DEX uses the <span class="bold">OpenID connect</span> (<span class="bold">OIDC</span>) (<a href="http://openid.net/connect/">http://openid.net/connect/</a>) standard, which is built on OAuth 2.0. OAuth 2.0 is used by Google to sign in to their services such as Gmail.</li><a/><li value="2" class="calibre_13">DEX supports multiple identity providers using the Connectors module. Currently, DEX supports the local connector using local servers and a OIDC connector such as Google.</li><li value="3" class="calibre_13">There is a plan to add authorization, user management, and multiple other connectors such as LDAP and GitHub.</li><li value="4" class="calibre_13">DEX is used as an identity provider in the Tectonic project.</li></ul><p class="calibre_8">DEX is still in its early<a/> stages and under active development.</p><div class="mbp_pagebreak" id="calibre_pb_250"/>


<p id="filepos823059" class="calibre_9"><span class="calibre3"><span class="bold">Clair</span></span></p><p class="calibre_8">Clair is an open<a/> source project started by CoreOS to detect Container vulnerabilities. The following are some properties of Clair:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Clair scans Container images stored in the Quay Container repository for vulnerabilities</li><a/><li value="2" class="calibre_13">Each Container layer contains information about packages installed in that layer and this is provided by the corresponding Linux package manager</li><a/><li value="3" class="calibre_13">Clair analyzes each Container layer by querying the package manager-related files and compares them against the vulnerability database available in the particular Linux distribution to check whether the particular Container layer is vulnerable</li><a/><li value="4" class="calibre_13">Clair makes an index-directed graph of each Container layer, and this speeds up the analysis of a lot of Container images sharing layers</li><li value="5" class="calibre_13">Clair currently supports CentOS, Ubuntu, and Debian Linux distributions</li></ul><p class="calibre_8">Clair is still in its early stages and under active development.</p><div class="mbp_pagebreak" id="calibre_pb_251"/>


<p id="filepos824491" class="calibre_"><span class="calibre1"><span class="bold">The Docker roadmap</span></span></p><p class="calibre_8">Docker has transitioned<a/> from providing Container runtime to a Container platform. Docker provides both open source solutions as well as commercial products around Containers.</p><p class="calibre_8">The following diagram shows different Docker products around Core Docker, Security, Orchestration, Registry, and Deployment as of November 2015:</p><p class="calibre_9"><img src="images/00210.jpg" class="calibre_433"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are<a/> some new projects announced recently by Docker.</p><div class="mbp_pagebreak" id="calibre_pb_252"/>


<p id="filepos825307" class="calibre_9"><span class="calibre3"><span class="bold">Tutum</span></span></p><p class="calibre_8">Tutum makes it easy<a/> to build, deploy, and manage Containerized<a/> applications and is available as a SaaS application. An application can be a single- or multi-container application. Tutum integrates well with the Docker hub.</p><div class="mbp_pagebreak" id="calibre_pb_253"/>


<p id="filepos825698" class="calibre_9"><span class="calibre3"><span class="bold">UCP</span></span></p><p class="calibre_8">UCP is Docker's commercial<a/> offering to provide on-premise Container <a/>deployment solutions. UCP integrates with the Docker trusted registry as well as with <a/>enterprise services such as LDAP and <span class="bold">Role-based access control</span> (<span class="bold">RBAC</span>). UCP also integrates with all other Docker services such as Networking, Compose, and Swarm. UCP is in the beta phase currently.</p><div class="mbp_pagebreak" id="calibre_pb_254"/>


<p id="filepos826237" class="calibre_9"><span class="calibre3"><span class="bold">Nautilus</span></span></p><p class="calibre_8">This project is<a/> targeted towards Container vulnerability detection. This is similar to the Clair project from<a/> CoreOS. Nautilus is still in the very early stages.</p><div class="mbp_pagebreak" id="calibre_pb_255"/>


<p id="filepos826555" class="calibre_"><span class="calibre1"><span class="bold">Microservices infrastructure</span></span></p><p class="calibre_8">In this section, we <a/>will cover an overview of microservice infrastructure components and examples of a few solution providers.</p><div class="mbp_pagebreak" id="calibre_pb_256"/>


<p id="filepos826866" class="calibre_9"><span class="calibre3"><span class="bold">Platform choices</span></span></p><p class="calibre_8">The following are some design decisions/platform choices that customers who are developing and deploying<a/> microservices need to make. The following examples are just a sample set and do not cover all the providers.</p><p class="calibre_8"><span class="bold">IaaS vs PaaS</span>: This choice<a/> applies for local data centers as well as for Cloud providers. In the earlier section, we covered the comparison between Container and PaaS models. Here, the trade-offs are flexibility versus time-to-market.</p><p class="calibre_8"><span class="bold">Local data center versus cloud providers</span>: This<a/> is mostly a cost versus time trade-off.</p><p class="calibre_8"><span class="bold">Base OS</span>: The choice here is <a/>either going with Container-optimized OSes such as CoreOS, Rancher, or Atomic or traditional OSes such as Ubuntu or Fedora. For pure microservice architecture, Container-optimized OSes are definitely worth pursuing.</p><p class="calibre_8"><span class="bold">VM Orchestration</span>: VMs and <a/>Containers have different use cases and will continue to live together. There will be scenarios where VMs will be used standalone or Containers will run on top of VMs. There are open source solutions such as Openstack and commercial solutions from VMWare for VM Orchestration.</p><p class="calibre_8"><span class="bold">Container runtime</span>: Choices here<a/> are Docker, Rkt, or LXC.</p><p class="calibre_8"><span class="bold">Networking</span>: Container orchestration systems such as Kubernetes typically integrate networking. As networking support is provided as plugins, it can be swapped with a different implementation<a/> if necessary. Some examples of networking plugins include Weave, Calico, and Contiv.</p><p class="calibre_8"><span class="bold">Storage</span>: We need to<a/> evaluate between dedicated storage versus stateful Containers. Choices for stateful Containers are GlusterFS, Ceph, or Flocker.</p><p class="calibre_8"><span class="bold">Container Orchestration</span>: Choices<a/> here are Kubernetes, Docker Swarm, Mesos, and so on.</p><p class="calibre_8"><span class="bold">Service discovery and DNS</span>: This <a/>can be built manually using building blocks mentioned in previous sections, or if we choose a container orchestration system such as Kubernetes, it's already<a/> integrated with this.</p><p class="calibre_8"><span class="bold">CI and CD</span>: This can be <a/>manually built or we can use packaged solutions from Codeship, CircleCI, or Shippable.</p><p class="calibre_8"><span class="bold">Monitoring and logging system</span>: Examples<a/> are Sysdig or Logentries. We covered more details on Monitoring and logging in <a href="index_split_219.html#filepos708963">Chapter 10</a>, <span class="italic">CoreOS and Containers - Troubleshooting and Debugging</span>.</p><div class="mbp_pagebreak" id="calibre_pb_257"/>


<p id="filepos829873" class="calibre_9"><span class="calibre3"><span class="bold">Solution providers</span></span></p><p class="calibre_8">As we have seen throughout this book, there are many hardware and software components that comprise the infrastructure to create and deploy microservices. We can think of each component as a LEGO block and there are numerous ways of bringing these LEGO blocks together. Customers have the following three choices:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Integrating all the infrastructure components by themselves</li><a/><li value="2" class="calibre_13">Going with solution providers who integrate these components and give an opinionated architecture</li><li value="3" class="calibre_13">Choosing a hybrid solution between the previous two options, where we can choose reference architecture and replace a few components based on specific needs</li></ul><p class="calibre_8">The following are some commercial and open source integrated solutions available. The list is not extensive <a/>and some of these solutions do not integrate all the components:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Tectonic Enterprise from CoreOS.</li><li value="2" class="calibre_13">Google Container service</li><li value="3" class="calibre_13">AWS Container service</li><li value="4" class="calibre_13">Cisco's Mantl project</li><li value="5" class="calibre_13">Openstack Magnum</li></ul><div class="mbp_pagebreak" id="calibre_pb_258"/>


<p id="filepos831568" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered some of the production considerations in deploying microservice-based distributed infrastructure, and this includes CoreOS, Docker, and the associated ecosystem. Cloud companies such as Google, Amazon, and Facebook have used microservices and Container-based technologies for quite a long time and they have learned the best practices and pitfalls based on their experience.</p><p class="calibre_8">The issue till now has been the replication of approaches and not having a common standard/approach. The trend in the last few years has been that these companies as well as many start-ups such as CoreOS and Docker are willing to develop technologies and work together in an open manner that helps the entire industry. A big contributor to this is open source software development, and many big companies are willing to develop software in the open now. Obviously, commercial solutions around open source technologies will continue to thrive as the industry still needs to make money to survive.</p><p class="calibre_8">Container technology and microservices are the biggest trends in the software industry currently. Customers have many options and this includes both open source and commercial solutions. At this point, there is a need to put together different technologies/products to create a complete solution for microservices infrastructure. As these technologies mature, integrated open solutions with a pluggable architecture will win over the long term.</p><div class="mbp_pagebreak" id="calibre_pb_259"/>


<p id="filepos833241" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Registrator: <a href="http://gliderlabs.com/registrator/latest/user/quickstart/">http://gliderlabs.com/registrator/latest/user/quickstart/</a></li><li value="2" class="calibre_13">Ansible reference: <a href="https://docs.ansible.com/">https://docs.ansible.com/</a></li><a/><li value="3" class="calibre_13">Managing CoreOS with Ansible: <a href="https://coreos.com/blog/managing-coreos-with-ansible/">https://coreos.com/blog/managing-coreos-with-ansible/</a> and <a href="https://github.com/defunctzombie/ansible-coreos-bootstrap">https://github.com/defunctzombie/ansible-coreos-bootstrap</a></li><a/><li value="4" class="calibre_13">Ansible Docker module: <a href="http://docs.ansible.com/ansible/docker_module.html">http://docs.ansible.com/ansible/docker_module.html</a></li><a/><li value="5" class="calibre_13">CoreOS and Docker: <a href="https://developer.rackspace.com/blog/ansible-and-docker/">https://developer.rackspace.com/blog/ansible-and-docker/</a> and <a href="http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html">http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html</a></li><a/><li value="6" class="calibre_13">CI pipeline with Docker: <a href="https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf">https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf</a></li><a/><li value="7" class="calibre_13">Containers and PaaS: <a href="http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/">http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/</a> and <a href="http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/">http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/</a></li><a/><li value="8" class="calibre_13">Container security with SELinux and CoreOS: <a href="https://coreos.com/blog/container-security-selinux-coreos/">https://coreos.com/blog/container-security-selinux-coreos/</a></li><a/><li value="9" class="calibre_13">CoreOS Ignition: <a href="https://github.com/coreos/ignition">https://github.com/coreos/ignition</a> and <a href="https://coreos.com/ignition/docs/latest/examples.html">https://coreos.com/ignition/docs/latest/examples.html</a></li><a/><li value="10" class="calibre_13">CoreOS DEX: <a href="https://github.com/coreos/dex">https://github.com/coreos/dex</a>, <a href="https://coreos.com/blog/announcing-dex/">https://coreos.com/blog/announcing-dex/</a>, and <a href="https://www.youtube.com/watch?v=QZgkJQiI_gE">https://www.youtube.com/watch?v=QZgkJQiI_gE</a></li><a/><li value="11" class="calibre_13">Clair for Container vulnerability analysis: <a href="https://coreos.com/blog/vulnerability-analysis-for-containers/">https://coreos.com/blog/vulnerability-analysis-for-containers/</a> and <a href="https://github.com/coreos/clair">https://github.com/coreos/clair</a></li><a/><li value="12" class="calibre_13">Docker Tutum and UCP: <a href="https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/">https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/</a>, <a href="https://www.docker.com/tutum">https://www.docker.com/tutum</a>, and <a href="https://www.docker.com/universal-control-plane">https://www.docker.com/universal-control-plane</a></li><a/><li value="13" class="calibre_13">Mantl project: <a href="https://github.com/CiscoCloud/microservices-infrastructure">https://github.com/CiscoCloud/microservices-infrastructure</a> and <a href="http://mantl.io/">http://mantl.io/</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_260"/>


<p id="filepos837239" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Service discovery: <a href="http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/">http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/</a></li><a/><li value="2" class="calibre_13">Ansible with Docker on Rancher: <a href="http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/">http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/</a></li><a/><li value="3" class="calibre_13">Stateful Containers: <a href="http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/">http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/</a></li><a/><li value="4" class="calibre_13">Codeship, Shippable, and CircleCI: <a href="https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci">https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci</a>, <a href="https://circleci.com/docs/docker">https://circleci.com/docs/docker</a>, <a href="https://blog.codeship.com/continuous-integration-and-delivery-with-docker/">https://blog.codeship.com/continuous-integration-and-delivery-with-docker/</a>, and <a href="http://docs.shippable.com/">http://docs.shippable.com/</a></li><a/><li value="5" class="calibre_13">Comparing CI/CD solutions: <a href="https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker">https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker</a></li><a/><li value="6" class="calibre_13">Containers and PaaS: <a href="https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/">https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/</a> and <a href="https://www.youtube.com/watch?v=YydhEEgOoDg">https://www.youtube.com/watch?v=YydhEEgOoDg</a></li><a/><li value="7" class="calibre_13">Ignition presentation: <a href="https://www.youtube.com/watch?v=ly3uwn0HzBI">https://www.youtube.com/watch?v=ly3uwn0HzBI</a></li><a/><li value="8" class="calibre_13">Jenkins Docker plugin: <a href="https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin">https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin</a></li><a/><li value="9" class="calibre_13">Continuous delivery with Docker and Jenkins: <a href="https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf">https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf</a> and <a href="https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf">https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_261"/>
</body></html>