<html><head></head><body>
<p id="filepos509414" class="calibre_"><span class="calibre1"><span class="bold">Chapter 7. Container Integration with CoreOS – Docker and Rkt</span></span></p><p class="calibre_8">Containers have drastically changed application development and deployment, and are the biggest trend in the computer industry currently. We have talked about Containers in almost all the chapters of this book. In this chapter, we will focus on the Container standards, advanced Docker topics, and basics of the Rkt Container runtime and how all these topics integrate with CoreOS. As Docker is pretty mature, we have covered only advanced Docker topics in this chapter. As the Rkt container runtime is still evolving, we have covered the basics of Rkt in this chapter. Even though Docker started as a Container runtime, Docker has evolved into a Container platform providing orchestration, networking, storage, and security solutions around containers.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Container standards—<span class="bold">App Container</span> (<span class="bold">appc</span>) specification , <span class="bold">Open Container Initiative</span> (<span class="bold">OCI</span>), Libnetwork, <span class="bold">Container Network Interface</span> (<span class="bold">CNI</span>), and <span class="bold">Cloud Native Computing Foundation</span> (<span class="bold">CNCF</span>)</li><li value="2" class="calibre_13">The Docker daemon configuration, Docker registry, Docker image signing, and basic Docker debugging</li><li value="3" class="calibre_13">Rkt basics and how to use Rkt with image signing, systemd, and Flannel</li></ul><div class="mbp_pagebreak" id="calibre_pb_164"/>


<p id="filepos511116" class="calibre_14"><span class="calibre1"><span class="bold">Container standards</span></span></p><p class="calibre_8">Standards <a/>are an important part of any technology. Standards and specifications allow products and technologies from different vendors to interoperate with each other. As developments in the Container space happened very fast in the last 1-2 years, there was limited attention paid to standards and specifications. In the recent past, the industry has been working toward standards for Container runtime, Container networking, and Container orchestration. In majority of these cases, there are runtime implementations that get released along with the specification and this encourages faster adoption. The following <a/>are the standards' categories covered in this section:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Container image and runtime: APPC and OCI</li><li value="2" class="calibre_13">Container Networking: Libnetwork and CNI</li><li value="3" class="calibre_13">Container orchestration: CNCF</li></ul><div class="mbp_pagebreak" id="calibre_pb_165"/>


<p id="filepos512308" class="calibre_14"><span class="calibre3"><span class="bold">App container specification</span></span></p><p class="calibre_8">The <a/>APPC specification provides you with a standard to <a/>describe the container image format, Container image discovery, Container grouping or Pods, and Container execution environment. Different Container runtimes implementing the APPC specification will be interoperable with each other. The APPC specification is primarily driven by CoreOS along with a few other community members. Rkt, Kurma, and Jetpack are examples of Container runtime implementing APPC. The following are some important components of APPC.</p><p id="filepos512980" class="calibre_9"><span class="calibre3"><span class="bold">The Container image format</span></span></p><p class="calibre_8">This <a/>describes the container image layout, manifest file <a/>with image details, and image signing.</p><p class="calibre_8"><span class="bold">Application Container Image</span> (<span class="bold">ACI</span>) is<a/> a Container image created according to the APPC specification. For example, the <tt class="calibre2">nginx.aci</tt> image is an ACI for the nginx Container. To understand Container image format, let's look at what is contained within nginx.aci APPC image. The following command extracts the contents of the <tt class="calibre2">nginx.aci</tt> image to the <tt class="calibre2">nginx</tt> directory: (We got the <tt class="calibre2">nginx.aci</tt> image from the <tt class="calibre2">docker2aci</tt> tool that will be covered later in the chapter.)</p><p class="calibre_8"><tt class="calibre2"><span class="bold">tar -xvf nginx.aci -C nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following set of screenshots shows you the base layout and rootfs layout for the <tt class="calibre2">nginx.aci</tt> image:</p><p class="calibre_9"><img src="images/00119.jpg" class="calibre_295"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00121.jpg" class="calibre_296"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are <a/>some important sections in the <tt class="calibre2">nginx.aci</tt> manifest. The first screenshot shows the container name and version. The second screenshot describes the exposed ports, mountpoints, environment variables, and so on:</p><p class="calibre_9"><img src="images/00073.jpg" class="calibre_297"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00077.jpg" class="calibre_298"/></p><p class="calibre_8">
</p><p class="calibre_8">From the preceding screenshot, we can see that nginx ACI image is exposing ports <tt class="calibre2">80</tt> and <tt class="calibre2">443</tt> and it has mount <tt class="calibre2">point /var/cache/nginx</tt>. Container image signing is done using GPG (<a href="https://www.gnupg.org/">https://www.gnupg.org/</a>). GPG is a public key cryptography implementation that can be used for the encryption of messages as well as image signing using a public and private key pair.</p><p id="filepos515458" class="calibre_9"><span class="bold">Container image discovery</span></p><p class="calibre_8">Container image discovery describes ways to find the location of Container images from image name. Container<a/> images use the URL format. Container image discovery describes ways to find the location of Container images from image name. The following is the image format used:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">https://{name}-{version}-{os}-{arch}.{ext}</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos515982" class="calibre_9"><span class="calibre5"><span class="bold">Simple discovery</span></span></p><p class="calibre_8">Here, the <a/>complete URL is mentioned to retrieve the ACI image. An example is as follows:</p><p class="calibre_8"><a href="https://github.com/coreos/etcd/releases/download/v2.0.0/etcd-v2.0.0-linux-amd64.aci">https://github.com/coreos/etcd/releases/download/v2.0.0/etcd-v2.0.0-linux-amd64.aci</a>
</p><p id="filepos516439" class="calibre_9"><span class="calibre5"><span class="bold">Meta discovery</span></span></p><p class="calibre_8">Here, the<a/> image URL and public key is discovered automatically by using the meta tag embedded in the HTTP location. The following example shows you how to retrieve the meta tag and the ACI image from meta tags for the CoreOS etcd container image.</p><p class="calibre_8">The first step is to retrieve the meta tags. The <tt class="calibre2">https://coreos.com/etcd</tt> location contains the <tt class="calibre2">ac-discovery</tt> meta tag that contains the image location and the <tt class="calibre2">ac-discovery-pubkeys</tt> meta tag that contains the public key.</p><p class="calibre_8">The link, <a href="https://coreos.com/etcd/">https://coreos.com/etcd/</a>, contains the following meta tags that can be retrieved as an HTTP request:</p><p class="calibre_8"><tt class="calibre2">&lt;meta name="ac-discovery" content="coreos.com/etcd https://github.com/coreos/etcd/releases/download/{version}/etcd-{version}-{os}-{arch}.{ext}"&gt;<br class="calibre4"/>&lt;meta name="ac-discovery-pubkeys" content="coreos.com/etcd https://coreos.com/dist/pubkeys/aci-pubkeys.gpg"&gt;</tt></p><p class="calibre_8">Using the preceding meta tag content, the Container image can be retrieved from the following:</p><p class="calibre_8"><tt class="calibre2">https://github.com/coreos/etcd/releases/download/{version}/etcd-{version}-{os}-{arch}.{ext}</tt></p><p class="calibre_8">The public key can be retrieved from the following:</p><p class="calibre_8"><tt class="calibre2">https://coreos.com/dist/pubkeys/aci-pubkeys.gpg</tt></p><p id="filepos518122" class="calibre_9"><span class="bold">The app container executor</span></p><p class="calibre_8">The app container executor<a/> takes care of the following to set up runtime for the Container:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">UUID setup: This is a Unique ID for the Pod that contains multiple containers. UUID is registered with the metadata service that allows other containers to find each other.</li><li value="2" class="calibre_13">Filesystem setup: A filesystem is created in its own namespace.</li><a/><li value="3" class="calibre_13">Volume setup: These are files to be mounted to the container.</li><li value="4" class="calibre_13">Networking: This specifies Container networking to the host and other Containers.</li><li value="5" class="calibre_13">Isolators: This controls the CPU and memory limit for the Container.</li></ul><p id="filepos519138" class="calibre_14"><span class="bold">App container pods</span></p><p class="calibre_8">The <a/>concept of pods comes from Kubernetes where related containers are packed together in a Pod. Containers within a pod share the process PID, network, and IPC namespace. A manifest can be created for the Pod in addition to individual containers in order to describe properties for the Pod.</p><p id="filepos519551" class="calibre_9"><span class="bold">The app container metadata service</span></p><p class="calibre_8">The <a/>app container metadata service is a service that runs externally, and container pods can register information about pods and applications. This metadata service can be used by pods to find information about other pods as well as by containers within a pod to find information about other containers.</p><p id="filepos519992" class="calibre_9"><span class="calibre3"><span class="bold">APPC tools</span></span></p><p class="calibre_8">APPC <a/>provides you with tools to create, validate, and <a/>convert ACI images.</p><p id="filepos520208" class="calibre_9"><span class="bold">Actool</span></p><p class="calibre_8">Using Actool for ACI validation: </p><p class="calibre_8">The <a/>following output shows you that the generated ACI image, <tt class="calibre2">busybox-latest.aci</tt>, is a valid APPC image:</p><p class="calibre_9"><img src="images/00081.jpg" class="calibre_299"/></p><p class="calibre_8">
</p><p class="calibre_8">Using Actool for ACI discovery:</p><p class="calibre_8">The following output shows you the discovery URL and public key from the ACI image:</p><p class="calibre_9"><img src="images/00086.jpg" class="calibre_300"/></p><p class="calibre_8">
</p><p class="calibre_8">Using Actool for checking manifest: </p><p class="calibre_8">The <a/>following output shows you how to see the manifest from the ACI image:</p><p class="calibre_9"><img src="images/00091.jpg" class="calibre_301"/></p><p class="calibre_8">
</p><p id="filepos521401" class="calibre_9"><span class="bold">Acbuild</span></p><p class="calibre_8">The <a/>Acbuild tool is used to build ACI images. The concept is similar to the Dockerfile approach to build Docker Container images, but Acbuild provides more flexibility to build Container images by having better integration with Linux tools such as makefile, environment variables, and others.</p><p class="calibre_8">The following is an example of building a container image from a GO executable <tt class="calibre2">hello</tt>. Before running the following commands, we need to link the <tt class="calibre2">hello</tt> executable in the current directory statically:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">acbuild begin</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild set-name example.com/hello</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild copy hello /bin/hello</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild set-exec /bin/hello</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild port add www tcp 5000</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild label add version 0.0.1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild annotation add authors "Sreenivas Makam&lt;sxxxm@yahoo.com&gt;"</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild write hello-0.0.1-linux-amd64.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild end</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If we run the preceding commands, it will create an APPC image, <tt class="calibre2">hello-0.0.1-linux-amd64.aci</tt>, which we can run with the Rkt Container runtime.</p><p class="calibre_8">The following is another example that is similar to the Dockerfile approach to build an ACI image. In <a/>this example, we take a base Ubuntu image, install Apache, and start Apache in a container to create the <tt class="calibre2">ubuntu-nginx.aci</tt> image:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">acbuild begin</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild dependency add quay.io/fermayo/ubuntu</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild run -- apt-get update</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild run -- apt-get -y install nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild set-exec -- /usr/sbin/nginx –g "daemon=off;"</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild set-name example.com/ubuntu-nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild write ubuntu-nginx.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">acbuild end</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">To run acbuild, it's necessary to have systemd-nspawn in the system. This is present by default in CoreOS nodes. The following is the APPC image that was created from the preceding script:</p><p class="calibre_9"><img src="images/00096.jpg" class="calibre_234"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's start the Container using Rkt:</p><p class="calibre_9"><img src="images/00101.jpg" class="calibre_302"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the running Container's status:</p><p class="calibre_9"><img src="images/00107.jpg" class="calibre_66"/></p><p class="calibre_8">
</p><p id="filepos524706" class="calibre_9"><span class="bold">Docker2aci</span></p><p class="calibre_8">The Docker2aci utility is used to convert Docker Containers to the ACI format. The following is an example that takes a docker <tt class="calibre2">busybox</tt> container and converts it to a <tt class="calibre2">busybox.aci</tt> image:</p><p class="calibre_9"><img src="images/00113.jpg" class="calibre_303"/></p><p class="calibre_8">
</p><p id="filepos525176" class="calibre_9"><span class="calibre3"><span class="bold">Open Container Initiative</span></span></p><p class="calibre_8">OCI is the<a/> Open Container Initiative open source project started in April 2015 by Docker and has members from all <a/>major companies including Docker and CoreOS. OCI defines the following:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Container image format: This describes the filesystem bundle along with <tt class="calibre2">config.json</tt> that describes the host-independent property of a container and <tt class="calibre2">runtime.json</tt> that describes the host-dependent property of a container.</li><li value="2" class="calibre_13">Runtime: This describes how a container can be started and stopped using namespaces and cgroups.</li></ul><p class="calibre_8">Docker's goal is to follow the OCI specification for its Container runtime.</p><p id="filepos526148" class="calibre_9"><span class="bold">Runc</span></p><p class="calibre_8">Runc is <a/>an implementation of the OCI specification. Docker engine uses runc to implement Container runtime in Docker. Runc can be installed using the procedure described at <a href="https://github.com/opencontainers/runc">https://github.com/opencontainers/runc</a>.</p><p class="calibre_8">The following procedure can be used to start a Ubuntu container using <tt class="calibre2">runc</tt>:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Docker pull Ubuntu</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker export $(docker create ubuntu) &gt; ubuntu.tar</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">mkdir rootfs</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">tar -C rootfs -xf ubuntu.tar</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">runc spec</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The first step pulls the Ubuntu Docker Container. The second step exports the Ubuntu Container to a <a/>filesystem. The third and fourth steps put the Ubuntu filesystem content in the <tt class="calibre2">rootfs</tt> directory. The last step generates <tt class="calibre2">config.json</tt> and <tt class="calibre2">runtime.json</tt>.</p><p class="calibre_8">The following output shows you the Ubuntu container started using <tt class="calibre2">runc</tt>:</p><p class="calibre_9"><img src="images/00118.jpg" class="calibre_304"/></p><p class="calibre_8">
</p><p id="filepos527616" class="calibre_9"><span class="bold">The relationship of OCI with APPC</span></p><p class="calibre_8">CoreOS, along <a/>with a few other community members, created the APPC specification to standardize the Container image format that makes Containers interoperable between different implementations. </p><p class="calibre_8">The original APPC container specification proposed by CoreOS covers four different elements of container management: packaging, signing, naming (sharing the container with others), and runtime. Docker felt the same need for interoperability and created OCI along with other community members including CoreOS. OCI focuses only on packaging and runtime currently, though this might change in the future. The goals of APPC and OCI are common even though specifics slightly differ. It is possible that these two standards will converge into one standard at some later point.</p><p id="filepos528563" class="calibre_9"><span class="calibre5"><span class="bold">OCI and APPC latest updates</span></span></p><p class="calibre_8">As per the<a/> latest CoreOS blog update (<a href="https://coreos.com/blog/making-sense-of-standards.html">https://coreos.com/blog/making-sense-of-standards.html</a>), APPC and OCI will intersect only in runtime and APPC will continue to focus on image format, signing, and distribution.</p><p id="filepos528999" class="calibre_9"><span class="calibre3"><span class="bold">Libnetwork</span></span></p><p class="calibre_8">Libnetwork<a/> was <a/>covered briefly in <a href="index_split_122.html#filepos342077">Chapter 5</a>, <span class="italic">CoreOS Networking and Flannel Internals</span>. Libnetwork is an open source project started by Docker and a few other community members with the following objectives:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Keep networking as a library separate from the Container runtime.</li><li value="2" class="calibre_13">Provide Container connectivity in the same host as well as across hosts.</li><li value="3" class="calibre_13">Networking implementation will be done as a plugin implemented by drivers. The plugin mechanism is provided to add new third-party drivers easily.</li><li value="4" class="calibre_13">Control IP address assignment for the Containers using local IPAM drivers and plugins.</li></ul><p class="calibre_8">Docker uses <a/>Libnetwork to provide Container networking.</p><p class="calibre_8">There are three primary components<a/> in Libnetwork:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Sandbox</span>: All networking functionality is encapsulated in a sandbox. This can be implemented using networking namespace or a similar function.</li><a/><li value="2" class="calibre_13"><span class="bold">Endpoint</span>: This attaches sandbox to the network.</li><a/><li value="3" class="calibre_13"><span class="bold">Network</span>: Multiple endpoints in the same network can talk to each other.</li><a/></ul><p class="calibre_8">The following diagram shows <span class="bold">Sandbox</span>, <span class="bold">Endpoint</span>, and <span class="bold">Network</span> and how two containers can talk to each other using these constructs:</p><p class="calibre_9"><img src="images/00020.jpg" class="calibre_305"/></p><p class="calibre_8">
</p><p class="calibre_8">Libnetwork supports <a/>local drivers such as null, bridge, and overlay. The bridge driver can be used for Container connectivity in a single host, and the overlay driver can be used for Container connectivity across hosts. Remote drivers such as Weave and Calico are also supported.</p><p id="filepos531459" class="calibre_9"><span class="calibre3"><span class="bold">CNI</span></span></p><p class="calibre_8">CNI was <a/>covered briefly in <a href="index_split_122.html#filepos342077">Chapter 5</a>, <span class="italic">CoreOS Networking and Flannel Internals</span>.</p><p class="calibre_8">CNI is the <a/>Container networking interface open source project developed by CoreOS along with a few other community members to provide networking facility for Containers as a pluggable and extensible mechanism. CoreOS's Container runtime, Rkt, uses CNI to establish Container networking. The objectives of Libnetwork and CNI are pretty much the same.</p><p class="calibre_8">The following are <a/>some notes on CNI:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The CNI interface calls the API of the CNI plugin to set up Container networking.</li><li value="2" class="calibre_13">The CNI plugin is responsible for creating the network interface to the container.</li><li value="3" class="calibre_13">The CNI plugin calls the IPAM plugin to set up the IP address for the container.</li><li value="4" class="calibre_13">The CNI plugin needs to implement an API for container network creation and deletion.</li><li value="5" class="calibre_13">The plugin type and parameters are specified as a JSON file that the Container runtime reads and sets up.</li><a/><li value="6" class="calibre_13">Available CNI plugins are Bridge, macvlan, ipvlan, and ptp. Available IPAM plugins are host-local and DHCP. CNI plugins and the IPAM plugin can be used in any combination.</li><li value="7" class="calibre_13">External CNI plugins such as Flannel and Weave are also supported. External plugins reuse the bridge plugin to set up the final networking.</li><li value="8" class="calibre_13">The following is a sample JSON configuration with the bridge CNI plugin and host-local IPAM plugin along with the IP allocation range:<p class="calibre_8"><tt class="calibre2">{<br class="calibre4"/>    "name": "mynet",<br class="calibre4"/>    "type": "bridge",<br class="calibre4"/>    "bridge": "mynet0",<br class="calibre4"/>    "isGateway": true,<br class="calibre4"/>    "ipMasq": true,<br class="calibre4"/>    "ipam": {<br class="calibre4"/>        "type": "host-local",<br class="calibre4"/>        "subnet": "10.10.0.0/16"<br class="calibre4"/>    }<br class="calibre4"/>}</tt></p></li><li value="9" class="calibre_31">The following is a sample JSON configuration that uses the Flannel CNI type:<p class="calibre_8"><tt class="calibre2">{<br class="calibre4"/>        "name": "containernet",<br class="calibre4"/>        "type": "flannel"<br class="calibre4"/>}</tt></p></li></ul><p class="calibre_8">The following figure shows you the relationship between Rkt, CNI, the CNI plugin, and IPAM plugin:</p><p class="calibre_9"><img src="images/00022.jpg" class="calibre_306"/></p><p class="calibre_8">
</p><p id="filepos534562" class="calibre_9"><span class="calibre3"><span class="bold">The relationship between Libnetwork and CNI</span></span></p><p class="calibre_8">Libnetwork <a/>and CNI have similar objectives. Docker uses Libnetwork and CoreOS, with Rkt, uses CNI. Libnetwork's overlay driver does something that is similar to CNI's flannel driver. The goal of external plugins such as Weave and Calico is to work with both Libnetwork and CNI.</p><p id="filepos535008" class="calibre_9"><span class="calibre3"><span class="bold">Cloud Native Computing Foundation</span></span></p><p class="calibre_8">The goal of CNCF is to make it easier to build Cloud native applications using Containers. CNCF will create reference architectures using best open source technologies around Containers for microservice based distributed application. The initial goal of CNCF is Container orchestration and the integration work is focused on Kubernetes with Mesos. CNCF will create the reference architecture for microservice development that can help enterprises to build on the reference architecture rather than integrating components by themselves. As per the latest CoreOS blog (<a href="https://coreos.com/blog/making-sense-of-standards.html">https://coreos.com/blog/making-sense-of-standards.html</a>), CoreOS will be donating etcd, flannel, and appc to CNCF.</p><div class="mbp_pagebreak" id="calibre_pb_166"/>


<p id="filepos535925" class="calibre_"><span class="calibre1"><span class="bold">Docker</span></span></p><p class="calibre_8">Even <a/>though Container technology has been available for a long time, Docker has revolutionized the Container technology by making the creation and transportation of Containers very user-friendly. Other than providing Container runtime, Docker provides you with networking, storage, and orchestration solutions for containers. For the majority of these solutions, Docker provides a pluggable model where the Docker native solution is provided, which can be swapped with any other third-party solution. This gives flexibility to the customer to use technologies that they are already comfortable with.</p><p class="calibre_8">In <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>, we covered the Docker architecture. As Docker technology is pretty mature, we will cover only the advanced Docker concepts in this chapter.</p><div class="mbp_pagebreak" id="calibre_pb_167"/>


<p id="filepos536935" class="calibre_9"><span class="calibre3"><span class="bold">The Docker daemon and an external connection</span></span></p><p class="calibre_8">Docker <a/>runs as a daemon and by default listens on the Unix socket, <tt class="calibre2">unix:///var/run/docker.sock</tt>. Docker start options are specified in <tt class="calibre2">/etc/default/docker</tt>.</p><p class="calibre_8">To allow external <a/>Docker clients to talk to the Docker daemon, the following procedure is to be performed in the Ubuntu node:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Add the TCP server with the local address and port number:<p class="calibre_8"><tt class="calibre2"><span class="bold">DOCKER_OPTS="-D -H unix:///var/run/docker.sock -H tcp://192.168.56.101:2376"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Restart the docker daemon:<p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo service docker restart</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="3" class="calibre_31">Now, we can see that the Docker daemon is exposing external connectivity on the IP address <tt class="calibre2">192.168.56.101</tt> and TCP port number <tt class="calibre2">2376</tt>:<p class="calibre_"><img src="images/00026.jpg" class="calibre_36"/></p><p class="calibre_8">
</p></li></ol><p class="calibre_8">We can connect from external Docker clients as follows:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker –H tcp://192.168.56.101:2376 ps</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following image shows that apache container is running:</p><p class="calibre_9"><img src="images/00027.jpg" class="calibre_307"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_168"/>


<p id="filepos538863" class="calibre_9"><span class="calibre3"><span class="bold">Dockerfile</span></span></p><p class="calibre_8">Dockerfile <a/>is used to <a/>create Docker Container images using specified instructions in Dockerfile. Typically, Dockerfile starts with a base container image, installs the necessary applications, and starts the process associated with the container.</p><p class="calibre_8">For Dockerfile best practices, you <a/>can refer to the following link:</p><p class="calibre_8"><a href="https://docs.docker.com/engine/articles/dockerfile_best-practices/">https://docs.docker.com/engine/articles/dockerfile_best-practices/</a>
</p><p class="calibre_8">The following is an example Dockerfile for creating an Apache container from the Ubuntu base image. This Dockerfile installs the Apache package and exposes port <tt class="calibre2">80</tt> to the outside world from the Container:</p><p class="calibre_8"><tt class="calibre2">FROM ubuntu:14.04<br class="calibre4"/>MAINTAINER Sreenivas Makam &lt;sxxxm@yahoo.com&gt;<br class="calibre4"/># Update<br class="calibre4"/>RUN apt-get update<br class="calibre4"/># Install apache2<br class="calibre4"/>RUN apt-get install -y apache2<br class="calibre4"/># Expose necessary ports<br class="calibre4"/>EXPOSE 80<br class="calibre4"/># Start application<br class="calibre4"/>ENTRYPOINT ["/usr/sbin/apache2ctl"]<br class="calibre4"/>CMD ["-D", "FOREGROUND"]</tt></p><p class="calibre_8">To create a Docker image, execute the following command in the directory where the preceding Dockerfile is present. In the example below, <tt class="calibre2">smakam/apache1</tt> is the name of the Container image. The default convention for Container image name is <tt class="calibre2">username/imagename:tag</tt>.</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker build -t smakam/apache1 .</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot shows you the created Apache container image:</p><p class="calibre_9"><img src="images/00030.jpg" class="calibre_36"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_169"/>


<p id="filepos540986" class="calibre_9"><span class="calibre3"><span class="bold">The Docker Image repository</span></span></p><p class="calibre_8">The <a/>Docker<a/> image repository is used to save and restore Docker Container images from a common server location. There are three possible solutions that Docker provides for storing Container images:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Docker hub</span>: This is the Docker image repository service that's hosted by Docker itself at <a href="https://hub.docker.com/">https://hub.docker.com/</a>. This is a free service provided by Docker.</li><a/><li value="2" class="calibre_13"><span class="bold">Docker registry</span>: This is an open source project (<a href="https://github.com/docker/distribution">https://github.com/docker/distribution</a>) that allows customers to host the Docker registry in their own premises. The latest Docker registry is version 2.0. Docker registry 2.0 overcomes some of the shortcomings of Docker registry 1.<span class="italic">x</span> for better security and performance.</li><a/><li value="3" class="calibre_13"><span class="bold">Docker Trusted registry</span>: This is Docker's commercial implementation (<a href="https://www.docker.com/docker-trusted-registry">https://www.docker.com/docker-trusted-registry</a>) of the Docker registry and adds features such as role-based user authentication, integration with an external directory service such as LDAP, GUI-based administrative management, support, and so on. Both the Docker registry and Docker Trusted registry support integration with external storage drivers such as AWS, Azure, and Swift to store Docker images.</li><a/></ul><p class="calibre_8">The following <a/>diagram captures the three Docker image repository types:</p><p class="calibre_9"><img src="images/00031.jpg" class="calibre_308"/></p><p class="calibre_8">
</p><p class="calibre_8">Docker images <a/>have this format:</p><p class="calibre_8"><tt class="calibre2">[REGISTRYHOST/][USERNAME/]NAME[:TAG]</tt>
</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">REGISTRYHOST</tt>: The registry server address</li><li value="2" class="calibre_13"><tt class="calibre2">USERNAME</tt>: The username that created the image</li><li value="3" class="calibre_13"><tt class="calibre2">NAME</tt>: The Container image name</li><li value="4" class="calibre_13"><tt class="calibre2">TAG</tt>: The version of the Container image</li><li value="5" class="calibre_13">Except <tt class="calibre2">NAME</tt>, the other arguments are optional</li></ul><p class="calibre_8">For example, the following command will pull a standard Ubuntu container image from the Docker hub; <tt class="calibre2">registry-1.docker.io/library</tt> is the registry host, the name is <tt class="calibre2">Ubuntu</tt>, and the tag is <tt class="calibre2">latest</tt>:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker pull registry-1.docker.io/library/Ubuntu:latest</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Similar to the Docker registry, CoreOS has the Quay registry (<a href="https://quay.io/">https://quay.io/</a>) to store Docker and Rkt images, and they have a public and enterprise version available.</p><p id="filepos544410" class="calibre_9"><span class="calibre3"><span class="bold">Creating your own Docker registry</span></span></p><p class="calibre_8">It is useful to <a/>create a local registry to share images in a particular company or group. This is important from a security perspective since there is no need to access Internet to access registry. The Docker registry provides you with options for authentication, backend storage drivers (for example, S3, Azure, and Swift), logging, and so on.</p><p class="calibre_8">To start a local registry, use the following command:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker run -d -p 5000:5000 --restart=always --name registry registry:2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following <a/>screenshot shows you the registry running as container. The registry service is exposed on port <tt class="calibre2">5000</tt> in the localhost:</p><p class="calibre_9"><img src="images/00033.jpg" class="calibre_309"/></p><p class="calibre_8">
</p><p class="calibre_8">The registry configuration is specified either as an environment variable as part of starting the registry container or using a YAML file with the configuration and mounting this YAML file to <tt class="calibre2">/etc/config/registry/config.yaml</tt> in the container.</p><p class="calibre_8">The following set of commands pulls a busybox container from the Docker hub, pushes the busybox container in the local registry, and then pulls it out from the local registry:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker pull busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker tag busybox localhost:5000/mybusybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker push localhost:5000/mybusybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker pull localhost:5000/mybusybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot shows you the <tt class="calibre2">mybusybox</tt> container that has been pulled from the local registry:</p><p class="calibre_9"><img src="images/00034.jpg" class="calibre_260"/></p><p class="calibre_8">
</p><p class="calibre_8">The following screenshot shows you the instantiation of the <tt class="calibre2">mybusybox</tt> container from the local registry:</p><p class="calibre_9"><img src="images/00035.jpg" class="calibre_310"/></p><p class="calibre_8">
</p><p id="filepos547000" class="calibre_9"><span class="calibre3"><span class="bold">Continuous integration</span></span></p><p class="calibre_8">When <a/>we push Docker images to the Docker Hub, Dockerfile does not get pushed. To push the Dockerfile and use it for automated Container builds, we need to link it with a repository management tool such as GitHub or Bitbucket. The steps are as follows:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Get an account in GitHub or Bitbucket.</li><li value="2" class="calibre_13">From Docker Hub, we can link either to GitHub or Bitbucket.</li><li value="3" class="calibre_13">Push the Dockerfile to GitHub.</li><li value="4" class="calibre_13">From DockerHub, when we create a repository, select automated build, and select the location from GitHub where Dockerfile is present. This will build the image automatically. Additionally, when there are changes to Dockerfile committed to GitHub, automatic builds are triggered.</li></ol><p class="calibre_8">The following diagram shows you the CI sequence using Dockerfile, from staging to production:</p><p class="calibre_9"><img src="images/00038.jpg" class="calibre_311"/></p><p class="calibre_8">
</p><p class="calibre_8">The following screenshot shows the automated build creation in Docker Hub after creating Dockerfile in GitHub. In the following example, Dockerfile is present in <a href="https://github.com/smakam/docker.git">https://github.com/smakam/docker.git</a> under the Apache directory:</p><p class="calibre_9"><img src="images/00039.jpg" class="calibre_312"/></p><p class="calibre_8">
</p><p class="calibre_8">When any <a/>changes are made, an automatic container image is built. The following screenshot shows the successful container image build log for <tt class="calibre2">smakam/apacheauto</tt>:</p><p class="calibre_9"><img src="images/00041.jpg" class="calibre_78"/></p><p class="calibre_8">
</p><p class="calibre_8">The following screenshot shows a successful pull of the <tt class="calibre2">smakam/apacheauto</tt> container image:</p><p class="calibre_9"><img src="images/00043.jpg" class="calibre_313"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_170"/>


<p id="filepos549591" class="calibre_9"><span class="calibre3"><span class="bold">The Docker content trust</span></span></p><p class="calibre_8">The Docker content trust <a/>provides you with a mechanism to sign and publish Docker images so that the client who pulls the image can be guaranteed that the image is from a trusted source and has not been modified by any man-in-the-middle attack. The <a/>following are some features of the Docker content trust:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Docker content trust is an implementation of the Notary open source project (<a href="https://github.com/docker/notary">https://github.com/docker/notary</a>). The Notary open source project is based on <span class="bold">The Update Framework</span> (<span class="bold">TUF</span>) project (<a href="https://theupdateframework.github.io/">https://theupdateframework.github.io/</a>). TUF provides a mechanism to secure software updates.</li><a/><li value="2" class="calibre_13">Compared to the GPG approach of signing keys, TUF has some unique differentiators. TUF takes care of the freshness of keys so that the client always knows that they are getting the latest content. Key compromise can be handled better with TUF using the key rotation scheme, which clients need not be aware of. TUF also provides you with the capability of signing collections rather than individual software.</li><li value="3" class="calibre_13">There are four keys with Notary—the Timestamp key to maintain the freshness of the image, the Snapshot key to sign image collections, the Target key for the regular signing of images, and the Offline key for key rotation.</li><li value="4" class="calibre_13">The Docker content trust has been released with Docker version 1.8. The default option is trust-disabled and can be enabled using the <tt class="calibre2">DOCKER_CONTENT_TRUST</tt> environment variable. At some later point, the default option would be to keep the trust enabled.</li></ul><p class="calibre_8">The following figure shows you the relationship between TUF, Notary, and the Docker content trust:</p><p class="calibre_9"><img src="images/00044.jpg" class="calibre_314"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the workflow<a/> with the Docker content trust:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Docker registry needs to support the Docker content trust. The Docker Hub supports the content trust. The Docker trusted registry and private registry do not yet support the content trust; this will be added soon.</li><li value="2" class="calibre_13">The usual Docker commands can be used for push and pull, and care has been taken not to change Docker commands. For advanced key management, the Notary CLI can be used.</li><li value="3" class="calibre_13">When the publisher pushes the image for the first time using <tt class="calibre2">docker push</tt>, there is a need to enter a passphrase for the root key and tagging key. All other keys are generated automatically. These keys need to be stored safely.</li><li value="4" class="calibre_13">For any further image publishing, only the tagging key is necessary.</li><li value="5" class="calibre_13">The client has the option to pull signed or unsigned images. With the Docker trust enabled, the client will get an error if they try to pull unsigned images.</li></ul><p id="filepos553241" class="calibre_14"><span class="calibre3"><span class="bold">Pushing secure image</span></span></p><p class="calibre_8">First, we <a/>enable the Docker content trust using the <tt class="calibre2">DOCKER_CONTENT_TRUST</tt> environment variable. The following is the output when the Docker content trust is enabled and we are publishing the image for the first time. Here, we <a/>are pushing the signed <tt class="calibre2">smakam/mybusybox:v1</tt> container:</p><p class="calibre_9"><img src="images/00045.jpg" class="calibre_315"/></p><p class="calibre_8">
</p><p id="filepos553852" class="calibre_9"><span class="calibre3"><span class="bold">Pulling secure image</span></span></p><p class="calibre_8">The following is<a/> the output when we are pulling the same secure image, <tt class="calibre2">smakam/mybusybox:v1</tt>, from the Docker hub:</p><p class="calibre_9"><img src="images/00048.jpg" class="calibre_316"/></p><p class="calibre_8">
</p><p id="filepos554281" class="calibre_9"><span class="calibre3"><span class="bold">Pulling same image with no security </span></span><a/></p><p class="calibre_8">The following is the output when we try to pull the same image, <tt class="calibre2">smakam/mybusybox:v1</tt>, with no Docker content trust. In this case, image verification is not done:</p><p class="calibre_9"><img src="images/00001.jpg" class="calibre_317"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the error message that we will get if we enable the trust and try to pull Docker images that are not signed. As <tt class="calibre2">smakam/hellocounter</tt> is not signed and we have <tt class="calibre2">DOCKER_CONTENT_TRUST</tt> enabled, we get an error:</p><p class="calibre_9"><img src="images/00003.jpg" class="calibre_318"/></p><p class="calibre_8">
</p><p class="calibre_8">Recently, Docker has <a/>enabled the content trust using hardware keys (<a href="https://blog.docker.com/2015/11/docker-content-trust-yubikey/">https://blog.docker.com/2015/11/docker-content-trust-yubikey/</a>). This is currently in the experimental mode.</p><div class="mbp_pagebreak" id="calibre_pb_171"/>


<p id="filepos555553" class="calibre_9"><span class="calibre3"><span class="bold">Container debugging</span></span></p><p class="calibre_8">The following <a/>are some basic Container debugging approaches.</p><p id="filepos555758" class="calibre_9"><span class="calibre3"><span class="bold">Logs</span></span></p><p class="calibre_8">The <a/>following command will show container logs. This can be a useful debugging tool. In <a href="index_split_219.html#filepos708963">Chapter 10</a>, <span class="italic">CoreOS and Containers - Troubleshooting and Debugging</span>, you will learn how to aggregate and analyze Container logs from a central location.</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker logs &lt;containername or id&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos556281" class="calibre_9"><span class="calibre3"><span class="bold">Login inside Container</span></span></p><p class="calibre_8">The <tt class="calibre2">docker exec</tt> command can <a/>be used to log in to the container. The following is an example:</p><p class="calibre_9"><img src="images/00006.jpg" class="calibre_319"/></p><p class="calibre_8">
</p><p class="calibre_8">Common Linux commands can be executed from the Container shell.</p><p id="filepos556798" class="calibre_9"><span class="calibre3"><span class="bold">Container properties</span></span></p><p class="calibre_8">The following<a/> command will show container properties such as mount points, resource limits, and so on:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker inspect &lt;containername or id&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos557171" class="calibre_9"><span class="calibre3"><span class="bold">Container processes</span></span></p><p class="calibre_8">The <a/>following command will show processes running in the container sorted by the process CPU usage:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker top &lt;containername or id&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is a sample output for a <tt class="calibre2">redis</tt> container:</p><p class="calibre_9"><img src="images/00009.jpg" class="calibre_320"/></p><p class="calibre_8">
</p><p id="filepos557806" class="calibre_9"><span class="calibre3"><span class="bold">The Container's CPU and memory usage</span></span></p><p class="calibre_8">The following <a/>command will show the resource usage of a Container:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">docker stats &lt;containername or id&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is a sample output for the Apache container:</p><p class="calibre_9"><img src="images/00013.jpg" class="calibre_224"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_172"/>


<p id="filepos558437" class="calibre_"><span class="calibre1"><span class="bold">Rkt</span></span></p><p class="calibre_8">Rkt is the <a/>Container runtime from CoreOS based on the APPC specification. The following are some differences in Rkt compared to Docker:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Rkt is daemonless. The problem of Containers going away if the Docker daemon restarts does not exist with Rkt.</li><li value="2" class="calibre_13">Rkt integrates well with systemd so that container resource limits can be set easily for the Containers.</li></ul><p class="calibre_8">There are three <a/>stages in the Rkt execution:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">Stage0</tt>: This does the image discovery and retrieval and sets up a filesystem for stages 1 and 2.</li><li value="2" class="calibre_13"><tt class="calibre2">Stage1</tt>: This sets up the execution environment for the container execution using the filesystem set up by <tt class="calibre2">stage0</tt>. Rkt uses systemd-nspawn to set up cgroups, networking, and so on in this stage. The goal here is to keep <tt class="calibre2">stage1</tt> swappable by other implementations.</li><li value="3" class="calibre_13"><tt class="calibre2">Stage2</tt>: This is the actual execution of the Container pod and application itself using the execution environment set up by <tt class="calibre2">stage1</tt> and filesystem set up by <tt class="calibre2">stage0</tt>.</li></ul><p class="calibre_8">The following <a/>example illustrates the stages. Let's start the <tt class="calibre2">hello</tt> ACI image using Rkt:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt --insecure-skip-verify run hello-0.0.1-linux-amd64.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00015.jpg" class="calibre_199"/></p><p class="calibre_8">
</p><p class="calibre_8">The following shows the <tt class="calibre2">stage1</tt> filesystem setup by <tt class="calibre2">stage0</tt>:</p><p class="calibre_9"><img src="images/00018.jpg" class="calibre_321"/></p><p class="calibre_8">
</p><p class="calibre_8">The manifest here shows the Rkt <tt class="calibre2">stage1</tt> ACI that sets up the container environment:</p><p class="calibre_9"><img src="images/00021.jpg" class="calibre_322"/></p><p class="calibre_8">
</p><p class="calibre_8">The following shows the <tt class="calibre2">stage2</tt> filesystem:</p><p class="calibre_9"><img src="images/00025.jpg" class="calibre_36"/></p><p class="calibre_8">
</p><p class="calibre_8">The manifest here shows the hello Rkt container image:</p><p class="calibre_9"><img src="images/00002.jpg" class="calibre_323"/></p><p class="calibre_8">
</p><p class="calibre_8">The following shows the filesystem for the hello application:</p><p class="calibre_9"><img src="images/00004.jpg" class="calibre_204"/></p><p class="calibre_8">
</p><p class="calibre_8">Rkt application is available in the CoreOS base image. Rkt can also be installed in any Linux system using the procedure described at <a href="https://github.com/coreos/rkt">https://github.com/coreos/rkt</a>. The following is the Rkt version running in the Ubuntu 14.04 system:</p><p class="calibre_9"><img src="images/00007.jpg" class="calibre_324"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the Rkt and APPC version used in the CoreOS alpha image 815.0.0:</p><p class="calibre_9"><img src="images/00010.jpg" class="calibre_325"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_173"/>


<p id="filepos562627" class="calibre_9"><span class="calibre3"><span class="bold">Basic commands</span></span></p><p class="calibre_8">The <a/>following <a/>are some basic commands to manipulate Rkt Containers.</p><p id="filepos562841" class="calibre_9"><span class="calibre3"><span class="bold">Fetch image</span></span></p><p class="calibre_8">The following <a/>command fetches a Container image from the repository in the ACI format:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt --insecure-skip-verify fetch docker://busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00014.jpg" class="calibre_326"/></p><p class="calibre_8">
</p><p id="filepos563381" class="calibre_9"><span class="calibre3"><span class="bold">List images</span></span></p><p class="calibre_8">The <a/>following command lists Rkt Container images:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt image list</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00016.jpg" class="calibre_41"/></p><p class="calibre_8">
</p><p id="filepos563847" class="calibre_9"><span class="calibre3"><span class="bold">Run image</span></span></p><p class="calibre_8">The following <a/>command runs the Rkt Container image:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt run --insecure-skip-verify --interactive docker://busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">By default, signature verification is turned on; we disable signature verification using the <tt class="calibre2">skip-verify</tt> option:</p><p class="calibre_9"><img src="images/00019.jpg" class="calibre_327"/></p><p class="calibre_8">
</p><p id="filepos564529" class="calibre_9"><span class="calibre3"><span class="bold">List pods</span></span></p><p class="calibre_8">The <a/>following command lists the running pods:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt list pods</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00422.jpg" class="calibre_328"/></p><p class="calibre_8">
</p><p id="filepos564988" class="calibre_9"><span class="calibre3"><span class="bold">Garbage collection</span></span></p><p class="calibre_8">The <a/>following screenshot shows two pods that have exited:</p><p class="calibre_9"><img src="images/00423.jpg" class="calibre_329"/></p><p class="calibre_8">
</p><p class="calibre_8">The exited Containers will be garbage collected periodically. To force garbage collection, we can perform the following command:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">rkt gc --grace-period=0</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00426.jpg" class="calibre_330"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can see that there are no active pods:</p><p class="calibre_9"><img src="images/00427.jpg" class="calibre_331"/></p><p class="calibre_8">
</p><p id="filepos566058" class="calibre_9"><span class="calibre3"><span class="bold">Delete image</span></span></p><p class="calibre_8">The following <a/>command deletes the local Container image:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt image rm sha512-cf74c26d8d35555066dce70bd94f513b90cbef6e7e9c01ea0c971f4f6d689848</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot shows the deletion of the busybox image using UUID:</p><p class="calibre_9"><img src="images/00429.jpg" class="calibre_41"/></p><p class="calibre_8">
</p><p id="filepos566724" class="calibre_9"><span class="calibre3"><span class="bold">Export image</span></span></p><p class="calibre_8">The following <a/>command converts a Docker image to the ACI format:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt image export nginx nginx.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00431.jpg" class="calibre_332"/></p><p class="calibre_8">
</p><p id="filepos567225" class="calibre_9"><span class="calibre3"><span class="bold">The nginx container with volume mounting and port forwarding</span></span></p><p class="calibre_8">The <a/>following command starts the nginx container forwarding the container port <tt class="calibre2">80</tt> to the host port <tt class="calibre2">8080</tt> and <a/>setting up the host volume. The volume directory and port name are as specified in the manifest file:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt run --insecure-skip-verify --private-net --port=80-tcp:8080 --volume volume-var-cache-nginx,kind=host,source=/home/core docker://nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00433.jpg" class="calibre_145"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>screenshot shows<a/> successful web page access using the nginx container and host port <tt class="calibre2">8080</tt>:</p><p class="calibre_9"><img src="images/00434.jpg" class="calibre_333"/></p><p class="calibre_8">
</p><p id="filepos568386" class="calibre_9"><span class="calibre3"><span class="bold">Pod status</span></span></p><p class="calibre_8">The following command lists the status of a particular Pod using UUID:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt status 2b165196</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00436.jpg" class="calibre_334"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_174"/>


<p id="filepos568901" class="calibre_9"><span class="calibre3"><span class="bold">Rkt image signing</span></span></p><p class="calibre_8">Container <a/>image signing allows us to verify that the image is coming from a trusted location and has not been tampered with. I used the procedure at <a href="https://github.com/coreos/rkt/blob/master/Documentation/signing-and-verification-guide.md">https://github.com/coreos/rkt/blob/master/Documentation/signing-and-verification-guide.md</a> to sign the ACI image and use Rkt to run the signed image.</p><p class="calibre_8">The following is a sample <tt class="calibre2">nginx.service</tt> systemd unit file:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Generate keys:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">gpg --batch --gen-key gpg-batch</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">In case you get this error message, <tt class="calibre2">Not enough random bytes available. Please do some other work to give the OS a chance to collect more entropy!</tt>, it can be solved by the following rngd tool that can be run in parallel:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">apt-get install rng-tools</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rngd -r /dev/urandom</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Trust the keys:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">gpg --no-default-keyring --secret-keyring ./rkt.sec --keyring ./rkt.pub --edit-key 1FEEF0ED trust</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="3" class="calibre_31">Export the public key:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">gpg --no-default-keyring --armor \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--secret-keyring ./rkt.sec --keyring ./rkt.pub \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--export &lt;email&gt; &gt; pubkeys.gpg</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="4" class="calibre_31">Sign the image using the public key:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">gpg --no-default-keyring --armor \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--secret-keyring ./rkt.sec --keyring ./rkt.pub \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--output hello-0.0.1-linux-amd64.aci.asc \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--detach-sig hello-0.0.1-linux-amd64.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="5" class="calibre_31">Host the web server with the ACI image, public key, and signature. The following are the contents in my web server location:<p class="calibre_"><img src="images/00437.jpg" class="calibre_76"/></p><p class="calibre_8">
</p></li><li value="6" class="calibre_13">The following is the <tt class="calibre2">index.html</tt> content:<p class="calibre_8"><tt class="calibre2">&lt;head&gt;<br class="calibre4"/>&lt;meta name="ac-discovery" content="example.com/hello http://example.com/hello-0.0.1-linux-amd64.aci"&gt;<br class="calibre4"/>&lt;meta name="ac-discovery-pubkeys" content="example.com/hello http://example.com/pubkeys.gpg"&gt;<br class="calibre4"/>&lt;/head&gt;</tt></p></li><li value="7" class="calibre_31">Trust the web server location and key:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt trust --prefix=example.com/hello http://example.com/pubkeys.gpg --insecure-allow-http</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="8" class="calibre_31">Modify <tt class="calibre2">/etc/hosts</tt> to point <tt class="calibre2">example.com</tt> to localhost.</li><li value="9" class="calibre_13">Start a simple web server:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo python -m SimpleHTTPServer 80</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><p class="calibre_8">Now, we <a/>can run the Rkt image with signature verification:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo rkt run --debug http://example.com/hello-0.0.1-linux-amd64.aci</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot shows the signature being verified. The signature location and public key are provided by the hosted web server at <tt class="calibre2">example.com</tt>:</p><p class="calibre_9"><img src="images/00439.jpg" class="calibre_48"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_175"/>


<p id="filepos573546" class="calibre_9"><span class="calibre3"><span class="bold">Rkt with systemd</span></span></p><p class="calibre_8">Systemd <a/>provides you with a lot of control over how processes are managed. Rkt pods can be managed by systemd. With systemd, we can control the process execution order, restartability, resource limit, and so on.</p><p class="calibre_8">The following is a sample <tt class="calibre2">nginx.service</tt> systemd unit file:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=nginx<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/># Resource limits<br class="calibre4"/>CPUShares=512<br class="calibre4"/>MemoryLimit=1G<br class="calibre4"/># Prefetch the image<br class="calibre4"/>ExecStartPre=/usr/bin/rkt fetch --insecure-skip-verify docker://nginx<br class="calibre4"/>ExecStart=/usr/bin/rkt run --insecure-skip-verify --private-net --port=80-tcp:8080 --volume volume-var-cache-nginx,kind=host,source=/home/co<br class="calibre4"/>re docker://nginx<br class="calibre4"/>KillMode=mixed<br class="calibre4"/>Restart=always</tt></p><p class="calibre_8">In the <a/>preceding service file, we started the nginx container and also limited the CPU and memory usage for this <tt class="calibre2">nginx.service</tt> using the systemd construct.</p><p class="calibre_8">To start the service, it's necessary to place <tt class="calibre2">nginx.service</tt> in <tt class="calibre2">/etc/systemd/system</tt>. The service can be started as follows:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Sudo systemctl start nginx.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot shows you the status of <tt class="calibre2">nginx.service</tt>:</p><p class="calibre_9"><img src="images/00442.jpg" class="calibre_335"/></p><p class="calibre_8">
</p><p class="calibre_8">To show the power of integration with systemd, let's kill the Rkt nginx process and demonstrate restartability:</p><p class="calibre_9"><img src="images/00444.jpg" class="calibre_39"/></p><p class="calibre_8">
</p><p class="calibre_8">Systemd will restart the nginx container because <tt class="calibre2">restart</tt> is turned on in <tt class="calibre2">nginx.service</tt>.</p><p class="calibre_8">From the<a/> following <tt class="calibre2">journalctl</tt> logs on <tt class="calibre2">nginx.service</tt>, we can see that the service has been restarted:</p><p class="calibre_9"><img src="images/00445.jpg" class="calibre_336"/></p><p class="calibre_8">
</p><p class="calibre_8">In the following screenshot, we can see that the Rkt nginx process is running with a different PID:</p><p class="calibre_9"><img src="images/00448.jpg" class="calibre_111"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_176"/>


<p id="filepos576487" class="calibre_9"><span class="calibre3"><span class="bold">Rkt with Flannel</span></span></p><p class="calibre_8">Rkt uses <a/>the CNI interface to talk to the Flannel plugin to establish container networking across hosts.</p><p class="calibre_8">The following example sets up a three-node CoreOS cluster using Rkt and Flannel for Container networking. The following is the necessary <tt class="calibre2">cloud-config</tt>:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/><br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    #generate a new token for each unique cluster from https://discovery.etcd.io/new<br class="calibre4"/>    discovery: &lt;your token&gt;<br class="calibre4"/>    # multi-region and multi-cloud deployments need to use $public_ipv4<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    # legacy ports can be omitted if your application doesn't depend on them<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "network": "10.1.0.0/16" }'<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: docker-tcp.socket<br class="calibre4"/>      command: start<br class="calibre4"/>      enable: true<br class="calibre4"/>      content: |<br class="calibre4"/>        [Unit]<br class="calibre4"/>        Description=Docker Socket for the API<br class="calibre4"/><br class="calibre4"/>        [Socket]<br class="calibre4"/>        ListenStream=2375<br class="calibre4"/>        Service=docker.service<br class="calibre4"/>        BindIPv6Only=both<br class="calibre4"/><br class="calibre4"/>        [Install]<br class="calibre4"/>        WantedBy=sockets.target<br class="calibre4"/>write_files:<br class="calibre4"/>  - path: "/etc/rkt/net.d/10-containernet.conf"<br class="calibre4"/>    permissions: "0644"<br class="calibre4"/>    owner: "root"<br class="calibre4"/>    content: |<br class="calibre4"/>      {<br class="calibre4"/>        "name": "containernet",<br class="calibre4"/>        "type": "flannel"<br class="calibre4"/>      }</tt></p><p class="calibre_8">The <tt class="calibre2">/etc/rkt/net.d/10-containernet.conf</tt> file sets up the CNI plugin type as Flannel.</p><p class="calibre_8">Flannel gets <a/>an individual subnet for each host using the IP range specified in the flannel configuration <tt class="calibre2">10.1.0.0/16</tt>.</p><p class="calibre_8">The following output shows you the subnet allocated in <tt class="calibre2">node1</tt> and <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00449.jpg" class="calibre_337"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00451.jpg" class="calibre_338"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's create Rkt containers in each node and check inter-container connectivity:</p><p class="calibre_9"><img src="images/00452.jpg" class="calibre_327"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows you that the busybox container in <tt class="calibre2">core-01</tt> got the IP, <tt class="calibre2">10.1.74.4</tt>, which is in the <tt class="calibre2">10.1.74.1/24</tt> range allocated for <tt class="calibre2">core-01</tt>:</p><p class="calibre_9"><img src="images/00454.jpg" class="calibre_339"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>output shows you that the busybox container in <tt class="calibre2">core-03</tt> got the IP, <tt class="calibre2">10.1.3.2</tt>, which is in the <tt class="calibre2">10.1.3.1/24</tt> range allocated for <tt class="calibre2">core-03</tt>:</p><p class="calibre_9"><img src="images/00455.jpg" class="calibre_339"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows you a successful ping from container 1 on <tt class="calibre2">core-01</tt> to container 2 on <tt class="calibre2">core-03</tt>:</p><p class="calibre_9"><img src="images/00457.jpg" class="calibre_340"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_177"/>


<p id="filepos581460" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered different Container standards for Container runtime, networking, and orchestration. Having these standards is important from the industry perspective for interoperability reasons. Container runtime systems like Docker and Rkt were covered in detail. For Docker, the focus was on advanced concepts, and for Rkt, we covered the basics as Rkt is still in the early stages. Even though CoreOS is actively developing Rkt, CoreOS is committed to supporting Docker in its OS. It will be interesting to see how Docker and Rkt run together in CoreOS and how customers adopt the two Container runtime technologies. In the next chapter, we will cover Container orchestration.</p><div class="mbp_pagebreak" id="calibre_pb_178"/>


<p id="filepos582294" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Notary GitHub: <a href="https://github.com/docker/notary">https://github.com/docker/notary</a></li><li value="2" class="calibre_13">Docker registry: <a href="https://github.com/docker/distribution">https://github.com/docker/distribution</a></li><li value="3" class="calibre_13">Docker content trust documentation: <a href="https://docs.docker.com/security/trust/content_trust/">https://docs.docker.com/security/trust/content_trust/</a></li><li value="4" class="calibre_13">Docker content trust blog: <a href="https://blog.docker.com/2015/08/content-trust-docker-1-8/">https://blog.docker.com/2015/08/content-trust-docker-1-8/</a></li><li value="5" class="calibre_13">The Update framework: <a href="http://theupdateframework.com/">http://theupdateframework.com/</a></li><li value="6" class="calibre_13">Cloud native compute foundation: <a href="https://cncf.io">https://cncf.io</a></li><li value="7" class="calibre_13">Open container initiative: <a href="https://github.com/opencontainers">https://github.com/opencontainers</a></li><li value="8" class="calibre_13">APPC specification: <a href="https://github.com/appc">https://github.com/appc</a></li><li value="9" class="calibre_13">Libnetwork: <a href="https://github.com/docker/libnetwork">https://github.com/docker/libnetwork</a></li><li value="10" class="calibre_13">Docker2aci: <a href="https://github.com/appc/docker2aci">https://github.com/appc/docker2aci</a></li><li value="11" class="calibre_13">CoreOS Rkt documentation: <a href="https://coreos.com/rkt/docs/latest/">https://coreos.com/rkt/docs/latest/</a></li><li value="12" class="calibre_13">Acbuild: <a href="https://github.com/appc/acbuild">https://github.com/appc/acbuild</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_179"/>


<p id="filepos584409" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The CNI presentation: <a href="https://www.youtube.com/watch?v=_-9kItVUUCw">https://www.youtube.com/watch?v=_-9kItVUUCw</a></li><li value="2" class="calibre_13">Docker registry presentations: <a href="https://www.youtube.com/watch?v=RnO9JnEO8tY">https://www.youtube.com/watch?v=RnO9JnEO8tY</a> and <a href="https://www.youtube.com/watch?v=cVsUhoJFPvQ">https://www.youtube.com/watch?v=cVsUhoJFPvQ</a></li><li value="3" class="calibre_13">The Docker Notary presentation: <a href="https://www.youtube.com/watch?v=JvjdfQC8jxM">https://www.youtube.com/watch?v=JvjdfQC8jxM</a></li><li value="4" class="calibre_13">The Container standards presentation: <a href="http://containersummit.io/events/sf-2015/videos/container-ecosystem-standards-needs-and-progress">http://containersummit.io/events/sf-2015/videos/container-ecosystem-standards-needs-and-progress</a></li><li value="5" class="calibre_13">The Rkt and APPC presentation: <a href="https://www.youtube.com/watch?v=C8Qpdrpm16Y">https://www.youtube.com/watch?v=C8Qpdrpm16Y</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_180"/>
</body></html>