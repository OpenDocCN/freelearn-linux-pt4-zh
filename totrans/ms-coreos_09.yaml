- en: Chapter 9. OpenStack Integration with Containers and CoreOS
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack is an open source cloud operating system for managing public and private
    clouds. It is a pretty mature technology that is supported by the majority of
    the vendors and is used in a wide variety of production deployments. Running CoreOS
    in the OpenStack environment will give OpenStack users a Container-based Micro
    OS to deploy their distributed applications. Having Container orchestration integrated
    with OpenStack gives OpenStack users a single management solution to manage VMs
    and Containers. There are multiple projects ongoing in OpenStack currently to
    integrate Container management and Container networking with OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running CoreOS in OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Options to run Containers in OpenStack—the Nova Docker driver, Heat Docker plugin,
    and Magnum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container networking using OpenStack Kuryr and Neutron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: Just like an OS for a desktop or server manages the resources associated with
    it, a cloud OS manages the resources associated with the cloud. Major cloud resources
    are compute, storage, and network. Compute includes servers and hypervisors associated
    with the servers that allows VM creation. Storage includes the local storage,
    Storage Area Network (SAN), and object storage.
  prefs: []
  type: TYPE_NORMAL
- en: Network includes vlans, firewalls, load balancers, and routers. A cloud OS is
    also responsible for other infrastructure-related items such as image management,
    authentication, security, billing, and so on. A cloud OS also provides some automated
    characteristics such as elasticity, a self service provisioning model, and others.
    Currently, the most popular open source cloud OS in the market is OpenStack. OpenStack
    has a lot of momentum going for it along with a great industry backing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some key OpenStack services:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nova: Compute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swift: Object storage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cinder: Block storage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neutron: Networking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Glance: Image management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keystone: Authentication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heat: Orchestration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceilometer: Metering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horizon: Web interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack can be downloaded from [https://wiki.openstack.org/wiki/Get_OpenStack](https://wiki.openstack.org/wiki/Get_OpenStack).
    It is pretty complex to install OpenStack as there are multiple components involved.
    Similar to Linux distributions provided by Linux vendors, there are multiple vendors
    offering OpenStack distributions. The best way to try out OpenStack is using Devstack
    ([http://devstack.org/](http://devstack.org/)). Devstack offers a scripted approach
    to install and can be installed on a laptop or VM. Devstack can be used to create
    a single-node cluster or multi-node cluster.
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS on OpenStack
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS can be run as a VM on OpenStack. CoreOS OpenStack images are available
    for alpha, beta, and stable versions.
  prefs: []
  type: TYPE_NORMAL
- en: Here, I have described the procedure to install CoreOS on OpenStack running
    in the Devstack environment. The procedure is based on the CoreOS OpenStack documentation
    ([https://coreos.com/os/docs/latest/booting-on-openstack.html](https://coreos.com/os/docs/latest/booting-on-openstack.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get OpenStack Kilo running in Devstack. In my case, I installed Devstack in
    the Ubuntu 14.04 VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the keys for authentication and a security group for SSH access.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up external network access and DNS for the VM. This is necessary as the
    CoreOS nodes need to discover each other using the token service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the appropriate CoreOS image and upload to OpenStack using the Glance
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get a discovery token and update it in the user data configuration file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start CoreOS instances using custom user data specifying necessary services
    to be started and the number of instances to be started.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get OpenStack Kilo running in Devstack
  prefs: []
  type: TYPE_NORMAL
- en: 'The following blog covers the procedure in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://sreeninet.wordpress.com/2015/02/21/openstack-juno-install-using-devstack/](https://sreeninet.wordpress.com/2015/02/21/openstack-juno-install-using-devstack/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the `local.conf` file that I used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[[local|localrc]] DEST=/opt/stack  # Logging LOGFILE=$DEST/logs/stack.sh.log
    VERBOSE=True SCREEN_LOGDIR=$DEST/logs/screen OFFLINE=True  # HOST #EDITME HOST_IP=<EDITME>  # Networking
    FIXED_RANGE=10.0.0.0/24 disable_service n-net enable_service q-svc enable_service q-agt
    enable_service q-dhcp enable_service q-meta enable_service q-l3 #ml2 Q_PLUGIN=ml2
    Q_AGENT=openvswitch # vxlan Q_ML2_TENANT_NETWORK_TYPE=vxlan  # Credentials ADMIN_PASSWORD=openstack
    MYSQL_PASSWORD=openstack RABBIT_PASSWORD=openstack SERVICE_PASSWORD=openstack
    SERVICE_TOKEN=tokentoken  #scheduler enable_service n-sch SCHEDULER=nova.scheduler.chance.ChanceScheduler  #vnc
    enable_service n-novnc enable_service n-cauth`'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up keys and a security group
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the commands that I used to create a keypair and to expose
    port SSH and ICMP port of the VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nova keypair-add heattest > ~/Downloads/heattest.pem``nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0``nova secgroup-add-rule default tcp 1 65535 0.0.0.0/0`'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up external network access
  prefs: []
  type: TYPE_NORMAL
- en: 'The first command sets up the NAT rule for VM external access and the second
    command sets up a DNS server:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE``neutron subnet-update  <subnet> --dns-nameservers list=true <dns address>`'
  prefs: []
  type: TYPE_NORMAL
- en: (Find `<subnet>` using `nova subnet-list` and `<dns address>` from the running
    host machine).
  prefs: []
  type: TYPE_NORMAL
- en: Download the CoreOS image and upload to Glance
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command is used to download the latest alpha image and upload
    to OpenStack glance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`wget http://alpha.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2``bunzip2 coreos_production_openstack_image.img.bz2``glance image-create --name CoreOS \``  --container-format bare \``  --disk-format qcow2 \``  --file coreos_production_openstack_image.img \``  --is-public True`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `glance image-list` output and we can see the CoreOS image
    uploaded to Glance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00231.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Updating the user data to be used for CoreOS
  prefs: []
  type: TYPE_NORMAL
- en: I had some issues using the default user data to start CoreOS because there
    were issues with CoreOS determining the system IP. I raised a case ([https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4](https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4))
    and the CoreOS team provided a sample user data where IP addresses are determined
    using a script inside the user data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the user data that I used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config  write_files:   - path: /tmp/ip.sh     permissions: 0755     content: |
          #!/bin/sh       get_ipv4() {           IFACE="${1}"            local ip
              while [ -z "${ip}" ]; do               ip=$(ip -4 -o addr show dev "${IFACE}" scope global | gawk ''{split ($4, out, "/"); print out[1]}'')
                  sleep .1           done            echo "${ip}"       }       echo "IPV4_PUBLIC=$(get_ipv4 eth0)" > /run/metadata
          echo "IPV4_PRIVATE=$(get_ipv4 eth0)" >> /run/metadata  coreos:   units:
        - name: populate-ips.service       command: start       runtime: true       content: |
            [Service]         Type=oneshot         ExecStart=/tmp/ip.sh     - name: etcd2.service
          command: start       runtime: true       drop-ins:         - name: custom.conf
              content: |             [Unit]             Requires=populate-ips.service
                After=populate-ips.service              [Service]             EnvironmentFile=/run/metadata
                ExecStart=             ExecStart=/usr/bin/etcd2 --initial-advertise-peer-urls=http://${IPV4_PRIVATE}:2380 --listen-peer-urls=http://${IPV4_PRIVATE}:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://${IPV4_PUBLIC}:2379 --discovery=https://discovery.etcd.io/0cbf57ced1c56ac028af8ce7e32264ba
        - name: fleet.service       command: start`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding user data does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `populate-ips.service` unit file is used to update the IP address. It reads
    the IP manually and updates `/run/metadata` with the IP address.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discovery token is updated so that nodes can discover each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd2 service is started using the IP address set in `/run/metadata`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet service is started using fleet unit file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following command is used to start two CoreOS instances using the preceding
    user data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nova boot \``--user-data ./user-data1.yaml \``--image 8ae5223c-1742-47bf-9bb3-873374e61a64 \``--key-name heattest \``--flavor m1.coreos \``--num-instances 2 \``--security-groups default coreos`'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: For the CoreOS instance, I have used a custom flavor `m1.coreos` with
    1 vcpu, 2 GB memory, and 10 GB hard disk. If these resource requirements are not
    met, instance creation will fail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the list of VMs. We can see the two CoreOS instances in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00232.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command shows the CoreOS version running in OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00234.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command shows the etcd member list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command shows the fleet machines showing the two CoreOS nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00237.jpg)'
  prefs: []
  type: TYPE_IMG
- en: OpenStack and Containers
  prefs: []
  type: TYPE_NORMAL
- en: Even though OpenStack has supported VMs and baremetal for quite some time, Containers
    are pretty new to OpenStack. The initial focus in OpenStack was to extend VM Orchestration
    to also manage Containers. The Nova Docker driver and Heat Docker plugin are examples
    of this. This was not widely adopted as some of the Container functionality was
    missing in this approach. The OpenStack Magnum project addresses some of the limitations
    and manages Containers as a first-class citizen like a VM.
  prefs: []
  type: TYPE_NORMAL
- en: The Nova Docker driver
  prefs: []
  type: TYPE_NORMAL
- en: Nova typically manages VMs. In this approach, the Nova driver is extended to
    spawn Docker Containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram describes the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00238.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Nova is configured to use the Nova Docker driver for Containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Nova Docker driver talks to the Docker daemon using the REST API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker images are imported to Glance and the Nova Docker driver uses these images
    to spawn Containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Nova Docker driver is not present in the mainstream OpenStack installation
    and has to be installed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Nova Driver
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will cover the installation and usage of the Nova
    Docker driver to create Containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to have a Ubuntu 14.04 VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Docker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the Nova docker plugin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the stacking of Devstack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install nova-docker rootwrap filters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create Docker images and export to Glance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spawn Docker containers from Nova.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Docker
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Docker version running in my system after the Docker installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00240.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Install the Nova Docker plugin
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to install the plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone -b stable/kilo https://github.com/stackforge/nova-docker.git``cd nova-docker``sudo pip install .`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Docker driver version after installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00241.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Devstack installation
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used a stable Kilo release with the following `local.conf`. This sets
    up Nova to use the Docker driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[[local|localrc]] # HOST HOST_IP=<EDITME>  ADMIN_PASSWORD=openstack DATABASE_PASSWORD=$ADMIN_PASSWORD
    RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD SERVICE_TOKEN=super-secret-admin-token
    VIRT_DRIVER=novadocker.virt.docker.DockerDriver  # Logging VERBOSE=True DEST=$HOME/stack
    SCREEN_LOGDIR=$DEST/logs/screen SERVICE_DIR=$DEST/status DATA_DIR=$DEST/data LOGFILE=$DEST/logs/stack.sh.log
    LOGDIR=$DEST/logs OFFLINE=false  # Networking FIXED_RANGE=10.0.0.0/24  # This enables Neutron
    disable_service n-net enable_service q-svc enable_service q-agt enable_service q-dhcp
    enable_service q-l3 enable_service q-meta  # Introduce glance to docker images
    [[post-config|$GLANCE_API_CONF]] [DEFAULT] container_formats=ami,ari,aki,bare,ovf,ova,docker  # Configure nova to use the nova-docker driver
    [[post-config|$NOVA_CONF]] [DEFAULT] compute_driver=novadocker.virt.docker.DockerDriver`'
  prefs: []
  type: TYPE_NORMAL
- en: 'For installing the nova-docker rootwrap filters run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo cp nova-docker/etc/nova/rootwrap.d/docker.filters \``  /etc/nova/rootwrap.d/`'
  prefs: []
  type: TYPE_NORMAL
- en: 'For uploading the Docker image to Glance run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker save nginx |  glance image-create --is-public=True --container-format=docker --disk-format=raw --name nginx`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the Glance image list; we can see the nginx container image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00243.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s create the nginx container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nova boot --flavor m1.small --image nginx nginxtest`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the Nova instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00244.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see the running Container using the Docker native command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00246.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Heat Docker plugin
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the items that the Nova Docker driver cannot do currently:'
  prefs: []
  type: TYPE_NORMAL
- en: Passing environment variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linking containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestrating and scheduling the containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These missing functionalities are important and unique for Containers. The Heat
    Docker plugin solves these problems partially, except for the orchestration part.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the Heat Docker orchestration architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00247.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Heat uses the Heat Docker plugin to talk to Docker. The Docker plugin uses the
    REST API to talk to the Docker engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no direct interaction of Heat with the Docker registry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Heat orchestration script, we can use all the features of the Docker
    engine. The disadvantage of this approach is that there is no direct integration
    of Docker with other OpenStack modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing the Heat plugin
  prefs: []
  type: TYPE_NORMAL
- en: I used the procedure at [https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/](https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/)
    and [https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat](https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat)
    to do the OpenStack Heat Docker plugin integration with OpenStack Icehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Heat plugin, we can spawn Docker containers either in the localhost
    or VM created by OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: I have used a Ubuntu 14.04 VM with Icehouse installed using Devstack. I used
    the procedure in the preceding links to install the Heat Docker plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command output shows that the Heat plugin is successfully installed
    in the localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ heat resource-type-list | grep Docker``| DockerInc::Docker::Container     `'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a heat template file to spawn the nginx container in the localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '`heat_template_version: 2013-05-23 description: >   Heat template to deploy Docker containers to an existing host
    resources:   nginx-01:     type: DockerInc::Docker::Container     properties:
          image: nginx       docker_endpoint: ''tcp://192.168.56.102:2376''`'
  prefs: []
  type: TYPE_NORMAL
- en: We have specified the endpoint as the localhost IP address and Docker engine
    port number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command is used to create the Container using the preceding heat
    template:'
  prefs: []
  type: TYPE_NORMAL
- en: '`heat stack-create -f ~/heat/docker_temp.yml nginxheat1`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the heat stack installation is complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ heat stack-list``+--------------------------------------+---------------+-----------------+----------------------+``| id                                   | stack_name    | stack_status    | creation_time        |``+--------------------------------------+---------------+-----------------+----------------------+``| d878d8c1-ce17-4f29-9203-febd37bd8b7d | nginxheat1    | CREATE_COMPLETE | 2015-06-14T13:27:54Z |``+--------------------------------------+---------------+-----------------+----------------------`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the successful running container in the localhost:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ docker -H :2376 ps``CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES``624ff5de9240        nginx:latest        "nginx -g ''daemon of   2 minutes ago       Up 2 minutes        80/tcp, 443/tcp     trusting_pasteur   `'
  prefs: []
  type: TYPE_NORMAL
- en: We can use the Heat plugin approach to run Containers on OpenStack VMs by changing
    the endpoint IP address from the localhost to the VM's IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Magnum
  prefs: []
  type: TYPE_NORMAL
- en: With Nova Driver and Heat Orchestration, Containers were not a first-class citizen
    in OpenStack and Container specifics were not easy to manage with these approaches.
    Magnum is a generic Container management solution being developed in OpenStack
    to manage Docker as well as other Container technologies. Magnum supports Kubernetes,
    Docker Swarm, and Mesos for Orchestration currently. Other orchestration solutions
    will be added in the future. Magnum supports Docker Containers currently. The
    architecture allows it to support other Container runtime such as Rkt in the future.
    Magnum is still in the early stages and is available as a beta feature in the
    OpenStack Liberty release.
  prefs: []
  type: TYPE_NORMAL
- en: The Magnum architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the different layers in Magnum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00249.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the Magnum architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: The Magnum client talks to the Magnum API server, which in turn talks to the
    Magnum conductor. The Magnum conductor is responsible for interacting with Kubernetes,
    Docker Swarm, and Heat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heat takes care of interacting with other OpenStack modules such as Nova, Neutron,
    Keystone, and Glance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nova is used to create nodes in the Bay and they can run different Micro OSes
    such as CoreOS and Atomic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenStack Magnum uses the following constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bay model: This is a cluster definition that describes properties of the cluster,
    such as the node flavor, node OS, and orchestration engine to be used. The following
    is an example bay model template that uses the node flavor as `m1.small`, fedora
    atomic as the base OS for the node, and Kubernetes as the orchestration engine:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`magnum baymodel-create --name k8sbaymodel \``                       --image-id fedora-21-atomic-5 \``                       --keypair-id testkey \``                       --external-network-id public \``                       --dns-nameserver 8.8.8.8 \``                       --flavor-id m1.small \``                       --docker-volume-size 5 \``                       --network-driver flannel \``                       --coe kubernetes`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Bay: Bays are instantiated based on the bay model with the number of nodes
    necessary in Bay.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nodes, Pods, and Containers: Nodes are the individual VM instances. Pods are
    a collection of containers that share common properties and are scheduled together.
    Containers run within a Pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the relationship between the Bay model, Bay, Node,
    Pod, and Container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00250.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the advantages of using OpenStack Magnum versus a native
    orchestration solution such as Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: For customers who are already using OpenStack, this provides an integrated solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack provides multitenancy at all layers. This can be extended for Containers
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack Magnum allows interaction with other OpenStack modules such as Neutron,
    Keystone, Glance, Swift, and Cinder. Some of these integrations are planned for
    the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMs and Containers have different purposes and most likely, they will coexist.
    OpenStack with the Magnum project provides you with an orchestration solution
    covering both VMs and Containers and this makes it very attractive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Magnum
  prefs: []
  type: TYPE_NORMAL
- en: 'Magnum can be installed using the procedure at [https://github.com/openstack/magnum/blob/master/doc/source/dev/quickstart.rst](https://github.com/openstack/magnum/blob/master/doc/source/dev/quickstart.rst).
    The following is a summary of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the OpenStack development environment with Devstack enabling the Magnum
    service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By default, the Fedora Atomic image gets downloaded to Glance as part of the
    Devstack installation. If the CoreOS image is necessary, we need to download it
    manually to Glance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Bay model. A Bay model is like a template with a specific set of parameters
    using which multiple bays can be created. In the Bay model, we can specify the
    Bay type (currently supported Bay types are Kubernetes and Swarm), base image
    type (currently supported base images are Fedora Atomic and CoreOS), networking
    model (Flannel), instance size, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Bay using the Bay model as a template. While creating a Bay, we can
    specify the number of nodes that need to be created. Node is a VM on top of which
    the base image is installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Containers using either Kubernetes or Swarm on top of the created Bay.
    Kubernetes or Swarm will take care of scheduling the Containers among the different
    nodes in the Bay.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note: It is recommended that you avoid running Magnum in a VM. It is necessary
    to have a beefy machine as each Fedora instance requires at least 1 or 2 GB of
    RAM and 8 GB of hard disk space.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Container networking using OpenStack Kuryr
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover how Container networking can be done with OpenStack
    Neutron using the OpenStack Kuryr project.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Neutron
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenStack Neutron provides the networking functionality for OpenStack clusters.
    The following are some properties of OpenStack Neutron:'
  prefs: []
  type: TYPE_NORMAL
- en: Neutron provides networking as an API service with backends or plugins doing
    the implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron can be used for baremetal networking as well as VM networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Neutron constructs are Neutron network, Port, Subnet, and Router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Neutron backends are OVS, OVN, and Linux bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron also provides advanced networking services such as load balancing as
    a service, Firewall as a service, Routing as a service, and VPN as a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers and networking
  prefs: []
  type: TYPE_NORMAL
- en: We covered the details of Container networking in the earlier chapters. Some
    of the common technologies used were Flannel, Docker Libnetwork, Weave, and Calico.
    Most of these technologies use the Overlay network to provide Container networking.
  prefs: []
  type: TYPE_NORMAL
- en: OpenStack Kuryr
  prefs: []
  type: TYPE_NORMAL
- en: The goal of OpenStack Kuryr is to use Neutron to provide Container networking.
    Considering that Neutron is a mature technology, Kuryr aims to leverage the Neutron
    effort and make it easy for OpenStack users to adopt the Container technology.
    Kuryr is not a networking technology by itself; it aims to act as a bridge between
    Container networking and VM networking and enhancing Neutron to provide missing
    Container networking pieces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows you how Docker can be used with Neutron and where
    Kuryr fits in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00253.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the Kuryr architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Kuryr is implemented as the Docker libnetwork plugin. Container networking calls
    are mapped by Kuryr to appropriate Neutron API calls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutron uses OVN, Midonet, and Dragonflow as backends to implement the Neutron
    calls.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some advantages of OpenStack Kuryr:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides a common networking solution for both VMs and Containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With Magnum and Kuryr together, Containers and VMs can have a common Orchestration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering that the Neutron technology is already mature, Containers can leverage
    all the Neutron functionalities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With default Container networking, there is a double encapsulation problem when
    Containers are deployed over a VM. Container networking does the first level of
    encapsulation and VM networking does the next level of encapsulation. This can
    cause performance overhead. With Kuryr, the double encapsulation problem can be
    avoided because Containers and VMs share the same network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kuryr can integrate well with other OpenStack components to provide a complete
    Container solution with built-in multitenant support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table shows the mapping between the Neutron and Libnetwork abstraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Neutron | Libnetwork |'
  prefs: []
  type: TYPE_TB
- en: '| Neutron network | Network |'
  prefs: []
  type: TYPE_TB
- en: '| Port | Endpoint |'
  prefs: []
  type: TYPE_TB
- en: '| Subnet | IPAM |'
  prefs: []
  type: TYPE_TB
- en: '| Plugin API (plug/unplug) | Plugin API (Join/leave) |'
  prefs: []
  type: TYPE_TB
- en: 'The following diagram shows you how Kuryr can provide a common networking solution
    for Containers, VMs, and bare metal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00254.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows you where Kuryr fits in with Magnum and Container
    orchestration projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00256.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The current state and roadmap of Kuryr
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kuryr project is pretty new, and the Mitaka release will be the first OpenStack
    release with Kuryr support. The following are the ongoing and future work items
    with Kuryr:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding missing Container features to Neutron, such as Port forwarding, resource
    tagging, and service discovery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling the nested container issue by integrating VM and Container networking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better integration with OpenStack Magnum and Kolla projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current integration is focused on Docker. There are integration plans with the
    Kubernetes networking model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered how Containers and CoreOS integrate with OpenStack.
    As CoreOS allows only applications running as Containers inside it, the OpenStack
    integration with CoreOS becomes more useful if OpenStack supports Container Orchestration.
    Even though the Nova driver and Heat plugin add Container support in OpenStack,
    the Magnum project seems like the correct solution treating Containers as a first-class
    citizen in OpenStack. We also covered how OpenStack Neutron can be used to provide
    Container networking using the Kuryr project. OpenStack Container integration
    is relatively new and there is still a lot of work ongoing to complete this integration.
    Managing VMs and Containers using single orchestration software gives tighter
    integration and eases the management and debugging capabilities. In the next chapter,
    we will cover CoreOS troubleshooting and debugging.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: 'Magnum: [https://wiki.openstack.org/wiki/Magnum](https://wiki.openstack.org/wiki/Magnum)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Magnum developer quick start: [https://github.com/openstack/magnum/blob/master/doc/source/dev/dev-quickstart.rst](https://github.com/openstack/magnum/blob/master/doc/source/dev/dev-quickstart.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS on OpenStack: [https://coreos.com/os/docs/latest/booting-on-openstack.html](https://coreos.com/os/docs/latest/booting-on-openstack.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The OpenStack Docker driver: [https://wiki.openstack.org/wiki/Docker](https://wiki.openstack.org/wiki/Docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing Nova-docker with OpenStack: [http://blog.oddbit.com/2015/02/11/installing-novadocker-with-devstack/](http://blog.oddbit.com/2015/02/11/installing-novadocker-with-devstack/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenStack and Docker driver: [https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-1/](https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-1/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenStack and Docker with Heat and Magnum: [https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/](https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The OpenStack Heat plugin for Docker: [https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat](https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenStack Kuryr: [https://github.com/openstack/kuryr](https://github.com/openstack/kuryr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenStack Kuryr background: [https://galsagie.github.io/sdn/openstack/docker/kuryr/neutron/2015/08/24/kuryr-part1/](https://galsagie.github.io/sdn/openstack/docker/kuryr/neutron/2015/08/24/kuryr-part1/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  prefs: []
  type: TYPE_NORMAL
- en: 'Private Cloud Dream Stack - OpenStack + CoreOS + Kubernetes: [https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/private-cloud-dream-stack-openstack-coreos-kubernetes](https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/private-cloud-dream-stack-openstack-coreos-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Magnum OpenStack presentations: [https://www.youtube.com/watch?v=BM6nFH7G8Vc](https://www.youtube.com/watch?v=BM6nFH7G8Vc)
    and [https://www.youtube.com/watch?v=_ZbebTIaS7M](https://www.youtube.com/watch?v=_ZbebTIaS7M)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuryr OpenStack presentations: [https://www.openstack.org/summit/tokyo-2015/videos/presentation/connecting-the-dots-with-neutron-unifying-network-virtualization-between-containers-and-vms](https://www.openstack.org/summit/tokyo-2015/videos/presentation/connecting-the-dots-with-neutron-unifying-network-virtualization-between-containers-and-vms)
    and [https://www.youtube.com/watch?v=crVi30bgOt0](https://www.youtube.com/watch?v=crVi30bgOt0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
