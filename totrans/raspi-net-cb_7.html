<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Clustering</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Installing a high-availability load balancer</li><li class="listitem" style="list-style-type: disc">Installing a distributed filesystem</li><li class="listitem" style="list-style-type: disc">Creating a super computer</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec61"/>Introduction</h1></div></div></div><p>The recipes in this chapter are for network clusters of Raspberry Pis.</p><p>A network cluster is more than one computer networked together as a single system. Computers are clustered for scaling and high availability. Clusters are used to scale performance by distributing the workload of the system across all of the computers in the cluster. In a highly available system, the network cluster continues to function even if one of the computers in the cluster goes down.</p><p>Clusters of Raspberry Pis can be used to keep a website up and running, even if one of the Raspberry Pis used to host the website fails. Raspberry Pi clusters can also be used to distribute processing and data storage across a number of Raspberry Pis to create a Raspberry Pi supercomputer.</p><p>The recipes in this chapter are not specific to the Raspberry Pi. They can be repeated on most (Debian-based) Linux operating systems. The recipes are included in the book to demonstrate the possibilities of clustering Raspberry Pi computers.</p><p>After completing the recipes in this chapter, you will have used load balancers to keep a website highly available, distributed files and data over the combined storage in a cluster of Raspberry Pis, and created a Raspberry Pi supercomputer.</p></div></div>
<div class="section" title="Installing a high-availability load balancer"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec62"/>Installing a high-availability load balancer</h1></div></div></div><p>This <a class="indexterm" id="id677"/>recipe turns four Raspberry Pis into a highly available website cluster.</p><p>Two Raspberry Pis are used as web servers sharing the load of hosting the website. The other two Raspberry Pis are load balancers and they distribute the load of the incoming web requests across the two web servers.</p><p>Only one load balancer is required to balance the load. The second is configured to replace the first, if the first load balancer should fail.</p><p>The web servers in this recipe use the Apache HTTP server to serve simple stateless websites that demonstrate load balancing in action.</p><p>The load<a class="indexterm" id="id678"/> balancers in this recipe use HA Proxy to balance web requests between the two web servers and <span class="strong"><strong>Keepalived</strong></span> to create a virtual IP address for the website that will be automatically redirected to the backup load balancer, if the current load balancer fails.</p><p>After completing this recipe, you will have created a highly available website.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec229"/>Getting ready </h2></div></div></div><p>These are the ingredients for this recipe:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Four basic networking setups for the Raspberry Pi all connected to the same network switch</li><li class="listitem" style="list-style-type: disc">Five available IP addresses on the local network</li></ul></div><p>This recipe does not require the desktop GUI and could either be run from the text-based console or from within an LXTerminal.</p><p>With the Secure Shell server running on each Raspberry Pi, this recipe can be completed remotely using a Secure Shell client. Websites are typically managed remotely.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec230"/>How to do it...</h2></div></div></div><p>The steps to building a highly available Raspberry Pi website cluster are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to each Raspberry Pi. Set its hostname and IP address. Name the two load balancers <code class="literal">lb1</code> and <code class="literal">lb2</code>. Name the two web servers <code class="literal">web1</code> and <code class="literal">web2</code>. Use IP addresses from your network.<p>The following hostnames and IP addresses are used in this recipe: <code class="literal">lb1</code> – 192.168.2.101; <code class="literal">lb2</code> – 192.168.2.102; <code class="literal">web1</code> – 192.168.2.111, and <code class="literal">web2</code> – 192.168.2.112.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>The <code class="literal">raspi-config</code> command can be used to change the hostname of your Raspberry Pi. <a class="link" href="ch02.html" title="Chapter 2. Administration">Chapter 2</a>, <span class="emphasis"><em>Administration</em></span>, has recipes for configuring the Raspberry Pi that use the <code class="literal">raspi-config</code> command.</p><p><a class="link" href="ch05.html" title="Chapter 5. Advanced Networking">Chapter 5</a>, <span class="emphasis"><em>Advanced Networking</em></span>, has a recipe for changing the static IP address.</p></div></div></li><li class="listitem">Next, we'll<a class="indexterm" id="id679"/> set up the web servers. Log in to each of the web servers: <code class="literal">web1</code> and <code class="literal">web2</code>.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>Repeat the following steps on both of the web servers: <code class="literal">web1</code> and <code class="literal">web2</code>.</p></div></div></li><li class="listitem">Now, we'll install Apache2 on each web server. Use the <code class="literal">apt-get install</code> command to install the Apache web server daemon (Apache2).<div class="informalexample"><pre class="programlisting">pi@web1 ~ $ <span class="strong"><strong>sudo apt-get install -y apache2</strong></span>

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following extra packages will be installed:
  apache2-bin apache2-data apache2-utils libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap liblua5.1-0 ssl-cert
Suggested packages:
  apache2-doc apache2-suexec-pristine apache2-suexec-custom openssl-blacklist
The following NEW packages will be installed:
  apache2 apache2-bin apache2-data apache2-utils libapr1 libaprutil1 libaprutil1-dbd-sqlite3 libaprutil1-ldap liblua5.1-0 ssl-cert
0 upgraded, 10 newly installed, 0 to remove and 0 not upgraded.
...</pre></div></li><li class="listitem">Create unique test pages for each web server.</li><li class="listitem">Change the directories (<code class="literal">cd</code>) to the web server root <code class="literal">(/var/www/html</code>).<div class="informalexample"><pre class="programlisting">pi@web1 ~ $ <span class="strong"><strong>cd /var/www/html</strong></span>

pi@web1 /var/www/html $ </pre></div></li><li class="listitem">Use the <code class="literal">chown</code> command to give the user <code class="literal">pi</code> ownership to the directory (<code class="literal">.</code>) and all of the files in it (<code class="literal">*</code>).<div class="informalexample"><pre class="programlisting">pi@web1 /var/www/html $ <span class="strong"><strong>sudo chown pi:www-data . *</strong></span>

pi@web1 /var/www/html $</pre></div></li><li class="listitem">Create a web page for the web server using the <code class="literal">echo</code> command.<div class="informalexample"><pre class="programlisting">pi@web1 /var/www/html $ <span class="strong"><strong>echo '&lt;html&gt;&lt;body&gt;web1&lt;/body&gt;&lt;/html&gt;' &gt; index.html</strong></span> 

pi@web1 /var/www/html $</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>During normal operation, both web servers will be serving identical content.</p><p>For testing, the page contents, <code class="literal">&lt;body&gt;web1&lt;/body&gt;</code>, should be unique for each web server. Use <code class="literal">&lt;body&gt;web2&lt;/body&gt;</code> for web server <code class="literal">web2</code>.</p></div></div></li><li class="listitem">Use<a class="indexterm" id="id680"/> the <code class="literal">touch</code> command to create a file (<code class="literal">lb-check.txt</code>) that can be used by the load balancers to validate that the web server is running.<div class="informalexample"><pre class="programlisting">pi@web1 /var/www/html $ <span class="strong"><strong>touch lb-check.txt</strong></span> 

pi@web1 /var/www/html $</pre></div></li><li class="listitem">Now, test the web servers. Use a web browser to test the web servers. Test both their hostnames: <code class="literal">http://web1.local/</code> and <code class="literal">http://web2.local/</code>, as well as their IP addresses: <code class="literal">http://192.168.2.111/</code> and <code class="literal">http://192.168.2.112/</code>.<div class="mediaobject"><img alt="How to do it..." src="graphics/B04745_07_01.jpg"/></div></li><li class="listitem">Set up the load balancers. Log in to each of the load balancers, <code class="literal">lb1</code> and <code class="literal">lb2</code>.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note35"/>Note</h3><p>Repeat the following steps on both <code class="literal">lb1</code> and <code class="literal">lb2</code>.</p></div></div></li><li class="listitem">Install<a class="indexterm" id="id681"/> HAProxy and Keepalived on each load balancer. Use the <code class="literal">apt-get install</code> command to download and install the <code class="literal">haproxy</code> and <code class="literal">keepalived</code> packages.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo apt-get install -y haproxy keepalived</strong></span>

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following extra packages will be installed:
  iproute ipvsadm libpci3 libperl5.20 libsensors4 libsnmp-base libsnmp30
Suggested packages:
  heartbeat ldirectord lm-sensors snmp-mibs-downloader vim-haproxy haproxy-doc
The following NEW packages will be installed:
  iproute ipvsadm keepalived libpci3 libperl5.20 libsensors4 libsnmp-base
  libsnmp30 haproxy
0 upgraded, 9 newly installed, 0 to remove and 0 not upgraded.

...</pre></div></li><li class="listitem">Configure HAProxy for each load balancer. Use the <code class="literal">cat</code> command to add the <code class="literal">listen stats</code> and <code class="literal">listen webfarm</code> sections to the bottom of the <code class="literal">/etc/haproxy/haproxy.cfg</code> configuration file.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo bash</strong></span>

root@lb1:/home/pi# <span class="strong"><strong>cat &lt;&lt;EOD &gt;&gt;/etc/haproxy/haproxy.cfg

listen stats
  bind 0.0.0.0:8880
  stats enable
  stats hide-version
  stats uri     /
  stats realm   HAProxy\ Statistics
  stats auth    pi:raspberry

listen webfarm 192.168.2.100:80
  mode http
  balance roundrobin
  option httpclose
  option httpchk HEAD /lb-check.txt HTTP/1.0
  server web1 192.168.2.111:80 check
  server web2 192.168.2.112:80 check

EOD

root@lb1:/home/pi# exit

pi@lb1 ~ $</strong></span></pre></div></li><li class="listitem">Use the <code class="literal">systemctl restart</code> command to restart <code class="literal">haproxy.service</code>.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo systemctl restart haproxy.service</strong></span> 

pi@lb1 ~ $ </pre></div></li><li class="listitem">Enable listening on virtual IP addresses for both load balancers. Add the configuration parameter <code class="literal">net.ipv4.ip_nonlocal_bind=1</code> to the bottom of the <code class="literal">sysctl.conf</code> configuration file.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo bash</strong></span>

root@lb1:/home/pi# <span class="strong"><strong>echo "net.ipv4.ip_nonlocal_bind=1" &gt;&gt;/etc/sysctl.conf</strong></span>

root@lb1:/home/pi# <span class="strong"><strong>exit</strong></span>

pi@lb1 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">sysctl –p</code> command to load the updated configuration.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo sysctl –p</strong></span>

kernel.printk = 3 4 1 3
vm.swappiness = 1
vm.min_free_kbytes = 8192
net.ipv4.ip_nonlocal_bind = 1

pi@lb1 ~ $</pre></div></li><li class="listitem">Configure Keepalived for both load balancers.</li><li class="listitem">Use <a class="indexterm" id="id682"/>the <code class="literal">cat</code> command to create the <code class="literal">keepalived.conf</code> configuration file that defines the following: a function to check the status of the HAProxy daemon (<code class="literal">chk_haproxy</code>); the network interface <code class="literal">eth0</code> to listen on; the load balancer's priority (the highest is the master load balancer); and the virtual IP address (192.168.2.100) that the load balancers share.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>It is important that the load balancers have different priorities.</p><p>On load balancer <code class="literal">lb1</code>, use <code class="literal">priority 101</code> (as shown next).</p><p>On load balancer <code class="literal">lb2</code>, use <code class="literal">priority 100</code>.</p></div></div><div class="informalexample"><pre class="programlisting">pi@lb2 ~ $ <span class="strong"><strong>sudo bash</strong></span>

root@lb2:/home/pi# <span class="strong"><strong>cat &lt;&lt;EOD &gt;/etc/keepalived/keepalived.conf

vrrp_script chk_haproxy 
{
        script "killall -0 haproxy"
        interval 2
        weight 2
}

vrrp_instance VI_1 
{
        interface eth0
        state MASTER
        virtual_router_id 51

        priority 101

        virtual_ipaddress 
        {
            192.168.2.100
        }

        track_script 
        {
            chk_haproxy
        }
}

EOD</strong></span>

root@lb2:/home/pi# exit

exit

pi@lb2 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">systemctl restart</code> command to restart <code class="literal">keepalived.service</code>.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>sudo systemctl restart keepalived.service</strong></span> 

pi@lb1 ~ $ </pre></div></li><li class="listitem">Test <a class="indexterm" id="id683"/>the cluster. Use a web browser to test the cluster. Browse to the cluster's virtual IP address (<code class="literal">http://192.168.2.100</code>).</li><li class="listitem">Notice that with each refresh of the browser, the web page displayed alternates between the web page from web server <code class="literal">web1</code> and the web page from <code class="literal">web2</code>. The cluster is working!<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>For an actual website, the web servers <code class="literal">web1</code> and <code class="literal">web2</code> should be serving the same content, stateless copies of the same website, or the same web service.</p></div></div></li><li class="listitem">Test web server failure. Log in to <code class="literal">web1</code> and use the <code class="literal">poweroff</code> command to shut it down.<div class="informalexample"><pre class="programlisting">golden-macbook:~ rick$ <span class="strong"><strong>ssh pi@web1.local</strong></span>

Warning: Permanently added the RSA host key for IP address 'fe80::ba27:ebff:fec9:7ea9%en5' to the list of known hosts.
pi@web1.local's password: 

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Sat Oct 24 14:34:34 2015 from fe80::12dd:b1ff:feee:dfc6%eth0

pi@web1 ~ $ <span class="strong"><strong>sudo poweroff</strong></span>

Connection to web1.local closed by remote host.
Connection to web1.local closed.

golden-macbook:~ rick$ </pre></div></li><li class="listitem">Use a web browser to validate that the virtual IP address of the cluster (192.1682.1.00) is still working.</li><li class="listitem">Notice that with every refresh of the browser, the web page displayed is from the only running web server, <code class="literal">web2</code>.</li><li class="listitem">Use a web browser to check the status of the HA Proxy on <code class="literal">lb1</code> (<code class="literal">http://lb1.local:8880</code>).<div class="mediaobject"><img alt="How to do it..." src="graphics/B04745_07_02.jpg"/></div></li><li class="listitem">Notice <a class="indexterm" id="id684"/>that the status of web server <code class="literal">web1</code> is displayed in red indicating that it is down. The status of <code class="literal">web2</code> is green because it is still running.</li><li class="listitem">Restart the web server, <code class="literal">web1</code>.</li><li class="listitem">Refresh the HAProxy status page (<code class="literal">http://lb1.local:8880</code>) and notice that the status of web server <code class="literal">web1</code> is once again green.</li><li class="listitem">Continually refresh the virtual IP address of the cluster (<code class="literal">http://192.168.2.100</code>) and notice that the web page displayed once again alternates between the web page from web server <code class="literal">web1</code> and the web page from web server <code class="literal">web2</code>.</li><li class="listitem">The cluster runs even if one web server fails!</li><li class="listitem">Test load balancer failure. Log in to the master load balancer, <code class="literal">lb1</code>.</li><li class="listitem">Use the <code class="literal">ip addr</code> command to show the IP addresses that share the network interface <code class="literal">eth0</code>.<div class="informalexample"><pre class="programlisting">pi@lb1 ~ $ <span class="strong"><strong>ip addr show eth0</strong></span>

2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether b8:27:eb:57:79:6d brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.101/24 brd 192.168.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet 192.168.2.100/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::9b32:5b03:f901:4777/64 scope link 
       valid_lft forever preferred_lft forever

pi@lb1 ~ $</pre></div></li><li class="listitem">Notice that there are two IPv4 (<code class="literal">inet</code>) addresses including the cluster's virtual IP address (<code class="literal">http://192.168.2.100</code>) assigned to network interface <code class="literal">eth0</code>.</li><li class="listitem">Now, log in to the failover load balancer <code class="literal">lb2</code>.</li><li class="listitem">Use the <code class="literal">ip addr</code> command to show the IP addresses that share the network interface <code class="literal">eth0</code> on load balancer <code class="literal">lb2</code>.<div class="informalexample"><pre class="programlisting">pi@lb2 ~ $ <span class="strong"><strong>ip addr show eth0</strong></span>

2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether b8:27:eb:42:f6:a2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.102/24 brd 192.168.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::9284:5714:5216:6cde/64 scope link 
       valid_lft forever preferred_lft forever

pi@lb2 ~ $ </pre></div></li><li class="listitem">Notice<a class="indexterm" id="id685"/> that there is only one IPv4 (<code class="literal">inet</code>) address assigned to the network interface <code class="literal">eth0</code> on load balancer <code class="literal">lb2</code> and it is not the cluster's virtual IP address.</li><li class="listitem">Remove the master load balancer <code class="literal">lb1</code> from the cluster by disconnecting its network cable.</li><li class="listitem">Use the <code class="literal">ip addr</code> command once again to show the IP addresses that are sharing the network interface <code class="literal">eth0</code> on load balancer <code class="literal">lb2</code>.<div class="informalexample"><pre class="programlisting">pi@lb2 ~ $ <span class="strong"><strong>ip addr sh eth0</strong></span>

2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether b8:27:eb:42:f6:a2 brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.102/24 brd 192.168.2.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet 192.168.2.100/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::9284:5714:5216:6cde/64 scope link 
       valid_lft forever preferred_lft forever

pi@lb2 ~ $ </pre></div></li><li class="listitem">Notice that there are now two IPv4 (<code class="literal">inet</code>) addresses including the cluster's virtual IP address (<code class="literal">http://192.168.2.100</code>) assigned to network interface <code class="literal">eth0</code> on <code class="literal">lb2</code>.</li><li class="listitem">Notice by continuously refreshing the cluster's virtual IP address (<code class="literal">http://192.168.2.100</code>) that load balancing still works—that the web page still alternates between <code class="literal">web1</code> and <code class="literal">web2</code>.</li><li class="listitem">The cluster runs even if one load balancer fails!</li><li class="listitem">Now, let's restore normal operation. Add <code class="literal">lb1</code> back to the cluster by connecting its network cable.</li><li class="listitem">Use <a class="indexterm" id="id686"/>the <code class="literal">ip addr</code> command on load balancer <code class="literal">lb2</code> to show that the cluster's virtual IP address (192.168.2.100) is no longer assigned to network interface <code class="literal">eth0</code> on load balancer <code class="literal">lb2</code> (see step <span class="emphasis"><em>32</em></span>).</li><li class="listitem">Use the <code class="literal">ip addr</code> command on load balancer <code class="literal">lb1</code> to show that load balancer <code class="literal">lb1</code> once again has the cluster's virtual IP address assigned to its network interface <code class="literal">eth0</code> (see step <span class="emphasis"><em>29</em></span>).</li><li class="listitem">The highly available website cluster is up and running!</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec231"/>How it works...</h2></div></div></div><p>The recipe begins by setting up four Raspberry Pis with new hostnames and IP addresses so that they can be used more effectively in a cluster.</p><p>The load balancers are named <code class="literal">lb1</code> and <code class="literal">lb2</code>; and their IP addresses are respectively set to 192.168.2.101 and 192.168.2.102. </p><p>The web servers are named <code class="literal">web1</code> and <code class="literal">web2</code> with their respective IP addresses set to 192.168.2.111 and 192.168.112.</p><p>The <code class="literal">raspi-config</code> command can be used to change the Raspberry Pi hostname (examples of using the <code class="literal">raspi-config</code> command can be found in <a class="link" href="ch02.html" title="Chapter 2. Administration">Chapter 2</a>, <span class="emphasis"><em>Administration</em></span>). </p><p>A recipe for <span class="emphasis"><em>Configuring a static IP address</em></span> can be found in <a class="link" href="ch05.html" title="Chapter 5. Advanced Networking">Chapter 5</a>, <span class="emphasis"><em>Advanced Networking</em></span>.</p><div class="section" title="Setting up the web servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec103"/>Setting up the web servers</h3></div></div></div><p>After <a class="indexterm" id="id687"/>each Raspberry Pi is properly named and addressed, the Apache HTTP daemon is set up on each of the two web servers, <code class="literal">web1</code> and <code class="literal">web2</code>.</p><div class="section" title="Installing Apache2 on each web server"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec06"/>Installing Apache2 on each web server</h4></div></div></div><p>The <code class="literal">apt-get install</code> command is used to install the <code class="literal">apache2</code> package on each web server.</p><p>Installation<a class="indexterm" id="id688"/> includes starting the Apache HTTP server and restarting it with each boot.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note38"/>Note</h3><p><a class="link" href="ch05.html" title="Chapter 5. Advanced Networking">Chapter 5</a>, <span class="emphasis"><em>Advanced Networking</em></span>, has a recipe for installing a web server with more detailed instructions on setting up a web server.</p></div></div></div><div class="section" title="Creating unique test web pages for each web server"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec07"/>Creating unique test web pages for each web server</h4></div></div></div><p>The <code class="literal">cd</code> command is used to change the web server's root directory, <code class="literal">/var/www/html</code>, where <a class="indexterm" id="id689"/>two files will be created: the default web page, <code class="literal">index.html</code>, and a file for the load balancers to check periodically to ensure that the web server is still running, <code class="literal">lb-check.txt</code>.</p><p>The <code class="literal">chown</code> command is used to change the ownership of the root directory (<code class="literal">.</code>) and all the files in it (<code class="literal">*</code>) to the user <code class="literal">pi</code>. After changing ownership, the user <code class="literal">pi</code> can create and delete files in the web server's root directory.</p><p>Two files are created on each web server: <code class="literal">index.html</code> and <code class="literal">lb-check.txt</code>. The <code class="literal">lb-check.txt</code> file can be empty. It just needs to exist.</p><p>The <code class="literal">echo</code> command is used to write the very simple <code class="literal">index.html</code> file, and the <code class="literal">touch</code> command is used to create an empty <code class="literal">lb-check.txt</code> file.</p><p>This recipe intentionally uses unique <code class="literal">index.html</code> files on each web server to demonstrate load balancing in action. On web server <code class="literal">web1</code>, the body of the web page is <code class="literal">web1</code> and on web server <code class="literal">web2</code> the <code class="literal">body</code> is <code class="literal">web2</code>.</p><p>During the normal operation of a website cluster, clients of the website should see the same web page regardless of which web server the load balancer has selected.</p><p>During the normal operation of a website cluster, each of the cluster's web servers will be identical. They will either have identical <code class="literal">index.html</code> files or they will be configured to serve the same web application.</p><p>This recipe uses two different <code class="literal">index.html</code> files to demonstrate load balancing.</p></div><div class="section" title="Testing the web servers"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec08"/>Testing the web servers</h4></div></div></div><p>A web <a class="indexterm" id="id690"/>browser is used to test that each web server is up and running. The hostname and IP address of both web servers are tested: </p><div class="informalexample"><pre class="programlisting">http://web1.local/  http://192.168.2.111/
http://web2.local/  http://192.168.2.112/</pre></div></div></div><div class="section" title="Setting up the load balancers"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec104"/>Setting up the load balancers</h3></div></div></div><p>After the <a class="indexterm" id="id691"/>web servers have been installed and tested, HA Proxy and Keepalived are set up on the two load balancers, <span class="strong"><strong>lb1</strong></span> and <span class="strong"><strong>lb2</strong></span>. HA Proxy is the load balancer service, and Keepalived is the failover service. HA Proxy distributes web requests between the two web servers and Keepalived replaces the master load balancer with another, if it fails.</p><div class="section" title="Install haproxy and keepalived on each load balancer"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec09"/>Install haproxy and keepalived on each load balancer</h4></div></div></div><p>The <code class="literal">apt-get install</code> command is used to install the HAProxy and Keepalived software distribution packages on each load balancer. </p><p>Installation <a class="indexterm" id="id692"/>includes starting and restarting both HAProxy and Keepalived with each boot. However, HAProxy and Keepalived still need to be configured.</p></div><div class="section" title="Configuring HAProxy for each load balancer"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec10"/>Configuring HAProxy for each load balancer</h4></div></div></div><p>The <a class="indexterm" id="id693"/>default HAProxy configuration file <code class="literal">(/etc/haproxy/haproxy.cfg</code>) needs two new sections: <code class="literal">listen stats</code> and <code class="literal">listen webfarm</code>.</p><p>The <code class="literal">listen stats</code> section creates a protected single-page web server on port <code class="literal">8880</code> for all network interfaces of the load balancer (0.0.0.0) including the virtual network interface for the cluster (192.168.2.100). The statistics web server is protected (<code class="literal">stats auth</code>) by a username (<code class="literal">pi</code>) and a password (<code class="literal">raspberry</code>).</p><p>The <code class="literal">listen webfarm</code> section defines the collection of web servers (server <code class="literal">web1</code>, server <code class="literal">web2</code>) that the HAProxy will load balance using the roundrobin load-balancing algorithm, as well as the method (<code class="literal">httpchk HEAD</code>) and URL (<code class="literal">/lb-check.txt</code>) that are used to test if the web servers are still running.</p><p>A secure shell (<code class="literal">sudo bash</code>) is used to update the HAProxy's configuration file (<code class="literal">/etc/haproxy/haproxy.cfg</code>).</p><p>Within the Secure Shell, the <code class="literal">cat</code> command is used to append the lines following the <code class="literal">cat</code> command up to the end of data mark (<code class="literal">&lt;&lt;EOD</code>) to the bottom of the file (<code class="literal">&gt;&gt;haproxy.cfg</code>).</p><p>The Secure Shell is released (<code class="literal">exit</code>) after the file is updated.</p><p>The <code class="literal">systemctl</code> command is used to restart the HAProxy service (<code class="literal">happroxy.service</code>) on each load balancer, so that the service on each load balancer can update its configuration.</p></div><div class="section" title="Enable listening on virtual IP addresses for both load balancers"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec11"/>Enable listening on virtual IP addresses for both load balancers</h4></div></div></div><p>The <a class="indexterm" id="id694"/>Raspberry Pi's Linux kernel is not by default configured to listen on the virtual IP addresses used by Keepalived. The system kernel configuration file (<code class="literal">/etc/sysctl.conf</code>) needs to be updated to permit non-local network binding.</p><p>A secure shell (<code class="literal">sudo bash</code>) is used to update the system kernel configuration file (<code class="literal">sysctl.conf</code>).</p><p>Within the Secure Shell, the <code class="literal">echo</code> command is used to enable virtual IP addresses by appending the statement <code class="literal">net.ipv4.ip_nonlocal_bind=1</code> to the bottom of the system kernel configuration file (<code class="literal">systctl.conf</code>) (<code class="literal">&gt;&gt;</code>). </p><p>The <a class="indexterm" id="id695"/>Secure Shell is released (<code class="literal">exit</code>) after the file is updated.</p><p>The <code class="literal">sysctl –p</code> command is used to load the updated kernel configuration.</p></div><div class="section" title="Configuring Keepalived for both load balancers"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec12"/>Configuring Keepalived for both load balancers</h4></div></div></div><p>Although<a class="indexterm" id="id696"/> Keepalived is installed and ready, it has not been configured.</p><p>A secure shell (<code class="literal">sudo bash</code>) is used to create a Keepalived configuration file (<code class="literal">/etc/keepalived/keepalived.conf</code>).</p><p>Within the secure shell, the <code class="literal">cat</code> command is used to create the configuration file by copying the lines following the <code class="literal">cat</code> command up to the end of data mark (<code class="literal">&lt;&lt;EOD</code>) to the configuration file (<code class="literal">&gt;keepalived.conf</code>).</p><p>The Secure Shell is released (<code class="literal">exit</code>) after the file is updated.</p><p>Keepalived configuration has two sections: <code class="literal">vrrp_script chk_haproxy</code> and <code class="literal">vrrp_instance VI_1</code>.</p><p>The <code class="literal">vrrp_script chk_haproxy</code> section defines a script (<code class="literal">killall -0 haproxy</code>) that will complete with an OK status so long as a process named <code class="literal">haproxy</code> is running.</p><p>The command name <code class="literal">killall</code> is misleading; the <code class="literal">-0</code> parameter tells the command to do nothing more than exit with an OK status. The <code class="literal">killall</code> command can also be used to <code class="literal">kill</code> processes; however, that is not its purpose here.</p><p>The <code class="literal">vrrp_instance VI_1</code> section defines the <code class="literal">virtual_ipaddress</code> that is shared by the two load balancers (192.168.2.100). This section also defines the network interface (<code class="literal">eth0</code>) that is used to bind the virtual IP address, the <code class="literal">track_script</code> (<code class="literal">chk_haproxy</code>) that is used to keep track of the <code class="literal">haproxy</code> process, and a priority that is used to determine which of the load balancers is the <code class="literal">MASTER</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note39"/>Note</h3><p>The <code class="literal">priority</code> parameter should be different on the two load balancers.</p><p>The master load balancer, <code class="literal">lb1</code>, should have a higher priority (priority <code class="literal">101</code>) than the failover load balancer, <code class="literal">lb2</code> (priority <code class="literal">100</code>).</p></div></div><p>The priority should be different on each of the load balancers. In this recipe, load balancer <code class="literal">lb1</code> has the priority <code class="literal">101</code> and load balancer <code class="literal">lb2</code> has the priority <code class="literal">100</code>. The load balancer with the highest priority (<code class="literal">lb1</code>) is used as the master and the other load balancer (<code class="literal">lb2</code>) is used as a failover slave.</p><p>Only the master load balancer (<code class="literal">lb1</code>) listens on the defined virtual network address (192.168.2.100). The failover load balancer (<code class="literal">lb2</code>) does not.</p><p>The HAProxy running on the master load balancer (<code class="literal">lb1</code>) is the service used by the cluster to balance web requests between the web servers. The HAProxy on the failover load balancer (<code class="literal">lb2</code>) is still running, but it is not used by the cluster because the failover load balancer (<code class="literal">lb2</code>) is not listening on the cluster's virtual IP address (192.168.2.100).</p><p>If the master load balancer (<code class="literal">lb1</code>) does fail, the load balancer with the next highest priority (<code class="literal">lb2</code>) becomes the master.</p><p>If the<a class="indexterm" id="id697"/> master <code class="literal">track_script</code> reports of load balancer (<code class="literal">lb1</code>) indicates that the master's <code class="literal">haproxy</code> process is no longer running, the master transfers control of the virtual IP address (192.168.2.100) to the failover load balancer (<code class="literal">lb2</code>).</p><p>If the failover load balancer (<code class="literal">lb2</code>) can no longer connect to the master load balancer (<code class="literal">lb1</code>), the failover load balancer (<code class="literal">lb2</code>) will attempt to take over the virtual IP address. </p><p>The <code class="literal">virtual_router_id</code> parameter defines a unique ID (<code class="literal">51</code>) that is used by the load balancers keeping the same virtual IP address up and running.</p><p>The Secure Shell is released (<code class="literal">exit</code>) after the file is created.</p><p>The <code class="literal">systemctl</code> command is used to restart the Keepalived service (<code class="literal">keepalived.service</code>) on both load balancers, so that the service on each load balancer can update its configuration.</p></div><div class="section" title="Testing the cluster"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec13"/>Testing the cluster</h4></div></div></div><p>A web<a class="indexterm" id="id698"/> browser is used to validate that the website cluster is up and running on the defined virtual IP address (192.168.2.100).</p><p>When the website URL (<code class="literal">http://192.168.2.100/</code>) is refreshed in the browser, the page displayed in the browser alternates between the default web page (<code class="literal">index.html</code>) from web server <code class="literal">web1</code> and the default page from web server <code class="literal">web2</code>. The master load balancer (<code class="literal">lb1</code>) is alternating web requests (round robin) between the two web servers.</p><p>The cluster is working!</p></div></div><div class="section" title="Testing web server failure"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec105"/>Testing web server failure</h3></div></div></div><p>In order<a class="indexterm" id="id699"/> to test that the website cluster's virtual IP address still responds to web requests after the failure of a single web server, web server <code class="literal">web1</code> is shut down using the <code class="literal">poweroff</code> command.</p><p>After web server <code class="literal">web1</code> has been shut down, a web browser is used to validate that the website cluster is still up and running on the defined virtual IP address (<code class="literal">192.168.2.100</code>).</p><p>When the website URL (<code class="literal">http://192.168.2.100/</code>) is refreshed in the browser, the web page displayed in the browser no longer alternates between the two web servers. Now, only the default page (<code class="literal">index.html</code>) from web server <code class="literal">web2</code> is displayed. The master load balancer (<code class="literal">lb1</code>) is still running but can only serve web requests from web server web2.</p><p>Then, the <a class="indexterm" id="id700"/>web browser is used to browse the URL of the HAProxy statistics page (<code class="literal">http://lb1.local:8880/</code>) of the master load balancer (<code class="literal">lb1</code>). The statistics page shows that web server <code class="literal">web1</code> is no longer available by displaying the statistics for the web server with a red background color. Web server <code class="literal">web2</code> is still running, so its statistics are displayed with a green background color.</p><p>The website continues to work properly, even if one web server is down.</p><p>Next, web server <code class="literal">web1</code> is restarted.</p><p>After web server <code class="literal">web1</code> has restarted, the master load balancer (<code class="literal">lb1</code>) detects the availability of the web server's tracking file (<code class="literal">http://lb1.local/chk_haproxy.txt</code>) and web server <code class="literal">web1</code> is added back to the load balancer's <code class="literal">webfarm</code>.</p><p>A refresh of the HAProxy statistics page shows that the statistics from web server <code class="literal">web1</code> are once again green, and continually refreshing the website's virtual URL (<code class="literal">http://192.168.2.100/</code>) once again alternates between <code class="literal">web1</code> and <code class="literal">web2</code>.</p><p>The website is protected from web server failure and web servers can be added on demand to handle more requests!</p><div class="section" title="Testing load balancer failure"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec14"/>Testing load balancer failure</h4></div></div></div><p>Removing the master load balancer (<code class="literal">lb1</code>) from the network by disconnecting its network cable tests the failover of the master load balancer. </p><p>Before<a class="indexterm" id="id701"/> the master load balancer (<code class="literal">lb1</code>) is disconnected from the network, the <code class="literal">ip addr</code> command is used to show that the website cluster's virtual IP address (<code class="literal">192.168.2.100</code>) is bound to the master load balancer's network interface (<code class="literal">eth0</code>). The <code class="literal">ip addr</code> command is also used on the failover load balancer (<code class="literal">lb2</code>) to show that it does not have the cluster's virtual IP address bound to its network interface (<code class="literal">eth0</code>).</p><p>After the master load balancer (<code class="literal">lb1</code>) has been disconnected from the network, the <code class="literal">ip addr</code> command is run again on the failover load balancer (<code class="literal">lb2</code>). Now that the master load balancer (<code class="literal">lb1</code>) is disconnected from the network, the failover load balancer (<code class="literal">lb2</code>) has taken over the cluster's virtual IP address (<code class="literal">192.168.2.100</code>).</p><p>While the master load balancer (<code class="literal">lb1</code>) is disconnected from the network, a web browser is used to validate that the website cluster is still up and running on the defined virtual IP address (<code class="literal">192.168.2.100</code>).</p><p>When the website URL (<code class="literal">http://192.168.2.100/</code>) is refreshed in the browser, the web page displayed in the browser continues to alternate between the two web servers. Load balancing still works even though the master load balancer (<code class="literal">lb1</code>) is offline. The failover load balancer (<code class="literal">lb2</code>) has taken over load balancing successfully!</p><p>The <a class="indexterm" id="id702"/>website cluster continues to work properly, even if one load balancer is down!</p></div><div class="section" title="Restoring normal operation"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec15"/>Restoring normal operation</h4></div></div></div><p>Normal <a class="indexterm" id="id703"/>operation is restored to the website cluster by reconnecting the master load balancer (<code class="literal">lb1</code>) to the network.</p><p>After the master load balancer (<code class="literal">lb1</code>) has been reconnected, the <code class="literal">ip addr</code> command is again run on each load balancer. The master load balancer (<code class="literal">lb1</code>) once again has the cluster's virtual IP address (<code class="literal">192.168.2.100</code>) bound to its network interface (<code class="literal">eth0</code>) and the failover load balancer (<code class="literal">lb2</code>) no longer has the virtual IP address bound to its network interface.</p><p>The highly available website cluster is up and running!</p></div></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec232"/>There's more…</h2></div></div></div><p>This recipe is a very simple example of a highly available website cluster that can be used to serve any stateless website such as a collection of static web pages or a website created with a <a class="indexterm" id="id704"/>website generator like <span class="strong"><strong>Jekyll</strong></span> (<a class="ulink" href="http://jekyllrb.com/">http://jekyllrb.com/</a>).</p><div class="section" title="Scaling horizontally by adding more web servers"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec106"/>Scaling horizontally by adding more web servers</h3></div></div></div><p>A cluster is scaled horizontally by adding more servers.</p><p>The<a class="indexterm" id="id705"/> website cluster in this recipe can be scaled horizontally by adding more Raspberry Pi web servers. Each additional web server added to the cluster should be configured exactly the same as the existing web servers (see steps <span class="emphasis"><em>2</em></span> through <span class="emphasis"><em>8</em></span>).</p><p>Scaling a Raspberry Pi cluster vertically is limited by the amount of memory available in a Raspberry Pi. The memory allocated by the GPU can be reduced freeing more memory for use by services; however, the physical memory of a Raspberry Pi cannot be increased.</p><p>The fixed memory size of the Raspberry Pi puts limits on scaling Raspberry Pi clusters. They can be easily scaled horizontally, but not vertically.</p></div><div class="section" title="Session cookies"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec107"/>Session cookies</h3></div></div></div><p>Many<a class="indexterm" id="id706"/> websites are stateful, not stateless. Stateful websites use session cookies to create unique sessions that require a login. The HAProxy configuration in this recipe is for stateless websites and does not recognize session cookies. </p><p>A user session is stored in a web application server and the session cookie is a unique key that is used to identify each unique user session in the web server. In most situations, sessions cannot be shared across web servers. The load balancer needs to ensure that once a user starts a session with one web server, all requests to the website cluster are directed to that web server and not to any other.</p><p>Web application servers and frameworks like Apache Tomcat and PHP depend on session cookies. Apache Tomcat uses the session cookie <code class="literal">JSESSIONID</code>, and PHP uses the <code class="literal">PHPSESSID</code> session cookie.</p><p>For websites<a class="indexterm" id="id707"/> that depend on session cookies, the load balancer for the website cluster needs to ensure that web requests from the same unique user (as identified by the session cookie) are always sent to the same web server because only that web server has the user's session.</p><p>To enable the HAProxy servers in this recipe to recognize session cookies for Apache Tomcat (or other Java application servers), replace the two server configuration parameters in the HAProxy configuration file (<code class="literal">/etc/haproxy/haproxy.cfg</code>) with the following three lines:</p><div class="informalexample"><pre class="programlisting">  <span class="strong"><strong>option cookie JSESSIONID prefix
  server web1 192.168.2.111:80 check cookie web1
  server web2 192.168.2.112:80 check cookie web2</strong></span></pre></div><p>The first line turns on the cookie tracking option using the <code class="literal">JSESSIONID</code> cookie plus a unique prefix for each web server. The two server configuration parameters have been updated to set a unique <code class="literal">cookie</code> prefix for each server (<code class="literal">web1</code> and <code class="literal">web2</code>).</p><p>After restarting the HAProxy service (<code class="literal">systemctl restart haproxy.service</code>), the cluster will recognize session cookies.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec233"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Computer cluster</strong></span> (<a class="ulink" href="https://en.wikipedia.org/wiki/Computer_cluster">https://en.wikipedia.org/wiki/Computer_cluster</a>): This Wikipedia <a class="indexterm" id="id708"/>article describes the concepts and history of computer clusters.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Keepalived</strong></span> (<a class="ulink" href="http://www.keepalived.org/">http://www.keepalived.org/</a>): The main goal of this project is <a class="indexterm" id="id709"/>to provide simple and robust facilities for load balancing and high availability to a Linux system and Linux-based infrastructures.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>HAProxy</strong></span> (<a class="ulink" href="http://www.haproxy.org/">http://www.haproxy.org/</a>): HAProxy is a free, very fast, and reliable<a class="indexterm" id="id710"/> solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>systemctl – control the systemd system and service manager</strong></span> (<a class="ulink" href="http://manpages.debian.org/cgi-bin/man.cgi?query=systemctl">http://manpages.debian.org/cgi-bin/man.cgi?query=systemctl</a>): The Debian manual page for <code class="literal">systemctl</code> describes <a class="indexterm" id="id711"/>the command and its options.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>sysctl – read/write system parameters </strong></span>(<a class="ulink" href="http://manpages.debian.org/cgi-bin/man.cgi?query=sysctl&amp;sektion=8">http://manpages.debian.org/cgi-bin/man.cgi?query=sysctl&amp;sektion=8</a>): The Debian manual page for <code class="literal">sysctl</code> describes<a class="indexterm" id="id712"/> the command and its options.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>killall – kill processes by name</strong></span> (<a class="ulink" href="http://manpages.debian.org/cgi-bin/man.cgi?query=killall">http://manpages.debian.org/cgi-bin/man.cgi?query=killall</a>): The Debian manual page for <code class="literal">killall</code> describes<a class="indexterm" id="id713"/> the command and its options.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Jekyll</strong></span> (<a class="ulink" href="http://jekyllrb.com/">http://jekyllrb.com/</a>): Transform your plain text into static websites <a class="indexterm" id="id714"/>and blogs.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scalability</strong></span> (<a class="ulink" href="https://en.wikipedia.org/wiki/Scalability">https://en.wikipedia.org/wiki/Scalability</a>): This Wikipedia<a class="indexterm" id="id715"/> article defines scalability, both horizontal and vertical.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Session cookie</strong></span> (<a class="ulink" href="https://en.wikipedia.org/wiki/HTTP_cookie#Session_cookie">https://en.wikipedia.org/wiki/HTTP_cookie#Session_cookie</a>): This <a class="indexterm" id="id716"/>Wikipedia article about HTTP cookies also defines session cookies.</li></ul></div></div></div>
<div class="section" title="Installing a distributed filesystem"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec63"/>Installing a distributed filesystem</h1></div></div></div><p>This recipe<a class="indexterm" id="id717"/> turns four Raspberry Pis into a highly available distributed filesystem using GlusterFS.</p><p>GlusterFS is <a class="indexterm" id="id718"/>a scalable network filesystem suitable for data-intensive tasks such as cloud storage and media streaming. GlusterFS is free and open source software and can utilize common off-the-shelf hardware like the Raspberry Pi.</p><p>After completing this recipe, you will have clustered four Raspberry Pis to create a highly available distributed filesystem.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec234"/>Getting ready</h2></div></div></div><p>Here are the ingredients for this recipe:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Four basic networking setups for the Raspberry Pi</li><li class="listitem" style="list-style-type: disc">Four available IP addresses on the local network</li></ul></div><p>This recipe does not require the desktop GUI and could either be run from the text-based console or from within an LXTerminal.</p><p>With the Secure Shell server running on each Raspberry Pi, this recipe can be completed remotely using a Secure Shell client. A distributed filesystem is typically managed remotely.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec235"/>How to do it...</h2></div></div></div><p>The steps to building a highly available Raspberry Pi distributed filesystem are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log<a class="indexterm" id="id719"/> in to each of the four Raspberry Pis and set their hostnames. Name the Raspberry Pis <code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note40"/>Note</h3><p>The <code class="literal">raspi-config</code> command can be used to change the hostname of your Raspberry Pi. <a class="link" href="ch02.html" title="Chapter 2. Administration">Chapter 2</a>, <span class="emphasis"><em>Administration</em></span>, has recipes for configuring the Raspberry Pi that use the <code class="literal">raspi-config</code> command.</p></div></div></li></ol></div><div class="section" title="Installing the GlusterFS server on each Raspberry Pi"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec108"/>Installing the GlusterFS server on each Raspberry Pi</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log<a class="indexterm" id="id720"/> in to each of the four<a class="indexterm" id="id721"/> Raspberry Pis: <code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>.</li><li class="listitem">Use the <code class="literal">apt-get install</code> command to install the GlusterFS server (<code class="literal">glusterfs-server</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo apt-get install -y glusterfs-server</strong></span>

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following extra packages will be installed:
  glusterfs-client glusterfs-common libaio1 libibverbs1 librdmacm1
The following NEW packages will be installed:
  glusterfs-client glusterfs-common glusterfs-server libaio1 libibverbs1
  librdmacm1
0 upgraded, 6 newly installed, 0 to remove and 0 not upgraded.
Need to get 7,604 kB of archives.
After this operation, 13.6 MB of additional disk space will be used.
…</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note41"/>Note</h3><p>Repeat the installation of <code class="literal">glusterfs-server</code> on each of the four Raspberry Pis: <code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">glsuter4</code>.</p></div></div></li><li class="listitem">Now, let's create a trusted storage pool. After each of the Raspberry Pis has had GlusterFS installed, log in to <code class="literal">gluster1</code> and use the <code class="literal">gluster peer probe</code> command to link the other three Raspberry Pis into a single trusted storage pool.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster peer probe gluster2.local</strong></span>

peer probe: success. 

pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster peer probe gluster3.local</strong></span>

peer probe: success. 

pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster peer probe gluster4.local</strong></span>
peer probe: success. 

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">gluster</code> <code class="literal">peer status</code> command to check that the storage pool has been created successfully.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster peer status</strong></span>

Number of Peers: 3

Hostname: gluster2.local
Uuid: 5b969ed1-01c2-406e-9710-944436c41c98
State: Peer in Cluster (Connected)

Hostname: gluster3.local
Uuid: 9a00c151-af6e-44c5-9d14-5607270b4038
State: Peer in Cluster (Connected)

Hostname: gluster4.local
Uuid: f036b1c4-7a51-49eb-aa63-81babb843b7e
State: Peer in Cluster (Connected)

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Also use the <code class="literal">gluster peer status</code> from another peer in the storage pool (<code class="literal">gluster2</code>) to validate the storage pool.<div class="informalexample"><pre class="programlisting">pi@gluster2 ~ $ <span class="strong"><strong>sudo gluster peer status</strong></span>

Number of Peers: 3

Hostname: 192.168.2.12
Uuid: 4147586b-a723-4068-b8cb-d417df6766d8
State: Peer in Cluster (Connected)

Hostname: gluster3.local
Uuid: 9a00c151-af6e-44c5-9d14-5607270b4038
State: Peer in Cluster (Connected)

Hostname: gluster4.local
Uuid: f036b1c4-7a51-49eb-aa63-81babb843b7e
State: Peer in Cluster (Connected)

pi@gluster2 ~ $ </pre></div></li><li class="listitem">Notice<a class="indexterm" id="id722"/> that the <code class="literal">Hostname</code> displayed for <code class="literal">gluster1</code> is an IP address (<code class="literal">192.168.2.12</code>).</li><li class="listitem">Use <a class="indexterm" id="id723"/>the <code class="literal">gluster peer probe</code> command on any other peer (<code class="literal">gluster2</code>) to add the hostname of <code class="literal">gluster1</code> to the storage pool.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note42"/>Note</h3><p>Do not use the <code class="literal">gluster peer probe</code> command to add itself to the trusted server pool! <span class="emphasis"><em>A storage peer cannot add itself to the pool!</em></span></p><p>Any attempt for a peer to add itself could damage the entire storage pool.</p></div></div><div class="informalexample"><pre class="programlisting">pi@gluster2 ~ $ <span class="strong"><strong>sudo gluster peer probe gluster1.local</strong></span>

peer probe: success. 

pi@gluster2 ~ $ <span class="strong"><strong>sudo gluster peer status</strong></span>

Number of Peers: 3

Hostname: gluster1.local
Uuid: 4147586b-a723-4068-b8cb-d417df6766d8
State: Peer in Cluster (Connected)

Hostname: gluster3.local
Uuid: 9a00c151-af6e-44c5-9d14-5607270b4038
State: Peer in Cluster (Connected)

Hostname: gluster4.local
Uuid: f036b1c4-7a51-49eb-aa63-81babb843b7e
State: Peer in Cluster (Connected)

pi@gluster2 ~ $ </pre></div></li><li class="listitem">It's <a class="indexterm" id="id724"/>time to create a striped <a class="indexterm" id="id725"/>replicated volume from the trusted storage pool. From any peer in the trusted storage pool (<code class="literal">gluster1</code>), use the <code class="literal">gluster volume create</code> command to create a distributed striped replicated volume (<code class="literal">stripe 2 replica 2</code>) using the four peers of the trusted storage pool (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>). On each peer, the <code class="literal">/srv/vol0</code> directory is used to store the GlusterFS configuration and data for the new volume (<code class="literal">vol0</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster volume create vol0 stripe 2 replica 2 
gluster1.local:/srv/vol0 gluster2.local:/srv/vol0 gluster3.local:/srv/vol0 gluster4.local:/srv/vol0 force</strong></span>

volume create: vol0: success: please start the volume to access data

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">gluster volume start</code> command to start the newly created volume (<code class="literal">vol0</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster volume start vol0</strong></span>

volume start: vol0: success

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Now, let's mount the distributed striped replicated volume. Use the <code class="literal">mount</code> command to mount the <code class="literal">glusterfs</code> volume <code class="literal">vol0</code> from the peer, <code class="literal">gluster1.local</code>, on the local mount point, <code class="literal">/mnt</code>.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo mount -t glusterfs gluster1.local:/vol0 /mnt</strong></span>
pi@gluster1 ~ $ </pre></div></li><li class="listitem">Test the striped replicated volume. Use the <code class="literal">cp</code> command to copy a large file (<code class="literal">/boot/kernel.img</code>) to the local mount point (<code class="literal">/mnt</code>) of the newly created distributed striped replicated volume (<code class="literal">vol0</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>ls -l /boot/kernel.img</strong></span> 

-rwxr-xr-x 1 root root 4056224 Sep 23 16:10 /boot/kernel.img

pi@gluster1 ~ $ <span class="strong"><strong>sha1sum /boot/kernel.img</strong></span> 

d5e64d892b308b99e9c2c55deaa39c579a2335ec  /boot/kernel.img

pi@gluster1 ~ $ <span class="strong"><strong>sudo cp /boot/kernel.img /mnt/</strong></span>


pi@gluster1 ~ $ <span class="strong"><strong>ls -l /mnt/kernel.img</strong></span> 

-rwxr-xr-x 1 root root 4056224 Oct 31 23:37 /mnt/kernel.img

pi@gluster1 ~ $ <span class="strong"><strong>sha1sum /mnt/kernel.img</strong></span> 

d5e64d892b308b99e9c2c55deaa39c579a2335ec  /mnt/kernel.img

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Notice<a class="indexterm" id="id726"/> that the copied file (<code class="literal">/mnt/kernel.img</code>) has the same size (<code class="literal">4056224</code>) and checksum (<code class="literal">d5e64…35ec</code>) as the original file.</li><li class="listitem">Use<a class="indexterm" id="id727"/> the <code class="literal">ls -la</code> command to display the entire contents of the GlusterFS storage directory for the distributed volume (<code class="literal">/srv/vol0</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 2068
drwxr-xr-x 3 root root    4096 Oct 31 23:37 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 7 root root    4096 Oct 31 23:37 .glusterfs
-rwxr-xr-x 2 root root 2090144 Oct 31 23:37 kernel.img

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Notice that only part of the data from the large file (<code class="literal">kernel.img</code>) is stored on this peer (<code class="literal">gluster1</code>). The size of the file (<code class="literal">2090144</code>) in the storage directory (<code class="literal">/srv/vol0</code>) is significantly smaller than the size (<code class="literal">4056224</code>) of the original file (<code class="literal">/boot/kernel.img</code>).</li><li class="listitem">Log in to each of the other three peers (<code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>) and use the <code class="literal">ls –l</code> command to check the size of the files in each of the other storage directories for the volume (<code class="literal">/srv/vol0</code>).<p>[log in to peer <code class="literal">gluster2</code>]</p><div class="informalexample"><pre class="programlisting">pi@gluster2 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 2068
drwxr-xr-x 3 root root    4096 Oct 31 23:37 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 7 root root    4096 Oct 31 23:37 .glusterfs
-rwxr-xr-x 2 root root 2090144 Oct 31 23:37 kernel.img

pi@gluster2 ~ $ 

[log in to peer gluster3]
pi@gluster3 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 1936
drwxr-xr-x 3 root root    4096 Oct 31 23:37 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 7 root root    4096 Oct 31 23:37 .glusterfs
-rwxr-xr-x 2 root root 1966080 Oct 31 23:37 kernel.img
pi@gluster3 ~ $ 

[log in to peer gluster4]
pi@gluster4 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 1936
drwxr-xr-x 3 root root    4096 Oct 31 23:37 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 7 root root    4096 Oct 31 23:37 .glusterfs
-rwxr-xr-x 2 root root 1966080 Oct 31 23:37 kernel.img

pi@gluster4 ~ $ </pre></div></li><li class="listitem">Notice<a class="indexterm" id="id728"/> that there are two different file sizes (<code class="literal">2090144</code> and <code class="literal">1966080</code>) for the data storage file (<code class="literal">/srv/vol0/kernel.img</code>) on each of the peers.</li><li class="listitem">Notice <a class="indexterm" id="id729"/>that the data storage files (<code class="literal">/srv/vol0/kernel.img</code>) on peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code> have the same size; and that the data storage files on peers <code class="literal">gluster3</code> and <code class="literal">gluster4</code> also have the same size. This is an example of how a replicated volume duplicates storage across replicated peers in case one of the replicated peers goes down.</li><li class="listitem">Notice that the sum of the two file sizes is equal to the size of the original file (<code class="literal">4056224</code>). This is an example of how a striped volume divides the data of large files across striped peers.</li><li class="listitem">Test the high availability of the cluster. Remove one of the Raspberry Pis (<code class="literal">gluster4</code>) from the network by disconnecting its network cable.</li><li class="listitem">Use the <code class="literal">gluster</code> peer status command on one of the remaining peers (<code class="literal">gluster1</code>) to check the status of the distributed filesystem's secure storage pool.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo gluster peer status</strong></span>

Number of Peers: 3

Hostname: gluster2.local
Uuid: 5b969ed1-01c2-406e-9710-944436c41c98
State: Peer in Cluster (Connected)

Hostname: gluster3.local
Uuid: 9a00c151-af6e-44c5-9d14-5607270b4038
State: Peer in Cluster (Connected)

Hostname: gluster4.local
Uuid: f036b1c4-7a51-49eb-aa63-81babb843b7e
State: Peer in Cluster (Disconnected)

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Notice that <code class="literal">Hostname: gluster4.local</code> is shown as <code class="literal">Disconnected</code>.</li><li class="listitem">Use the <code class="literal">sha1sum</code> command on <code class="literal">gluster1</code> to validate the large file stored in the filesystem (<code class="literal">/mnt/kernel.img</code>) has not changed.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ sha1sum /mnt/kernel.img 

d5e64d892b308b99e9c2c55deaa39c579a2335ec  /mnt/kernel.img

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Notice that the checksum (<code class="literal">d5e64…35ec</code>) is still the same.</li><li class="listitem">The distributed filesystem functions, even if one peer is down!</li><li class="listitem">Test <a class="indexterm" id="id730"/>the healing of replicated <a class="indexterm" id="id731"/>peers. While <code class="literal">gluster4</code> is still disconnected from the cluster, use the <code class="literal">cp</code> command to copy another large file (<code class="literal">/boot/kernel7.img</code>) to the self-healing distributed striped replicated filesystem.<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo cp /boot/kernel7.img /mnt/</strong></span>

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">ls</code> and <code class="literal">sha1sum</code> commands to check that the copied file (<code class="literal">/mnt/kernel7.img</code>) is identical to the original file (<code class="literal">/boot/kernel7.img</code>).<div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>ls -l /boot/kernel7.img</strong></span> 

-rwxr-xr-x 1 root root 4032544 Sep 23 16:10 /boot/kernel7.img

pi@gluster1 ~ $ <span class="strong"><strong>sha1sum /boot/kernel7.img</strong></span> 

8a2b1f065fc9459de79ba40e7cb791216f2f501b  /boot/kernel7.img

pi@gluster1 ~ $ <span class="strong"><strong>ls -la /mnt</strong></span>

total 7912
drwxr-xr-x  3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 21 root root    4096 Oct 31 20:29 ..
-rwxr-xr-x  1 root root 4032544 Nov  1 00:47 kernel7.img
-rwxr-xr-x  1 root root 4056224 Oct 31 23:37 kernel.img

pi@gluster1 ~ $ <span class="strong"><strong>sha1sum /mnt/kernel7.img</strong></span> 

8a2b1f065fc9459de79ba40e7cb791216f2f501b  /mnt/kernel7.img

pi@gluster1 ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">ls –l</code> command to check the file sizes in the storage directory (<code class="literal">/srv/vol0</code>) of each peer to validate that the newly copied file (<code class="literal">kernel7.img</code>) is also striped and replicated.<p>[log in to peer <code class="literal">gluster1</code>]</p><div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 4092
drwxr-xr-x 3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 8 root root    4096 Nov  1 00:47 .glusterfs
-rwxr-xr-x 2 root root 2066464 Nov  1 00:47 kernel7.img
-rwxr-xr-x 2 root root 2090144 Oct 31 23:37 kernel.img

pi@gluster1 ~ $ 
</pre></div><p>[log in to peer <code class="literal">gluster2</code>]</p><div class="informalexample"><pre class="programlisting">pi@gluster2 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 4092
drwxr-xr-x 3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 8 root root    4096 Nov  1 00:47 .glusterfs
-rwxr-xr-x 2 root root 2066464 Nov  1 00:47 kernel7.img
-rwxr-xr-x 2 root root 2090144 Oct 31 23:37 kernel.img

pi@gluster2 ~ $ </pre></div><p>[log in to peer <code class="literal">gluster3</code>]</p><div class="informalexample"><pre class="programlisting">pi@gluster3 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 3864
drwxr-xr-x 3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 8 root root    4096 Nov  1 00:47 .glusterfs
-rwxr-xr-x 2 root root 1966080 Nov  1 00:47 kernel7.img
-rwxr-xr-x 2 root root 1966080 Oct 31 23:37 kernel.img

pi@gluster3 ~ $ </pre></div></li><li class="listitem">Notice<a class="indexterm" id="id732"/> that the data storage file (<code class="literal">/srv/vol0/kernel7.img</code>) on peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code> has<a class="indexterm" id="id733"/> the same size and that the total size of the striped files (<code class="literal">2066464 + 1966080</code>) is equal to the size of the original file (<code class="literal">4032544</code>). The distributed filesystem continues to stripe and replicate files even if one peer is down!</li><li class="listitem">Reconnect peer <code class="literal">gluster4</code> to the network.</li><li class="listitem">Immediately use the <code class="literal">ls –l</code> command to check the files in the data storage directory (<code class="literal">/srv/vol0</code>) on reconnected peer <code class="literal">gluster4</code>.<div class="informalexample"><pre class="programlisting">pi@gluster4 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 1940
drwxr-xr-x 3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 8 root root    4096 Nov  1 00:50 .glusterfs
-rwxr-xr-x 2 root root       0 Nov  1 00:47 kernel7.img
-rwxr-xr-x 2 root root 1966080 Oct 31 23:37 kernel.img

pi@gluster4 ~ $ </pre></div></li><li class="listitem">Notice that the newly copied file (<code class="literal">kernel7.img</code>) has been created in the data storage directory (<code class="literal">/srv/vol0</code>), but the file size is empty (<code class="literal">0</code>).</li><li class="listitem">After waiting five minutes, use the <code class="literal">ls –l</code> command to once again check the data storage directory on <code class="literal">gluster4</code>.<div class="informalexample"><pre class="programlisting">pi@gluster4 ~ $ <span class="strong"><strong>ls -la /srv/vol0/</strong></span>

total 3864
drwxr-xr-x 3 root root    4096 Nov  1 00:47 .
drwxr-xr-x 3 pi   pi      4096 Oct 31 23:12 ..
drw------- 8 root root    4096 Nov  1 00:50 .glusterfs
-rwxr-xr-x 2 root root 1966080 Nov  1 00:47 kernel7.img
-rwxr-xr-x 2 root root 1966080 Oct 31 23:37 kernel.img

pi@gluster4 ~ $ </pre></div></li><li class="listitem">Notice that the storage file <code class="literal">kernel7.img</code> is no longer empty. The data storage file (<code class="literal">kernel7.img</code>) on <code class="literal">gluster4</code> is now the same size (<code class="literal">1966080</code>) as the storage file on peer <code class="literal">gluster3</code>.</li><li class="listitem">The distributed filesystem has healed itself!</li><li class="listitem">This<a class="indexterm" id="id734"/> cluster of four Raspberry Pis is now a highly available distributed filesystem!</li></ol></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec236"/>How it works...</h2></div></div></div><p>The<a class="indexterm" id="id735"/> recipe begins by changing the hostnames of four Raspberry Pis that are linked together on the same network. The new hostnames of the Raspberry Pis are <code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>.</p><div class="section" title="Installing the GlusterFS server on each Raspberry Pi"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec109"/>Installing the GlusterFS server on each Raspberry Pi</h3></div></div></div><p>After <a class="indexterm" id="id736"/>the Raspberry Pis are renamed, the <code class="literal">apt-get install</code> command is used on each of the Raspberry Pis to install the GlusterFS server software distribution package (<code class="literal">glusterfs-server</code>).</p><p>The installation of the <code class="literal">glusterfs-server</code> package includes starting the GlusterFS server on each of the Raspberry Pis: <code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>.</p><p>The <code class="literal">gluster peer probe</code> command is used from the <code class="literal">gluster1</code> Raspberry Pi to link the other Raspberry Pis (<code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>) into a trusted peer relationship. </p><p>The first peer in the storage pool (<code class="literal">gluster1</code>) establishes the trusted peer relationship with the other storage pool peers (<code class="literal">gluster2</code>, <code class="literal">gluster3</code>, <code class="literal">gluster4</code>). However, once the trusted relationship is established, any peer can be used as the storage pool master—to manage storage volumes, to manage the trusted peer relationships, or to be mounted as the distributed filesystem's network attachable endpoint.</p><p>The <code class="literal">gluster peer status</code> command is used on both <code class="literal">gluster1</code> and <code class="literal">gluster2</code> to validate the trusted storage pool is up and running. On both peers, the other three storage pool peers are displayed as being part of the storage pool.</p><p>The <code class="literal">gluster peer status</code> command on <code class="literal">gluster2</code>, however, displays an IP address for the <code class="literal">Hostname</code> field of the first peer, <code class="literal">gluster1</code>. So, the <code class="literal">gluster peer</code> command is used on <code class="literal">gluster2</code> to add the hostname of <code class="literal">gluster1</code> to the metadata of the storage pool.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>Do not have a peer add itself to the trusted storage pool!</p><p>A peer using the command <code class="literal">gluster peer probe</code> with its own hostname could<a class="indexterm" id="id737"/> damage the trusted storage pool!</p></div></div></div><div class="section" title="Creating a striped replicated volume in the trusted storage pool"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec110"/>Creating a striped replicated volume in the trusted storage pool</h3></div></div></div><p>The<a class="indexterm" id="id738"/> <code class="literal">cluster volume create</code> <a class="indexterm" id="id739"/>command is used from <code class="literal">gluster1</code> to create a new striped replicated volume that is distributed across the four storage pool peers (<code class="literal">gluster1.local</code>, <code class="literal">gluster2.local</code>, <code class="literal">gluster3.local</code>, <code class="literal">gluster4.local</code>).</p><p>The new volume is named <code class="literal">vol0</code>. It has two stripes (<code class="literal">stripe 2</code>) and two replicas (replica 2). It uses the same storage directory (<code class="literal">/srv/vol0</code>) on each of the storage pool peers (<code class="literal">gluster1.local</code>, <code class="literal">gluster2.local</code>, <code class="literal">gluster3.local</code>, and <code class="literal">gluster4.local</code>).</p><p>Using a storage directory on a peer's root filesystem (<code class="literal">/</code>) is not recommended, nor is it allowed by default. The <code class="literal">force</code> keyword is used to override the default behavior.</p><p>This recipe uses the root filesystem to keep the recipe simple. For a more robust, reliable distributed filesystem with higher performance, attach a high-speed external USB disk to each Raspberry Pi and configure the storage directory for the volume to be on the external disk instead of on the root filesystem. <a class="link" href="ch04.html" title="Chapter 4. File Sharing">Chapter 4</a>, <span class="emphasis"><em>File Sharing</em></span>, has more than one recipe for mounting an external USB disk on a Raspberry Pi.</p><p>After the volume (<code class="literal">vol0</code>) is created, the <code class="literal">gluster volume start</code> command is used to start sharing the newly created volume with GlusterFS clients. The <code class="literal">gluster volume start</code> command could be run from any peer in the cluster. In this case, it is run from <code class="literal">gluster1</code>.</p></div><div class="section" title="Mount the distributed striped replicated volume"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec111"/>Mount the distributed striped replicated volume</h3></div></div></div><p>Now that the <a class="indexterm" id="id740"/>distributed striped replicated volume (<code class="literal">vol0</code>) has been created and started, it is time for a GlusterFS client to mount the newly created volume. </p><p>To keep this recipe simple, <code class="literal">gluster1</code> is used as the client. However, any computer on the local network with the GlusterFS client software installed should now be able to mount the distributed volume (<code class="literal">vol0</code>) from any trusted peer in the GlusterFS storage pool.</p><p>The <code class="literal">mount –t glusterfs</code> command is used from <code class="literal">gluster1</code> to mount the distributed volume (<code class="literal">vol0</code>) from the trusted storage peer <code class="literal">gluster1.local</code> on its local directory, <code class="literal">/mnt</code>. The <a class="indexterm" id="id741"/>Raspberry Pi named <code class="literal">gluster1</code> is both the client and the server of the distributed volume (<code class="literal">vol0</code>).</p></div><div class="section" title="Testing the striped replicated volume"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec112"/>Testing the striped replicated volume</h3></div></div></div><p>The <code class="literal">cp</code> command is<a class="indexterm" id="id742"/> used to copy a large file from the local filesystem (<code class="literal">/boot/kernel.img</code>) to the distributed striped replicated volume mounted at <code class="literal">/mnt</code>.</p><p>The <code class="literal">ls –l</code> command and the <code class="literal">sha1sum</code> command are used to validate that the copied file (<code class="literal">/mnt/kernel.img</code>) has been copied successfully by checking that its size (<code class="literal">4056224</code>) and checksum (<code class="literal">d5e64…35ec</code>) are the same as the original file (<code class="literal">/boot/kernel.img</code>).</p><p>The <code class="literal">ls –la</code> command is used on each peer of the storage pool (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>) to display the contents of the peer's storage directory (<code class="literal">/srv/vol0</code>).</p><p>None of the peer's storage directories has a file (<code class="literal">/srv/kernel.img</code>) as large as the original file (<code class="literal">/boot/kernel.img</code>).</p><div class="section" title="Replication"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec16"/>Replication</h4></div></div></div><p>There <a class="indexterm" id="id743"/>are two sets of storage files (<code class="literal">/srv/kernel.img</code>) with the same size. The first set of storage files with the same size (<code class="literal">2090144</code>) can be found on peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code>. The second set of peers, <code class="literal">gluster3</code> and <code class="literal">gluster4</code>, also have storage files that are the same size (<code class="literal">1966080</code>). The peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code> are replicas of each other; <code class="literal">gluster3</code> and <code class="literal">gluster4</code> are also replicas.</p><p>Data replication is used to keep the distributed volume highly available. If one of the trusted storage peers goes down or is disconnected from the network, a replica of the unavailable peer's stored data is still available. If <code class="literal">gluster1</code> were to go down, <code class="literal">gluster2</code> would still have a replica of the stored data. If <code class="literal">gluster4</code> were disconnected from the network, <code class="literal">gluster3</code> would still have a replica of its data.</p></div></div><div class="section" title="Striping"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec113"/>Striping</h3></div></div></div><p>The sum<a class="indexterm" id="id744"/> of the two different file sizes (<code class="literal">2090144</code> and <code class="literal">1966080</code>) equals the size of the original file (<code class="literal">4056224</code>). The original file has been distributed (striped) across the trusted storage peers. The trusted storage peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code> are replicating one part (<code class="literal">2090144</code>) of the large file (<code class="literal">kernel.img</code>), and the trusted peers <code class="literal">gluster3</code> and <code class="literal">gluster4</code> are storing replicas of the other part (<code class="literal">1966080</code>).</p><p>Data striping is a technique for distributing large files across multiple storage peers. Parts of the file (stripes) are distributed evenly across the striped storage peers so that sequentially reading (or writing) a large amount of data from a single large file does not continuously put a load on only one of the trusted storage peers. Striping a file distributes the load to access<a class="indexterm" id="id745"/> the file across the striped storage peers by distributing the data across the peers. Striping increases the data transfer rate of the distributed volume.</p></div><div class="section" title="Testing the high availability of the cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec114"/>Testing the high availability of the cluster</h3></div></div></div><p>In order to <a class="indexterm" id="id746"/>test the cluster's ability to remain available when one of the trusted data storage peers goes down, the trusted data storage peer <code class="literal">gluster4.local</code> is removed from the cluster by disconnecting its network cable.</p><p>After the network cable has been removed from <code class="literal">gluster4</code>, the <code class="literal">gluster peer status</code> command is used (on any remaining peer) to show that trusted storage peer <code class="literal">gluster4.local</code> has been <code class="literal">Disconnected</code> from the cluster.</p><p>The <code class="literal">sha1sum</code> command is used to validate that the checksum (<code class="literal">d5e64…35ec</code>) of the distributed file (<code class="literal">/mnt/kernel.img</code>) still matches the checksum of the original file (<code class="literal">/boot/kernel.img</code>).</p><p>The GlusterFS distributed filesystem still functions properly when one peer is removed from the storage pool! The cluster is highly available!</p></div><div class="section" title="Testing the healing of replicated peers"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec115"/>Testing the healing of replicated peers</h3></div></div></div><p>While the<a class="indexterm" id="id747"/> trusted storage peer <code class="literal">gluster4</code> is still disconnected from the cluster, the <code class="literal">cp</code> command is used to copy another large file (<code class="literal">/boot/kernel7.img</code>) to the distributed storage volume (<code class="literal">vol0</code>) mounted locally on <code class="literal">gluster1</code> at <code class="literal">/mnt</code>.</p><p>The checksum of the copied file (<code class="literal">/mnt/kernel7.img</code>) is compared to the checksum of the original file (<code class="literal">/boot/kernel7.img</code>) using the <code class="literal">sha1sum</code> command. The files are identical.</p><p>The <code class="literal">ls –la</code> command is used on each of the remaining trusted storage peers (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, and <code class="literal">gluster3</code>) to validate that the new large file (<code class="literal">kermel7.img</code>) has also been striped and replicated across the storage directories (<code class="literal">/srv/vol0</code>) of the distributed volume. The trusted storage peers <code class="literal">gluster1</code> and <code class="literal">gluster2</code> have replicas of one portion of the file while peer <code class="literal">gluster3</code> has the other portion of the file.</p><p>After the new large file has been striped and replicated across the distributed volume, the trusted peer <code class="literal">gluster4</code> is once again connected to the cluster.</p><p>Immediately after the peer <code class="literal">gluster4</code> has been reconnected to the cluster, the <code class="literal">ls –la</code> command is used to display the contents of the data storage directory (<code class="literal">/srv/vol0</code>) on <code class="literal">gluster4</code>. The file copied to the distributed volume (<code class="literal">vol0</code>) while <code class="literal">gluster4</code> was disconnected from the cluster (<code class="literal">kernel7.img</code>) has been created in the data storage directory; however, the file is empty (<code class="literal">0</code>).</p><p>After waiting a few minutes for the GlusterFS healing service to finish replicating the striped portion of the new large file (<code class="literal">kernel7.img</code>) from peer <code class="literal">gluster3</code> to peer <code class="literal">gluster4</code>, the <code class="literal">ls –la</code> command is used once again to validate that peer <code class="literal">gluster4</code> has replicated its portion of the striped file and that the distributed volume (<code class="literal">vol0</code>) has been healed.</p><p>The four Raspberry Pis are now a self-healing highly available distributed filesystem.</p></div></div><div class="section" title="There's more …"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec237"/>There's more …</h2></div></div></div><p>GlusterFS is a peer-based distributed filesystem. There is no master server in a GlusterFS trusted storage pool. In this recipe, <code class="literal">gluster1</code> was the first peer in the trusted storage pool and invited the other trusted peers to join the pool. Even though it was the first peer in the storage pool, <code class="literal">gluster1</code> is still an equal peer and not the master.</p><p>On the<a class="indexterm" id="id748"/> other hand, the current recipe only allows a GlusterFS client to mount a filesystem endpoint from one of the trusted storage pool peers (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, or <code class="literal">gluster4</code>). In this recipe, <code class="literal">gluster1</code> was the peer providing the distributed filesystem endpoint. </p><p>Should the mounted peer (<code class="literal">gluster1</code>) go down, the client would not be able to access the distributed filesystem even if the other peers in the cluster have kept the filesystem available. In the recipe, <code class="literal">gluster1</code> was also the client, so this issue was not possible.</p><p>A GlusterFS distributed filesystem is normally accessed from outside the cluster, not from a trusted storage peer within the cluster. Mounting the distributed filesystem from one of the storage peers directly defeats the high availability of the cluster by making the client dependent on a single trusted peer instead of being dependent on the cluster as a whole.</p><div class="section" title="Using Keepalived to create a virtual filesystem endpoint"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec116"/>Using Keepalived to create a virtual filesystem endpoint</h3></div></div></div><p>The <a class="indexterm" id="id749"/>virtual IP address service <a class="indexterm" id="id750"/>Keepalived can be used to create a distributed filesystem endpoint that is kept alive by all peers of the trusted storage pool. The previous recipe, <span class="emphasis"><em>Installing a high-availability load balancer</em></span>, shows how to install and configure Keepalived for use with the HA Proxy.</p><p>Keepalived can also be used with GlusterFS to create a virtual IP for accessing the distributed filesystem that will remain available so long as the distributed filesystem remains available.</p><p>Using Keepalived, a virtual endpoint (IP address) is created for the distributed filesystem. Clients will mount the virtual endpoint of the filesystem instead of mounting the endpoint directly from a trusted storage peer. </p><p>Even though the trusted storage peer currently serving the virtual filesystem endpoint may fail, the virtual endpoint provided by Keepalived will not fail; instead, another trusted storage peer will be selected to replace the peer that did fail. The virtual filesystem endpoint will remain alive no matter which trusted peer goes down.</p><p>To use Keepalived with this recipe, first enable (<code class="literal">=1</code>) the kernel parameter that permits listening for virtual IP addresses (<code class="literal">net.ipv4.ip_nonlocal_bind</code>) on each of the trusted storage peers in the cluster (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, and <code class="literal">gluster4</code>).</p><div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo -i bash</strong></span>

root@gluster1:~# <span class="strong"><strong>echo "net.ipv4.ip_nonlocal_bind=1" &gt;&gt;/etc/sysctl.conf</strong></span>

root@gluster1:~# <span class="strong"><strong>sysctl -p</strong></span>

kernel.printk = 3 4 1 3
vm.swappiness = 1
vm.min_free_kbytes = 8192
net.ipv4.ip_nonlocal_bind = 1

root@gluster1:~# <span class="strong"><strong>exit</strong></span>

exit

pi@gluster1 ~ $ </pre></div><p>Next, install the <code class="literal">keepalived</code> software distribution package using the <code class="literal">apt-get install</code> command.</p><div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo apt-get install -y keepalived</strong></span>

Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following extra packages will be installed:
  iproute ipvsadm libpci3 libperl5.20 libsensors4 libsnmp-base libsnmp30
Suggested packages:
  heartbeat ldirectord lm-sensors snmp-mibs-downloader
The following NEW packages will be installed:
  iproute ipvsadm keepalived libpci3 libperl5.20 libsensors4 libsnmp-base libsnmp30
0 upgraded, 8 newly installed, 0 to remove and 0 not upgraded.
Need to get 3,902 kB of archives.
After this operation, 7,093 kB of additional disk space will be used.

...</pre></div><p>Use a<a class="indexterm" id="id751"/> configuration for Keepalived that<a class="indexterm" id="id752"/> allows any of the trusted storage peers (<code class="literal">gluster1</code>, <code class="literal">gluster2</code>, <code class="literal">gluster3</code>, or <code class="literal">gluster4</code>) to take over the cluster's virtual IP address whenever the current peer serving the virtual IP address fails.</p><div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo bash</strong></span>

root@gluster1:/home/pi# <span class="strong"><strong>cat &lt;&lt;EOD &gt;/etc/keepalived/keepalived.conf

vrrp_script chk_glusterd 
{
        script "killall -0 glusterd"
        interval 2
        weight 2
}

vrrp_instance VI_1 
{
        interface eth0
        state MASTER
        virtual_router_id 13

        priority 100

        virtual_ipaddress 
        {
            192.168.2.100
        }

        track_script 
        {
            chk_glusterd
        }
}

EOD</strong></span>

root@gluster1:/home/pi# exit

exit

pi@gluster1 ~ $ </pre></div><p>Finally, restart<a class="indexterm" id="id753"/> the Keepalived <a class="indexterm" id="id754"/>service.</p><div class="informalexample"><pre class="programlisting">pi@gluster1 ~ $ <span class="strong"><strong>sudo systemctl restart keepalived.service</strong></span> 

pi@gluster1 ~ $ </pre></div><p>Now, the virtual endpoint can be mounted instead of a trusted peer.</p><div class="informalexample"><pre class="programlisting">pi@gluster4 ~ $ <span class="strong"><strong>sudo mount -t glusterfs 192.168.2.100:/vol0 /mnt</strong></span>

pi@gluster4 ~ $ <span class="strong"><strong>ls -l /mnt</strong></span>
total 7900
-rwxr-xr-x 1 root root 4032544 Nov  1 00:47 kernel7.img
-rwxr-xr-x 1 root root 4056224 Oct 31 23:37 kernel.img

pi@gluster4 ~ $ </pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec238"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>GlusterFS</strong></span> (<a class="ulink" href="http://www.glusterfs.org/">http://www.glusterfs.org/</a>): GlusterFS is a scalable network<a class="indexterm" id="id755"/> filesystem. Using common off-the-shelf hardware, you <a class="indexterm" id="id756"/>can create large, distributed storage solutions for media streaming, data analysis, and other data and bandwidth-intensive tasks. GlusterFS is free and open source software.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>GlusterFS</strong></span> (<a class="ulink" href="https://en.wikipedia.org/wiki/GlusterFS">https://en.wikipedia.org/wiki/GlusterFS</a>): This Wikipedia <a class="indexterm" id="id757"/>article describes the GlusterFS design.</li></ul></div></div></div>
<div class="section" title="Creating a supercomputer"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec64"/>Creating a supercomputer</h1></div></div></div><p>This <a class="indexterm" id="id758"/>recipe turns four Raspberry Pis into a super computer using Apache Spark. </p><p>Apache Spark is a fast and general engine for large-scale data processing. In this recipe, Apache Spark is installed on four Raspberry Pis that have been networked into a small computer cluster. The <a class="indexterm" id="id759"/>cluster is then used to demonstrate the speed of super computing by calculating the value of pi using a Monte Carlo algorithm.</p><p>After completing this recipe, you will have a Raspberry Pi super computer.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec239"/>Getting ready </h2></div></div></div><p>The following ingredients are required to create a supercomputer:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Four basic networking setups for the Raspberry Pi</li><li class="listitem" style="list-style-type: disc">A high-speed network switch</li></ul></div><p>This recipe does not require the desktop GUI and could either be run from the text-based console or from within LXTerminal.</p><p>With the Secure Shell server running on each Raspberry Pi, this recipe can be completed remotely using a Secure Shell client. Typically, a website is managed remotely.</p><p>All the Raspberry Pis should be connected directly to the same network switch.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec240"/>How to do it...</h2></div></div></div><p>Perform the following steps to build a Raspberry Pi supercomputer:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to each Raspberry Pi and set its hostname. One Raspberry Pi will be the Spark master server, and the other three will be Spark slaves. Name the four Raspberry Pis <code class="literal">spark-master</code>, <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>.</li><li class="listitem">Now, let's set up secure communication between master and slaves. Use the <code class="literal">ssh-keygen</code> command on <code class="literal">spark-master</code> to generate a pair of SSH keys. Press <code class="literal">&lt;enter&gt;</code> to accept the default file location (<code class="literal">/home/pi/.ssh/id_rsa</code>). Then, press <code class="literal">&lt;enter&gt;</code> twice to use an empty passphrase (the Spark automation requires an empty passphrase).<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>ssh-keygen</strong></span>

Generating public/private rsa key pair.
Enter file in which to save the key (/home/pi/.ssh/id_rsa): 

Enter passphrase (empty for no passphrase): 

Enter same passphrase again: 

Your identification has been saved in /home/pi/.ssh/id_rsa.
Your public key has been saved in /home/pi/.ssh/id_rsa.pub.
The key fingerprint is:
29:0e:95:61:a6:e6:30:8f:23:66:cd:68:d3:c4:0c:8e pi@spark-master

The key's randomart image is:
+---[RSA 2048]----+
| .    +          |
|o +  + o         |
|E.o+o o          |
|  *B .   .       |
|.*o++ . S        |
|+... o .         |
|      .          |
|                 |
|                 |
+-----------------+

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">ssh-copy-id</code> command to copy the newly created public key from <code class="literal">spark-master</code> to each of the Spark slaves (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c)</code>.<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>ssh-copy-id pi@spark-slave-a.local</strong></span>

The authenticity of host 'spark-slave-a.local (192.168.2.6)' can't be established.
ECDSA key fingerprint is e9:55:ff:6c:69:be:5d:8f:80:de:b2:d9:85:eb:1b:90.

Are you sure you want to continue connecting (yes/no)? <span class="strong"><strong>yes</strong></span>

/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys

pi@spark-slave-a.local's password: 

Number of key(s) added: 1</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note44"/>Note</h3><p>Repeat step <span class="emphasis"><em>3</em></span> for each of the slaves: <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>.</p></div></div></li><li class="listitem">Note<a class="indexterm" id="id760"/> that a secure shell login (<code class="literal">ssh</code>) from <code class="literal">spark-master</code> to the slaves no longer requires a password for authentication:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ ssh spark-slave-a.local

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
Last login: Mon Nov  2 21:29:28 2015 from 192.168.2.1

pi@spark-slave-a ~ $ </pre></div></li><li class="listitem">Now, it's about downloading the Apache Spark software distribution. Use a web browser to locate the correct Apache Spark software distribution package<a class="indexterm" id="id761"/> on the Apache Spark website's download page (<a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>), which is shown in the following screenshot:<div class="mediaobject"><img alt="How to do it..." src="graphics/B04745_07_03.jpg"/></div></li><li class="listitem">On the <a class="indexterm" id="id762"/>download page, use the following drop-down options:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Choose a Spark release:</strong></span> <span class="strong"><strong>1.5.1 (Oct 02 2015)</strong></span></li><li class="listitem"><span class="strong"><strong>Choose a package type:</strong></span> Pre-built for Hadoop 2.6 and later</li><li class="listitem"><span class="strong"><strong>Choose a download type</strong></span>: Select Apache Mirror</li></ol></div><p>Once the correct choices have been made for <span class="strong"><strong>1</strong></span>, <span class="strong"><strong>2</strong></span>, and <span class="strong"><strong>3</strong></span>, click on the link (<code class="literal">spark-1.5.1-bin-haddop2.6.tgz</code>) that appears at <span class="strong"><strong>4</strong></span>. Download Spark.</p><div class="mediaobject"><img alt="How to do it..." src="graphics/B04745_07_04.jpg"/></div></li><li class="listitem">Note<a class="indexterm" id="id763"/> that the next web page displays the actual download link for the correct Apache Spark software distribution package (<a class="ulink" href="http://www.eu.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz">http://www.eu.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz</a>).</li><li class="listitem">Use <a class="indexterm" id="id764"/>the <code class="literal">wget</code> command on <code class="literal">spark-master</code> to download the Apache Spark software distribution page, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>wget http://www.eu.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz</strong></span>

--2015-11-05 17:41:01--  http://www.eu.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz
Resolving www.eu.apache.org (www.eu.apache.org)... 88.198.26.2, 2a01:4f8:130:2192::2
Connecting to www.eu.apache.org (www.eu.apache.org)|88.198.26.2|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 280901736 (268M) [application/x-gzip]
Saving to: 'spark-1.5.1-bin-hadoop2.6.tgz'

spark-1.5.1-bin-hadoop2.6.tgz   100%[============&gt;] 267.89M   726KB/s   in 4m 38s 

2015-11-05 17:45:39 (987 KB/s) - 'spark-1.5.1-bin-hadoop2.6.tgz' saved [280901736/280901736]

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">tar</code> command to unpack the Apache Spark software distribution on each Raspberry Pi (<code class="literal">spark-master</code>, <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>), as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>scp spark-1.5.1-bin-hadoop2.6.tgz spark-slave-a.local:.</strong></span>

spark-1.5.1-bin-hadoop2.6.tgz                       100%  268MB   4.4MB/s   01:01    

pi@spark-master ~ $ <span class="strong"><strong>scp spark-1.5.1-bin-hadoop2.6.tgz spark-slave-b.local:.</strong></span>

spark-1.5.1-bin-hadoop2.6.tgz                       100%  268MB   4.0MB/s   01:07    

pi@spark-master ~ $ <span class="strong"><strong>scp spark-1.5.1-bin-hadoop2.6.tgz spark-slave-c.local:.</strong></span>

spark-1.5.1-bin-hadoop2.6.tgz                       100%  268MB   5.2MB/s   00:47    

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">tar</code> command to unpack the Apache Spark software distribution on each Raspberry Pi (<code class="literal">spark-master</code>, <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>), as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>tar xvfz spark-1.5.1-bin-hadoop2.6.tgz</strong></span> 

spark-1.5.1-bin-hadoop2.6/
spark-1.5.1-bin-hadoop2.6/NOTICE
spark-1.5.1-bin-hadoop2.6/CHANGES.txt
spark-1.5.1-bin-hadoop2.6/python/
spark-1.5.1-bin-hadoop2.6/python/run-tests.py
spark-1.5.1-bin-hadoop2.6/python/test_support/
spark-1.5.1-bin-hadoop2.6/python/test_support/userlibrary.py
spark-1.5.1-bin-hadoop2.6/python/test_support/userlib-0.1.zip
spark-1.5.1-bin-hadoop2.6/python/test_support/sql/
spark-1.5.1-bin-hadoop2.6/python/test_support/sql/people.json
spark-1.5.1-bin-hadoop2.6/python/test_support/sql/orc_partitioned/
spark-1.5.1-bin-hadoop2.6/python/test_support/sql/orc_partitioned/b=1/
spark-1.5.1-bin-hadoop2.6/python/test_support/sql/orc_partitioned/b=1/c=1/

...</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>Repeat step <span class="emphasis"><em>10</em></span> on each Raspberry Pi, namely <code class="literal">spark-master</code>, <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>.</p></div></div></li><li class="listitem">Use <a class="indexterm" id="id765"/>the <code class="literal">mv</code> command to move the Apache Spark installation directory (<code class="literal">spark-1.5.1-bin-hadoop2.6</code>) to a more convenient location on each Raspberry Pi (<code class="literal">/opt/spark</code>), as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>sudo mv spark-1.5.1-bin-hadoop2.6 /opt/spark</strong></span>

pi@spark-master ~ $ </pre></div></li><li class="listitem">Now, configure the Spark master. Use the <code class="literal">cat</code> command on <code class="literal">spark-master</code> to create a list of slaves, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~/ $ <span class="strong"><strong>cat &lt;&lt;EOD &gt;/opt/spark/conf/slaves

spark-slave-a.local
spark-slave-b.local
spark-slave-c.local

EOD</strong></span>

pi@spark-master ~/ $ </pre></div></li><li class="listitem"> Use the <code class="literal">scp</code> command on <code class="literal">spark-master</code> to copy the Spark execution environment configuration file (<code class="literal">spark-env.sh</code>) to each Spark slave (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>), as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>echo "SPARK_MASTER_IP=`hostname -I`" &gt;/opt/spark/conf/spark-env.sh</strong></span></pre></div></li><li class="listitem">Use the <code class="literal">scp</code> command on <code class="literal">spark-master</code> to copy the Spark execution environment configuration file (<code class="literal">spark-env.sh</code>) to each Spark slave (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>), as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>scp /opt/spark/conf/spark-env.sh spark-slave-a:/opt/spark/conf/spark-env.sh</strong></span>

spark-env.sh                                  100%   30     0.0KB/s   00:00    

pi@spark-master ~ $ <span class="strong"><strong>scp /opt/spark/conf/spark-env.sh spark-slave-b.local:/opt/spark/conf/spark-env.sh</strong></span>

spark-env.sh                                  100%   30     0.0KB/s   00:00    

pi@spark-master ~ $ <span class="strong"><strong>scp /opt/spark/conf/spark-env.sh spark-slave-c.local:/opt/spark/conf/spark-env.sh</strong></span>

spark-env.sh                                  100%   30     0.0KB/s   00:00    

pi@spark-master ~ $ </pre></div></li><li class="listitem"> Use the <code class="literal">echo</code> command on <code class="literal">spark-master</code> to append an additional memory constraint (<code class="literal">SPARK_DRIVER_MEMORY=512m</code>) to the execution environment (<code class="literal">spark-env.sh</code>) of the Spark master server (<code class="literal">spark-master</code>) so that enough memory remains free on the master server to run spark jobs, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ echo "SPARK_DRIVER_MEMORY=512m" &gt;&gt;/opt/spark/conf/spark-env.sh

pi@spark-master ~ $ </pre></div></li><li class="listitem"> Use the <code class="literal">echo</code> command on <code class="literal">spark-master</code> to append the local IP address (<code class="literal">SPARK_LOCAL_IP</code>) to the execution environment (<code class="literal">spark-env.sh</code>). This reduces the warnings in the output from Spark jobs:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>echo "SPARK_LOCAL_IP=$(hostname -I)" &gt;&gt;/opt/spark/conf/spark-env.sh</strong></span>

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use<a class="indexterm" id="id766"/> the <code class="literal">sed</code> command to change the logging level of Spark jobs from <code class="literal">INFO</code> (which produces a lot of informational output) to <code class="literal">WARN</code> (which produces a lot less output).<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>sed 's/rootCategory=INFO/rootCategory=WARN/' 
spark/conf/log4j.properties.template &gt;/opt/spark/conf/log4j.properties</strong></span>

pi@spark-master ~ $ </pre></div></li><li class="listitem">At this point, the Spark cluster is ready to start.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note46"/>Note</h3><p>The next steps calculate pi both with and without the Spark cluster so that the duration of the two calculation methods can be compared.</p></div></div></li><li class="listitem">Now, calculate pi without using the Spark cluster. Use the <code class="literal">cat</code> command on <code class="literal">spark-master</code> to create a simple Python script to calculate pi without using the Spark cluster, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>cat &lt;&lt;EOD &gt;pi.py

from operator import add
from random   import random
from time     import clock


MSG  = "Python estimated Pi at %f in %f seconds"

n = 1000000


def f(_):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &lt; 1 else 0


def main():
    st = clock()
    tries = map( f, range( 1, n + 1 ) )
    count = reduce( add, tries )
    et = clock()
    print( MSG % ( 4.0 * count / n, et - st ) )    


if __name__ == "__main__":
    main()


EOD</strong></span>

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use the <code class="literal">python</code> command on <code class="literal">spark-master</code> to run the script for calculating pi (<code class="literal">pi.py</code>) without a Spark cluster.<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>python pi.py</strong></span> 

Python esitmated PI at 3.141444 in 13.430613 seconds

pi@spark-master ~ $</pre></div></li><li class="listitem">Note<a class="indexterm" id="id767"/> that it took one Raspberry Pi (<code class="literal">spark-master</code>) more than 13 seconds (<code class="literal">13.430613</code> seconds) to calculate pi without using Spark.</li><li class="listitem">Now, calculate pi using the Spark cluster. Use the <code class="literal">cat</code> command on <code class="literal">spark-master</code> to create a simple Python script that parallelizes the calculation of pi for use on the Spark cluster, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>cat &lt;&lt;EOD &gt;pi-spark.py

from __future__ import print_function

from operator   import add
from random     import random
from sys        import argv
from time       import clock

from pyspark    import SparkConf, SparkContext


APP_NAME = "MonteCarloPi"
MSG      = "Spark estimated Pi at %f in %f seconds using %i partitions"

master     =      argv[ 1 ]   if len( argv ) &gt; 1 else "local"
partitions = int( argv[ 2 ] ) if len( argv ) &gt; 2 else 2

n = 1000000


def f(_):
    x = random() * 2 - 1
    y = random() * 2 - 1
    return 1 if x ** 2 + y ** 2 &lt; 1 else 0


def main(sc):
    st    = clock()
    tries = sc.parallelize( range( 1, n + 1 ), partitions ).map( f )
    count = tries.reduce( add )
    et    = clock()
    print( MSG % ( 4.0 * count / n, et - st, partitions ) )    


if __name__ == "__main__":
    conf = SparkConf()
    conf.setMaster( master )
    conf.setAppName( APP_NAME )
    sc = SparkContext( conf = conf )
    main( sc )
    sc.stop()


EOD</strong></span>

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use<a class="indexterm" id="id768"/> the <code class="literal">start-all.sh</code> shell script on <code class="literal">spark-master</code> to start the Apache Spark cluster. Starting the cluster may take 30 seconds.<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>/opt/spark/sbin/start-all.sh</strong></span>

starting org.apache.spark.deploy.master.Master, logging to /home/pi/spark/sbin/../logs/spark-pi-org.apache.spark.deploy.master.Master-1-spark-master.out
spark-slave-c.local: starting org.apache.spark.deploy.worker.Worker, logging to /home/pi/spark/sbin/../logs/spark-pi-org.apache.spark.deploy.worker.Worker-1-spark-slave-c.out
spark-slave-b.local: starting org.apache.spark.deploy.worker.Worker, logging to /home/pi/spark/sbin/../logs/spark-pi-org.apache.spark.deploy.worker.Worker-1-spark-slave-b.out
spark-slave-a.local: starting org.apache.spark.deploy.worker.Worker, logging to /home/pi/spark/sbin/../logs/spark-pi-org.apache.spark.deploy.worker.Worker-1-spark-slave-a.out

pi@spark-master ~ $ </pre></div></li><li class="listitem">Use a web browser to view the status of the cluster by browsing to the cluster status page at <code class="literal">http://spark-master.local:8080/</code>.<div class="mediaobject"><img alt="How to do it..." src="graphics/B04745_07_05.jpg"/></div></li><li class="listitem">Wait<a class="indexterm" id="id769"/> until the Spark master server and all three slaves have started. Three worker IDs will be displayed on the status page when the cluster is ready to compute. Refresh the page, if necessary.</li><li class="listitem">Submit the Python script (<code class="literal">pi-spark.py</code>) that is used to calculate pi to the Spark cluster, as follows:<div class="informalexample"><pre class="programlisting">pi@spark-master ~ $ <span class="strong"><strong>export SPARK_MASTER_URL="http://$(hostname –I | tr –d [:space:]):7077"</strong></span>

pi@spark-master ~ $ <span class="strong"><strong>export PATH=/opt/spark/bin:$PATH</strong></span>


pi@spark-master ~ $ <span class="strong"><strong>spark-submit pi-spark.py $SPARK_MASTER_URL 24</strong></span>

15/11/04 21:39:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/04 21:39:51 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
[Stage 0:&gt;                                                         (0 + 0) / 24]15/11/04 21:40:00 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes
15/11/04 21:40:05 WARN TaskSetManager: Stage 0 contains a task of very large size (122 KB). The maximum recommended task size is 100 KB.

Spark esitmated Pi at 3.143368 in 0.720023 seconds using 24 partitions          

pi@spark-master ~ $ </pre></div></li><li class="listitem">Notice that it took the Spark cluster less than a second (<code class="literal">0.720023 seconds</code>) to calculate pi. That's more than 185 times faster!!</li><li class="listitem">The Raspberry Pi super computer is working!</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec241"/>How it works...</h2></div></div></div><p>This <a class="indexterm" id="id770"/>recipe has the following six parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Setting up secure communication between the master and slaves</li><li class="listitem" style="list-style-type: disc">Downloading the Apache Spark software distribution</li><li class="listitem" style="list-style-type: disc">Installing Apache Spark on each Raspberry Pi in the cluster</li><li class="listitem" style="list-style-type: disc">Configuring the Spark master</li><li class="listitem" style="list-style-type: disc">Calculating pi without using the Spark cluster</li><li class="listitem" style="list-style-type: disc">Calculating pi using the Spark cluster</li></ul></div><p>The recipe begins by setting the hostnames of the four Raspberry Pi computers. One Raspberry Pi is selected as the Spark master (<code class="literal">spark-master</code>), the other three Raspberry Pis are the Spark slaves (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><div class="section" title="Setting up secure communication between master and slaves"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec117"/>Setting up secure communication between master and slaves</h3></div></div></div><p>After the hostnames<a class="indexterm" id="id771"/> have been set, the <code class="literal">ssh-keygen</code> and <code class="literal">ssh-copy-id</code> commands are used to establish a secure communication link between the Spark master (<code class="literal">spark-master</code>) and each of its slaves (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><p>The <code class="literal">ssh-keygen</code> command is used to create a secure key pair (<code class="literal">/home/pi/.ssh/id_rsa</code> and <code class="literal">/home/pi/.ssh/id_rsa.pub</code>). The <code class="literal">ssh-copy-id</code> command is used to copy the public key (<code class="literal">id_rsa.pub</code>) from <code class="literal">spark-master</code> to each of the slaves.</p><p>After the public key of <code class="literal">spark-master</code> has been copied to each slave, it is possible to log in from <code class="literal">spark-master</code> to each slave without using a password. Having a secure login from a master to a slave without a password is a requirement for the automation of the startup of the cluster.</p></div><div class="section" title="Downloading the Apache Spark software distribution"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec118"/>Downloading the Apache Spark software distribution</h3></div></div></div><p>The<a class="indexterm" id="id772"/> Apache Spark <a class="indexterm" id="id773"/>download page (<a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>) presents a number of choices that are used to determine the correct software distribution.</p><p>This recipe uses the 1.5.1 (Oct 02 2015) release of Spark that has been pre-built for Hadoop 2.6 and later. Once the correct choices have been made, a link is presented (<code class="literal">spark-1.5.1-bin-hadoop2.6.tgz</code>), which leads to the actual download page.</p><p>The <code class="literal">wget</code> command is<a class="indexterm" id="id774"/> used to download the Spark software distribution from the actual download page to <code class="literal">spark-master</code> using the link presented on the actual download page (<a class="ulink" href="http://www.us.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz">http://www.us.apache.org/dist/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz</a>).</p><p>The<a class="indexterm" id="id775"/> software distribution has a size of 280 MB. It will take a while to download.</p><p>Once the Spark software distribution (<code class="literal">spark-1.5.1-bin-hadoop2.6.tgz</code>) is downloaded to <code class="literal">spark-master</code>, it is then copied using the <code class="literal">scp</code> command to the three slaves (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p></div><div class="section" title="Installing Apache Spark on each Raspberry Pi in the cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec119"/>Installing Apache Spark on each Raspberry Pi in the cluster</h3></div></div></div><p>The <code class="literal">tar</code> command is use to unpack the Apache Spark software distribution (<code class="literal">spark-1.5.1-bin-hadoop2.6.tgz</code>) on each Raspberry Pi in the cluster (<code class="literal">spark-master</code>, <code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><p>After <a class="indexterm" id="id776"/>the software distribution has been unpacked in the home directory of the user, <code class="literal">pi</code>, it is moved by using the <code class="literal">mv</code> command to a more central location (<code class="literal">/opt/spark</code>).</p></div><div class="section" title="Configuring the Spark master"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec120"/>Configuring the Spark master</h3></div></div></div><p>The <code class="literal">cat</code> command is<a class="indexterm" id="id777"/> used to create a list of slaves (<code class="literal">/opt/spark/conf/slaves</code>). This list is used during the cluster startup to automatically start the <a class="indexterm" id="id778"/>slaves when <code class="literal">spark-master</code> is started. All the lines after the <code class="literal">cat</code> command up to the <span class="strong"><strong>end-of-data</strong></span> (<span class="strong"><strong>EOD</strong></span>) mark are copied to the list of slaves.</p><p>The <code class="literal">echo</code> command is used to create the Spark runtime environment file (<code class="literal">spark-env.sh</code> under <code class="literal">/opt/spark/conf/</code>) with one environment variable (<code class="literal">SPARK_MASTER_IP</code>) that is set to the IP address of <code class="literal">spark-master</code> (<code class="literal">hostname –I</code>).</p><p>The Spark runtime environment configuration file, <code class="literal">spark-env.sh</code>, is then copied from the <code class="literal">spark-master</code> to each slave (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><p>After the configuration file (<code class="literal">spark-env.sh</code>) has been copied to the slaves, two additional configuration parameters specific to <code class="literal">spark-master</code> are added to the file.</p><p>The <code class="literal">echo</code> command is used to append (<code class="literal">&gt;&gt;</code>) the <code class="literal">SPARK_DRIVER_MEMORY</code> parameter to the bottom of the configuration file. This parameter is used to limit the amount of memory used by the <code class="literal">spark-master</code> to <code class="literal">512m</code> (512 MB). This leaves room in the <code class="literal">spark-master</code> memory pool to run the Spark jobs.</p><p>The <code class="literal">echo</code> command is also used to append the <code class="literal">SPARK_LOCAL_IP</code> parameter to the bottom of the configuration file (<code class="literal">spark-env.sh</code>). This parameter is set to the IP address of the <code class="literal">spark-master</code> (<code class="literal">hostname –I</code>). Setting this parameter eliminates some of the warning messages that occur when running the Spark jobs.</p><p>The <code class="literal">sed</code> command is<a class="indexterm" id="id779"/> used to modify the logging parameters of <code class="literal">spark-master</code>. The <code class="literal">log4j.properties</code> file is changed so that <code class="literal">INFO</code> messages are no longer displayed. Only warning messages (<code class="literal">WARN</code>) and error messages are displayed. This greatly reduces the output of the Spark jobs.</p><p>At this point, the Spark cluster is fully configured and ready to start.</p></div><div class="section" title="Calculating pi without using the Spark cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec121"/>Calculating pi without using the Spark cluster</h3></div></div></div><p>Before <a class="indexterm" id="id780"/>the Spark cluster is started, a simple Python script (<code class="literal">pi.py</code>) is created using the <code class="literal">cat</code> command to calculate pi without using the Spark cluster.</p><p>This script (<code class="literal">pi.py</code>) uses the Monte Carlo method to estimate the value of pi by randomly generating 1 million data points and testing each data point for inclusion in a circle. The ratio of points inside the circle to the total number of points will be approximately equal to <span class="emphasis"><em>Pi/4</em></span>.</p><p>More <a class="indexterm" id="id781"/>information on calculating the value of Pi, including how to use the Monte Carlo method, can be found on Wikipedia (<a class="ulink" href="https://en.wikipedia.org/wiki/Pi">https://en.wikipedia.org/wiki/Pi</a>).</p><p>The Python script that is used to estimate the value of pi takes more than 13 seconds to run on a single standalone Raspberry Pi.</p></div><div class="section" title="Calculating pi using the Spark cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec122"/>Calculating pi using the Spark cluster</h3></div></div></div><p>Another <a class="indexterm" id="id782"/>Python script (<code class="literal">pi-spark.py</code>) is created<a class="indexterm" id="id783"/> using the <code class="literal">cat</code> command.</p><p>This new script (<code class="literal">pi-spark.py</code>) uses the same Monte Carlo method to estimate the value of pi using 1 million random data points. However, this script uses the <code class="literal">SparkContext</code> (<code class="literal">sc</code>) to <a class="indexterm" id="id784"/>create a <span class="strong"><strong>resilient distributed dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>) that parallelizes the million data points (<code class="literal">range( 1, n + 1 )</code>) so that they can be distributed among the slaves for the actual calculation (<code class="literal">f</code>).</p><p>After the script is created, the Spark cluster is started (<code class="literal">/opt/spark/sbin/start-all.sh</code>). The startup script (<code class="literal">start-all.sh</code>) uses the contents of the <code class="literal">/opt/conf/slaves</code> file to locate and start the Spark slaves (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><p>A web browser is used to validate that all the slaves have started properly. The <code class="literal">spark-master</code> produces a small website (<code class="literal">http://spark-master.local:8080/</code>) that displays the status of the cluster. The Spark cluster's status page is not refreshed automatically, so you will need to continually refresh the page until all the workers have started.</p><p>Each Spark slave is given a Worker ID when it connects to <code class="literal">spark-master</code>. You will need to wait until there are three workers before you can submit the Spark jobs, with one worker for each slave (<code class="literal">spark-slave-a</code>, <code class="literal">spark-slave-b</code>, and <code class="literal">spark-slave-c</code>).</p><p>Once all <a class="indexterm" id="id785"/>the slaves (workers) have started, the <code class="literal">pi-spark.py</code> Python script can be submitted to the cluster using the <code class="literal">spark-submit</code> command.</p><p>The <code class="literal">spark-submit</code> command passes two parameters, namely <code class="literal">$SPARK_MASTER_URL</code> and <code class="literal">24</code>, to the <code class="literal">pi-spark.py</code> script.</p><p>The value of the <code class="literal">SPARK_MASTER_URL</code> is used to configure (<code class="literal">SparkConf conf</code>) the location of the Spark master (<code class="literal">conf.setMaster( master )</code>).</p><p>The second parameter of the <code class="literal">pi-spark.py</code> script (<code class="literal">24</code>) determines the number of compute partitions that are used to parallelize the calculations. Partitions divide the total number of calculations into compute groups (24 distinct groups).</p><p>The number of partitions should be a factor of the number of available computer cores. Here, we are using 2 partitions for each available computer core (24 = 2 x 12). There are twelve cores available—four cores in each of three Raspberry Pi slaves.</p><p>The <code class="literal">SPARK_MASTER_URL</code> and <code class="literal">PATH</code> environment variables are updated to simplify the <code class="literal">spark-submit</code> command line.</p><p>The <code class="literal">SPARK_MASTER_URL</code> is set to the IP address of the <code class="literal">spark-master</code> using the <code class="literal">hostname –I</code> command. The <code class="literal">tr</code> command is used to strip (<code class="literal">-d</code>) the trailing space (<code class="literal">[:space:]</code>) from the output of the <code class="literal">hostname –I</code> command.</p><p>The location of the Spark command directory (<code class="literal">/opt/spark/bin</code>) is prepended to the front of the <code class="literal">PATH</code> environment variable so that the Spark commands can be used without requiring their complete path.</p><p>Submitting the <code class="literal">pi-spark.py</code> script to the cluster for calculation takes a few seconds. However, once the calculation is distributed among the workers (slaves), it takes less than a second (<code class="literal">0.720023</code> seconds) to estimate the value of pi. The Spark cluster is more than 185 times faster than a standalone Raspberry Pi.</p><p>The Raspberry Pi supercomputer is running!</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec242"/>There's more...</h2></div></div></div><p>This recipe only begins to explore the possibility of creating a supercomputer from low-cost Raspberry Pi computers. For Spark (and Hadoop, on which Spark is built), there are numerous packages for statistical calculation and data visualization. More information<a class="indexterm" id="id786"/> on supercomputing using Spark (and Hadoop) can <a class="indexterm" id="id787"/>be found on the Apache Software Foundation website (<a class="ulink" href="http://www.apache.org">http://www.apache.org</a>).</p></div><div class="section" title="See Also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec243"/>See Also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Apache Spark</strong></span> (<a class="ulink" href="http://spark.apache.org/">http://spark.apache.org/</a>): Apache Spark™ is a fast and <a class="indexterm" id="id788"/>general engine for large-scale data processing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Apache Hadoop (</strong></span><a class="ulink" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a>): The Apache™ Hadoop<sup>®</sup> project <a class="indexterm" id="id789"/>develops open-source software for reliable, scalable, and distributed computing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ssh-copy-id</strong></span> (<a class="ulink" href="http://manpages.debian.org/cgi-bin/man.cgi?query=ssh-copy-id">http://manpages.debian.org/cgi-bin/man.cgi?query=ssh-copy-id</a>): Uses locally available keys to authorize logins<a class="indexterm" id="id790"/> on a remote machine. The Debian man page for <code class="literal">ssh-copy-id</code> describes the command and its options.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>tr</strong></span> (<a class="ulink" href="http://manpages.debian.org/cgi-bin/man.cgi?query=tr">http://manpages.debian.org/cgi-bin/man.cgi?query=tr</a>): This is used to<a class="indexterm" id="id791"/> translate or delete characters. The Debian man page for tr describes the command and its options.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monte Carlo methods for estimating pi</strong></span> (<a class="ulink" href="https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods">https://en.wikipedia.org/wiki/Pi#Monte_Carlo_methods</a>): This Wikipedia<a class="indexterm" id="id792"/> article on pi describes a number of ways to calculate pi, including the Monte Carlo method.</li></ul></div></div></div></body></html>