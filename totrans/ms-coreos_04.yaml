- en: Chapter 4. CoreOS Primary Services – Etcd, Systemd, and Fleet
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章 CoreOS 主要服务 – Etcd、Systemd 和 Fleet
- en: 'This chapter will cover the internals of CoreOS'' critical services—Etcd, Systemd,
    and Fleet. For each of the services, we will cover installation, configuration,
    and their applications. CoreOS ships with Etcd, Systemd, and Fleet by default.
    They can also be installed as standalone components in any Linux system. The following
    topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 CoreOS 关键服务——Etcd、Systemd 和 Fleet 的内部工作。对于每个服务，我们将涵盖安装、配置以及它们的应用。CoreOS
    默认包括 Etcd、Systemd 和 Fleet。这些服务也可以作为独立组件安装在任何 Linux 系统上。本章将涵盖以下主题：
- en: Etcd—installation, access methods, configuration options, use cases, tuning,
    cluster management, security, authentication, and debugging
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd——安装、访问方法、配置选项、使用案例、调优、集群管理、安全性、认证和调试
- en: Systemd—unit types, specifiers, templates, and special units
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Systemd——单元类型、说明符、模板和特殊单元
- en: Fleet—installation, access methods, templates, scheduling, HA, and debugging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet——安装、访问方法、模板、调度、高可用性和调试
- en: Service discovery options using Etcd and Fleet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Etcd 和 Fleet 的服务发现选项
- en: Etcd
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd
- en: Etcd is a distributed key-value store used by all the machines in the CoreOS
    cluster to read/write and exchange data. An overview of etcd is provided in [Chapter
    1](index_split_023.html#filepos77735), CoreOS Overview. This section will cover
    the internals of etcd.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 是一个分布式键值存储，CoreOS 集群中的所有机器都使用它来读写和交换数据。[第一章](index_split_023.html#filepos77735)中提供了
    Etcd 的概述。本节将介绍 Etcd 的内部工作。
- en: Versions
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 版本
- en: 'Etcd is under continuous development, and frequent releases are done to add
    enhancements as well as fix bugs. The following are some major updates from recent
    etcd releases:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 正在持续开发中，频繁发布版本以添加新功能并修复漏洞。以下是最近版本更新的主要内容：
- en: Version 2.0 is the first stable release and was released in January 2015\. Pre-version
    2.0 is available as etcd and post-version 2.0 is available as etcd2 in CoreOS
    nodes.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.0 是第一个稳定版本，于 2015 年 1 月发布。版本 2.0 之前的版本作为 etcd 提供，而版本 2.0 之后的版本在 CoreOS
    节点中作为 etcd2 提供。
- en: Version 2.0 added IANA-assigned ports `2379` for client-to-server communication
    and `2380` for server-to-server communication. Previously, port `4001` was used
    for client-to-server communication and port `7001` was used for server-to-server
    communication.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.0 增加了 IANA 分配的端口 `2379` 用于客户端与服务器的通信，`2380` 用于服务器与服务器的通信。之前，端口 `4001` 用于客户端与服务器的通信，端口
    `7001` 用于服务器与服务器的通信。
- en: Version 2.1 introduced authentication and metrics collection features and these
    are in experimental mode.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.1 引入了认证和度量收集功能，这些功能处于实验模式中。
- en: The latest release as of September 2015 is 2.2.0.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至 2015 年 9 月，最新版本为 2.2.0。
- en: An experimental v3 API (some examples are multikey reads, range reads, and binary
    keys) is available now as a preview and will be available officially in version
    2.3.0 scheduled at the end of October 2015.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个实验性的 v3 API（一些示例包括多键读取、范围读取和二进制键）现在作为预览版可用，预计将在 2015 年 10 月底的 2.3.0 版本中正式发布。
- en: All examples in this chapter are based on etcd version 2.1.0 and above.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有示例均基于 etcd 版本 2.1.0 及以上版本。
- en: Installation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 安装
- en: 'CoreOS ships with etcd. Both the etcd and etcd2 versions are available in the
    base CoreOS image. The following are the etcd versions available in the CoreOS
    alpha image 779.0.0:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 自带 etcd。etcd 和 etcd2 版本都包含在基础 CoreOS 镜像中。以下是 CoreOS alpha 镜像 779.0.0
    中提供的 etcd 版本：
- en: '![](img/00049.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpg)'
- en: Standalone installation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 独立安装
- en: 'Etcd can also be installed on any Linux machine. The following is the installation
    command tried out on Ubuntu 14.04 to install etcd version 2.2:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 也可以安装在任何 Linux 机器上。以下是尝试在 Ubuntu 14.04 上安装 etcd 版本 2.2 的安装命令：
- en: '`curl -L  https://github.com/coreos/etcd/releases/download/v2.2.0/etcd-v2.2.0-linux-amd64.tar.gz -o etcd-v2.2.0-linux-amd64.tar.gz``tar xzvf etcd-v2.2.0-linux-amd64.tar.gz`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl -L https://github.com/coreos/etcd/releases/download/v2.2.0/etcd-v2.2.0-linux-amd64.tar.gz
    -o etcd-v2.2.0-linux-amd64.tar.gz``tar xzvf etcd-v2.2.0-linux-amd64.tar.gz`'
- en: 'The following example shows you how to try out etcd in the standalone mode.
    To start the server run the following command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何在独立模式下尝试使用 etcd。启动服务器时运行以下命令：
- en: '`etcd -name etcdtest`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd -name etcdtest`'
- en: 'Now, check whether we can connect to the etcd server using some basic commands:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查是否可以通过一些基本命令连接到 etcd 服务器：
- en: '`etcdctl cluster-health`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl cluster-health`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前述命令的输出：
- en: '![](img/00053.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpg)'
- en: 'The following is an example of a simple set and get operation using the curl
    interface:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用curl接口进行简单的set和get操作的示例：
- en: '`curl –L –X PUT http://127.0.0.1:2379/v2/keys/message -d value="hello"``curl –L http://127.0.0.1:2379/v2/keys/message`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L –X PUT http://127.0.0.1:2379/v2/keys/message -d value="hello"``curl
    –L http://127.0.0.1:2379/v2/keys/message`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00055.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpg)'
- en: Accessing etcd
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 访问etcd
- en: 'Etcd can be accessed using either etcdctl CLI or REST API. This applies to
    both the standalone etcd as well as etcd in CoreOS. The following figure shows
    you the different ways to access etcd:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过etcdctl CLI或REST API访问Etcd。这适用于独立的etcd以及CoreOS中的etcd。下图展示了访问etcd的不同方式：
- en: '![](img/00058.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpg)'
- en: REST
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: REST
- en: The etcd database can be accessed and modified through the REST API. The etcd
    database can be accessed either locally or remotely using this approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过REST API访问和修改etcd数据库。使用这种方式可以本地或远程访问etcd数据库。
- en: 'The following example shows the `curl` method to access the CoreOS node to
    get all the keys:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用`curl`方法访问CoreOS节点以获取所有键：
- en: '`curl –L http://localhost:2379/v2/keys/?recursive=true`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L http://localhost:2379/v2/keys/?recursive=true`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00060.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpg)'
- en: 'The following example shows the `curl` method to access the remote CoreOS node
    to get all the keys:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用`curl`方法访问远程CoreOS节点以获取所有键：
- en: '`curl –L http://172.17.8.101:2379/v2/keys/?recursive=true`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L http://172.17.8.101:2379/v2/keys/?recursive=true`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00063.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpg)'
- en: Etcdctl
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Etcdctl
- en: Etcdctl is a CLI wrapper on top of the REST interface. Etcdctl can be used for
    local or remote access.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Etcdctl是REST接口之上的CLI封装。Etcdctl可以用于本地或远程访问。
- en: 'The following example shows etcdctl method to access the CoreOS node to get
    all the keys:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用etcdctl方法访问CoreOS节点以获取所有键：
- en: '`etcdctl ls / --recursive`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl ls / --recursive`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00067.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpg)'
- en: 'The following example shows etcdctl method to access the remote CoreOS node
    to get all the keys:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用etcdctl方法访问远程CoreOS节点以获取所有键：
- en: '`etcdctl --peers=http://172.17.8.101:2379 ls / --recursive`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --peers=http://172.17.8.101:2379 ls / --recursive`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00070.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpg)'
- en: Etcd configuration
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd配置
- en: 'Etcd configuration parameters can be used to modify the etcd member property
    or cluster-wide property. Etcd options can be set either in the command line or
    using environment variables. The command line will override the environment variables.
    The following are the broad categories and their critical configuration parameters/environment
    variables:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd配置参数可以用来修改etcd成员属性或集群范围属性。Etcd选项可以通过命令行或使用环境变量进行设置。命令行设置会覆盖环境变量。以下是一些广泛的分类及其关键配置参数/环境变量：
- en: 'Member: Name, data-dir, and heartbeat interval'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成员：名称、数据目录和心跳间隔
- en: 'Cluster: Discovery token and initial cluster nodes'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群：发现令牌和初始集群节点
- en: 'Proxy: Proxy on/off and proxy intervals'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：代理开/关和代理间隔
- en: 'Security: Certificate and key'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全：证书和密钥
- en: 'Logging: Enable/disable logging and logging levels'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志：启用/禁用日志记录和日志级别
- en: Experimental
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验性
- en: 'The following is an etcd invocation example, where we use some of the preceding
    configuration parameters:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个etcd调用示例，其中使用了一些前述配置参数：
- en: '`etcd -name infra0 -data-dir infra0  --cacert=~/.etcd-ca/ca.crt -cert-file=/home/smakam14/infra0.crt -key-file=/home/smakam14/infra0.key.insecure  -advertise-client-urls=https://192.168.56.104:2379 -listen-client-urls=https://192.168.56.104:2379`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd -name infra0 -data-dir infra0  --cacert=~/.etcd-ca/ca.crt -cert-file=/home/smakam14/infra0.crt
    -key-file=/home/smakam14/infra0.key.insecure  -advertise-client-urls=https://192.168.56.104:2379
    -listen-client-urls=https://192.168.56.104:2379`'
- en: 'Etcd environment variables can also be specified in `cloud-config`. The following
    is a `cloud-config` example to specify etcd environment variables:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd环境变量也可以在`cloud-config`中指定。以下是一个`cloud-config`示例，用于指定etcd环境变量：
- en: '`etcd2:     #generate a new token for each unique cluster from https://discovery.etcd.io/new
        discovery: https://discovery.etcd.io/d93c8c02eedadddd3cf14828f9bec01c     # multi-region and multi-cloud deployments need to use $public_ipv4
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2:     #为每个唯一集群生成一个新令牌，来自https://discovery.etcd.io/new     discovery: https://discovery.etcd.io/d93c8c02eedadddd3cf14828f9bec01c
        # 多区域和多云部署需要使用$public_ipv4     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # 同时监听官方端口和遗留端口
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001`'
- en: 'Etcd2 environment variables from cloud-config are stored in the following directory:
    `/run/systemd/system/etcd2.service.d`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从cloud-config中获取的etcd2环境变量存储在以下目录：`/run/systemd/system/etcd2.service.d`。
- en: The etcd2 service needs to be restarted if the environment variables are changed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境变量发生更改，etcd2服务需要重新启动。
- en: A complete list of configuration parameters and environment variables for etcd
    can be found at [https://coreos.com/etcd/docs/latest/configuration.html](https://coreos.com/etcd/docs/latest/configuration.html).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://coreos.com/etcd/docs/latest/configuration.html](https://coreos.com/etcd/docs/latest/configuration.html)找到完整的etcd配置参数和环境变量列表。
- en: Etcd operations
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 操作
- en: 'The following are some examples of major operations that can be done using
    etcd:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以使用etcd执行的主要操作示例：
- en: Set, get, and delete operations of a key-value pair
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键值对的设置、获取和删除操作
- en: Set a key with timeout where the key expires automatically
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置带有超时的键，其中该键会自动过期
- en: Set a key based on the atomic condition check
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据原子条件检查设置一个键
- en: Hidden keys
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏键
- en: Watching and waiting for key changes
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视并等待键的变化
- en: Creating in-order keys
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建有序的键
- en: 'Using these operations, etcd can be used for a variety of distributed application
    use cases. The following is an example TTL use case where we check for the liveliness
    of the Apache service and update service details such as the IP address and port
    number in etcd, which other applications can use to determine if the service is
    running or not. If the Apache service dies, the etcd key-value pair will be deleted
    after 30 seconds in this case:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些操作，etcd可以用于各种分布式应用场景。以下是一个TTL使用场景的示例，其中我们检查Apache服务的存活性，并更新如IP地址和端口号等服务详情到etcd，其他应用可以使用这些信息来判断服务是否正在运行。如果Apache服务停止，在这种情况下，etcd中的键值对将在30秒后被删除：
- en: '`## Test whether service is accessible and then register useful information like IP address, port
    ExecStart=/bin/bash -c ''\   while true; do \     curl -f ${COREOS_PUBLIC_IPV4}:%i; \
        if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        fi; \     sleep 20; \   done''`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`## 测试服务是否可访问，然后注册有用的信息，如IP地址，端口 ExecStart=/bin/bash -c ''\   while true; do \
        curl -f ${COREOS_PUBLIC_IPV4}:%i; \     if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        fi; \     sleep 20; \   done''`'
- en: We can find statistics about the etcd node as well as the key-related operations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以找到关于etcd节点的统计信息，以及与键相关的操作。
- en: 'The following output shows the etcd node statistics:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了etcd节点的统计信息：
- en: '`curl http://127.0.0.1:2379/v2/stats/self | jq .`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl http://127.0.0.1:2379/v2/stats/self | jq .`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00072.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpg)'
- en: 'The following output shows the etcd key statistics:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了etcd键的统计信息：
- en: '`curl http://127.0.0.1:2379/v2/stats/store | jq .`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl http://127.0.0.1:2379/v2/stats/store | jq .`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00076.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpg)'
- en: Etcd tuning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 调优
- en: 'The following are some etcd parameters that can be tuned to achieve optimum
    cluster performance based on the operating environment:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以调整的etcd参数，以根据操作环境实现最佳集群性能：
- en: 'Cluster size: A bigger cluster size provides you with better redundancy. The
    disadvantage with big cluster sizes is that updates can take a long time. In [Chapter
    1](index_split_023.html#filepos77735), CoreOS Overview, we saw the failure tolerance
    limit with different cluster sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群大小：较大的集群可以提供更好的冗余。大集群的缺点是更新可能需要较长时间。在[第1章](index_split_023.html#filepos77735)，CoreOS概述中，我们看到了不同集群大小下的容错极限。
- en: 'Heartbeat interval: This is the time interval at which the master node sends
    a heartbeat message to its followers. The default heartbeat interval is 100 ms.
    It is necessary to choose a heartbeat interval based on the average round-trip
    time taken for the ping between nodes. If the nodes are geographically distributed,
    then the round-trip time will be longer. The suggested heartbeat interval is 0.5-1.5
    x the average round-trip time. If we choose a small heartbeat interval, the overhead
    will be a higher number of packets. If we choose a large heartbeat interval, it
    will take a longer time to detect leader failure. The heartbeat interval can be
    set using the `heartbeat-interval` parameter in the etcd command line or the `ETCD_HEARTBEAT_INTERVAL`
    environment variable.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 心跳间隔：这是主节点向其跟随节点发送心跳消息的时间间隔。默认的心跳间隔为 100 毫秒。选择心跳间隔时需要根据节点之间 ping 的平均往返时间来决定。如果节点地理分布较广，往返时间会更长。建议的心跳间隔是平均往返时间的
    0.5-1.5 倍。如果选择较小的心跳间隔，数据包的开销会更高；如果选择较大的心跳间隔，则需要更长的时间才能检测到领导者故障。心跳间隔可以通过 `heartbeat-interval`
    参数在 etcd 命令行中设置，或者通过 `ETCD_HEARTBEAT_INTERVAL` 环境变量设置。
- en: 'Election timeout: When the follower nodes fail to get a heartbeat message for
    the election timeout value, they become the leader node. The default election
    timeout is 1,000 ms. The suggested value for election timeout is 10 times the
    heartbeat interval. Keeping the election timeout too low can cause false leader
    election. The election timeout can be set using the `election-timeout` parameter
    in the etcd command line or the `ETCD_ELECTION_TIMEOUT` environment variable.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举超时：当跟随节点在选举超时值内未能接收到心跳消息时，它们会成为领导者节点。默认的选举超时为 1,000 毫秒。建议的选举超时值是心跳间隔的 10 倍。将选举超时设置得过低可能会导致错误的领导者选举。选举超时可以通过
    `election-timeout` 参数在 etcd 命令行中设置，或者通过 `ETCD_ELECTION_TIMEOUT` 环境变量设置。
- en: Etcd proxy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 代理
- en: An etcd proxy is used when worker nodes want to use the master node or master
    cluster to provide etcd service. In this case, all etcd requests from the worker
    node are proxied to the master node and the master node replies to the worker
    node.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作节点需要使用主节点或主集群提供 etcd 服务时，会使用 etcd 代理。在这种情况下，所有来自工作节点的 etcd 请求都将被代理到主节点，主节点再回复工作节点。
- en: 'Let''s say that we have a working three-node master cluster as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个正在运行的三节点主集群，配置如下：
- en: '![](img/00080.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpg)'
- en: 'The following example shows the `cloud-config` for the fourth node that is
    a worker node and acting as a proxy. Here, the master cluster members are mentioned
    statically:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了作为工作节点并充当代理的第四个节点的 `cloud-config` 配置。在此，主集群成员是静态指定的：
- en: '`#cloud-config coreos:   etcd2:     proxy: on     listen-client-urls: http://localhost:2379
        initial-cluster: etcdserver=http://172.17.8.101:2380, http://172.17.8.102:2380, http://172.17.8.103:2380
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     proxy: on     listen-client-urls: http://localhost:2379
        initial-cluster: etcdserver=http://172.17.8.101:2380, http://172.17.8.102:2380, http://172.17.8.103:2380
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: In the preceding Etcd configuration section, we have turned on the proxy and
    pointed to the `etcd_server` cluster. The fourth node needs to be started with
    the preceding `cloud-config`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Etcd 配置部分中，我们启用了代理并指向了 `etcd_server` 集群。第四个节点需要使用上述 `cloud-config` 配置启动。
- en: 'The following example shows the `cloud-config` for the fourth node that is
    acting as a proxy and using a discovery token. We need to use the same discovery
    token as we did for the three-node cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了作为代理并使用发现令牌的第四个节点的 `cloud-config` 配置。我们需要使用与三节点集群相同的发现令牌：
- en: '`#cloud-config coreos:   etcd2:     proxy: on     # use the same discovery token as for master, these nodes will proxy to master
        discovery: <your token>     # listen on both the official ports and the legacy ports
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001   fleet:     etcd_servers: "http://localhost:2379"
        public-ip: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     proxy: on     # 使用与主节点相同的发现令牌，这些节点将代理到主节点
        discovery: <your token>     # 监听官方端口和旧版端口     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: 'The following is the etcd member output in the new node. As we can see, the
    etcd cluster is composed of only three nodes and the new node is proxying to the
    master etcd cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是新节点上的etcd成员输出。如我们所见，etcd集群由三个节点组成，且新节点正在代理主etcd集群：
- en: '![](img/00085.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpg)'
- en: 'The following is the Fleet machine''s output in the new node. As we can see,
    there are four nodes and this includes the fourth worker node and the three-node
    etcd cluster:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是新节点上的Fleet机器输出。如我们所见，共有四个节点，包括第四个工作节点和三节点的etcd集群：
- en: '![](img/00482.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00482.jpg)'
- en: Adding and removing nodes from a cluster
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 向集群中添加和移除节点
- en: There will be scenarios where we need to add and remove nodes from a working
    etcd cluster. This section illustrates how to add and remove nodes in a working
    etcd cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要从一个正在运行的etcd集群中添加或移除节点。本节将演示如何在运行中的etcd集群中添加和移除节点。
- en: 'Let''s say that we have a three-node working cluster and we want to add a fourth
    node to the cluster. The following command can be executed in one of the three
    working nodes to add the fourth node detail:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个三节点的工作集群，并且想要向集群中添加第四个节点。可以在任一三节点中的一台上执行以下命令，添加第四个节点的详细信息：
- en: '![](img/00093.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpg)'
- en: 'The following `cloud-config` can be used to start the new fourth node:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`cloud-config`可以用来启动新的第四节点：
- en: '`#cloud-config coreos:   etcd2:     name: core-04     initial_cluster: "core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380,core-04=http://172.17.8.104:2380"
        initial_cluster_state: existing     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   units:     # Note: this requires a release that contains etcd2
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-04     initial_cluster: "core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380,core-04=http://172.17.8.104:2380"     initial_cluster_state:
    existing     advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls:
    http://$private_ipv4:2380     # listen on both the official ports and the legacy
    ports     # legacy ports can be omitted if your application doesn''t depend on
    them     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls:
    http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:     public-ip: $public_ipv4   units:     #
    Note: this requires a release that contains etcd2     - name: etcd2.service       command:
    start     - name: fleet.service       command: start`'
- en: 'From the following output, we can see that the new member has been successfully
    added:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出可以看出，新的节点已经成功添加：
- en: '![](img/00095.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpg)'
- en: 'The following command can be used to remove the fourth number that we added
    before:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令移除之前添加的第四个节点：
- en: '![](img/00138.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.jpg)'
- en: 'Let''s check the member list and cluster health now. We can see that the three
    nodes are part of the cluster and that the fourth node has been removed:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查成员列表和集群健康状况。我们可以看到，三个节点是集群的一部分，第四个节点已被移除：
- en: '![](img/00103.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpg)'
- en: Node migration and backup
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 节点迁移与备份
- en: Node migration is necessary to handle failure of the node and cluster and also
    to replicate the cluster to a different location.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 节点迁移是处理节点和集群故障以及将集群复制到不同位置时所必需的。
- en: 'To take a backup of the etcd database, we can perform the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了备份etcd数据库，我们可以执行以下操作：
- en: '`Sudo etcdctl backup --data-dir=/var/lib/etcd2 --backup-dir=/tmp/etcd2`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo etcdctl backup --data-dir=/var/lib/etcd2 --backup-dir=/tmp/etcd2`'
- en: This approach allows us to reuse the backed-up etcd data in another cluster.
    In this approach, `nodeid` and `clusterid` are overwritten in the backup directory
    to prevent unintentional addition of a new node to the old cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许我们在另一个集群中重新使用备份的etcd数据。在这种方法中，备份目录中的`nodeid`和`clusterid`被覆盖，以防止无意中将新节点添加到旧集群。
- en: To preserve the node ID and cluster ID, we have to manually make a copy, and
    the copy can be used to restart the service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留节点ID和集群ID，我们必须手动复制，且复制品可以用来重启服务。
- en: 'The following are the steps to move the etcd2 data directory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是移动etcd2数据目录的步骤：
- en: 'Stop the service:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止服务：
- en: '![](img/00106.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00106.jpg)'
- en: 'Make a copy of the `/var/lib/etcd2` etcd data directory in `/tmp/etcd2_backup`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`/var/lib/etcd2`的etcd数据目录复制到`/tmp/etcd2_backup`：
- en: '![](img/00196.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00196.jpg)'
- en: 'Start etcd2 manually using the new data directory, `/tmp/etcd2_backup`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的数据目录`/tmp/etcd2_backup`手动启动etcd2：
- en: '![](img/00115.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00115.jpg)'
- en: 'There are two approaches to handle the migration:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 处理迁移有两种方法：
- en: Add a new member and remove the old member. We can use `etcdctl member add`
    and `etcdctl member remove`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个新成员并移除旧成员。我们可以使用`etcdctl member add`和`etcdctl member remove`。
- en: Make a copy of the etcd database, move it to the new node, and update it.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制etcd数据库，将其移动到新节点并更新。
- en: With the first approach, the new member has a different identity. With the second
    approach, we can have the new node retain the same old identity. With the first
    approach, there is no need to stop the etcd service, while we need to stop the
    etcd service before taking the backup in the second approach.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法中新成员有不同的身份。第二种方法则可以让新节点保持相同的旧身份。在第一种方法中，不需要停止etcd服务，而在第二种方法中，在进行备份之前需要停止etcd服务。
- en: Etcd security
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd安全性
- en: 'A secure etcd is needed to ensure that the client-to-server communication and
    server-to-server communication are secure. The following figure shows you the
    different components involved in providing etcd security. Certificate authority
    is used to provide and verify certificates for the etcd client-to-server and server-to-server
    communication:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个安全的etcd来确保客户端与服务器之间的通信以及服务器之间的通信是安全的。下图展示了提供etcd安全性所涉及的不同组件。证书颁发机构用于提供和验证etcd客户端到服务器以及服务器到服务器的通信证书：
- en: '![](img/00117.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpg)'
- en: Certificate authority – etcd-ca
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 证书颁发机构 – etcd-ca
- en: Certificate authority is a trusted source that issues certificates to a trusted
    server. Other than using standard certificate authorities (CA), etcd allows for
    a custom CA. Etcd-ca ([https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca))
    is a GO application that can be used as a CA for testing purposes. Recently, etcd
    has migrated to CFSSL ([https://github.com/cloudflare/cfssl](https://github.com/cloudflare/cfssl))
    as the official tool for certificates.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 证书颁发机构是一个可信的来源，用于向可信服务器颁发证书。除了使用标准的证书颁发机构（CA）外，etcd还允许使用自定义CA。Etcd-ca ([https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca))
    是一个GO应用程序，可以作为测试目的使用的CA。最近，etcd已迁移至CFSSL ([https://github.com/cloudflare/cfssl](https://github.com/cloudflare/cfssl))
    作为官方证书工具。
- en: Installing etcd-ca
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 安装etcd-ca
- en: 'I installed etcd-ca in my Linux VM running Ubuntu 14.04 using the following
    steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我在运行Ubuntu 14.04的Linux虚拟机上使用以下步骤安装了etcd-ca：
- en: '`git clone https://github.com/coreos/etcd-ca``cd etcd-ca``./build`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/etcd-ca``cd etcd-ca``./build`'
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: The GO application needs to be installed before the etcd-ca installation.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在安装etcd-ca之前，需要先安装GO应用程序。
- en: 'Following three steps are needed to setup etcd-ca:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 设置etcd-ca需要以下三个步骤：
- en: Creating a CA using etcd-ca.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用etcd-ca创建CA。
- en: Creating server keys.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建服务器密钥。
- en: Creating client keys.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建客户端密钥。
- en: 'The following command, `etcd-ca init`, is used to create a CA. This is a one-time
    procedure. The following screenshot shows you the output when creating a CA:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令`etcd-ca init`用于创建一个CA。这是一个一次性的过程。以下截图展示了创建CA时的输出：
- en: '![](img/00317.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00317.jpg)'
- en: 'The following commands can be used to create a server certificate:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可以用于创建服务器证书：
- en: '`etcd-ca new-cert -ip  172.17.8.101 core-01``etcd-ca sign core-01``etcd-ca chain core-01``etcd-ca export --insecure core-01 | tar xvf –`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd-ca new-cert -ip 172.17.8.101 core-01``etcd-ca sign core-01``etcd-ca chain
    core-01``etcd-ca export --insecure core-01 | tar xvf –`'
- en: In the preceding command, `172.17.8.101` is the CoreOS node IP and `core-01`
    is the node name. These steps will create `core-01.crt` and `core-01.key.insecure`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`172.17.8.101`是CoreOS节点的IP，`core-01`是节点名称。这些步骤将创建`core-01.crt`和`core-01.key.insecure`。
- en: 'The following commands can be used to create a client certificate:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可以用于创建客户端证书：
- en: '`etcd-ca new-cert -ip 192.168.56.104 client``etcd-ca sign client``etcd-ca chain client``etcd-ca export --insecure client | tar xvf -`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd-ca new-cert -ip 192.168.56.104 client``etcd-ca sign client``etcd-ca chain
    client``etcd-ca export --insecure client | tar xvf -`'
- en: In the preceding command, `192.168.56.104` is the client node IP. These steps
    will create `client.crt` and `client.key.insecure`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`192.168.56.104`是客户端节点的IP。这些步骤将创建`client.crt`和`client.key.insecure`。
- en: Etcd secure client-to-server communication using a server certificate
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务器证书实现etcd安全的客户端与服务器通信
- en: 'A server certificate is used by the client to ensure the server''s identity.
    The following command starts the etcd server using a server certificate and the
    key that was generated in the previous section:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端使用服务器证书来确保服务器的身份。以下命令使用在前一部分生成的服务器证书和密钥启动 etcd 服务器：
- en: '`etcd2 -name core-01 --cert-file=/home/core/core-01.crt --key-file=/home/core/core-01.key  --advertise-client-urls=https://172.17.8.101:2379 --listen-client-urls=https://172.17.8.101:2379`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2 -name core-01 --cert-file=/home/core/core-01.crt --key-file=/home/core/core-01.key  --advertise-client-urls=https://172.17.8.101:2379
    --listen-client-urls=https://172.17.8.101:2379`'
- en: 'The following is an example to set a key and retrieve it using a secure mechanism:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示如何使用安全机制设置密钥并检索它：
- en: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo
    -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo`'
- en: 'The following example uses etcdctl to do the key retrieval:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用 `etcdctl` 进行密钥检索：
- en: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --peers https://172.17.8.101:2379 get /foo`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --peers https://172.17.8.101:2379
    get /foo`'
- en: Etcd secure client-to-server communication using server certificate and client
    certificate
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 使用服务器证书和客户端证书进行安全的客户端到服务器通信
- en: 'In the previous example, only the server had a certificate. In this example,
    we will generate a client certificate so that the server can verify the client''s
    identity. The following command starts the etcd server using a server certificate
    and key and enabling client authentication. The server certificate and keys are
    the same as generated in the previous section:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，只有服务器有证书。在这个示例中，我们将生成一个客户端证书，以便服务器验证客户端的身份。以下命令使用服务器证书和密钥启动 etcd 服务器，并启用客户端身份验证。服务器证书和密钥与前一部分生成的相同：
- en: '`etcd2 -name core-01 --data-dir=core-01 -client-cert-auth -trusted-ca-file=/home/core/ca.crt -cert-file=/home/core/key.crt  -key-file=/home/core/key.key -advertise-client-urls https://172.17.8.101:2379 -listen-client-urls https://172.17.8.101:2379`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2 -name core-01 --data-dir=core-01 -client-cert-auth -trusted-ca-file=/home/core/ca.crt
    -cert-file=/home/core/key.crt  -key-file=/home/core/key.key -advertise-client-urls
    https://172.17.8.101:2379 -listen-client-urls https://172.17.8.101:2379`'
- en: 'The following is an example to set a key and retrieve it using a secure client
    and server mechanism. The client certificate and key are the same as generated
    in the previous section:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示如何使用安全的客户端和服务器机制设置密钥并检索它。客户端证书和密钥与前一部分生成的相同：
- en: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure -L https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure https://172.17.8.101:2379/v2/keys/foo`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt
    --key /home/smakam14/client.key.insecure -L https://172.17.8.101:2379/v2/keys/foo
    -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt
    --key /home/smakam14/client.key.insecure https://172.17.8.101:2379/v2/keys/foo`'
- en: 'The following example uses etcdctl to do the key retrieval:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用 `etcdctl` 进行密钥检索：
- en: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --cert-file /home/smakam14/client.crt --key-file /home/smakam14/client.key.insecure --peers https://172.17.8.101:2379 get /foo`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --cert-file /home/smakam14/client.crt
    --key-file /home/smakam14/client.key.insecure --peers https://172.17.8.101:2379
    get /foo`'
- en: A secure cloud-config
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个安全的 `cloud-config`
- en: 'The following is a sample `cloud-config` that sets up the etcd security environment
    variables as well as the necessary certificate and keys:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例的 `cloud-config`，它设置了 etcd 的安全环境变量以及所需的证书和密钥：
- en: '`cloud-config write_files:   - path: /run/systemd/system/etcd2.service.d/30-configuration.conf
       permissions: 0644    content: |      [Service]      Environment=ETCD_NAME=core-01
         Environment=ETCD_VERBOSE=1      # Encryption      Environment=ETCD_CLIENT_CERT_AUTH=1
         Environment=ETCD_TRUSTED_CA_FILE=/home/core/ca.crt      Environment=ETCD_CERT_FILE=/home/core/server.crt
         Environment=ETCD_KEY_FILE=/home/core/server.key       - path: /home/core/ca.crt
       permissions: 0644    content: |      -----BEGIN CERTIFICATE-----       -----END CERTIFICATE-----
         - path: /home/core/server.crt    permissions: 0644    content: |      -----BEGIN CERTIFICATE-----
         -----END CERTIFICATE-----   - path: /home/core/server.key    permissions: 0644
       content: |      -----BEGIN RSA PRIVATE KEY-----      -----END RSA PRIVATE KEY-----  coreos:
     etcd2:     # Static cluster     initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://$private_ipv4:2380
        initial-cluster-state: new     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://$public_ipv4:2379     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
     units:    - name: etcd2.service      command: start`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`cloud-config write_files:   - path: /run/systemd/system/etcd2.service.d/30-configuration.conf
       permissions: 0644    content: |      [Service]      Environment=ETCD_NAME=core-01
         Environment=ETCD_VERBOSE=1      # Encryption      Environment=ETCD_CLIENT_CERT_AUTH=1
         Environment=ETCD_TRUSTED_CA_FILE=/home/core/ca.crt      Environment=ETCD_CERT_FILE=/home/core/server.crt
         Environment=ETCD_KEY_FILE=/home/core/server.key       - path: /home/core/ca.crt
       permissions: 0644    content: |      -----BEGIN CERTIFICATE-----       -----END CERTIFICATE-----
         - path: /home/core/server.crt    permissions: 0644    content: |      -----BEGIN CERTIFICATE-----
         -----END CERTIFICATE-----   - path: /home/core/server.key    permissions: 0644
       content: |      -----BEGIN RSA PRIVATE KEY-----      -----END RSA PRIVATE KEY-----  coreos:
     etcd2:     # Static cluster     initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://$private_ipv4:2380
        initial-cluster-state: new     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports 可以省略，如果您的应用不依赖于它们     listen-client-urls: http://$public_ipv4:2379
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001  units:
       - name: etcd2.service      command: start`'
- en: Authentication
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 认证
- en: Before the introduction of the authentication feature, there were no restrictions
    on access to the etcd database. The authentication feature was introduced as an
    experimental feature in etcd 2.1.0 and allows access to a specific set of keys
    based on the username and password.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入认证功能之前，etcd 数据库的访问没有任何限制。认证功能作为实验性功能在 etcd 2.1.0 中引入，并允许基于用户名和密码访问特定的密钥集。
- en: 'There are two entities associated with authentication:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 认证涉及两个实体：
- en: 'Users: Users can be created with a username and password. Before enabling the
    authentication feature, a root user needs to be created. The root user has substantially
    more privileges/permissions to add users and roles and assign role permissions.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户：用户可以通过用户名和密码创建。在启用认证功能之前，必须先创建一个 root 用户。root 用户拥有更多的权限，可以添加用户和角色并分配角色权限。
- en: 'Roles: Roles can be used to restrict access to a specific key or directory
    that holds multiple keys. Roles are assigned to users, and manipulations of the
    keys can be done based on the username.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色：角色用于限制对特定密钥或包含多个密钥的目录的访问。角色分配给用户，密钥的操作可以根据用户名进行。
- en: To get started with authentication, we need to first create a root user and
    then enable authentication.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用认证，我们需要先创建一个 root 用户，然后启用认证功能。
- en: 'Create a root user first, as shown in the following screenshot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个 root 用户，如下图所示：
- en: '![](img/00424.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00424.jpg)'
- en: 'Enable authentication as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式启用认证：
- en: '![](img/00428.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00428.jpg)'
- en: The following example illustrates the etcd authentication.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了 etcd 认证。
- en: 'Let''s create a sample keyset, user, and role:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个示例密钥集、用户和角色：
- en: Create `/dir1/key1` and `/dir2/key2` keys.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `/dir1/key1` 和 `/dir2/key2` 密钥。
- en: Create a `role_dir1` role that has access to `/dir1/*` only.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅能访问 `/dir1/*` 的 `role_dir1` 角色。
- en: Create a `role_dir2` role that has access to `/dir2/*` only.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅能访问 `/dir2/*` 的 `role_dir2` 角色。
- en: Create `user1` and grant the `role_dir1` role.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `user1` 并授予 `role_dir1` 角色。
- en: Create `user2` and grant the `role_dir2` role.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `user2` 并授予 `role_dir2` 角色。
- en: At this point, `user1` will be able to access `/dir1/*` only and `user2` will
    be able to access `/dir2/*` only.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`user1` 只能访问 `/dir1/*`，`user2` 只能访问 `/dir2/*`。
- en: 'The following is a breakdown of the steps:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤的详细说明：
- en: 'Create `/dir1/key1` and `/dir2/key2` keys:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00432.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: '![](img/00435.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Create a `role_dir1` role that has access to `/dir1/*` only:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00438.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: '![](img/00443.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: 'Create a `role_dir2` role that has access to `/dir2/*` only:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00446.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: '![](img/00450.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Create `user1` and grant the `role_dir1` role:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00453.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: '![](img/00456.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Create `user2` and grant the `role_dir2` role:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00459.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: '![](img/00463.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can verify that `user1` has access only to `/dir1/key1`. As shown in
    the following screenshot, `user1` is not able to access `/dir2/key1`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00466.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, `user2` has access only to `/dir2/key1`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00471.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Etcd debugging
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Etcd log files can be checked using the following command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '`Journalctl –u etcd2.service`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Default logging is set to `INFO`. For more elaborate logging, we can set `ETCD_DEBUG=1`
    in the environment file or use the `-debug` command-line option.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, it''s useful to check the curl command associated with the etcdctl
    CLI command. This can be achieved using the `--debug` option. The following is
    an example:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00475.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Systemd
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: An overview of Systemd was provided in [Chapter 1](index_split_023.html#filepos77735),
    CoreOS Overview. Systemd is the init system used by CoreOS and is always on by
    default. In this section, we will walk through some of the Systemd internals.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Unit types
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Units describe a particular task along with its dependencies and the execution
    order. Some units are started on the CoreOS system by default. CoreOS users can
    also start their own units. System-started units are at `/usr/lib64/systemd/system`
    and user-started units are at `/etc/systemd/system`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the common unit types:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Service unit: This is used to start a particular daemon or process. Examples
    are `sshd.service` and `docker.service`. The `sshd.service` unit starts the SSH
    service, and `docker.service` starts the docker daemon.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Socket unit: This is used for local IPC or network communication. Examples
    are `systemd-journald.socket` and `docker.socket`. There is a corresponding service
    associated with a socket that manages the socket. For example, `docker.service`
    manages `docker.socket`. In `docker.service`, `docker.socket` is mentioned as
    a dependency. `Docker.socket` provides remote connectivity to the docker engine.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target unit: This is used mainly to group related units so that they can be
    started together. All user-created services are in `multi-user.target`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mount unit: This is used to mount disks to the filesystem. Examples are `tmp.mount`
    and `usr-share-oem.mount`. The following is a relevant section of `usr-share-oem.mount`
    that mounts `/usr/share/oem`:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00479.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Timer unit: These are units that are started periodically based on the interval
    specified. Examples are `update-engine-stub.timer` and `logrotate.timer`. The
    following is a relevant section of `update-engine-stub.timer`, where `update-engine-stub.service`
    is invoked every `41 minutes` to check for CoreOS updates:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00483.jpg)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Unit specifiers
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: When writing systemd units, it is useful to access system environment variables
    such as hostname, username, IP address, and so on so that we can avoid hardcoding
    and use the same systemd unit across systems. For this, systemd provides you with
    unit specifiers, which are shortcuts to get to the system environment.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some common unit specifiers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '`%H`: Hostname'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%m`: Machine ID'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`%u`: Username'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complete list of unit specifiers is specified at [http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers](http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'The following service example illustrates the usage of unit specifiers. In
    this example, we are setting the key-value pair associated with different specifiers
    in etcd in ExecStartPre. In ExecStartPost, we are getting the key-value and then
    cleaning up in the end:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=My Service  [Service] KillMode=none ExecStartPre=/usr/bin/etcdctl set hostname %H ; /usr/bin/etcdctl set machinename %m ; /usr/bin/etcdctl set bootid %b ; /usr/bin/etcdctl set unitname %n ; /usr/bin/etcdctl set username %u
    ExecStart=/bin/echo hello, set done, will echo and remove ExecStartPost=/usr/bin/etcdctl get hostname ; /usr/bin/etcdctl get machinename ; /usr/bin/etcdctl get bootid ; /usr/bin/etcdctl get unitname ; /usr/bin/etcdctl get username ;
    ExecStartPost=/usr/bin/etcdctl rm hostname ; /usr/bin/etcdctl rm machinename ; /usr/bin/etcdctl rm bootid ; /usr/bin/etcdctl rm unitname ; /usr/bin/etcdctl rm username ;  [Install]
    WantedBy=multi-user.target`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute this service, it is necessary to execute all the following operations
    with sudo:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Create the `unitspec.service` file in `/etc/systemd/system` with the preceding
    content.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable the service with `systemctl enable unitspec.service`.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the service with `systemctl start unitspec.service`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we change the service after this, we need to execute command `systemctl daemon-reload`
    before starting the service.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are the `journalctl` logs associated with the service where we
    can see the key being set and retrieved and the corresponding unit specifier value:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '`journalctl –u unitspec.service`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00487.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: Unit templates
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Systemd units can be created as a template, and the same template unit can be
    used to instantiate multiple units based on the invocation of templates.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Templates are created as `unitname@.service`. The invocation of templates can
    be done using `unitname@instanceid.service`. In the unit file, the `unit name`
    can be accessed with `%p` and `instanceid` can be accessed using `%i`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example template file, `unitspec@.service`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=My Service  [Service] ExecStartPre=/usr/bin/etcdctl set instance%i %i ; /usr/bin/etcdctl set prefix %p
    ExecStart=/bin/echo Demonstrate systemd template  [Install] WantedBy=multi-user.target`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute this service, it is necessary to execute all the following operations
    with sudo:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Create the `unitspec@.service` file in `/etc/systemd/system`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable the service with `systemctl enable unitspec@.service`.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start multiple services with `systemctl start unitspec@1.service` and `systemctl
    start unitspec@2.service`.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we look at the etcd content, we can see that the instance value gets updated
    based on the `%i` argument supplied in the unit name and creates the `instance1`
    and `instance2` keys:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00490.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'The following example gives a more practical example of instantiated units.
    It uses a template nginx service, `nginx@.service`, where the port number of the
    web service is passed dynamically:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=Apache web server service After=etcd.service After=docker.service  [Service]
    TimeoutStartSec=0 Restart=always EnvironmentFile=/etc/environment ExecStartPre=-/usr/bin/docker kill nginx%i
    ExecStartPre=-/usr/bin/docker rm nginx%i ExecStartPre=/usr/bin/docker pull nginx
    ExecStart=/usr/bin/docker run --name nginx%i -p ${COREOS_PUBLIC_IPV4}:%i:80 nginx
    ExecStop=/usr/bin/docker stop nginx%i  [Install] WantedBy=multi-user.target`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two service options used in the preceding code:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '`Timeoutstartsec`: This specifies the time taken to start the service, and
    if the service is not started by this time, it gets killed. The `none` parameter
    disables this option and is useful when downloading big containers.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Restart`: This controls the restartability of the service. Here we have specified
    `always` to restart the service in case there is a failure associated with this
    service.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create two instances of this service using the following commands:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '`Sudo systemctl enable nginx@.service``Sudo systemctl start nginx@8080.service``Sudo systemctl start nginx@8081.service`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This creates two docker containers with nginx service; the first one exposing
    port `8080` and the second one exposing port `8081`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at `docker ps`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00494.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the status of the two units. As we can see in the following
    screenshot, the units are in an active (running) state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00497.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: '![](img/00258.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Drop-in units
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Drop-in units are useful to change system unit properties at runtime. There
    are four ways of creating drop-in units.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Default cloud-config drop-in units
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameters specified in the `cloud-config` user data will automatically be
    configured as drop-in units. For example, let''s look at the `etcd2.service cloud-config`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00260.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the `etcd2.service` status:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00262.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding output, the default drop-in unit is `20-cloudinit.conf`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '`20-cloudinit.conf` will contain the parameters specified in `etcd2 cloud-config`
    as environment variables, as shown in the following screenshot:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00265.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Cloud-config custom drop-in units
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'We can specify the drop-in unit as part of the `cloud-config`. The following
    is an example of the `fleet.service` drop-in unit, where we change the default
    `Restart` parameter from `Always` to `No`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00267.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'When we use this `cloud-config`, the `norestart.conf` drop-in file gets automatically
    created as can be seen from the `fleet.service` status:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00269.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: This configuration change will keep `fleet.service` non-restartable.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Runtime drop-in unit – specific parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'We can change specific properties of the service using the drop-in configuration
    file. The following is the service section of `fleet.service`, which shows the
    default parameters:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00272.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: This specifies that the service needs to be started in 10 seconds in case the
    service dies because of some error. Let's check whether the restart works by killing
    the Fleet service.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We can kill the service as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '`Sudo kill -9 <fleet pid>`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a log showing the Fleet service restarting in 10 seconds,
    which is due to `Restartsec` specified in the service configuration:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00274.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: To prove the runtime drop-in configuration change, let's create a configuration
    file where we disable the restart for the Fleet service.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `norestart.conf` under `/etc/systemd.system/system/fleet.service.d`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00276.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s restart the systemd configuration:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '`Sudo systemd daemon-reload`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check the status of `fleet.service` now:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00279.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: We can see that other than `20-cloudinit.conf`, we also have a `norestart.conf`
    drop-in unit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we kill the Fleet service, it does not get restarted as the restart
    option has been disabled by the `restart.conf` drop-in unit. `Fleet.service` stays
    in a failed state, as shown in the following screenshot:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00281.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: Runtime drop-in unit – full service
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'In this approach, we can replace the complete system service using our own
    service. Let''s change the restart option by creating this `fleet.service` file
    in `/etc/systemd/system`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=fleet daemon  After=etcd.service After=etcd2.service  Wants=fleet.socket
    After=fleet.socket  [Service] ExecStart=/usr/bin/fleetd Restart=no  [Install]
    WantedBy=multi-user.target`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start the `fleet.service` as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '`Sudo systemctl start fleet.service`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the status of `fleet.service`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00283.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we can see that `fleet.service` is picked up from
    `/etc/systemd/system`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: If we compare this option (a drop-in unit with a complete service change) with
    the previous option (a drop-in unit with a specific parameter change), the previous
    option gives the flexibility to change specific parameters and not touch the original
    set. This makes it easier to handle upgrades when new versions of the service
    allow additional options.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Network units
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'The `systemd-networkd` service manages networks. System-configured networks
    are specified in `/usr/lib64/systemd/network` and user-configured networks are
    specified in `/etcd/systemd/network`. The following is a sample Vagrant configured
    systemd-network file to configure the `eth1` IP address:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00286.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
- en: 'The `ifconfig` output associated with `eth1` shows the IP address that Vagrant
    configured, as shown in the following screenshot:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00288.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: 'As an example, let''s try to change the `eth1` IP address. There are three
    steps:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Stop `systemd-networkd.service`.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flush the IP address.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a new IP address.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's create a service file to flush the `eth1` IP address and another network
    file specifying the new IP address for `eth1`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Create a service file to flush the `eth1` IP address as follows. We need to
    place this in `/etc/systemd/system/down-eth1.service`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=eth1 flush  [Service] Type=oneshot ExecStart=/usr/bin/ip link set eth1 down
    ExecStart=/usr/bin/ip addr flush dev eth1  [Install] WantedBy=multi-user.target`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the network file to specify the `eth1` new address. We need
    to place this in `/etc/systemd/network/40-eth1.network`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '`[Match] Name=eth1  [Network] Address=172.17.8.110/24 Gateway=172.17.8.1`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to change the IP address are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Stop the system-networkd service by `sudo systemctl stop systemd-networkd.service`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flush the `eth1` IP address by `sudo systemctl start down-eth1.service`.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start `systemd-networkd.service` by `sudo systemctl start systemd-networkd.service`.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we look at the ifconfig output now, we should see the new IP address `172.17.8.110`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00290.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Fleet
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Fleet is a cluster manager/scheduler that controls service creation at the CoreOS
    cluster level. We can think of Fleet as Systemd for the cluster. For an overview
    of Fleet, refer to [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview.
    Fleet is used mainly for the orchestration of critical system services, while
    other orchestration solutions such as Kubernetes are used for application service
    orchestration. Fleet is not under active development and is mostly under the maintenance
    mode.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Fleet is installed and started by default in CoreOS. The following is the Fleet
    version in the CoreOS stable 766.3.0 release:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00293.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: Fleet can also be installed in a standalone Linux machine. Fleet releases can
    be found at [https://github.com/coreos/fleet/releases](https://github.com/coreos/fleet/releases).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Fleet
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: The following are different approaches to access Fleet.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Local fleetctl
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: The `fleetctl` command is present in each CoreOS node and can be used to control
    Fleet services.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Remote fleetctl
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fleetctl` command can be used to access non-local CoreOS nodes by specifying
    an endpoint argument. The following is an example:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '`Fleetctl --endpoint=http://172.17.8.101:2379 list-machines`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00295.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: Remote fleetctl with an SSH tunnel
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: The previous example did not use any authentication. To make access to fleetctl
    secure, we can use the SSH authentication scheme. It is necessary to add the CoreOS
    node private key to the local SSH authentication agent for this mode. For a Vagrant
    CoreOS cluster, the private key is stored in `~/.vagrant.d/insecure_private_key`.
    For an AWS CoreOS cluster, the private key can be downloaded as part of the initial
    key creation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a private key to the authentication agent:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '`` eval `ssh-agent -s` ```ssh-add <private key>`'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00298.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: '![](img/00301.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can use fleetctl to use a secure SSH to access the CoreOS cluster:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '`Fleetctl --tunnel=http://172.17.8.101 list-unit-files`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00303.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: Remote HTTP
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Remote HTTP Fleet API access is disabled by default.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable remote access, create a `.socket` file to expose the Fleet API port.
    The following is an example Fleet configuration file to expose port `49153` for
    external API access:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00305.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: 'It is necessary to restart the systemd, `fleet.socket`, and `fleet.service`
    after creating the remote API configuration file for it to take effect:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '`Sudo systemctl daemon-reload``Sudo systemctl restart fleet.socket``Sudo systemctl restart fleet.service`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can access the remote API. The following is an example using `fleetctl`
    and `curl`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '`Fleetctl --endpoint=http://172.17.8.101:49153 list-units`'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00356.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows you the unit list using the Fleet HTTP API. The
    following curl output is truncated to show partial output:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '`Curl –s http://172.17.8.101:49153/fleet/v1/units | jq .`'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00310.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: Using etcd security
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the secure etcd approach to access Fleet. Setting up a secure
    etcd is covered in the section on Etcd security. The following example shows the
    `fleetctl` command with a server certificate:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '`fleetctl --debug --ca-file ca.crt  --endpoint=https://172.17.8.101:2379 list-machines`'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Templates, scheduling, and HA
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Fleet supports unit specifiers and templates similar to systemd. A unit specifier
    provides you with shortcuts within a service file, and templates provide reusable
    service files. The earlier section on systemd covered details on unit specifiers
    and templates. [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview
    covered the basics of Fleet scheduling and HA.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Fleet metadata for a node can be specified in the Fleet section of `cloud-config`.
    The following example sets the Fleet node metadata for `role` as `web`. Metadata
    can be used in Fleet service files to control scheduling:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '`metadata: "role=services"`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Fleet uses a pretty simple scheduling algorithm, and X-fleet options are used
    to specify constraints while scheduling the service. The following are the available
    X-fleet options:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '`MachineMetaData`: Service gets scheduled based on matching metadata.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineId`: Service gets scheduled based on the specified `MachineId`.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MachineOf`: Service gets scheduled based on other services running in the
    same node. This can be used to schedule tightly coupled services in the same node.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Conflict`: This option can be used to avoid scheduling conflicting services
    in the same node.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Global`: The same service gets scheduled in all the nodes of the cluster.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example uses unit specifiers and templates and illustrates Fleet
    scheduling and HA. The following are some details of the application:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: An application consists of a WordPress container and MySQL container
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WordPress container uses the database from the MySQL container and is linked
    using Docker container linking
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linking across containers is done using the `--link` option, and it works only
    if both containers are on the same host
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet's template feature will be used to launch multiple services using a single
    WordPress and MySQL template, and Fleet's X-fleet constraint feature will be used
    to launch the related containers on the same host
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When one of the nodes in the cluster dies, Fleet's HA mechanism will take care
    of rescheduling the failed units, and we will see it working in this example
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MySQL template service is as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=app-mysql  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill mysql%i
    ExecStartPre=-/usr/bin/docker rm mysql%i ExecStartPre=/usr/bin/docker pull mysql
    ExecStart=/usr/bin/docker run --name mysql%i -e MYSQL_ROOT_PASSWORD=mysql mysql
    ExecStop=/usr/bin/docker stop mysql%i`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The WordPress template service is as follows:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=wordpress  [Service] Restart=always RestartSec=15 ExecStartPre=-/usr/bin/docker kill wordpress%i
    ExecStartPre=-/usr/bin/docker rm wordpress%i ExecStartPre=/usr/bin/docker pull wordpress
    ExecStart=/usr/bin/docker run --name wordpress%i --link mysql%i:mysql wordpress
    ExecStop=/usr/bin/docker stop wordpress%i  [X-Fleet] MachineOf=mysql@%i.service`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some notes on the service:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: We have used `%i` as an instance specifier
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WordPress has an X-fleet constraint to schedule the corresponding MySQL container
    in the same node
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first step is to submit the services:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00311.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: 'The next step is to load each instance of the service:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00425.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: 'Let''s check whether all the services are running. As can be seen in the following
    screenshot, we have three instances of the WordPress application and the associated
    MySQL database:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00313.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
- en: To demonstrate HA, let's kill CoreOS `node2`. This can be done by shutting down
    the node.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, there are only two nodes in the cluster now:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00467.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: 'From the following new service output, we can see that the services running
    on the old `node2` have been moved to `node3` now as `node2` is not available:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00316.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: Debugging
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'The status of the Fleet service can be checked using `fleetctl status`. The
    following is an example:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00011.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: 'Logs of the Fleet service can be checked using `fleetctl journal`. The following
    is an example:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00319.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
- en: 'For debugging and to get the REST API corresponding to the `fleetctl` command,
    we can use the `--debug` option as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00040.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: Service discovery
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Microservices are dynamic, and it is important for services to discover other
    services dynamically to find the IP address, port number, and metadata about the
    services. There are multiple schemes available to discover services, and in this
    section, we will cover a few schemes using etcd and Fleet for service discovery.
    In the later chapters of the book, we will cover advanced service discovery options.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Simple etcd-based discovery
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows you the simplest possible service discovery mechanism,
    where a service updates etcd with service-related details that other services
    can access from etcd:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00323.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: 'The following is an example Apache service, `apacheupdateetcd@.service`, that
    updates the hostname and port number in etcd when the service is started:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=My Advanced Service After=etcd2.service After=docker.service  [Service]
    TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill apache%i ExecStartPre=-/usr/bin/docker rm apache%i
    ExecStartPre=/usr/bin/docker pull coreos/apache ExecStart=/usr/bin/docker run --name apache%i -p %i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND
    ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running ExecStop=/usr/bin/docker stop apache%i
    ExecStopPost=/usr/bin/etcdctl rm /domains/example.com/%H:%i  [X-Fleet] # Don''t schedule on the same machine as other Apache instances
    X-Conflicts=apache*@*.service`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the service and create two instances:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00324.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: '![](img/00327.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can verify that etcd gets updated with the service details of the two
    services:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00330.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: Sidekick discovery
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding scheme, there is no way to know if the service is alive and
    running after it has been started. The following figure shows you a slightly advanced
    service discovery scheme where a sidekick service updates etcd with the details
    of the service:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00333.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: The purpose of the Side kick container is to monitor the main service and update
    etcd only if the Service is active. It is important to run the Side kick container
    in the same node as the main service that Side kick is monitoring.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: The following is a Sidekick example using the Apache service and a sidekick
    for the Apache service.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the `Apache.service` unit file:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=Apache web server service on port %i  # Requirements Requires=etcd2.service
    Requires=docker.service Requires=apachet-discovery@%i.service  # Dependency ordering
    After=etcd2.service After=docker.service Before=apachet-discovery@%i.service  [Service]
    # Let processes take awhile to start up (for first run Docker containers) TimeoutStartSec=0  # Change killmode from "control-group" to "none" to let Docker remove
    # work correctly. KillMode=none  # Get CoreOS environmental variables EnvironmentFile=/etc/environment  # Pre-start and Start
    ## Directives with "=-" are allowed to fail without consequence ExecStartPre=-/usr/bin/docker kill apachet.%i
    ExecStartPre=-/usr/bin/docker rm apachet.%i ExecStartPre=/usr/bin/docker pull coreos/apache
    ExecStart=/usr/bin/docker run --name apachet.%i -p ${COREOS_PUBLIC_IPV4}:%i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND  # Stop
    ExecStop=/usr/bin/docker stop apachet.%i`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the Apache sidekick service unit file:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=Apache Sidekick  # Requirements Requires=etcd2.service
    Requires=apachet@%i.service  # Dependency ordering and binding After=etcd2.service
    After=apachet@%i.service BindsTo=apachet@%i.service  [Service]  # Get CoreOS environmental variables
    EnvironmentFile=/etc/environment  # Start ## Test whether service is accessible and then register useful information
    ExecStart=/bin/bash -c ''\   while true; do \     curl -f ${COREOS_PUBLIC_IPV4}:%i; \
        if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        else \       etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}; \     fi; \
        sleep 20; \   done''  # Stop ExecStop=/usr/bin/etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}  [X-Fleet]
    # Schedule on the same machine as the associated Apache service X-ConditionMachineOf=apachet@%i.service`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: The preceding Side kick container service does a periodic ping to the main service
    and updates the etcd output. If the main service is not reachable, the service-related
    details are removed from etcd.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start two instances of the service:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00337.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see the etcd output. As shown in the following screenshot, etcd reflects
    the two nodes where Apache is running:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00341.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see the docker output in node1:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00345.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
- en: 'To demonstrate the sidekick service, let''s stop the docker container and check
    whether the sidekick service updates etcd in order to remove the appropriate service:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00349.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
- en: Let's check the status of the units. As can be seen below, `apachet@2.service`
    has failed and the associated sidekick service `apachet-discovery@2.service` is
    inactive.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00352.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: 'From the following output, we can see that the `apachet@2.service` details
    are removed from etcd:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00355.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
- en: ELB service discovery
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: This is a variation of the Sidekick discovery in which, instead of Sidekick
    updating etcd, Sidekick updates the IP address to the load balancer and the load
    balancer redirects the web query to the active nodes. In this example, we will
    use the AWS Elastic load balancer and CoreOS elb-presence container available
    in the Quay repository. The elb-presence container takes care of checking the
    health of the nginx container and updates AWS ELB with the container's IP address.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows you a high-level architecture of this approach:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00264.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
- en: 'The first step is to create ELB in AWS, as shown in the following screenshot.
    Here we have used AWS CLI to create ELB, `testlb`:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpg)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
- en: We need to use `testlb` created in the preceding screenshot in the Sidekick
    service.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the `nginx.service` unit file:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=nginx  [Service] ExecStartPre=-/usr/bin/docker kill nginx-%i
    ExecStartPre=-/usr/bin/docker rm nginx-%i ExecStart=/usr/bin/docker run --rm --name nginx-%i -p 80:80 nginx
    ExecStop=/usr/bin/docker stop nginx-%i  [X-Fleet] Conflicts=nginx@*.service`'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the nginx sidekick service that updates AWS ELB based on the health
    of `nginx.service`:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=nginx presence service BindsTo=nginx@%i.service  [Service]
    ExecStartPre=-/usr/bin/docker kill nginx-presence-%i ExecStartPre=-/usr/bin/docker rm nginx-presence-%i
    ExecStart=/usr/bin/docker run --rm --name nginx-presence-%i -e AWS_ACCESS_KEY=<key> -e AWS_SECRET_KEY=<secretkey> -e AWS_REGION=us-west-2 -e ELB_NAME=testlb quay.io/coreos/elb-presence
    ExecStop=/usr/bin/docker stop nginx-presence-%i  [X-Fleet] MachineOf=nginx@%i.service`'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Fleet status after creating two instances of the service:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
- en: 'As we can see in the following screenshot, AWS ELB has both the instances registered,
    and it will load-balance between these two instances:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
- en: At this point, if we stop any instance of the nginx service, the Sidekick service
    will take care of removing this instance from ELB.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered the internals of Etcd, Systemd, and Fleet with sufficient
    hands-on examples, which will allow you to get comfortable with configuring and
    using these services. By keeping the development of the critical services open
    source, CoreOS has encouraged the usage of these services outside CoreOS as well.
    We also covered the basic service discovery options using Etcd, Systemd, and Fleet.
    In the next chapter, we will cover container networking and Flannel.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'Etcd docs: [https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fleet docs: [https://coreos.com/fleet/docs/latest/](https://coreos.com/fleet/docs/latest/)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Systemd docs: [http://www.freedesktop.org/wiki/Software/systemd/](http://www.freedesktop.org/wiki/Software/systemd/)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fleet service discovery: [https://coreos.com/fleet/docs/latest/examples/service-discovery.html](https://coreos.com/fleet/docs/latest/examples/service-discovery.html)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Etcd-ca: [https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Etcd security: [https://github.com/coreos/etcd/blob/master/Documentation/security.md](https://github.com/coreos/etcd/blob/master/Documentation/security.md)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'Etcd security and authentication: [http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/](http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Etcd administration: [https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md](https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md)'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Why systemd: [http://blog.jorgenschaefer.de/2014/07/why-systemd.html](http://blog.jorgenschaefer.de/2014/07/why-systemd.html)'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparing init systems: [http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html](http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Systemd talk by the Systemd creator: [https://www.youtube.com/watch?v=VIPonFvPlAs](https://www.youtube.com/watch?v=VIPonFvPlAs)'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Service discovery overview: [http://www.gomicro.services/articles/service-discovery-overview](http://www.gomicro.services/articles/service-discovery-overview)
    and [http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Highly available Docker services using CoreOS and Consul: [http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/](http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/)
    and [http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/](http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
