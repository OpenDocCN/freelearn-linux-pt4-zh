- en: Chapter 4. CoreOS Primary Services – Etcd, Systemd, and Fleet
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章 CoreOS 主要服务 – Etcd、Systemd 和 Fleet
- en: 'This chapter will cover the internals of CoreOS'' critical services—Etcd, Systemd,
    and Fleet. For each of the services, we will cover installation, configuration,
    and their applications. CoreOS ships with Etcd, Systemd, and Fleet by default.
    They can also be installed as standalone components in any Linux system. The following
    topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 CoreOS 关键服务——Etcd、Systemd 和 Fleet 的内部工作。对于每个服务，我们将涵盖安装、配置以及它们的应用。CoreOS
    默认包括 Etcd、Systemd 和 Fleet。这些服务也可以作为独立组件安装在任何 Linux 系统上。本章将涵盖以下主题：
- en: Etcd—installation, access methods, configuration options, use cases, tuning,
    cluster management, security, authentication, and debugging
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd——安装、访问方法、配置选项、使用案例、调优、集群管理、安全性、认证和调试
- en: Systemd—unit types, specifiers, templates, and special units
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Systemd——单元类型、说明符、模板和特殊单元
- en: Fleet—installation, access methods, templates, scheduling, HA, and debugging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet——安装、访问方法、模板、调度、高可用性和调试
- en: Service discovery options using Etcd and Fleet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Etcd 和 Fleet 的服务发现选项
- en: Etcd
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd
- en: Etcd is a distributed key-value store used by all the machines in the CoreOS
    cluster to read/write and exchange data. An overview of etcd is provided in [Chapter
    1](index_split_023.html#filepos77735), CoreOS Overview. This section will cover
    the internals of etcd.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 是一个分布式键值存储，CoreOS 集群中的所有机器都使用它来读写和交换数据。[第一章](index_split_023.html#filepos77735)中提供了
    Etcd 的概述。本节将介绍 Etcd 的内部工作。
- en: Versions
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 版本
- en: 'Etcd is under continuous development, and frequent releases are done to add
    enhancements as well as fix bugs. The following are some major updates from recent
    etcd releases:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 正在持续开发中，频繁发布版本以添加新功能并修复漏洞。以下是最近版本更新的主要内容：
- en: Version 2.0 is the first stable release and was released in January 2015\. Pre-version
    2.0 is available as etcd and post-version 2.0 is available as etcd2 in CoreOS
    nodes.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.0 是第一个稳定版本，于 2015 年 1 月发布。版本 2.0 之前的版本作为 etcd 提供，而版本 2.0 之后的版本在 CoreOS
    节点中作为 etcd2 提供。
- en: Version 2.0 added IANA-assigned ports `2379` for client-to-server communication
    and `2380` for server-to-server communication. Previously, port `4001` was used
    for client-to-server communication and port `7001` was used for server-to-server
    communication.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.0 增加了 IANA 分配的端口 `2379` 用于客户端与服务器的通信，`2380` 用于服务器与服务器的通信。之前，端口 `4001` 用于客户端与服务器的通信，端口
    `7001` 用于服务器与服务器的通信。
- en: Version 2.1 introduced authentication and metrics collection features and these
    are in experimental mode.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本 2.1 引入了认证和度量收集功能，这些功能处于实验模式中。
- en: The latest release as of September 2015 is 2.2.0.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至 2015 年 9 月，最新版本为 2.2.0。
- en: An experimental v3 API (some examples are multikey reads, range reads, and binary
    keys) is available now as a preview and will be available officially in version
    2.3.0 scheduled at the end of October 2015.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个实验性的 v3 API（一些示例包括多键读取、范围读取和二进制键）现在作为预览版可用，预计将在 2015 年 10 月底的 2.3.0 版本中正式发布。
- en: All examples in this chapter are based on etcd version 2.1.0 and above.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有示例均基于 etcd 版本 2.1.0 及以上版本。
- en: Installation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 安装
- en: 'CoreOS ships with etcd. Both the etcd and etcd2 versions are available in the
    base CoreOS image. The following are the etcd versions available in the CoreOS
    alpha image 779.0.0:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 自带 etcd。etcd 和 etcd2 版本都包含在基础 CoreOS 镜像中。以下是 CoreOS alpha 镜像 779.0.0
    中提供的 etcd 版本：
- en: '![](img/00049.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00049.jpg)'
- en: Standalone installation
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 独立安装
- en: 'Etcd can also be installed on any Linux machine. The following is the installation
    command tried out on Ubuntu 14.04 to install etcd version 2.2:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 也可以安装在任何 Linux 机器上。以下是尝试在 Ubuntu 14.04 上安装 etcd 版本 2.2 的安装命令：
- en: '`curl -L  https://github.com/coreos/etcd/releases/download/v2.2.0/etcd-v2.2.0-linux-amd64.tar.gz -o etcd-v2.2.0-linux-amd64.tar.gz``tar xzvf etcd-v2.2.0-linux-amd64.tar.gz`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl -L https://github.com/coreos/etcd/releases/download/v2.2.0/etcd-v2.2.0-linux-amd64.tar.gz
    -o etcd-v2.2.0-linux-amd64.tar.gz``tar xzvf etcd-v2.2.0-linux-amd64.tar.gz`'
- en: 'The following example shows you how to try out etcd in the standalone mode.
    To start the server run the following command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何在独立模式下尝试使用 etcd。启动服务器时运行以下命令：
- en: '`etcd -name etcdtest`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd -name etcdtest`'
- en: 'Now, check whether we can connect to the etcd server using some basic commands:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查是否可以通过一些基本命令连接到 etcd 服务器：
- en: '`etcdctl cluster-health`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl cluster-health`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前述命令的输出：
- en: '![](img/00053.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00053.jpg)'
- en: 'The following is an example of a simple set and get operation using the curl
    interface:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用curl接口进行简单的set和get操作的示例：
- en: '`curl –L –X PUT http://127.0.0.1:2379/v2/keys/message -d value="hello"``curl –L http://127.0.0.1:2379/v2/keys/message`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L –X PUT http://127.0.0.1:2379/v2/keys/message -d value="hello"``curl
    –L http://127.0.0.1:2379/v2/keys/message`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00055.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00055.jpg)'
- en: Accessing etcd
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 访问etcd
- en: 'Etcd can be accessed using either etcdctl CLI or REST API. This applies to
    both the standalone etcd as well as etcd in CoreOS. The following figure shows
    you the different ways to access etcd:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过etcdctl CLI或REST API访问Etcd。这适用于独立的etcd以及CoreOS中的etcd。下图展示了访问etcd的不同方式：
- en: '![](img/00058.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpg)'
- en: REST
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: REST
- en: The etcd database can be accessed and modified through the REST API. The etcd
    database can be accessed either locally or remotely using this approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过REST API访问和修改etcd数据库。使用这种方式可以本地或远程访问etcd数据库。
- en: 'The following example shows the `curl` method to access the CoreOS node to
    get all the keys:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用`curl`方法访问CoreOS节点以获取所有键：
- en: '`curl –L http://localhost:2379/v2/keys/?recursive=true`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L http://localhost:2379/v2/keys/?recursive=true`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00060.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00060.jpg)'
- en: 'The following example shows the `curl` method to access the remote CoreOS node
    to get all the keys:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用`curl`方法访问远程CoreOS节点以获取所有键：
- en: '`curl –L http://172.17.8.101:2379/v2/keys/?recursive=true`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl –L http://172.17.8.101:2379/v2/keys/?recursive=true`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00063.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00063.jpg)'
- en: Etcdctl
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Etcdctl
- en: Etcdctl is a CLI wrapper on top of the REST interface. Etcdctl can be used for
    local or remote access.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Etcdctl是REST接口之上的CLI封装。Etcdctl可以用于本地或远程访问。
- en: 'The following example shows etcdctl method to access the CoreOS node to get
    all the keys:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用etcdctl方法访问CoreOS节点以获取所有键：
- en: '`etcdctl ls / --recursive`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl ls / --recursive`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00067.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00067.jpg)'
- en: 'The following example shows etcdctl method to access the remote CoreOS node
    to get all the keys:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用etcdctl方法访问远程CoreOS节点以获取所有键：
- en: '`etcdctl --peers=http://172.17.8.101:2379 ls / --recursive`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --peers=http://172.17.8.101:2379 ls / --recursive`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00070.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00070.jpg)'
- en: Etcd configuration
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd配置
- en: 'Etcd configuration parameters can be used to modify the etcd member property
    or cluster-wide property. Etcd options can be set either in the command line or
    using environment variables. The command line will override the environment variables.
    The following are the broad categories and their critical configuration parameters/environment
    variables:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd配置参数可以用来修改etcd成员属性或集群范围属性。Etcd选项可以通过命令行或使用环境变量进行设置。命令行设置会覆盖环境变量。以下是一些广泛的分类及其关键配置参数/环境变量：
- en: 'Member: Name, data-dir, and heartbeat interval'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成员：名称、数据目录和心跳间隔
- en: 'Cluster: Discovery token and initial cluster nodes'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群：发现令牌和初始集群节点
- en: 'Proxy: Proxy on/off and proxy intervals'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理：代理开/关和代理间隔
- en: 'Security: Certificate and key'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全：证书和密钥
- en: 'Logging: Enable/disable logging and logging levels'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志：启用/禁用日志记录和日志级别
- en: Experimental
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验性
- en: 'The following is an etcd invocation example, where we use some of the preceding
    configuration parameters:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个etcd调用示例，其中使用了一些前述配置参数：
- en: '`etcd -name infra0 -data-dir infra0  --cacert=~/.etcd-ca/ca.crt -cert-file=/home/smakam14/infra0.crt -key-file=/home/smakam14/infra0.key.insecure  -advertise-client-urls=https://192.168.56.104:2379 -listen-client-urls=https://192.168.56.104:2379`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd -name infra0 -data-dir infra0  --cacert=~/.etcd-ca/ca.crt -cert-file=/home/smakam14/infra0.crt
    -key-file=/home/smakam14/infra0.key.insecure  -advertise-client-urls=https://192.168.56.104:2379
    -listen-client-urls=https://192.168.56.104:2379`'
- en: 'Etcd environment variables can also be specified in `cloud-config`. The following
    is a `cloud-config` example to specify etcd environment variables:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd环境变量也可以在`cloud-config`中指定。以下是一个`cloud-config`示例，用于指定etcd环境变量：
- en: '`etcd2:     #generate a new token for each unique cluster from https://discovery.etcd.io/new
        discovery: https://discovery.etcd.io/d93c8c02eedadddd3cf14828f9bec01c     # multi-region and multi-cloud deployments need to use $public_ipv4
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2:     #为每个唯一集群生成一个新令牌，来自https://discovery.etcd.io/new     discovery: https://discovery.etcd.io/d93c8c02eedadddd3cf14828f9bec01c
        # 多区域和多云部署需要使用$public_ipv4     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # 同时监听官方端口和遗留端口
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001`'
- en: 'Etcd2 environment variables from cloud-config are stored in the following directory:
    `/run/systemd/system/etcd2.service.d`.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从cloud-config中获取的etcd2环境变量存储在以下目录：`/run/systemd/system/etcd2.service.d`。
- en: The etcd2 service needs to be restarted if the environment variables are changed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境变量发生更改，etcd2服务需要重新启动。
- en: A complete list of configuration parameters and environment variables for etcd
    can be found at [https://coreos.com/etcd/docs/latest/configuration.html](https://coreos.com/etcd/docs/latest/configuration.html).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://coreos.com/etcd/docs/latest/configuration.html](https://coreos.com/etcd/docs/latest/configuration.html)找到完整的etcd配置参数和环境变量列表。
- en: Etcd operations
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 操作
- en: 'The following are some examples of major operations that can be done using
    etcd:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以使用etcd执行的主要操作示例：
- en: Set, get, and delete operations of a key-value pair
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键值对的设置、获取和删除操作
- en: Set a key with timeout where the key expires automatically
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置带有超时的键，其中该键会自动过期
- en: Set a key based on the atomic condition check
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据原子条件检查设置一个键
- en: Hidden keys
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏键
- en: Watching and waiting for key changes
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监视并等待键的变化
- en: Creating in-order keys
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建有序的键
- en: 'Using these operations, etcd can be used for a variety of distributed application
    use cases. The following is an example TTL use case where we check for the liveliness
    of the Apache service and update service details such as the IP address and port
    number in etcd, which other applications can use to determine if the service is
    running or not. If the Apache service dies, the etcd key-value pair will be deleted
    after 30 seconds in this case:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些操作，etcd可以用于各种分布式应用场景。以下是一个TTL使用场景的示例，其中我们检查Apache服务的存活性，并更新如IP地址和端口号等服务详情到etcd，其他应用可以使用这些信息来判断服务是否正在运行。如果Apache服务停止，在这种情况下，etcd中的键值对将在30秒后被删除：
- en: '`## Test whether service is accessible and then register useful information like IP address, port
    ExecStart=/bin/bash -c ''\   while true; do \     curl -f ${COREOS_PUBLIC_IPV4}:%i; \
        if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        fi; \     sleep 20; \   done''`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`## 测试服务是否可访问，然后注册有用的信息，如IP地址，端口 ExecStart=/bin/bash -c ''\   while true; do \
        curl -f ${COREOS_PUBLIC_IPV4}:%i; \     if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        fi; \     sleep 20; \   done''`'
- en: We can find statistics about the etcd node as well as the key-related operations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以找到关于etcd节点的统计信息，以及与键相关的操作。
- en: 'The following output shows the etcd node statistics:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了etcd节点的统计信息：
- en: '`curl http://127.0.0.1:2379/v2/stats/self | jq .`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl http://127.0.0.1:2379/v2/stats/self | jq .`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00072.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00072.jpg)'
- en: 'The following output shows the etcd key statistics:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了etcd键的统计信息：
- en: '`curl http://127.0.0.1:2379/v2/stats/store | jq .`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl http://127.0.0.1:2379/v2/stats/store | jq .`'
- en: 'The following screenshot is the output of the preceding command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的输出：
- en: '![](img/00076.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00076.jpg)'
- en: Etcd tuning
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 调优
- en: 'The following are some etcd parameters that can be tuned to achieve optimum
    cluster performance based on the operating environment:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可以调整的etcd参数，以根据操作环境实现最佳集群性能：
- en: 'Cluster size: A bigger cluster size provides you with better redundancy. The
    disadvantage with big cluster sizes is that updates can take a long time. In [Chapter
    1](index_split_023.html#filepos77735), CoreOS Overview, we saw the failure tolerance
    limit with different cluster sizes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群大小：较大的集群可以提供更好的冗余。大集群的缺点是更新可能需要较长时间。在[第1章](index_split_023.html#filepos77735)，CoreOS概述中，我们看到了不同集群大小下的容错极限。
- en: 'Heartbeat interval: This is the time interval at which the master node sends
    a heartbeat message to its followers. The default heartbeat interval is 100 ms.
    It is necessary to choose a heartbeat interval based on the average round-trip
    time taken for the ping between nodes. If the nodes are geographically distributed,
    then the round-trip time will be longer. The suggested heartbeat interval is 0.5-1.5
    x the average round-trip time. If we choose a small heartbeat interval, the overhead
    will be a higher number of packets. If we choose a large heartbeat interval, it
    will take a longer time to detect leader failure. The heartbeat interval can be
    set using the `heartbeat-interval` parameter in the etcd command line or the `ETCD_HEARTBEAT_INTERVAL`
    environment variable.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 心跳间隔：这是主节点向其跟随节点发送心跳消息的时间间隔。默认的心跳间隔为 100 毫秒。选择心跳间隔时需要根据节点之间 ping 的平均往返时间来决定。如果节点地理分布较广，往返时间会更长。建议的心跳间隔是平均往返时间的
    0.5-1.5 倍。如果选择较小的心跳间隔，数据包的开销会更高；如果选择较大的心跳间隔，则需要更长的时间才能检测到领导者故障。心跳间隔可以通过 `heartbeat-interval`
    参数在 etcd 命令行中设置，或者通过 `ETCD_HEARTBEAT_INTERVAL` 环境变量设置。
- en: 'Election timeout: When the follower nodes fail to get a heartbeat message for
    the election timeout value, they become the leader node. The default election
    timeout is 1,000 ms. The suggested value for election timeout is 10 times the
    heartbeat interval. Keeping the election timeout too low can cause false leader
    election. The election timeout can be set using the `election-timeout` parameter
    in the etcd command line or the `ETCD_ELECTION_TIMEOUT` environment variable.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举超时：当跟随节点在选举超时值内未能接收到心跳消息时，它们会成为领导者节点。默认的选举超时为 1,000 毫秒。建议的选举超时值是心跳间隔的 10 倍。将选举超时设置得过低可能会导致错误的领导者选举。选举超时可以通过
    `election-timeout` 参数在 etcd 命令行中设置，或者通过 `ETCD_ELECTION_TIMEOUT` 环境变量设置。
- en: Etcd proxy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 代理
- en: An etcd proxy is used when worker nodes want to use the master node or master
    cluster to provide etcd service. In this case, all etcd requests from the worker
    node are proxied to the master node and the master node replies to the worker
    node.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当工作节点需要使用主节点或主集群提供 etcd 服务时，会使用 etcd 代理。在这种情况下，所有来自工作节点的 etcd 请求都将被代理到主节点，主节点再回复工作节点。
- en: 'Let''s say that we have a working three-node master cluster as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个正在运行的三节点主集群，配置如下：
- en: '![](img/00080.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00080.jpg)'
- en: 'The following example shows the `cloud-config` for the fourth node that is
    a worker node and acting as a proxy. Here, the master cluster members are mentioned
    statically:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了作为工作节点并充当代理的第四个节点的 `cloud-config` 配置。在此，主集群成员是静态指定的：
- en: '`#cloud-config coreos:   etcd2:     proxy: on     listen-client-urls: http://localhost:2379
        initial-cluster: etcdserver=http://172.17.8.101:2380, http://172.17.8.102:2380, http://172.17.8.103:2380
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     proxy: on     listen-client-urls: http://localhost:2379
        initial-cluster: etcdserver=http://172.17.8.101:2380, http://172.17.8.102:2380, http://172.17.8.103:2380
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: In the preceding Etcd configuration section, we have turned on the proxy and
    pointed to the `etcd_server` cluster. The fourth node needs to be started with
    the preceding `cloud-config`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Etcd 配置部分中，我们启用了代理并指向了 `etcd_server` 集群。第四个节点需要使用上述 `cloud-config` 配置启动。
- en: 'The following example shows the `cloud-config` for the fourth node that is
    acting as a proxy and using a discovery token. We need to use the same discovery
    token as we did for the three-node cluster:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了作为代理并使用发现令牌的第四个节点的 `cloud-config` 配置。我们需要使用与三节点集群相同的发现令牌：
- en: '`#cloud-config coreos:   etcd2:     proxy: on     # use the same discovery token as for master, these nodes will proxy to master
        discovery: <your token>     # listen on both the official ports and the legacy ports
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001   fleet:     etcd_servers: "http://localhost:2379"
        public-ip: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     proxy: on     # 使用与主节点相同的发现令牌，这些节点将代理到主节点
        discovery: <your token>     # 监听官方端口和旧版端口     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
      fleet:     etcd_servers: "http://localhost:2379"     public-ip: $public_ipv4
      units:     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: 'The following is the etcd member output in the new node. As we can see, the
    etcd cluster is composed of only three nodes and the new node is proxying to the
    master etcd cluster:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是新节点上的etcd成员输出。如我们所见，etcd集群由三个节点组成，且新节点正在代理主etcd集群：
- en: '![](img/00085.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpg)'
- en: 'The following is the Fleet machine''s output in the new node. As we can see,
    there are four nodes and this includes the fourth worker node and the three-node
    etcd cluster:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是新节点上的Fleet机器输出。如我们所见，共有四个节点，包括第四个工作节点和三节点的etcd集群：
- en: '![](img/00482.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00482.jpg)'
- en: Adding and removing nodes from a cluster
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 向集群中添加和移除节点
- en: There will be scenarios where we need to add and remove nodes from a working
    etcd cluster. This section illustrates how to add and remove nodes in a working
    etcd cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要从一个正在运行的etcd集群中添加或移除节点。本节将演示如何在运行中的etcd集群中添加和移除节点。
- en: 'Let''s say that we have a three-node working cluster and we want to add a fourth
    node to the cluster. The following command can be executed in one of the three
    working nodes to add the fourth node detail:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个三节点的工作集群，并且想要向集群中添加第四个节点。可以在任一三节点中的一台上执行以下命令，添加第四个节点的详细信息：
- en: '![](img/00093.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00093.jpg)'
- en: 'The following `cloud-config` can be used to start the new fourth node:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`cloud-config`可以用来启动新的第四节点：
- en: '`#cloud-config coreos:   etcd2:     name: core-04     initial_cluster: "core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380,core-04=http://172.17.8.104:2380"
        initial_cluster_state: existing     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   units:     # Note: this requires a release that contains etcd2
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-04     initial_cluster: "core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380,core-04=http://172.17.8.104:2380"     initial_cluster_state:
    existing     advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls:
    http://$private_ipv4:2380     # listen on both the official ports and the legacy
    ports     # legacy ports can be omitted if your application doesn''t depend on
    them     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls:
    http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:     public-ip: $public_ipv4   units:     #
    Note: this requires a release that contains etcd2     - name: etcd2.service       command:
    start     - name: fleet.service       command: start`'
- en: 'From the following output, we can see that the new member has been successfully
    added:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出可以看出，新的节点已经成功添加：
- en: '![](img/00095.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00095.jpg)'
- en: 'The following command can be used to remove the fourth number that we added
    before:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令移除之前添加的第四个节点：
- en: '![](img/00138.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.jpg)'
- en: 'Let''s check the member list and cluster health now. We can see that the three
    nodes are part of the cluster and that the fourth node has been removed:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查成员列表和集群健康状况。我们可以看到，三个节点是集群的一部分，第四个节点已被移除：
- en: '![](img/00103.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpg)'
- en: Node migration and backup
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 节点迁移与备份
- en: Node migration is necessary to handle failure of the node and cluster and also
    to replicate the cluster to a different location.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 节点迁移是处理节点和集群故障以及将集群复制到不同位置时所必需的。
- en: 'To take a backup of the etcd database, we can perform the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了备份etcd数据库，我们可以执行以下操作：
- en: '`Sudo etcdctl backup --data-dir=/var/lib/etcd2 --backup-dir=/tmp/etcd2`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo etcdctl backup --data-dir=/var/lib/etcd2 --backup-dir=/tmp/etcd2`'
- en: This approach allows us to reuse the backed-up etcd data in another cluster.
    In this approach, `nodeid` and `clusterid` are overwritten in the backup directory
    to prevent unintentional addition of a new node to the old cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法允许我们在另一个集群中重新使用备份的etcd数据。在这种方法中，备份目录中的`nodeid`和`clusterid`被覆盖，以防止无意中将新节点添加到旧集群。
- en: To preserve the node ID and cluster ID, we have to manually make a copy, and
    the copy can be used to restart the service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留节点ID和集群ID，我们必须手动复制，且复制品可以用来重启服务。
- en: 'The following are the steps to move the etcd2 data directory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是移动etcd2数据目录的步骤：
- en: 'Stop the service:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止服务：
- en: '![](img/00106.jpg)'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00106.jpg)'
- en: 'Make a copy of the `/var/lib/etcd2` etcd data directory in `/tmp/etcd2_backup`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`/var/lib/etcd2`的etcd数据目录复制到`/tmp/etcd2_backup`：
- en: '![](img/00196.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00196.jpg)'
- en: 'Start etcd2 manually using the new data directory, `/tmp/etcd2_backup`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的数据目录`/tmp/etcd2_backup`手动启动etcd2：
- en: '![](img/00115.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00115.jpg)'
- en: 'There are two approaches to handle the migration:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 处理迁移有两种方法：
- en: Add a new member and remove the old member. We can use `etcdctl member add`
    and `etcdctl member remove`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个新成员并移除旧成员。我们可以使用`etcdctl member add`和`etcdctl member remove`。
- en: Make a copy of the etcd database, move it to the new node, and update it.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制etcd数据库，将其移动到新节点并更新。
- en: With the first approach, the new member has a different identity. With the second
    approach, we can have the new node retain the same old identity. With the first
    approach, there is no need to stop the etcd service, while we need to stop the
    etcd service before taking the backup in the second approach.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法中新成员有不同的身份。第二种方法则可以让新节点保持相同的旧身份。在第一种方法中，不需要停止etcd服务，而在第二种方法中，在进行备份之前需要停止etcd服务。
- en: Etcd security
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd安全性
- en: 'A secure etcd is needed to ensure that the client-to-server communication and
    server-to-server communication are secure. The following figure shows you the
    different components involved in providing etcd security. Certificate authority
    is used to provide and verify certificates for the etcd client-to-server and server-to-server
    communication:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个安全的etcd来确保客户端与服务器之间的通信以及服务器之间的通信是安全的。下图展示了提供etcd安全性所涉及的不同组件。证书颁发机构用于提供和验证etcd客户端到服务器以及服务器到服务器的通信证书：
- en: '![](img/00117.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00117.jpg)'
- en: Certificate authority – etcd-ca
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 证书颁发机构 – etcd-ca
- en: Certificate authority is a trusted source that issues certificates to a trusted
    server. Other than using standard certificate authorities (CA), etcd allows for
    a custom CA. Etcd-ca ([https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca))
    is a GO application that can be used as a CA for testing purposes. Recently, etcd
    has migrated to CFSSL ([https://github.com/cloudflare/cfssl](https://github.com/cloudflare/cfssl))
    as the official tool for certificates.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 证书颁发机构是一个可信的来源，用于向可信服务器颁发证书。除了使用标准的证书颁发机构（CA）外，etcd还允许使用自定义CA。Etcd-ca ([https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca))
    是一个GO应用程序，可以作为测试目的使用的CA。最近，etcd已迁移至CFSSL ([https://github.com/cloudflare/cfssl](https://github.com/cloudflare/cfssl))
    作为官方证书工具。
- en: Installing etcd-ca
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 安装etcd-ca
- en: 'I installed etcd-ca in my Linux VM running Ubuntu 14.04 using the following
    steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我在运行Ubuntu 14.04的Linux虚拟机上使用以下步骤安装了etcd-ca：
- en: '`git clone https://github.com/coreos/etcd-ca``cd etcd-ca``./build`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/etcd-ca``cd etcd-ca``./build`'
- en: Note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: The GO application needs to be installed before the etcd-ca installation.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在安装etcd-ca之前，需要先安装GO应用程序。
- en: 'Following three steps are needed to setup etcd-ca:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 设置etcd-ca需要以下三个步骤：
- en: Creating a CA using etcd-ca.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用etcd-ca创建CA。
- en: Creating server keys.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建服务器密钥。
- en: Creating client keys.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建客户端密钥。
- en: 'The following command, `etcd-ca init`, is used to create a CA. This is a one-time
    procedure. The following screenshot shows you the output when creating a CA:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令`etcd-ca init`用于创建一个CA。这是一个一次性的过程。以下截图展示了创建CA时的输出：
- en: '![](img/00317.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00317.jpg)'
- en: 'The following commands can be used to create a server certificate:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可以用于创建服务器证书：
- en: '`etcd-ca new-cert -ip  172.17.8.101 core-01``etcd-ca sign core-01``etcd-ca chain core-01``etcd-ca export --insecure core-01 | tar xvf –`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd-ca new-cert -ip 172.17.8.101 core-01``etcd-ca sign core-01``etcd-ca chain
    core-01``etcd-ca export --insecure core-01 | tar xvf –`'
- en: In the preceding command, `172.17.8.101` is the CoreOS node IP and `core-01`
    is the node name. These steps will create `core-01.crt` and `core-01.key.insecure`.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`172.17.8.101`是CoreOS节点的IP，`core-01`是节点名称。这些步骤将创建`core-01.crt`和`core-01.key.insecure`。
- en: 'The following commands can be used to create a client certificate:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可以用于创建客户端证书：
- en: '`etcd-ca new-cert -ip 192.168.56.104 client``etcd-ca sign client``etcd-ca chain client``etcd-ca export --insecure client | tar xvf -`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd-ca new-cert -ip 192.168.56.104 client``etcd-ca sign client``etcd-ca chain
    client``etcd-ca export --insecure client | tar xvf -`'
- en: In the preceding command, `192.168.56.104` is the client node IP. These steps
    will create `client.crt` and `client.key.insecure`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，`192.168.56.104`是客户端节点的IP。这些步骤将创建`client.crt`和`client.key.insecure`。
- en: Etcd secure client-to-server communication using a server certificate
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务器证书实现etcd安全的客户端与服务器通信
- en: 'A server certificate is used by the client to ensure the server''s identity.
    The following command starts the etcd server using a server certificate and the
    key that was generated in the previous section:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端使用服务器证书来确保服务器的身份。以下命令使用在前一部分生成的服务器证书和密钥启动 etcd 服务器：
- en: '`etcd2 -name core-01 --cert-file=/home/core/core-01.crt --key-file=/home/core/core-01.key  --advertise-client-urls=https://172.17.8.101:2379 --listen-client-urls=https://172.17.8.101:2379`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2 -name core-01 --cert-file=/home/core/core-01.crt --key-file=/home/core/core-01.key  --advertise-client-urls=https://172.17.8.101:2379
    --listen-client-urls=https://172.17.8.101:2379`'
- en: 'The following is an example to set a key and retrieve it using a secure mechanism:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示如何使用安全机制设置密钥并检索它：
- en: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo
    -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo`'
- en: 'The following example uses etcdctl to do the key retrieval:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用 `etcdctl` 进行密钥检索：
- en: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --peers https://172.17.8.101:2379 get /foo`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --peers https://172.17.8.101:2379
    get /foo`'
- en: Etcd secure client-to-server communication using server certificate and client
    certificate
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 使用服务器证书和客户端证书进行安全的客户端到服务器通信
- en: 'In the previous example, only the server had a certificate. In this example,
    we will generate a client certificate so that the server can verify the client''s
    identity. The following command starts the etcd server using a server certificate
    and key and enabling client authentication. The server certificate and keys are
    the same as generated in the previous section:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，只有服务器有证书。在这个示例中，我们将生成一个客户端证书，以便服务器验证客户端的身份。以下命令使用服务器证书和密钥启动 etcd 服务器，并启用客户端身份验证。服务器证书和密钥与前一部分生成的相同：
- en: '`etcd2 -name core-01 --data-dir=core-01 -client-cert-auth -trusted-ca-file=/home/core/ca.crt -cert-file=/home/core/key.crt  -key-file=/home/core/key.key -advertise-client-urls https://172.17.8.101:2379 -listen-client-urls https://172.17.8.101:2379`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcd2 -name core-01 --data-dir=core-01 -client-cert-auth -trusted-ca-file=/home/core/ca.crt
    -cert-file=/home/core/key.crt  -key-file=/home/core/key.key -advertise-client-urls
    https://172.17.8.101:2379 -listen-client-urls https://172.17.8.101:2379`'
- en: 'The following is an example to set a key and retrieve it using a secure client
    and server mechanism. The client certificate and key are the same as generated
    in the previous section:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示如何使用安全的客户端和服务器机制设置密钥并检索它。客户端证书和密钥与前一部分生成的相同：
- en: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure -L https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure https://172.17.8.101:2379/v2/keys/foo`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt
    --key /home/smakam14/client.key.insecure -L https://172.17.8.101:2379/v2/keys/foo
    -XPUT -d value=bar -v``curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt
    --key /home/smakam14/client.key.insecure https://172.17.8.101:2379/v2/keys/foo`'
- en: 'The following example uses etcdctl to do the key retrieval:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用 `etcdctl` 进行密钥检索：
- en: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --cert-file /home/smakam14/client.crt --key-file /home/smakam14/client.key.insecure --peers https://172.17.8.101:2379 get /foo`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --cert-file /home/smakam14/client.crt
    --key-file /home/smakam14/client.key.insecure --peers https://172.17.8.101:2379
    get /foo`'
- en: A secure cloud-config
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个安全的 `cloud-config`
- en: 'The following is a sample `cloud-config` that sets up the etcd security environment
    variables as well as the necessary certificate and keys:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例的 `cloud-config`，它设置了 etcd 的安全环境变量以及所需的证书和密钥：
- en: '`cloud-config write_files:   - path: /run/systemd/system/etcd2.service.d/30-configuration.conf
       permissions: 0644    content: |      [Service]      Environment=ETCD_NAME=core-01
         Environment=ETCD_VERBOSE=1      # Encryption      Environment=ETCD_CLIENT_CERT_AUTH=1
         Environment=ETCD_TRUSTED_CA_FILE=/home/core/ca.crt      Environment=ETCD_CERT_FILE=/home/core/server.crt
         Environment=ETCD_KEY_FILE=/home/core/server.key       - path: /home/core/ca.crt
       permissions: 0644    content: |      -----BEGIN CERTIFICATE-----       -----END CERTIFICATE-----
         - path: /home/core/server.crt    permissions: 0644    content: |      -----BEGIN CERTIFICATE-----
         -----END CERTIFICATE-----   - path: /home/core/server.key    permissions: 0644
       content: |      -----BEGIN RSA PRIVATE KEY-----      -----END RSA PRIVATE KEY-----  coreos:
     etcd2:     # Static cluster     initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://$private_ipv4:2380
        initial-cluster-state: new     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://$public_ipv4:2379     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
     units:    - name: etcd2.service      command: start`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`cloud-config write_files:   - path: /run/systemd/system/etcd2.service.d/30-configuration.conf
       permissions: 0644    content: |      [Service]      Environment=ETCD_NAME=core-01
         Environment=ETCD_VERBOSE=1      # Encryption      Environment=ETCD_CLIENT_CERT_AUTH=1
         Environment=ETCD_TRUSTED_CA_FILE=/home/core/ca.crt      Environment=ETCD_CERT_FILE=/home/core/server.crt
         Environment=ETCD_KEY_FILE=/home/core/server.key       - path: /home/core/ca.crt
       permissions: 0644    content: |      -----BEGIN CERTIFICATE-----       -----END CERTIFICATE-----
         - path: /home/core/server.crt    permissions: 0644    content: |      -----BEGIN CERTIFICATE-----
         -----END CERTIFICATE-----   - path: /home/core/server.key    permissions: 0644
       content: |      -----BEGIN RSA PRIVATE KEY-----      -----END RSA PRIVATE KEY-----  coreos:
     etcd2:     # Static cluster     initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://$private_ipv4:2380
        initial-cluster-state: new     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # listen on both the official ports and the legacy ports
        # legacy ports 可以省略，如果您的应用不依赖于它们     listen-client-urls: http://$public_ipv4:2379
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001  units:
       - name: etcd2.service      command: start`'
- en: Authentication
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 认证
- en: Before the introduction of the authentication feature, there were no restrictions
    on access to the etcd database. The authentication feature was introduced as an
    experimental feature in etcd 2.1.0 and allows access to a specific set of keys
    based on the username and password.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入认证功能之前，etcd 数据库的访问没有任何限制。认证功能作为实验性功能在 etcd 2.1.0 中引入，并允许基于用户名和密码访问特定的密钥集。
- en: 'There are two entities associated with authentication:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 认证涉及两个实体：
- en: 'Users: Users can be created with a username and password. Before enabling the
    authentication feature, a root user needs to be created. The root user has substantially
    more privileges/permissions to add users and roles and assign role permissions.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户：用户可以通过用户名和密码创建。在启用认证功能之前，必须先创建一个 root 用户。root 用户拥有更多的权限，可以添加用户和角色并分配角色权限。
- en: 'Roles: Roles can be used to restrict access to a specific key or directory
    that holds multiple keys. Roles are assigned to users, and manipulations of the
    keys can be done based on the username.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色：角色用于限制对特定密钥或包含多个密钥的目录的访问。角色分配给用户，密钥的操作可以根据用户名进行。
- en: To get started with authentication, we need to first create a root user and
    then enable authentication.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用认证，我们需要先创建一个 root 用户，然后启用认证功能。
- en: 'Create a root user first, as shown in the following screenshot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 首先创建一个 root 用户，如下图所示：
- en: '![](img/00424.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00424.jpg)'
- en: 'Enable authentication as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下方式启用认证：
- en: '![](img/00428.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00428.jpg)'
- en: The following example illustrates the etcd authentication.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了 etcd 认证。
- en: 'Let''s create a sample keyset, user, and role:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个示例密钥集、用户和角色：
- en: Create `/dir1/key1` and `/dir2/key2` keys.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `/dir1/key1` 和 `/dir2/key2` 密钥。
- en: Create a `role_dir1` role that has access to `/dir1/*` only.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅能访问 `/dir1/*` 的 `role_dir1` 角色。
- en: Create a `role_dir2` role that has access to `/dir2/*` only.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个仅能访问 `/dir2/*` 的 `role_dir2` 角色。
- en: Create `user1` and grant the `role_dir1` role.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `user1` 并授予 `role_dir1` 角色。
- en: Create `user2` and grant the `role_dir2` role.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `user2` 并授予 `role_dir2` 角色。
- en: At this point, `user1` will be able to access `/dir1/*` only and `user2` will
    be able to access `/dir2/*` only.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`user1` 只能访问 `/dir1/*`，`user2` 只能访问 `/dir2/*`。
- en: 'The following is a breakdown of the steps:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤的详细说明：
- en: 'Create `/dir1/key1` and `/dir2/key2` keys:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`/dir1/key1`和`/dir2/key2`密钥：
- en: '![](img/00432.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00432.jpg)'
- en: '![](img/00435.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00435.jpg)'
- en: 'Create a `role_dir1` role that has access to `/dir1/*` only:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个仅能访问`/dir1/*`的`role_dir1`角色：
- en: '![](img/00438.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00438.jpg)'
- en: '![](img/00443.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00443.jpg)'
- en: 'Create a `role_dir2` role that has access to `/dir2/*` only:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个仅能访问`/dir2/*`的`role_dir2`角色：
- en: '![](img/00446.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00446.jpg)'
- en: '![](img/00450.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00450.jpg)'
- en: 'Create `user1` and grant the `role_dir1` role:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`user1`并授予`role_dir1`角色：
- en: '![](img/00453.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00453.jpg)'
- en: '![](img/00456.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00456.jpg)'
- en: 'Create `user2` and grant the `role_dir2` role:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`user2`并授予`role_dir2`角色：
- en: '![](img/00459.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00459.jpg)'
- en: '![](img/00463.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00463.jpg)'
- en: 'Now, we can verify that `user1` has access only to `/dir1/key1`. As shown in
    the following screenshot, `user1` is not able to access `/dir2/key1`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以验证`user1`仅能访问`/dir1/key1`。如以下截图所示，`user1`无法访问`/dir2/key1`：
- en: '![](img/00466.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00466.jpg)'
- en: 'Similarly, `user2` has access only to `/dir2/key1`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，`user2`仅能访问`/dir2/key1`：
- en: '![](img/00471.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00471.jpg)'
- en: Etcd debugging
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd调试
- en: 'Etcd log files can be checked using the following command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令检查Etcd的日志文件：
- en: '`Journalctl –u etcd2.service`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`Journalctl –u etcd2.service`'
- en: Default logging is set to `INFO`. For more elaborate logging, we can set `ETCD_DEBUG=1`
    in the environment file or use the `-debug` command-line option.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 默认日志级别设置为`INFO`。要进行更详细的日志记录，我们可以在环境文件中设置`ETCD_DEBUG=1`，或者使用`-debug`命令行选项。
- en: 'Sometimes, it''s useful to check the curl command associated with the etcdctl
    CLI command. This can be achieved using the `--debug` option. The following is
    an example:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，检查与etcdctl CLI命令关联的curl命令是很有用的。可以使用`--debug`选项实现这一点。以下是一个示例：
- en: '![](img/00475.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00475.jpg)'
- en: Systemd
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Systemd
- en: An overview of Systemd was provided in [Chapter 1](index_split_023.html#filepos77735),
    CoreOS Overview. Systemd is the init system used by CoreOS and is always on by
    default. In this section, we will walk through some of the Systemd internals.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一章](index_split_023.html#filepos77735)《CoreOS概述》中提供了Systemd的概述。Systemd是CoreOS使用的初始化系统，默认始终开启。在本节中，我们将深入了解一些Systemd的内部机制。'
- en: Unit types
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 单元类型
- en: Units describe a particular task along with its dependencies and the execution
    order. Some units are started on the CoreOS system by default. CoreOS users can
    also start their own units. System-started units are at `/usr/lib64/systemd/system`
    and user-started units are at `/etc/systemd/system`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 单元描述了特定的任务及其依赖项和执行顺序。一些单元默认在CoreOS系统中启动。CoreOS用户也可以启动自己的单元。系统启动的单元位于`/usr/lib64/systemd/system`，用户启动的单元位于`/etc/systemd/system`。
- en: 'The following are some of the common unit types:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的单元类型：
- en: 'Service unit: This is used to start a particular daemon or process. Examples
    are `sshd.service` and `docker.service`. The `sshd.service` unit starts the SSH
    service, and `docker.service` starts the docker daemon.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务单元：用于启动特定的守护进程或进程。例如`sshd.service`和`docker.service`。`sshd.service`单元启动SSH服务，`docker.service`启动docker守护进程。
- en: 'Socket unit: This is used for local IPC or network communication. Examples
    are `systemd-journald.socket` and `docker.socket`. There is a corresponding service
    associated with a socket that manages the socket. For example, `docker.service`
    manages `docker.socket`. In `docker.service`, `docker.socket` is mentioned as
    a dependency. `Docker.socket` provides remote connectivity to the docker engine.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 套接字单元：用于本地IPC或网络通信。例如`systemd-journald.socket`和`docker.socket`。每个套接字都有一个对应的服务来管理该套接字。例如，`docker.service`管理`docker.socket`。在`docker.service`中，`docker.socket`作为依赖项被提到。`Docker.socket`提供与docker引擎的远程连接。
- en: 'Target unit: This is used mainly to group related units so that they can be
    started together. All user-created services are in `multi-user.target`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标单元：主要用于将相关单元分组，以便它们可以一起启动。所有用户创建的服务都位于`multi-user.target`中。
- en: 'Mount unit: This is used to mount disks to the filesystem. Examples are `tmp.mount`
    and `usr-share-oem.mount`. The following is a relevant section of `usr-share-oem.mount`
    that mounts `/usr/share/oem`:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挂载单元：用于将磁盘挂载到文件系统。例如`tmp.mount`和`usr-share-oem.mount`。以下是`usr-share-oem.mount`的相关部分，用于挂载`/usr/share/oem`：
- en: '![](img/00479.jpg)'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00479.jpg)'
- en: 'Timer unit: These are units that are started periodically based on the interval
    specified. Examples are `update-engine-stub.timer` and `logrotate.timer`. The
    following is a relevant section of `update-engine-stub.timer`, where `update-engine-stub.service`
    is invoked every `41 minutes` to check for CoreOS updates:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定时器单元：这些单元会根据指定的时间间隔定期启动。例如`update-engine-stub.timer`和`logrotate.timer`。以下是`update-engine-stub.timer`的相关部分，其中每`41分钟`调用一次`update-engine-stub.service`来检查CoreOS更新：
- en: '![](img/00483.jpg)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00483.jpg)'
- en: Unit specifiers
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 单元说明符
- en: When writing systemd units, it is useful to access system environment variables
    such as hostname, username, IP address, and so on so that we can avoid hardcoding
    and use the same systemd unit across systems. For this, systemd provides you with
    unit specifiers, which are shortcuts to get to the system environment.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写systemd单元时，访问系统环境变量（如主机名、用户名、IP地址等）非常有用，这样可以避免硬编码，并在不同系统间使用相同的systemd单元。为此，systemd为您提供了单元说明符，这些是访问系统环境的快捷方式。
- en: 'The following are some common unit specifiers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的单元说明符：
- en: '`%H`: Hostname'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%H`：主机名'
- en: '`%m`: Machine ID'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%m`：机器ID'
- en: '`%u`: Username'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`%u`：用户名'
- en: A complete list of unit specifiers is specified at [http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers](http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 单元说明符的完整列表可以在[http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers](http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers)找到。
- en: 'The following service example illustrates the usage of unit specifiers. In
    this example, we are setting the key-value pair associated with different specifiers
    in etcd in ExecStartPre. In ExecStartPost, we are getting the key-value and then
    cleaning up in the end:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下服务示例演示了单元说明符的使用。在此示例中，我们在ExecStartPre中设置与不同说明符相关的键值对，并将其存储在etcd中。在ExecStartPost中，我们获取该键值对，并在最后进行清理：
- en: '`[Unit] Description=My Service  [Service] KillMode=none ExecStartPre=/usr/bin/etcdctl set hostname %H ; /usr/bin/etcdctl set machinename %m ; /usr/bin/etcdctl set bootid %b ; /usr/bin/etcdctl set unitname %n ; /usr/bin/etcdctl set username %u
    ExecStart=/bin/echo hello, set done, will echo and remove ExecStartPost=/usr/bin/etcdctl get hostname ; /usr/bin/etcdctl get machinename ; /usr/bin/etcdctl get bootid ; /usr/bin/etcdctl get unitname ; /usr/bin/etcdctl get username ;
    ExecStartPost=/usr/bin/etcdctl rm hostname ; /usr/bin/etcdctl rm machinename ; /usr/bin/etcdctl rm bootid ; /usr/bin/etcdctl rm unitname ; /usr/bin/etcdctl rm username ;  [Install]
    WantedBy=multi-user.target`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=My Service  [Service] KillMode=none ExecStartPre=/usr/bin/etcdctl
    set hostname %H ; /usr/bin/etcdctl set machinename %m ; /usr/bin/etcdctl set bootid
    %b ; /usr/bin/etcdctl set unitname %n ; /usr/bin/etcdctl set username %u ExecStart=/bin/echo
    hello, set done, will echo and remove ExecStartPost=/usr/bin/etcdctl get hostname
    ; /usr/bin/etcdctl get machinename ; /usr/bin/etcdctl get bootid ; /usr/bin/etcdctl
    get unitname ; /usr/bin/etcdctl get username ; ExecStartPost=/usr/bin/etcdctl
    rm hostname ; /usr/bin/etcdctl rm machinename ; /usr/bin/etcdctl rm bootid ; /usr/bin/etcdctl
    rm unitname ; /usr/bin/etcdctl rm username ;  [Install] WantedBy=multi-user.target`'
- en: 'To execute this service, it is necessary to execute all the following operations
    with sudo:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此服务，必须使用sudo执行以下所有操作：
- en: Create the `unitspec.service` file in `/etc/systemd/system` with the preceding
    content.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/etc/systemd/system`中创建`unitspec.service`文件，并使用前述内容。
- en: Enable the service with `systemctl enable unitspec.service`.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`systemctl enable unitspec.service`启用服务。
- en: Start the service with `systemctl start unitspec.service`.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`systemctl start unitspec.service`启动服务。
- en: If we change the service after this, we need to execute command `systemctl daemon-reload`
    before starting the service.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在此之后修改服务，需要在启动服务之前执行命令`systemctl daemon-reload`。
- en: 'The following are the `journalctl` logs associated with the service where we
    can see the key being set and retrieved and the corresponding unit specifier value:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与服务相关的`journalctl`日志，我们可以看到键被设置和获取，以及相应的单元说明符值：
- en: '`journalctl –u unitspec.service`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`journalctl –u unitspec.service`'
- en: '![](img/00487.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00487.jpg)'
- en: Unit templates
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 单元模板
- en: Systemd units can be created as a template, and the same template unit can be
    used to instantiate multiple units based on the invocation of templates.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Systemd单元可以作为模板创建，并且可以使用相同的模板单元基于模板的调用实例化多个单元。
- en: Templates are created as `unitname@.service`. The invocation of templates can
    be done using `unitname@instanceid.service`. In the unit file, the `unit name`
    can be accessed with `%p` and `instanceid` can be accessed using `%i`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 模板创建为`unitname@.service`。模板的调用可以使用`unitname@instanceid.service`进行。在单元文件中，可以使用`%p`访问`unit
    name`，并使用`%i`访问`instanceid`。
- en: 'The following is an example template file, `unitspec@.service`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例模板文件，`unitspec@.service`：
- en: '`[Unit] Description=My Service  [Service] ExecStartPre=/usr/bin/etcdctl set instance%i %i ; /usr/bin/etcdctl set prefix %p
    ExecStart=/bin/echo Demonstrate systemd template  [Install] WantedBy=multi-user.target`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=My Service [Service] ExecStartPre=/usr/bin/etcdctl set
    instance%i %i ; /usr/bin/etcdctl set prefix %p ExecStart=/bin/echo Demonstrate
    systemd template [Install] WantedBy=multi-user.target`'
- en: 'To execute this service, it is necessary to execute all the following operations
    with sudo:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此服务，必须使用sudo执行以下所有操作：
- en: Create the `unitspec@.service` file in `/etc/systemd/system`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/etc/systemd/system`中创建`unitspec@.service`文件。
- en: Enable the service with `systemctl enable unitspec@.service`.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`systemctl enable unitspec@.service`启用服务。
- en: Start multiple services with `systemctl start unitspec@1.service` and `systemctl
    start unitspec@2.service`.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`systemctl start unitspec@1.service`和`systemctl start unitspec@2.service`启动多个服务。
- en: 'If we look at the etcd content, we can see that the instance value gets updated
    based on the `%i` argument supplied in the unit name and creates the `instance1`
    and `instance2` keys:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看etcd内容，我们可以看到，实例值会根据单元名称中提供的`%i`参数进行更新，并创建`instance1`和`instance2`键：
- en: '![](img/00490.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00490.jpg)'
- en: 'The following example gives a more practical example of instantiated units.
    It uses a template nginx service, `nginx@.service`, where the port number of the
    web service is passed dynamically:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例提供了一个更实际的实例化单元示例。它使用了一个模板 nginx 服务，`nginx@.service`，其中 web 服务的端口号是动态传递的：
- en: '`[Unit] Description=Apache web server service After=etcd.service After=docker.service  [Service]
    TimeoutStartSec=0 Restart=always EnvironmentFile=/etc/environment ExecStartPre=-/usr/bin/docker kill nginx%i
    ExecStartPre=-/usr/bin/docker rm nginx%i ExecStartPre=/usr/bin/docker pull nginx
    ExecStart=/usr/bin/docker run --name nginx%i -p ${COREOS_PUBLIC_IPV4}:%i:80 nginx
    ExecStop=/usr/bin/docker stop nginx%i  [Install] WantedBy=multi-user.target`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=Apache web server service After=etcd.service After=docker.service
    [Service] TimeoutStartSec=0 Restart=always EnvironmentFile=/etc/environment ExecStartPre=-/usr/bin/docker
    kill nginx%i ExecStartPre=-/usr/bin/docker rm nginx%i ExecStartPre=/usr/bin/docker
    pull nginx ExecStart=/usr/bin/docker run --name nginx%i -p ${COREOS_PUBLIC_IPV4}:%i:80
    nginx ExecStop=/usr/bin/docker stop nginx%i [Install] WantedBy=multi-user.target`'
- en: 'There are two service options used in the preceding code:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中使用了两个服务选项：
- en: '`Timeoutstartsec`: This specifies the time taken to start the service, and
    if the service is not started by this time, it gets killed. The `none` parameter
    disables this option and is useful when downloading big containers.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TimeoutStartSec`：此选项指定启动服务所需的时间。如果服务在此时间内未启动，它将被终止。`none`参数禁用此选项，适用于下载大容器时。'
- en: '`Restart`: This controls the restartability of the service. Here we have specified
    `always` to restart the service in case there is a failure associated with this
    service.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Restart`：此选项控制服务的重启性。在这里我们指定了`always`，以便在服务发生故障时重启该服务。'
- en: 'Let''s create two instances of this service using the following commands:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令创建该服务的两个实例：
- en: '`Sudo systemctl enable nginx@.service``Sudo systemctl start nginx@8080.service``Sudo systemctl start nginx@8081.service`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo systemctl enable nginx@.service``Sudo systemctl start nginx@8080.service``Sudo
    systemctl start nginx@8081.service`'
- en: This creates two docker containers with nginx service; the first one exposing
    port `8080` and the second one exposing port `8081`.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建两个带有 nginx 服务的 docker 容器，第一个容器暴露端口`8080`，第二个容器暴露端口`8081`。
- en: 'Let''s look at `docker ps`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看`docker ps`：
- en: '![](img/00494.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00494.jpg)'
- en: 'Let''s look at the status of the two units. As we can see in the following
    screenshot, the units are in an active (running) state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看两个单元的状态。如以下截图所示，这些单元处于活动（运行）状态：
- en: '![](img/00497.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00497.jpg)'
- en: '![](img/00258.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00258.jpg)'
- en: Drop-in units
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 插入单元
- en: Drop-in units are useful to change system unit properties at runtime. There
    are four ways of creating drop-in units.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 插入单元用于在运行时更改系统单元的属性。创建插入单元有四种方法。
- en: Default cloud-config drop-in units
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 cloud-config 插入单元
- en: 'Parameters specified in the `cloud-config` user data will automatically be
    configured as drop-in units. For example, let''s look at the `etcd2.service cloud-config`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cloud-config`用户数据中指定的参数将自动配置为插入单元。例如，我们来看一下`etcd2.service cloud-config`：
- en: '![](img/00260.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00260.jpg)'
- en: 'Let''s look at the `etcd2.service` status:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看`etcd2.service`的状态：
- en: '![](img/00262.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00262.jpg)'
- en: As we can see in the preceding output, the default drop-in unit is `20-cloudinit.conf`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上面的输出中看到的，默认的插入单元是`20-cloudinit.conf`。
- en: '`20-cloudinit.conf` will contain the parameters specified in `etcd2 cloud-config`
    as environment variables, as shown in the following screenshot:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`20-cloudinit.conf` 将包含在 `etcd2 cloud-config` 中指定的作为环境变量的参数，如下图所示：'
- en: '![](img/00265.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00265.jpg)'
- en: Cloud-config custom drop-in units
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config 自定义 drop-in 单元
- en: 'We can specify the drop-in unit as part of the `cloud-config`. The following
    is an example of the `fleet.service` drop-in unit, where we change the default
    `Restart` parameter from `Always` to `No`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 drop-in 单元作为 `cloud-config` 的一部分进行指定。以下是 `fleet.service` drop-in 单元的示例，我们将默认的
    `Restart` 参数从 `Always` 改为 `No`：
- en: '![](img/00267.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00267.jpg)'
- en: 'When we use this `cloud-config`, the `norestart.conf` drop-in file gets automatically
    created as can be seen from the `fleet.service` status:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用这个`cloud-config`时，`norestart.conf` drop-in 文件会自动创建，可以从 `fleet.service`
    状态中看到这一点：
- en: '![](img/00269.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00269.jpg)'
- en: This configuration change will keep `fleet.service` non-restartable.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置更改将使 `fleet.service` 无法重启。
- en: Runtime drop-in unit – specific parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时 drop-in 单元 – 特定参数
- en: 'We can change specific properties of the service using the drop-in configuration
    file. The following is the service section of `fleet.service`, which shows the
    default parameters:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 drop-in 配置文件更改服务的特定属性。以下是 `fleet.service` 的服务部分，显示了默认参数：
- en: '![](img/00272.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00272.jpg)'
- en: This specifies that the service needs to be started in 10 seconds in case the
    service dies because of some error. Let's check whether the restart works by killing
    the Fleet service.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示如果服务因某些错误死掉，需要在 10 秒内启动该服务。让我们通过杀死 Fleet 服务来检查重启是否有效。
- en: 'We can kill the service as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过如下方式杀死服务：
- en: '`Sudo kill -9 <fleet pid>`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo kill -9 <fleet pid>`'
- en: 'The following is a log showing the Fleet service restarting in 10 seconds,
    which is due to `Restartsec` specified in the service configuration:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是日志，显示了 Fleet 服务在 10 秒内重启，这是由于服务配置中指定的 `Restartsec`：
- en: '![](img/00274.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00274.jpg)'
- en: To prove the runtime drop-in configuration change, let's create a configuration
    file where we disable the restart for the Fleet service.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明运行时的 drop-in 配置更改，让我们创建一个配置文件，在其中禁用 Fleet 服务的重启。
- en: 'Create `norestart.conf` under `/etc/systemd.system/system/fleet.service.d`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在`/etc/systemd.system/system/fleet.service.d`下创建 `norestart.conf`：
- en: '![](img/00276.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00276.jpg)'
- en: 'Now, let''s restart the systemd configuration:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新加载 systemd 配置：
- en: '`Sudo systemd daemon-reload`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo systemd daemon-reload`'
- en: 'Let''s check the status of `fleet.service` now:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下现在 `fleet.service` 的状态：
- en: '![](img/00279.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00279.jpg)'
- en: We can see that other than `20-cloudinit.conf`, we also have a `norestart.conf`
    drop-in unit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，除了 `20-cloudinit.conf`，我们还拥有一个 `norestart.conf` drop-in 单元。
- en: 'Now, if we kill the Fleet service, it does not get restarted as the restart
    option has been disabled by the `restart.conf` drop-in unit. `Fleet.service` stays
    in a failed state, as shown in the following screenshot:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们杀死 Fleet 服务，它不会被重启，因为重启选项已通过 `restart.conf` drop-in 单元禁用。`Fleet.service`
    会保持失败状态，如下图所示：
- en: '![](img/00281.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00281.jpg)'
- en: Runtime drop-in unit – full service
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时 drop-in 单元 – 完整服务
- en: 'In this approach, we can replace the complete system service using our own
    service. Let''s change the restart option by creating this `fleet.service` file
    in `/etc/systemd/system`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以用自己的服务替换完整的系统服务。让我们通过在 `/etc/systemd/system` 中创建这个 `fleet.service`
    文件来更改重启选项：
- en: '`[Unit] Description=fleet daemon  After=etcd.service After=etcd2.service  Wants=fleet.socket
    After=fleet.socket  [Service] ExecStart=/usr/bin/fleetd Restart=no  [Install]
    WantedBy=multi-user.target`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=fleet daemon  After=etcd.service After=etcd2.service  Wants=fleet.socket
    After=fleet.socket  [Service] ExecStart=/usr/bin/fleetd Restart=no  [Install]
    WantedBy=multi-user.target`'
- en: 'We can start the `fleet.service` as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式启动 `fleet.service`：
- en: '`Sudo systemctl start fleet.service`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo systemctl start fleet.service`'
- en: 'Let''s see the status of `fleet.service`:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 `fleet.service` 的状态：
- en: '![](img/00283.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00283.jpg)'
- en: From the preceding output, we can see that `fleet.service` is picked up from
    `/etc/systemd/system`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到 `fleet.service` 是从 `/etc/systemd/system` 加载的。
- en: If we compare this option (a drop-in unit with a complete service change) with
    the previous option (a drop-in unit with a specific parameter change), the previous
    option gives the flexibility to change specific parameters and not touch the original
    set. This makes it easier to handle upgrades when new versions of the service
    allow additional options.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个选项（一个带有完整服务更改的掉入单元）与之前的选项（一个带有特定参数更改的掉入单元）进行比较，前一个选项提供了更改特定参数的灵活性，而不会触及原始设置。这使得在新版本的服务允许额外选项时，更容易处理升级。
- en: Network units
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 网络单元
- en: 'The `systemd-networkd` service manages networks. System-configured networks
    are specified in `/usr/lib64/systemd/network` and user-configured networks are
    specified in `/etcd/systemd/network`. The following is a sample Vagrant configured
    systemd-network file to configure the `eth1` IP address:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`systemd-networkd` 服务用于管理网络。系统配置的网络位于 `/usr/lib64/systemd/network`，用户配置的网络位于
    `/etcd/systemd/network`。以下是一个示例的 Vagrant 配置的 systemd-network 文件，用于配置 `eth1` 的
    IP 地址：'
- en: '![](img/00286.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00286.jpg)'
- en: 'The `ifconfig` output associated with `eth1` shows the IP address that Vagrant
    configured, as shown in the following screenshot:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `eth1` 相关的 `ifconfig` 输出显示了 Vagrant 配置的 IP 地址，如以下截图所示：
- en: '![](img/00288.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00288.jpg)'
- en: 'As an example, let''s try to change the `eth1` IP address. There are three
    steps:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，假设我们尝试更改 `eth1` 的 IP 地址。步骤如下：
- en: Stop `systemd-networkd.service`.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止 `systemd-networkd.service`。
- en: Flush the IP address.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刷新 IP 地址。
- en: Set a new IP address.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个新的 IP 地址。
- en: Let's create a service file to flush the `eth1` IP address and another network
    file specifying the new IP address for `eth1`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个服务文件来刷新 `eth1` 的 IP 地址，并创建另一个网络文件来为 `eth1` 指定新的 IP 地址。
- en: Create a service file to flush the `eth1` IP address as follows. We need to
    place this in `/etc/systemd/system/down-eth1.service`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个服务文件以刷新 `eth1` 的 IP 地址，如下所示。我们需要将其放置在 `/etc/systemd/system/down-eth1.service`
    中。
- en: '`[Unit] Description=eth1 flush  [Service] Type=oneshot ExecStart=/usr/bin/ip link set eth1 down
    ExecStart=/usr/bin/ip addr flush dev eth1  [Install] WantedBy=multi-user.target`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=eth1 flush  [Service] Type=oneshot ExecStart=/usr/bin/ip link set eth1 down
    ExecStart=/usr/bin/ip addr flush dev eth1  [Install] WantedBy=multi-user.target`'
- en: 'The following is the network file to specify the `eth1` new address. We need
    to place this in `/etc/systemd/network/40-eth1.network`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定 `eth1` 新地址的网络文件。我们需要将其放置在 `/etc/systemd/network/40-eth1.network` 中：
- en: '`[Match] Name=eth1  [Network] Address=172.17.8.110/24 Gateway=172.17.8.1`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Match] Name=eth1  [Network] Address=172.17.8.110/24 Gateway=172.17.8.1`'
- en: 'The steps to change the IP address are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 更改 IP 地址的步骤如下：
- en: Stop the system-networkd service by `sudo systemctl stop systemd-networkd.service`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `sudo systemctl stop systemd-networkd.service` 停止 systemd-networkd 服务。
- en: Flush the `eth1` IP address by `sudo systemctl start down-eth1.service`.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `sudo systemctl start down-eth1.service` 来刷新 `eth1` 的 IP 地址。
- en: Start `systemd-networkd.service` by `sudo systemctl start systemd-networkd.service`.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `sudo systemctl start systemd-networkd.service` 启动 `systemd-networkd.service`。
- en: 'If we look at the ifconfig output now, we should see the new IP address `172.17.8.110`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在查看 ifconfig 输出，应该能看到新的 IP 地址 `172.17.8.110`：
- en: '![](img/00290.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00290.jpg)'
- en: Fleet
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet
- en: Fleet is a cluster manager/scheduler that controls service creation at the CoreOS
    cluster level. We can think of Fleet as Systemd for the cluster. For an overview
    of Fleet, refer to [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview.
    Fleet is used mainly for the orchestration of critical system services, while
    other orchestration solutions such as Kubernetes are used for application service
    orchestration. Fleet is not under active development and is mostly under the maintenance
    mode.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet 是一个集群管理器/调度器，控制 CoreOS 集群级别的服务创建。我们可以将 Fleet 看作是集群的 Systemd。有关 Fleet 的概述，请参考
    [第 1 章](index_split_023.html#filepos77735)，CoreOS 概述。Fleet 主要用于关键系统服务的编排，而其他编排解决方案如
    Kubernetes 主要用于应用服务编排。Fleet 当前不再活跃开发，处于维护模式。
- en: Installation
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 安装
- en: 'Fleet is installed and started by default in CoreOS. The following is the Fleet
    version in the CoreOS stable 766.3.0 release:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet 在 CoreOS 中默认安装并启动。以下是 CoreOS 稳定版 766.3.0 发布中的 Fleet 版本：
- en: '![](img/00293.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00293.jpg)'
- en: Fleet can also be installed in a standalone Linux machine. Fleet releases can
    be found at [https://github.com/coreos/fleet/releases](https://github.com/coreos/fleet/releases).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet 也可以安装在独立的 Linux 机器上。Fleet 的发布版本可以在 [https://github.com/coreos/fleet/releases](https://github.com/coreos/fleet/releases)
    找到。
- en: Accessing Fleet
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Fleet
- en: The following are different approaches to access Fleet.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是访问 Fleet 的不同方式。
- en: Local fleetctl
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 本地 fleetctl
- en: The `fleetctl` command is present in each CoreOS node and can be used to control
    Fleet services.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`fleetctl` 命令在每个 CoreOS 节点中都存在，可以用于控制 Fleet 服务。'
- en: Remote fleetctl
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 远程 fleetctl
- en: 'The `fleetctl` command can be used to access non-local CoreOS nodes by specifying
    an endpoint argument. The following is an example:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`fleetctl` 命令可以通过指定端点参数来访问非本地的 CoreOS 节点。以下是示例：'
- en: '`Fleetctl --endpoint=http://172.17.8.101:2379 list-machines`'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fleetctl --endpoint=http://172.17.8.101:2379 list-machines`'
- en: '![](img/00295.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00295.jpg)'
- en: Remote fleetctl with an SSH tunnel
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 远程 fleetctl 与 SSH 隧道
- en: The previous example did not use any authentication. To make access to fleetctl
    secure, we can use the SSH authentication scheme. It is necessary to add the CoreOS
    node private key to the local SSH authentication agent for this mode. For a Vagrant
    CoreOS cluster, the private key is stored in `~/.vagrant.d/insecure_private_key`.
    For an AWS CoreOS cluster, the private key can be downloaded as part of the initial
    key creation.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的示例没有使用任何认证。为了确保 fleetctl 访问的安全性，我们可以使用 SSH 认证方案。在这种模式下，必须将 CoreOS 节点的私钥添加到本地
    SSH 认证代理。对于 Vagrant CoreOS 集群，私钥存储在 `~/.vagrant.d/insecure_private_key` 中。对于 AWS
    CoreOS 集群，私钥可以在初始密钥创建时下载。
- en: 'To add a private key to the authentication agent:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 要将私钥添加到认证代理：
- en: '`` eval `ssh-agent -s` ```ssh-add <private key>`'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`` eval `ssh-agent -s` ```ssh-add <private key>`'
- en: '![](img/00298.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00298.jpg)'
- en: '![](img/00301.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00301.jpg)'
- en: 'Now, we can use fleetctl to use a secure SSH to access the CoreOS cluster:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 fleetctl 通过安全的 SSH 访问 CoreOS 集群：
- en: '`Fleetctl --tunnel=http://172.17.8.101 list-unit-files`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fleetctl --tunnel=http://172.17.8.101 list-unit-files`'
- en: '![](img/00303.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00303.jpg)'
- en: Remote HTTP
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 远程 HTTP
- en: Remote HTTP Fleet API access is disabled by default.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，远程 HTTP Fleet API 访问被禁用。
- en: 'To enable remote access, create a `.socket` file to expose the Fleet API port.
    The following is an example Fleet configuration file to expose port `49153` for
    external API access:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用远程访问，创建 `.socket` 文件以暴露 Fleet API 端口。以下是示例 Fleet 配置文件，用于暴露端口 `49153` 供外部
    API 访问：
- en: '![](img/00305.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00305.jpg)'
- en: 'It is necessary to restart the systemd, `fleet.socket`, and `fleet.service`
    after creating the remote API configuration file for it to take effect:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建远程 API 配置文件后，必须重启 systemd、`fleet.socket` 和 `fleet.service`，以使其生效：
- en: '`Sudo systemctl daemon-reload``Sudo systemctl restart fleet.socket``Sudo systemctl restart fleet.service`'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo systemctl daemon-reload` `Sudo systemctl restart fleet.socket` `Sudo
    systemctl restart fleet.service`'
- en: 'Now, we can access the remote API. The following is an example using `fleetctl`
    and `curl`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以访问远程 API。以下是使用 `fleetctl` 和 `curl` 的示例：
- en: '`Fleetctl --endpoint=http://172.17.8.101:49153 list-units`'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fleetctl --endpoint=http://172.17.8.101:49153 list-units`'
- en: '![](img/00356.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00356.jpg)'
- en: 'The following output shows you the unit list using the Fleet HTTP API. The
    following curl output is truncated to show partial output:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了使用 Fleet HTTP API 的单元列表。以下 curl 输出已被截断，显示部分输出：
- en: '`Curl –s http://172.17.8.101:49153/fleet/v1/units | jq .`'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`Curl –s http://172.17.8.101:49153/fleet/v1/units | jq .`'
- en: '![](img/00310.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00310.jpg)'
- en: Using etcd security
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 etcd 安全性
- en: 'We can also use the secure etcd approach to access Fleet. Setting up a secure
    etcd is covered in the section on Etcd security. The following example shows the
    `fleetctl` command with a server certificate:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用安全的 etcd 方法来访问 Fleet。设置安全的 etcd 内容在 Etcd 安全部分中有说明。以下示例显示了带有服务器证书的 `fleetctl`
    命令：
- en: '`fleetctl --debug --ca-file ca.crt  --endpoint=https://172.17.8.101:2379 list-machines`'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '`fleetctl --debug --ca-file ca.crt --endpoint=https://172.17.8.101:2379 list-machines`'
- en: Templates, scheduling, and HA
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 模板、调度和高可用性
- en: Fleet supports unit specifiers and templates similar to systemd. A unit specifier
    provides you with shortcuts within a service file, and templates provide reusable
    service files. The earlier section on systemd covered details on unit specifiers
    and templates. [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview
    covered the basics of Fleet scheduling and HA.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet 支持类似 systemd 的单元规范符和模板。单元规范符为服务文件提供快捷方式，而模板提供可重用的服务文件。前面关于 systemd 的部分详细介绍了单元规范符和模板。[第一章](index_split_023.html#filepos77735)，CoreOS
    概述介绍了 Fleet 调度和高可用性的基础知识。
- en: 'Fleet metadata for a node can be specified in the Fleet section of `cloud-config`.
    The following example sets the Fleet node metadata for `role` as `web`. Metadata
    can be used in Fleet service files to control scheduling:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 `cloud-config` 的 Fleet 部分指定节点的 Fleet 元数据。以下示例将 `role` 设置为 `web` 的 Fleet
    节点元数据。元数据可用于 Fleet 服务文件中控制调度：
- en: '`metadata: "role=services"`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`metadata: "role=services"`'
- en: 'Fleet uses a pretty simple scheduling algorithm, and X-fleet options are used
    to specify constraints while scheduling the service. The following are the available
    X-fleet options:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: Fleet使用非常简单的调度算法，X-fleet选项用于在调度服务时指定约束条件。以下是可用的X-fleet选项：
- en: '`MachineMetaData`: Service gets scheduled based on matching metadata.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineMetaData`：服务根据匹配的元数据进行调度。'
- en: '`MachineId`: Service gets scheduled based on the specified `MachineId`.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineId`：服务根据指定的`MachineId`进行调度。'
- en: '`MachineOf`: Service gets scheduled based on other services running in the
    same node. This can be used to schedule tightly coupled services in the same node.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MachineOf`：服务根据在同一节点上运行的其他服务进行调度。这可以用于将紧密耦合的服务调度到同一节点。'
- en: '`Conflict`: This option can be used to avoid scheduling conflicting services
    in the same node.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conflict`：此选项可用于避免将冲突的服务调度到同一节点。'
- en: '`Global`: The same service gets scheduled in all the nodes of the cluster.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Global`：相同的服务会在集群的所有节点上进行调度。'
- en: 'The following example uses unit specifiers and templates and illustrates Fleet
    scheduling and HA. The following are some details of the application:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用单元标识符和模板，展示了Fleet调度和高可用性的实现。以下是该应用的一些细节：
- en: An application consists of a WordPress container and MySQL container
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个应用由WordPress容器和MySQL容器组成
- en: The WordPress container uses the database from the MySQL container and is linked
    using Docker container linking
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WordPress容器使用来自MySQL容器的数据库，并通过Docker容器链接进行连接
- en: Linking across containers is done using the `--link` option, and it works only
    if both containers are on the same host
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器之间的链接是通过`--link`选项完成的，并且只有当两个容器在同一主机上时才能工作
- en: Fleet's template feature will be used to launch multiple services using a single
    WordPress and MySQL template, and Fleet's X-fleet constraint feature will be used
    to launch the related containers on the same host
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet的模板功能将用于通过单一的WordPress和MySQL模板启动多个服务，Fleet的X-fleet约束功能将用于在同一主机上启动相关的容器
- en: When one of the nodes in the cluster dies, Fleet's HA mechanism will take care
    of rescheduling the failed units, and we will see it working in this example
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当集群中的一个节点宕机时，Fleet的HA机制会负责重新调度失败的单元，我们将在这个例子中看到它的工作原理
- en: 'The MySQL template service is as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL模板服务如下：
- en: '`[Unit] Description=app-mysql  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill mysql%i
    ExecStartPre=-/usr/bin/docker rm mysql%i ExecStartPre=/usr/bin/docker pull mysql
    ExecStart=/usr/bin/docker run --name mysql%i -e MYSQL_ROOT_PASSWORD=mysql mysql
    ExecStop=/usr/bin/docker stop mysql%i`'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=app-mysql  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill mysql%i
    ExecStartPre=-/usr/bin/docker rm mysql%i ExecStartPre=/usr/bin/docker pull mysql
    ExecStart=/usr/bin/docker run --name mysql%i -e MYSQL_ROOT_PASSWORD=mysql mysql
    ExecStop=/usr/bin/docker stop mysql%i`'
- en: 'The WordPress template service is as follows:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: WordPress模板服务如下：
- en: '`[Unit] Description=wordpress  [Service] Restart=always RestartSec=15 ExecStartPre=-/usr/bin/docker kill wordpress%i
    ExecStartPre=-/usr/bin/docker rm wordpress%i ExecStartPre=/usr/bin/docker pull wordpress
    ExecStart=/usr/bin/docker run --name wordpress%i --link mysql%i:mysql wordpress
    ExecStop=/usr/bin/docker stop wordpress%i  [X-Fleet] MachineOf=mysql@%i.service`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=wordpress  [Service] Restart=always RestartSec=15 ExecStartPre=-/usr/bin/docker kill wordpress%i
    ExecStartPre=-/usr/bin/docker rm wordpress%i ExecStartPre=/usr/bin/docker pull wordpress
    ExecStart=/usr/bin/docker run --name wordpress%i --link mysql%i:mysql wordpress
    ExecStop=/usr/bin/docker stop wordpress%i  [X-Fleet] MachineOf=mysql@%i.service`'
- en: 'The following are some notes on the service:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于服务的一些说明：
- en: We have used `%i` as an instance specifier
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了`%i`作为实例标识符
- en: WordPress has an X-fleet constraint to schedule the corresponding MySQL container
    in the same node
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WordPress具有X-fleet约束，用于在同一节点调度对应的MySQL容器
- en: 'The first step is to submit the services:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是提交服务：
- en: '![](img/00311.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00311.jpg)'
- en: 'The next step is to load each instance of the service:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载每个服务实例：
- en: '![](img/00425.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00425.jpg)'
- en: 'Let''s check whether all the services are running. As can be seen in the following
    screenshot, we have three instances of the WordPress application and the associated
    MySQL database:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下所有服务是否正在运行。从以下截图可以看到，我们有三个WordPress应用实例和相关的MySQL数据库：
- en: '![](img/00313.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00313.jpg)'
- en: To demonstrate HA, let's kill CoreOS `node2`. This can be done by shutting down
    the node.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示高可用性（HA），我们将关闭CoreOS的`node2`，可以通过关闭该节点来实现。
- en: 'As we can see, there are only two nodes in the cluster now:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当前集群中只有两个节点：
- en: '![](img/00467.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00467.jpg)'
- en: 'From the following new service output, we can see that the services running
    on the old `node2` have been moved to `node3` now as `node2` is not available:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下新的服务输出中，我们可以看到，原先运行在`node2`上的服务已经迁移到了`node3`，因为`node2`不可用：
- en: '![](img/00316.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00316.jpg)'
- en: Debugging
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 调试
- en: 'The status of the Fleet service can be checked using `fleetctl status`. The
    following is an example:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`fleetctl status`检查Fleet服务的状态。以下是一个示例：
- en: '![](img/00011.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpg)'
- en: 'Logs of the Fleet service can be checked using `fleetctl journal`. The following
    is an example:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`fleetctl journal`可以检查Fleet服务的日志。以下是一个示例：
- en: '![](img/00319.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00319.jpg)'
- en: 'For debugging and to get the REST API corresponding to the `fleetctl` command,
    we can use the `--debug` option as follows:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行调试并获取与`fleetctl`命令对应的REST API，我们可以使用`--debug`选项，如下所示：
- en: '![](img/00040.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00040.jpg)'
- en: Service discovery
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现
- en: Microservices are dynamic, and it is important for services to discover other
    services dynamically to find the IP address, port number, and metadata about the
    services. There are multiple schemes available to discover services, and in this
    section, we will cover a few schemes using etcd and Fleet for service discovery.
    In the later chapters of the book, we will cover advanced service discovery options.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务是动态的，服务需要动态发现其他服务以查找IP地址、端口号以及有关服务的元数据。服务发现有多种方案，在本节中，我们将介绍几种使用etcd和Fleet进行服务发现的方案。在本书的后续章节中，我们将介绍更高级的服务发现选项。
- en: Simple etcd-based discovery
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的基于etcd的服务发现
- en: 'The following figure shows you the simplest possible service discovery mechanism,
    where a service updates etcd with service-related details that other services
    can access from etcd:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了最简单的服务发现机制，其中一个服务将与服务相关的详细信息更新到etcd，其他服务可以从etcd中访问这些信息：
- en: '![](img/00323.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00323.jpg)'
- en: 'The following is an example Apache service, `apacheupdateetcd@.service`, that
    updates the hostname and port number in etcd when the service is started:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例Apache服务，`apacheupdateetcd@.service`，它会在服务启动时更新etcd中的主机名和端口号：
- en: '`[Unit] Description=My Advanced Service After=etcd2.service After=docker.service  [Service]
    TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill apache%i ExecStartPre=-/usr/bin/docker rm apache%i
    ExecStartPre=/usr/bin/docker pull coreos/apache ExecStart=/usr/bin/docker run --name apache%i -p %i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND
    ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running ExecStop=/usr/bin/docker stop apache%i
    ExecStopPost=/usr/bin/etcdctl rm /domains/example.com/%H:%i  [X-Fleet] # Don''t schedule on the same machine as other Apache instances
    X-Conflicts=apache*@*.service`'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=My Advanced Service After=etcd2.service After=docker.service  [Service]
    TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill apache%i ExecStartPre=-/usr/bin/docker rm apache%i
    ExecStartPre=/usr/bin/docker pull coreos/apache ExecStart=/usr/bin/docker run --name apache%i -p %i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND
    ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running ExecStop=/usr/bin/docker stop apache%i
    ExecStopPost=/usr/bin/etcdctl rm /domains/example.com/%H:%i  [X-Fleet] # Don''t schedule on the same machine as other Apache instances
    X-Conflicts=apache*@*.service`'
- en: 'Let''s start the service and create two instances:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动服务并创建两个实例：
- en: '![](img/00324.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00324.jpg)'
- en: '![](img/00327.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00327.jpg)'
- en: 'Now, we can verify that etcd gets updated with the service details of the two
    services:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以验证etcd是否已更新了两个服务的详细信息：
- en: '![](img/00330.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00330.jpg)'
- en: Sidekick discovery
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: Sidekick服务发现
- en: 'In the preceding scheme, there is no way to know if the service is alive and
    running after it has been started. The following figure shows you a slightly advanced
    service discovery scheme where a sidekick service updates etcd with the details
    of the service:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方案中，无法知道服务启动后是否还在运行。下图展示了一个稍微高级一点的服务发现方案，其中一个sidekick服务会将服务的详细信息更新到etcd中：
- en: '![](img/00333.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00333.jpg)'
- en: The purpose of the Side kick container is to monitor the main service and update
    etcd only if the Service is active. It is important to run the Side kick container
    in the same node as the main service that Side kick is monitoring.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: Sidekick容器的目的是监控主服务，并且只有当主服务处于活动状态时，才会更新etcd。重要的是，Sidekick容器需要与其监控的主服务运行在同一节点上。
- en: The following is a Sidekick example using the Apache service and a sidekick
    for the Apache service.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用Apache服务的Sidekick示例以及为Apache服务配置的sidekick。
- en: 'Following is the `Apache.service` unit file:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`Apache.service`单元文件：
- en: '`[Unit] Description=Apache web server service on port %i  # Requirements Requires=etcd2.service
    Requires=docker.service Requires=apachet-discovery@%i.service  # Dependency ordering
    After=etcd2.service After=docker.service Before=apachet-discovery@%i.service  [Service]
    # Let processes take awhile to start up (for first run Docker containers) TimeoutStartSec=0  # Change killmode from "control-group" to "none" to let Docker remove
    # work correctly. KillMode=none  # Get CoreOS environmental variables EnvironmentFile=/etc/environment  # Pre-start and Start
    ## Directives with "=-" are allowed to fail without consequence ExecStartPre=-/usr/bin/docker kill apachet.%i
    ExecStartPre=-/usr/bin/docker rm apachet.%i ExecStartPre=/usr/bin/docker pull coreos/apache
    ExecStart=/usr/bin/docker run --name apachet.%i -p ${COREOS_PUBLIC_IPV4}:%i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND  # Stop
    ExecStop=/usr/bin/docker stop apachet.%i`'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] 描述=Apache Web服务器服务，在端口 %i 上  # 需求 需要=etcd2.service 需要=docker.service
    需要=apachet-discovery@%i.service  # 依赖关系排序 在etcd2.service之后 在docker.service之后 在apachet-discovery@%i.service之前  [Service]
    # 让进程启动稍微延迟（用于首次运行的Docker容器） TimeoutStartSec=0  # 将killmode从"control-group"更改为"none"，以使Docker正确移除
    KillMode=none  # 获取CoreOS环境变量 环境文件=/etc/environment  # 启动前和启动 ## 带有"=-"的指令允许失败而不产生后果
    ExecStartPre=-/usr/bin/docker kill apachet.%i ExecStartPre=-/usr/bin/docker rm apachet.%i
    ExecStartPre=/usr/bin/docker pull coreos/apache ExecStart=/usr/bin/docker run --name apachet.%i -p ${COREOS_PUBLIC_IPV4}:%i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND  #
    停止 ExecStop=/usr/bin/docker stop apachet.%i`'
- en: 'Following is the Apache sidekick service unit file:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Apache侧kick服务单元文件：
- en: '`[Unit] Description=Apache Sidekick  # Requirements Requires=etcd2.service
    Requires=apachet@%i.service  # Dependency ordering and binding After=etcd2.service
    After=apachet@%i.service BindsTo=apachet@%i.service  [Service]  # Get CoreOS environmental variables
    EnvironmentFile=/etc/environment  # Start ## Test whether service is accessible and then register useful information
    ExecStart=/bin/bash -c ''\   while true; do \     curl -f ${COREOS_PUBLIC_IPV4}:%i; \
        if [ $? -eq 0 ]; then \       etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        else \       etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}; \     fi; \
        sleep 20; \   done''  # Stop ExecStop=/usr/bin/etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}  [X-Fleet]
    # Schedule on the same machine as the associated Apache service X-ConditionMachineOf=apachet@%i.service`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] 描述=Apache Sidekick  # 需求 需要=etcd2.service 需要=apachet@%i.service  #
    依赖关系排序和绑定 在etcd2.service之后 在apachet@%i.service之后 绑定到apachet@%i.service  [Service]  #
    获取CoreOS环境变量 环境文件=/etc/environment  # 启动 ## 测试服务是否可访问，然后注册有用的信息 ExecStart=/bin/bash
    -c ''\   while true; do \     curl -f ${COREOS_PUBLIC_IPV4}:%i; \     if [ $? -eq 0 ]; then \
          etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \''{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\'' --ttl 30; \
        else \       etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}; \     fi; \
        sleep 20; \   done''  # 停止 ExecStop=/usr/bin/etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}  [X-Fleet]
    # 在与相关的Apache服务相同的机器上调度 X-ConditionMachineOf=apachet@%i.service`'
- en: The preceding Side kick container service does a periodic ping to the main service
    and updates the etcd output. If the main service is not reachable, the service-related
    details are removed from etcd.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 上述Sidekick容器服务定期ping主服务并更新etcd输出。如果主服务无法访问，相关服务的详细信息将从etcd中删除。
- en: 'Let''s start two instances of the service:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动两个服务实例：
- en: '![](img/00337.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00337.jpg)'
- en: 'Let''s see the etcd output. As shown in the following screenshot, etcd reflects
    the two nodes where Apache is running:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看etcd输出。从以下截图中可以看到，etcd反映了Apache运行的两个节点：
- en: '![](img/00341.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00341.jpg)'
- en: 'Let''s see the docker output in node1:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看node1中的docker输出：
- en: '![](img/00345.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00345.jpg)'
- en: 'To demonstrate the sidekick service, let''s stop the docker container and check
    whether the sidekick service updates etcd in order to remove the appropriate service:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示侧kick服务，让我们停止docker容器并检查侧kick服务是否更新etcd，以删除相应的服务：
- en: '![](img/00349.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00349.jpg)'
- en: Let's check the status of the units. As can be seen below, `apachet@2.service`
    has failed and the associated sidekick service `apachet-discovery@2.service` is
    inactive.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下单元的状态。从下面可以看到，`apachet@2.service` 已经失败，相关的侧kick服务 `apachet-discovery@2.service`
    处于非活动状态。
- en: '![](img/00352.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00352.jpg)'
- en: 'From the following output, we can see that the `apachet@2.service` details
    are removed from etcd:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出可以看到，`apachet@2.service` 的详细信息已经从etcd中删除：
- en: '![](img/00355.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00355.jpg)'
- en: ELB service discovery
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ELB服务发现
- en: This is a variation of the Sidekick discovery in which, instead of Sidekick
    updating etcd, Sidekick updates the IP address to the load balancer and the load
    balancer redirects the web query to the active nodes. In this example, we will
    use the AWS Elastic load balancer and CoreOS elb-presence container available
    in the Quay repository. The elb-presence container takes care of checking the
    health of the nginx container and updates AWS ELB with the container's IP address.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Sidekick 发现的一个变体，在这种变体中，Sidekick 更新 IP 地址到负载均衡器，负载均衡器将 Web 查询重定向到活动节点。在本例中，我们将使用
    AWS 弹性负载均衡器和 Quay 仓库中提供的 CoreOS elb-presence 容器。elb-presence 容器负责检查 nginx 容器的健康状态，并更新
    AWS ELB 以使用容器的 IP 地址。
- en: 'The following figure shows you a high-level architecture of this approach:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这种方法的高级架构：
- en: '![](img/00264.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00264.jpg)'
- en: 'The first step is to create ELB in AWS, as shown in the following screenshot.
    Here we have used AWS CLI to create ELB, `testlb`:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在 AWS 中创建 ELB，如下截图所示。这里我们使用 AWS CLI 创建了 ELB，名为 `testlb`：
- en: '![](img/00124.jpg)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpg)'
- en: We need to use `testlb` created in the preceding screenshot in the Sidekick
    service.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 Sidekick 服务中使用之前截图创建的 `testlb`。
- en: 'Following is the `nginx.service` unit file:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `nginx.service` 单元文件：
- en: '`[Unit] Description=nginx  [Service] ExecStartPre=-/usr/bin/docker kill nginx-%i
    ExecStartPre=-/usr/bin/docker rm nginx-%i ExecStart=/usr/bin/docker run --rm --name nginx-%i -p 80:80 nginx
    ExecStop=/usr/bin/docker stop nginx-%i  [X-Fleet] Conflicts=nginx@*.service`'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=nginx  [Service] ExecStartPre=-/usr/bin/docker kill nginx-%i
    ExecStartPre=-/usr/bin/docker rm nginx-%i ExecStart=/usr/bin/docker run --rm --name nginx-%i -p 80:80 nginx
    ExecStop=/usr/bin/docker stop nginx-%i  [X-Fleet] Conflicts=nginx@*.service`'
- en: 'Following is the nginx sidekick service that updates AWS ELB based on the health
    of `nginx.service`:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更新 AWS ELB 基于 `nginx.service` 健康状态的 nginx Sidekick 服务：
- en: '`[Unit] Description=nginx presence service BindsTo=nginx@%i.service  [Service]
    ExecStartPre=-/usr/bin/docker kill nginx-presence-%i ExecStartPre=-/usr/bin/docker rm nginx-presence-%i
    ExecStart=/usr/bin/docker run --rm --name nginx-presence-%i -e AWS_ACCESS_KEY=<key> -e AWS_SECRET_KEY=<secretkey> -e AWS_REGION=us-west-2 -e ELB_NAME=testlb quay.io/coreos/elb-presence
    ExecStop=/usr/bin/docker stop nginx-presence-%i  [X-Fleet] MachineOf=nginx@%i.service`'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=nginx presence service BindsTo=nginx@%i.service  [Service]
    ExecStartPre=-/usr/bin/docker kill nginx-presence-%i ExecStartPre=-/usr/bin/docker rm nginx-presence-%i
    ExecStart=/usr/bin/docker run --rm --name nginx-presence-%i -e AWS_ACCESS_KEY=<key> -e AWS_SECRET_KEY=<secretkey> -e AWS_REGION=us-west-2 -e ELB_NAME=testlb quay.io/coreos/elb-presence
    ExecStop=/usr/bin/docker stop nginx-presence-%i  [X-Fleet] MachineOf=nginx@%i.service`'
- en: 'The following is the Fleet status after creating two instances of the service:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 创建两个服务实例后，以下是 Fleet 的状态：
- en: '![](img/00128.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.jpg)'
- en: 'As we can see in the following screenshot, AWS ELB has both the instances registered,
    and it will load-balance between these two instances:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在下面的截图中看到的，AWS ELB 已经注册了这两个实例，并且将在这两个实例之间进行负载均衡：
- en: '![](img/00130.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpg)'
- en: At this point, if we stop any instance of the nginx service, the Sidekick service
    will take care of removing this instance from ELB.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，如果停止 nginx 服务的任何实例，Sidekick 服务将负责从 ELB 中删除此实例。
- en: Summary
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered the internals of Etcd, Systemd, and Fleet with sufficient
    hands-on examples, which will allow you to get comfortable with configuring and
    using these services. By keeping the development of the critical services open
    source, CoreOS has encouraged the usage of these services outside CoreOS as well.
    We also covered the basic service discovery options using Etcd, Systemd, and Fleet.
    In the next chapter, we will cover container networking and Flannel.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细介绍了 Etcd、Systemd 和 Fleet 的内部结构，并通过充分的实际示例让您能够熟悉配置和使用这些服务。通过开源关键服务的开发，CoreOS
    鼓励在 CoreOS 之外使用这些服务。我们还介绍了使用 Etcd、Systemd 和 Fleet 的基本服务发现选项。在下一章中，我们将介绍容器网络和 Flannel。
- en: References
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Etcd docs: [https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd 文档：[https://coreos.com/etcd/docs/latest/](https://coreos.com/etcd/docs/latest/)
- en: 'Fleet docs: [https://coreos.com/fleet/docs/latest/](https://coreos.com/fleet/docs/latest/)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet 文档：[https://coreos.com/fleet/docs/latest/](https://coreos.com/fleet/docs/latest/)
- en: 'Systemd docs: [http://www.freedesktop.org/wiki/Software/systemd/](http://www.freedesktop.org/wiki/Software/systemd/)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Systemd 文档：[http://www.freedesktop.org/wiki/Software/systemd/](http://www.freedesktop.org/wiki/Software/systemd/)
- en: 'Fleet service discovery: [https://coreos.com/fleet/docs/latest/examples/service-discovery.html](https://coreos.com/fleet/docs/latest/examples/service-discovery.html)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet服务发现：[https://coreos.com/fleet/docs/latest/examples/service-discovery.html](https://coreos.com/fleet/docs/latest/examples/service-discovery.html)
- en: 'Etcd-ca: [https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd-ca：[https://github.com/coreos/etcd-ca](https://github.com/coreos/etcd-ca)
- en: 'Etcd security: [https://github.com/coreos/etcd/blob/master/Documentation/security.md](https://github.com/coreos/etcd/blob/master/Documentation/security.md)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd安全性：[https://github.com/coreos/etcd/blob/master/Documentation/security.md](https://github.com/coreos/etcd/blob/master/Documentation/security.md)
- en: Further reading and tutorials
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读和教程
- en: 'Etcd security and authentication: [http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/](http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd安全性和身份验证：[http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/](http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/)
- en: 'Etcd administration: [https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md](https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md)'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd管理：[https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md](https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md)
- en: 'Why systemd: [http://blog.jorgenschaefer.de/2014/07/why-systemd.html](http://blog.jorgenschaefer.de/2014/07/why-systemd.html)'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择Systemd：[http://blog.jorgenschaefer.de/2014/07/why-systemd.html](http://blog.jorgenschaefer.de/2014/07/why-systemd.html)
- en: 'Comparing init systems: [http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html](http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html)'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较初始化系统：[http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html](http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html)
- en: 'Systemd talk by the Systemd creator: [https://www.youtube.com/watch?v=VIPonFvPlAs](https://www.youtube.com/watch?v=VIPonFvPlAs)'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Systemd创始人的讲座：[https://www.youtube.com/watch?v=VIPonFvPlAs](https://www.youtube.com/watch?v=VIPonFvPlAs)
- en: 'Service discovery overview: [http://www.gomicro.services/articles/service-discovery-overview](http://www.gomicro.services/articles/service-discovery-overview)
    and [http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现概述：[http://www.gomicro.services/articles/service-discovery-overview](http://www.gomicro.services/articles/service-discovery-overview)
    和 [http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)
- en: 'Highly available Docker services using CoreOS and Consul: [http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/](http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/)
    and [http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/](http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/)'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CoreOS和Consul实现高可用Docker服务：[http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/](http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/)
    和 [http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/](http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/)
