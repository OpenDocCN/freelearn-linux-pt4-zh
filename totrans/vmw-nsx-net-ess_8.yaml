- en: Chapter 8.  NSX Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let me start this chapter with a famous quote from Antisthenes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Not to unlearn what you have learned is the most necessary kind of learning"*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I couldn't find a better quote than that for giving everyone a heads-up on how
    vital it is to ensure that we recollect what we have learned so far in previous
    chapters about how to approach a problem to see what the best solution is. For
    the best solution to also be the quickest, we truly need to know how to approach
    a scenario, where to start looking, what logs are useful, and lastly, when to
    engage the vendor for further troubleshooting. As we all know, our course is focused
    on NSX with vSphere. NSX is tightly integrated with vSphere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a real example, even a well-constructed building will not stand on a
    weak foundation. A bad vSphere design will have a direct impact on NSX components,
    no matter how good the NSX design is. This rule of thumb is the same for any VMware
    solution that runs on top of vSphere. In this chapter, we will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: NSX installation and registration issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log collection process and steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VXLAN troubleshooting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager installation and registration issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Installing NSX Manager is one of the easiest tasks, and the bitter truth is
    that anyone who is familiar with vSphere OVA/OVF deployment can easily deploy
    an NSX Manager without any prior knowledge of NSX products. We know for sure,
    that in a production environment, no one will follow that method. However, I still
    want to educate you all about the importance of NSX installation. Let''s carefully
    go through the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: There should not be any vCloud networking security (VCNS/vShield Manager) registered
    with the same vCenter when we are trying to register NSX Manager. If we find any
    such environments, we must ensure that we are unregistering one of the solutions;
    definitely VCNS/vShield, since that is an outdated solution compared with NSX
    Manager. That doesn't mean we can have two NSX Managers registered with the same
    vCenter Server. However, we can upgrade VCNS to NSX and I will be sharing the
    upgrade guide link in the chapter's final section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never import any previously used NSX Manager instance to a new environment and
    register it as a solution with a new vCenter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always check if NSX Manager is registered with how many vSphere solutions. For
    example, we might have a **vCloudAutomation Center** (**VCAC**) and** vCloud Director** (**VCD**)
    registered with NSX Manager A, which is also registered with a vCenter Server
    environment. The reason why I'm more curious about such solutions is that careful
    planning and design is required not only for installation but also for uninstallation
    of NSX products during break fix time. Each solution's integration demands separate
    steps while unregistering NSX Manager.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always take a backup of NSX Manager after initial deployment of the software.
    Never depend on the vSphere snapshot feature for this backup activity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager can be treated as a normal vSphere virtual machine for troubleshooting
    any network-related issues. For example, we can migrate NSX Manager from one host
    to another host, or check the ESXTOP command to know Tx and Rx counts for isolating
    a network issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While registering with vCenter Server, we have two options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lookup service registration**: Lookup service registration is an optional
    feature for importing SSO users. However, if we are integrating with an SSO identity
    source, we need to follow all vendor-specific best practices for identity source
    availability. But, it''s worth remembering that if SSO is down except for login
    to NSX Manager, it won''t have any impact on NSX components and their features.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vCenter Server Registration**: vCenter Server registration is the first and
    most critical integration. Hence, we need to ensure that we have proper connectivity
    and configuration for the following points:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS resolution should be configured between **NSX Manager** and **vCenter Server**.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NTP** should be configured properly; this point might be very familiar for
    most of the experts, but I will still reiterate it: The impact of wrong NTP is
    very high when we integrate the lookup service (SSO) and try to leverage SSO-based
    authentication.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Firewall** ports should be opened between NSX Manager and vCenter Server.
    Always verify VMware **Knowledge Base** (**KB**) article for port requirements.
    The following link leads to a VMware KB article, which talks about all the port
    requirements:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`https://kb.vmware.com/selfservice/search.do?cmd=displayKC&docType=kc&docTypeID=DT_KB_1_1&externalId=2079386`'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that we are using vCenter Server administrative user rights while registering
    with NSX Manager. We can certainly use the administrator@vsphere.local account
    to register NSX with vCenter, vCloud Director, and vRealize Automation products.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting NSX Manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the situation, we may have to collect diagnostic information for NSX
    Manager for VMware Support. Keep the following steps handy for such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting NSX Manager logs via GUI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to collect NSX Manager logs via GUI are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the ****NSX Manager**** virtual appliance through a web browser.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **NSX Manager Virtual Appliance Management**, click **Download Tech Support
    Log**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click ****Download**** | ****Save**** . The following screenshot depicts the
    NSX Manager logs download:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Collecting NSX Manager logs via GUI](img/image_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Collecting NSX Manager logs via CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There might be instances when the NSX Manager GUI is not working and we might
    need to depend on the CLI for collecting logs. For CLI haters, there is no escape
    this time; we need to go through the following steps to capture NSX Manager logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the **NSX Manager** virtual appliance through a SSH session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to ****Enable Mode**** , by typing `enable`**.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Issue the following command in **Enable Mode**, which will save the NSX Manager
    logs in a remote location based on the host name that we selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`export tech-support scp USERNAME@HOSTNAME:FILENAME`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates NSX CLI log capturing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting NSX Manager logs via CLI](img/image_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: VMware Installation Bundle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hypervisors are basically the backbone of network virtualization. Virtual machines
    are able to leverage NSX features primarily because the ESXi host is a network-virtualized
    host. One of the most critical pillars of an NSX installation is ESXi host preparation.
    If we don''t have the right modules running in the ESXi host, the whole purpose
    of leveraging NSX features will be defeated. Symptoms would be that we might not
    be able to install feature *X*, or we can configure feature *X*, but the functionality
    is impacted. Watch out for the following VIBs in the ESXi host:'
  prefs: []
  type: TYPE_NORMAL
- en: esx-vxlan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: esx-vsip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: esx-dvfilter-switch-security (starting from NSX 6.2.0, esx-dvfilter-switch-security
    is part of esx-vxlan vibs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the command to check if VIB is installed in ESXi host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Since these are VIBs, we can manually uninstall and install the same during
    break fix scenarios. But the real question is, who is pushing these VIBs? That's
    where I have seen the majority of issues. Behind the scenes, vCenter Server ESX
    Agent Managers (**EAM**) are responsible for installing these VIBs. So, first
    and foremost, the EAM service should be up and running. The following steps are
    useful for collecting EAM based upon the operating system and vCenter Server flavor.
  prefs: []
  type: TYPE_NORMAL
- en: EAM log location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following are the EAM log locations for respective vCenter Server and operating
    system versions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VMware vSphere 5.1.x/5.5.x (EAM is a part of the common Tomcat server):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\VMware
    VirtualCenter\Logs\eam.log`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windows 2008: Same as Windows 2003, the VC log directory is located at `C:\ProgramData\VMware\VMware
    VirtualCenter\Logs\`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vCenter Server Virtual Appliance** (**VCVA**): `/storage/log/vmware/vpx/eam.log`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'VMware vSphere 6.x (EAM is a standalone service and has embedded tcserver):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\CIS\logs\eam\eam.log`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows 2008: Same as Windows 2003, the VC log directory is located at `C:\ProgramData\VMware\VMware
    VirtualCenter\Logs\`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CloudVM: `/storage/log/vmware/eam/eam.log`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I have seen a lot of issues, especially when a vCenter Server installation
    is a Windows-based installation, with EAM trying to use port `80` for downloading
    VIBs. At times, we might have other applications or services running in VC, which
    might be leveraging port `80` and will cause VIB download failures, so we have
    to change the default EAM ports. However, starting from VMware vSphere 6.0, VIB
    downloads over port `443` (instead of port `80`) are supported. This port is opened
    and closed dynamically. The intermediate devices (firewalls) between the ESXi
    hosts and vCenter Server must allow traffic using this port. With that, we will
    move on to our next topic: control plane and data plane log collection.'
  prefs: []
  type: TYPE_NORMAL
- en: Control plane and data plane log collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Log collection is vital for proactive and root cause analysis. How many times
    have we ended up collecting the wrong set of logs or received feedback that we
    have to enable or increase certain logging levels to ensure that we have the right
    set of logs to analyze the root cause? Technically, that type of feedback is digestible.
    However, when it comes to production impact, it would be a disappointment to find
    that there is nothing conclusive, even after going through the logs. There is
    only one solution for this issue: we should know what logs need to be collected
    and most importantly, from which locality. First and foremost, we need to get
    some background knowledge on the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the physical topology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding the physical topology is not only important for overall NSX design
    and feature configuration, it is equally important to share effective feedback
    if there is a better way to approach the overall design. The following mentioned
    points are something that we need to keep handy:'
  prefs: []
  type: TYPE_NORMAL
- en: Physical Network design - Spine-leaf/layer2 architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing firewall deployments and rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall datacenter Routing and Switching topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere cluster design (placement in racks) and topology. In addition to that, how
    many clusters (single-site, multi-site), data centers and active-active or active-passive
    physical data center designs required currently or future configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere distributed Switch Uplink policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some recommendations based on the preceding points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical Network design - Spine-leaf/layer2 architecture**: Spine/Leaf architecture
    is the best and most widely used connectivity now a days because of full mesh
    connectivity, less latency, high bandwidth, ECMP routing and most importantly
    easy expansion of network is achievable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Existing firewall deployments and rules**: This is an important check primarily
    because of NSX Edge Firewall and Microsegmentation capabilities. My suggestion
    would be try to integrate vRealize Network-insight software to understand overall
    traffic growth in North-South and East-West direction and then decide what firewall
    policies to be configured at which points. In the closing section of the chapter
    i have given a small summary on vRealize Network-insight software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overall datacenter routing and switching topology**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we are mainly focusing on routing protocols used in physical network -
    for example OSPF, BGP, ISIS.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example based on the AREA types configured for OSPF, assuming that upstream
    router is **Area Border Router (ABR)** we need to know what routes should be injected
    to Upstream router from NSX Edge and appropriate firewall policies for allowing
    the traffic.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Layer 2 side ideally ESXI, VXLAN traffic is the most important parameters
    that we need to know for assigning/configuring VLAN-ID (Management, VMotion, Storage,
    VXLAN).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSphere cluster design (placement in racks) and topology**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster design should ideally be Management & Edge Cluster Separated from Compute
    Cluster with minimum of 4 Host in Management & Edge Cluster and maximum of 64
    host in Computer Cluster (only if environment is running vSphere 6.0)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For disaster recovery environment configured with SRM and NSX.It is recommended
    to maintain similar physical and virtual design in the DR site. Situations might
    demand workloads to be running in DR site for long time, hence we need to follow
    that strict rule.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSphere distributed Switch Uplink policy**: This is the same topic that we
    have discussed in [Chapter 3](ch03.html "Chapter 3. NSX Manager Installation and
    Configuration"), *NSX Manager Installation and Configuration*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we need to select and configure NIC teaming and failover policies. For
    example if we choose LACP configuration, we are limited with Single VTEP configuration,
    however route based on port and source MAC hash support multi VTEP configuration.
    Prevention is better that cure, it is recommended to load balance VXLAN traffic
    across all available uplinks rather than inviting performance issues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind these minor point discussion is to ensure all configurations/design
    aspects are taken care well in advance rather than spending time later to do configuration
    and performance tweaking.
  prefs: []
  type: TYPE_NORMAL
- en: NSX Controller log collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Controller is the real game changer component in the overall architecture of
    NSX. For the same reason, it remains a critical piece when it comes to troubleshooting.
    As we all know, controllers are deployed from NSX Manager in an **Open Virtualization
    Appliance** (**OVA**). In a worst-case scenario, even the deployment of controllers
    might fail, and that would be a showstopper for any NSX implementation. The majority
    of such failures happen for the following two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NTP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There should be proper DNS/NTP configuration between ESXi hosts, vCenter Server,
    and NSX Manager for a successful deployment of NSX Controller. Apart from this
    point, a successful deployment of any virtual machine in vSphere certainly needs
    enough compute and storage capacity, and NSX Controller is no exception, primarily
    because these are virtual machines from an ESXi host perspective. For collecting
    NSX Controller logs, we need to complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need to log in to vCenter Server using the vSphere web client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Networking and Security**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Installation** on the left-hand pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the **Manage** tab, select the controller you want to download logs from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Download Tech support logs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the NSX Controller log collection process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![NSX Controller log collection](img/image_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How about if the web client is down, or we don''t have access to the web client?
    What method we can follow to collect the logs? There are two options in those
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a vSphere client session, we can connect to vCenter Server or the ESXi
    host where the controller is running, and we can take a VM console session to
    controller to leverage the CLI command for log collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take an SSH session directly to controller and execute a CLI command for log
    collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collecting NSX Controller logs using CLI steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Firstly, log into the NSX Controller for which you want to gather logs using
    any of the previous steps, and execute the following command, as shown in the
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Collecting NSX Controller logs using CLI steps](img/image_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After the controller logs are captured, we can go ahead and copy the same set
    of logs to any machine that has got IP connectivity with the controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, I have copied the controller logs to one of the management
    ESXi host TMP locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting NSX Controller logs using CLI steps](img/image_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With that, we will move to NSX Edge and DLR log collection, and we will finish
    off with data plane log collection and a few important service statuses.
  prefs: []
  type: TYPE_NORMAL
- en: The processes for collecting Edge and DLR logs are almost the same.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting Edge and Distributed Logical Router logs through the web client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the steps for collecting distributed logical router logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, log into vCenter Server using the vSphere web client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Networking & Security** icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Edges** on the left-hand pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the right-hand pane, select the edge (**DLR/EDGE**) we want to download from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Actions** and select ****Download Tech support logs**** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see **Download Tech support log** highlighted
    for Distributed Logical Router, and I mentioned earlier that this is the same
    process as for collecting NSX Edge logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting Edge and Distributed Logical Router logs through the web client](img/image_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For collecting logs via CLI, we need to execute the following command by executing
    any of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a vSphere client session, we can connect to vCenter Server or the ESXi
    host where controller is running, and we can take a VM console session to controller
    to leverage the CLI command for log collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take an SSH session directly to controller and execute the CLI command for
    log collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have already discussed what EAM is and the role it plays in an NSX environment.
    Apart from that vSphere troubleshooting piece, we need to the status and logging
    level of two user world agents, which will be running in an NSX-prepared ESXi
    host.
  prefs: []
  type: TYPE_NORMAL
- en: NSX user world agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NSX Manager is responsible for deploying the NSX Controller cluster, ESXi hosts
    preparation by pushing **vSphere Installation Bundles** (**VIBs**) to enable VXLAN,
    distributed routing, distributed firewall, and a user world agent used to communicate
    at the control-plane level. The functionality of the user world agent is highly
    critical and any failures will have a direct impact on the control plane learning,
    which eventually affects data plane traffic. So, let's discuss these agents, along
    with basic health checks and log locations.
  prefs: []
  type: TYPE_NORMAL
- en: netcpa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is a user world agent that communicates with NSX control plane, and the
    netcpa service should be up and running on the NSX-prepared ESXi host. If the
    functionality is impacted, we will certainly experience routing and switching
    issues in the NSX environment, and the ESXi host won''t learn new routes from
    the time the netcpa service was down. So, this is extremely important: creating
    the routes alone on an NSX Edge VM won''t do the trick; unless the netcpa service
    is up and running, ESXi host won''t learn those routes. Complete the following
    steps in order to check for netcpa-related issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Check if the netcpa service is running on the host (this needs to be checked
    on the host where we are experiencing network or control-plane related issues).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following command to check the netcpa service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts the netcpa agent''s status:'
  prefs: []
  type: TYPE_NORMAL
- en: '![netcpa](img/image_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Check if the netcpa configuration file is showing all the controllers. Use
    the following command to check controller details in the configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts the config file output, with controller IP
    and SSL certificate thumbprint information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![netcpa](img/image_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the `/var/log/netcpa.log` file on the ESXi host, we can see the complete
    netcpa logs. The following screenshot depicts controller registration information,
    which is populated in netcpa logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![netcpa](img/image_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Anytime we are facing issues with the netcpa service, I would strongly recommend
    restarting the service to confirm if that fixes the issue. To restart the netcpa
    service, we need to complete the following steps in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in as root to the ESXi host through SSH or through the DCUI console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `/etc/init.d/netcpad` restart command to restart the netcpa agent on
    the ESXi host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vsfwd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NSX distributed firewall is a hypervisor integrated firewall and apart from
    the point that the host should have a firewall `vib` installed, there should be
    a vsfwd daemon process up and running for proper message bus communication with
    NSX Manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command and screenshot shows a stateful firewall status on the
    ESXi host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Vsfwd](img/image_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To check the active message bus session with NSX Manager the following command
    and screenshot depicts an active session with NSX Manager (172.16.1.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Vsfwd](img/image_08_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A potential failure can happen if port `**5671**` is not opened between the
    ESXi host and NSX Manager.
  prefs: []
  type: TYPE_NORMAL
- en: Vsfwd log location and collection process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NSX Distributed Firewall is a new generation firewall in vSphere environment
    used primarily because of its ability to filter traffic at the virtual machine
    NIC level. Hence, it is important to understand the log collection and a few troubleshooting
    steps related to this feature. So, let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to start with the prerequisites to run **Distributed Firewall**
    (**DFW**). There is no need for log collection, even if the following requirements
    are not met. The prerequisites for running DFW are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: VMware vCenter Server should be version 5.5 minimum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware ESXi version should be at 5.1, 5.5, 6.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware NSX for vSphere 6.0 and later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All logs related to vsfwd will be at the following location, and their representation
    is shown in the figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Vsfwd log location and collection process](img/image_08_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Collecting centralized logs from NSX Manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Firstly, we need to log in to NSX Manager using the admin credentials and execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the introduction of NSX 6.2.3, VMware has come up with an export host-tech-support
    command, which can be executed on the NSX Manager to collect the following information.
    I strongly believe they will be adding more and more log collection options, since
    this is a centralized way of collecting the logs, but a lot depends upon the type
    of failure. If we encounter an NSX Manager failure scenario, centralized logging
    functionality is also impacted, hence it is important to understand the next plan
    for such scenarios, which is the whole purpose of me explaining the log collection
    process so far. Following is the list of logs that are included in the centralized
    logs as of now:'
  prefs: []
  type: TYPE_NORMAL
- en: vmkernel and vsfwd log files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of dfw rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spoofguard details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host-related information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ipdiscovery-related information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMQ command outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security group and services profile and instance details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: esxcli-related outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VXLAN troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VXLAN is the overlay technology that is used in the VMware NSX environment
    when it comes to first-time testing and implementation, and most likely, we will
    end up with a few connectivity issues. Some of the common issues that we might
    face because of misconfiguration in both virtual and physical networks are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machines have no network connectivity, either between other machines
    in the same VXLAN network, or no egress connection with the physical world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent packet drops getting reported and the applications team facing poor
    performance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual Machines have proper network connectivity on some of the ESXi hosts,
    but when placed on another set of hosts, there is no connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal ping tests are fine, but when checking with VXLAN packets, packet drops
    occur
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual machines in VLAN networks have proper network connectivity; however,
    VXLAN networks are not working
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The majority of network-related issues related to VXLAN will be around the preceding
    list of issues; however, only by applying our knowledge will we have a clear picture
    of what type of networks customers have and what type of network issues they are
    facing. So, let's get started with learning what the possible symptoms for such
    issues are and what the necessary actions to resolve the issue are.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing a VXLAN solution, I would strongly recommend to checking
    GUI-level PING and VXLAN tests between all the NSX-prepared ESXi hosts, which
    is the best way to confirm if initial requirements are met for sending a VXLAN
    packet from one hypervisor to another hypervisor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to select one of the logical switches. Go to **Monitor** page, and
    there we will have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ping**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broadcast**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the **Ping** and **Broadcast** test options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![VXLAN troubleshooting](img/image_08_013.jpg)First, we will do a **Ping**
    test between the `172.16.1.94` and `172.16.1.96` ESXi hosts, and we will follow
    that with a VXLAN test. The following screenshot shows a successful Ping test:![VXLAN
    troubleshooting](img/image_08_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows a VXLAN test between the same ESXi host, and
    we can confirm that VXLAN packets sent from host `172.16.1.94` are successfully
    accepted by `172.16.1.96`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![VXLAN troubleshooting](img/image_08_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to the previous test, we can also perform a VXLAN ping test through
    an SSH session, and I have captured the output of this test between two ESXi hosts.
    The command to perform the test is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot depicts a VXLAN test performed through an SSH session
    to host 172.16.1.94:'
  prefs: []
  type: TYPE_NORMAL
- en: '![VXLAN troubleshooting](img/image_08_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding output is a clear indication that MTU is set properly in the
    VXLAN environment, so we are able to MTU more than 1500\. If there is an MTU misconfiguration
    issue, this test would fail, as shown in the following screenshot. I intentionally
    changed MTU in the distributed virtual switch from 1600 to 1500 to showcase the
    failure scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![VXLAN troubleshooting](img/image_08_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Network troubleshooting isn't complete without a packet capture. This is our
    final topic for this chapter, and I will showcase how to collect VXLAN packets.
    We will also take a quick walk-through to see what information is in the packet.
    Considering the knowledge we have gained so far, it should be a cakewalk for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Packet capturing and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting from ESXi 5.5, the pktcap-uw tool is embedded inside the hypervisor.
    Some of you will be familiar with the tcpdump tool, which was already available
    in ESXi; pktcap is a replacement for the same. The prime reason for integrating
    the pktcap tool captures packets are every layer which is extremely essential
    in NSX world. So, we are no longer limited by capturing packets at the vmkernel
    layer. I have been a big fan of this tool starting from the vCloud networking
    and security days and I strongly believe most of us will like this tool. Before
    jumping into packet capturing, let''s be clear about the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Pktcap`, by default, collects only incoming packets, and it is unidirectional.
    So, if we want to capture both ingress and egress traffic, we need to add certain
    parameters. If not, the whole purpose of capturing the packet will be defeated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic direction is mentioned as `-dir 0` for ingress packets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic direction is mentioned as `-dir 1` for egress packets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can capture a packet at the vmkernel, vmnic, and switch port levels (DVS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Detailed command syntax is available if we issue the following command in ESXi
    host:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Enough theory; let's get started by capturing the packet to analyze the VXLAN
    field. Before that, let me explain my lab setup and virtual machine details so
    that we can verify if those outputs match the captured packet details.
  prefs: []
  type: TYPE_NORMAL
- en: Lab environment details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this lab, I have two vSphere clusters, and we have a VXLAN network 5001,
    which is stretched across these two clusters. Both clusters have their own distributed
    switch.
  prefs: []
  type: TYPE_NORMAL
- en: The virtual machine IP in cluster A is `192.16.10.12`, with a VTEP IP of `172.16.1.32`
    running on ESXi `172.16.1.97`.
  prefs: []
  type: TYPE_NORMAL
- en: The virtual machine IP in cluster B is `192.16.10.14`, with a VTEP IP of `172.16.1.33`
    running on ESXi `172.16.1.94`.
  prefs: []
  type: TYPE_NORMAL
- en: VNIC packet capturing for egress traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start the packet capturing, we need to take an SSH session to host `172.16.1.97`
    and identify on which VNIC the VM is running:'
  prefs: []
  type: TYPE_NORMAL
- en: Issue the `ESXTOP` command and press the *n* key to show the network parameters.
    The following screenshot shows the ESXTOP screen. We have identified that our
    source virtual machine 192.16.10.12 is running on `vmnic0`:![VNIC packet capturing
    for egress traffic](img/image_08_018.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Issue the following command for egress traffic capturing. Output is saved in
    the ESXi host **tmp** directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initiate a `ping` request from the source virtual machine to the destination
    virtual machine, as shown in the following screenshot:![VNIC packet capturing
    for egress traffic](img/image_08_019.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the packet capture after some time by pressing *Ctrl *+ *C* in an ESXi
    putty session and see the following output:![VNIC packet capturing for egress
    traffic](img/image_08_020.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The saved packet `webAvxlan.pcap` needs to be imported to the Wireshark tool.
    I know for sure that everyone knows how to copy or download a file from the ESXi
    host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we import the file to Wireshark, we will get the following output:![VNIC
    packet capturing for egress traffic](img/image_08_021.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the **Virtual eXentsible Local Area Network** option, which will display
    the complete VXLAN header field as shown in the following screenshot:![VNIC packet
    capturing for egress traffic](img/image_08_022.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we can see from the highlighted field, we got the following output, which
    matches perfectly with the lab environment details that we mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: The inner IP `Src` is `192.16.10.12` and `Dst` is `192.16.10.14` (two machines
    which we have tested the ping command)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VXLAN network is `5001` (the logical switch where our virtual machines are
    running)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VXLAN UDP port is `4789`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The outer IP `SRC: 172.16.1.32` and `DST: 172.16.1.33` (VXLAN tunnel endpoint
    IP address)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I strongly believe this is the most precise VXLAN output, which will help us
    in many scenarios, and pktcap-uw is a great tool, which is not used by many people,
    primarily because of lack of knowledge. With the captured output imported to the
    **Wireshark** tool, it gives us granular-level information on all the fields.
    So, keep these steps handy and I bet this will be useful in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: NSX upgrade checklist and planning order
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is always better to stop something from happening in the first instance
    than spend time repairing the damage after it has happened. Every product upgrade
    should follow a step-by-step process with proper planning, and an NSX upgrade
    is no exception. The following steps are the proper order that needs to be followed
    while planning to upgrade the NSX environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Upgrade NSX Manager
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade NSX Controllers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade ESXi cluster prepared by NSX
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade Distributed Logical Router and Edge service gateway
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upgrade data security and guest introspection services
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First and foremost, we need to ensure the following pre-checks are met before
    doing any upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an NSX pre-upgrade checklist:'
  prefs: []
  type: TYPE_NORMAL
- en: Take backup for NSX Manager. In a cross-VC NSX environment, we need to take
    backup from all the NSX Managers and complete the following process for all NSX
    Managers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a snapshot of all NSX Managers. This is only for additional protection
    in case normal backups are not available or corrupted due to human error or catastrophic
    events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log in to NSX Manager and run show filesystems to show the `/dev/sda2` filesystem
    usage. If the filesystem usage is 100 percent, the upgrade process will certainly
    fail, and for such cases, we need to purge manager logs and NSX system commands
    and reboot the NSX Manager before starting with the upgrade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Issue the following commands in NSX Manager to purge NSX logs and system commands:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: purge log manager
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: purge log system
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reboot the NSX Manager appliance for the log cleanup to take effect.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX data security should be uninstalled before upgrading NSX Manager.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure all controllers are connected and don't plan to deploy any new controllers
    during the existing controller upgrade phase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Place the vSphere cluster ESXi host in maintenance mode, perform the upgrade,
    and reboot the host. Continue the same operation for the NSX-prepared ESXi host,
    so that new VIBS are pushed to all ESXi hosts, and finally, take the host out
    of maintenance mode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from NSX Manager 6.2.3 onwards the default VXLAN port is `4789`. Before
    NSX 6.2.3, the default VXLAN UDP port number was `8472`. So, if we are planning
    to continue with the new VXLAN port `4789`, please ensure that this port is allowed
    in your firewall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Edge and NSX Distributed Logical Router control VM can be upgraded in any
    order, and there is no repeated upgrade process for HA-enabled NSX Edge and control
    VM. Both appliances get upgraded at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we can go ahead and upgrade the guest introspection virtual machine
    and respective partner appliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the upgrade is complete, we should delete all snapshots taken for NSX Manager,
    after verifying all components and services are up and running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts all operationally-impacted tasks, and non-impacted
    tasks and services during respective NSX component upgrade phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **NSX Components** | **Operational impact** | **Not impacted** |'
  prefs: []
  type: TYPE_TB
- en: '| **NSX Manager** | NSX Manager GUI and API-related new tasks are blocked |
    Control plane and data plane continue to work |'
  prefs: []
  type: TYPE_TB
- en: '| **NSX Controllers** | No modification accepted for logical networks and no
    new logical network creation will be accepted | Management plane and data plane
    |'
  prefs: []
  type: TYPE_TB
- en: '| **vSphere Cluster** | No new VM provisioning will be accepted during this
    phase for that specific vSphere cluster. | Management plane, control plane, and
    data plane related tasks for other vSphere clusters will be working (if we are
    doing upgrades on one vSphere cluster at a time) |'
  prefs: []
  type: TYPE_TB
- en: '| **NSX Edge and Control VM** | All Edge and control VM services will be impacted
    during this operation. | Management plane, new Edges and control VM can be deployed.
    All existing configuration on old edges will be retained. |'
  prefs: []
  type: TYPE_TB
- en: '| **Guest introspection and data security** | Virtual machines will be unprotected
    during this phase | All other NSX related tasks and services will be intact. |'
  prefs: []
  type: TYPE_TB
- en: That concludes our final chapter, and I believe this network virtualization
    journey has been fantastic so far.
  prefs: []
  type: TYPE_NORMAL
- en: The future of NSX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The current and future choice of IT will certainly be Software Defined Data
    Center, no doubt about it. One of the primary reasons I believe this product and
    technology is the cherry on top of the VMware product portfolio is the proven
    and larger ecosystem that VMware has, and the fact that customers can leverage
    NSX in private, public (vCloud Air), and with the mix of both, a truly hybrid
    cloud platform. There are millions of workloads, which are protected by vCloud
    Air disaster recovery data center, and behind the scenes, these are vSphere environments
    that are fully network-virtualized with NSX. It''s no mistake that Gartner has
    recently recognized the vCloud Air disaster recovery solution as one of the best
    in the public Cloud market. The VMware NSX approach is very simple: *Follow the
    virtual machines wherever they go: private, public, or hybrid will always remain
    secured*. From a cross-vendor perspective, there are definitely some hard decisions
    taken by VMware, especially with products such as NEXUS 1000V, which is no longer
    supported with VMware NSX. However, in a greenfield deployment scenario, there
    is only one question that might be raised while looking for an NSX solution: what
    is the cost and manpower involved in developing or remodeling existing applications
    if NSX is getting integrated? Does it demand an overall change in networking?
    Does it demand any specific models of switches or routers? We all have the answers
    now: NSX doesn''t demand any significant change in overall physical networking.
    Already a recognized leader in software defined networking, VMware has made another
    wise move by acquiring Arkin on June 13, 2016, and that will further simplify
    a lot of NSX operational tasks. Arkin cross-domain visibility will help customers
    to get a granular visibility on physical to overlay mapping and other security
    parameters, and with that approach and output, day-to-day operational tasks will
    be much more simplified. How many times have we received questions about how a
    virtual machine is connected to the network? The traditional way of checking that
    would be by taking multiple connections to multiple products to get an end-to-end
    connectivity view. I strongly believe tasks like this will be simplified with
    the integration of Arkin with NSX, and adding to that, it will give us precise
    information on what types of traffic we have in the data center, as well as overall
    traffic percentages for East-West and North-South, Internet traffic, as a few
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: VMware vRealize network insight (Arkin) is an intelligent security and operations
    management solution for the network, which provides 360-degree visibility across
    virtual and physical networks using network flow analytics. It is available for
    download from 1^(st) August 2016\. There was also a recent announcement from VMware
    regarding a new release of NSX multi hypervisor called NSX transformers, which
    supports hypervisors such as KVM and vSphere, and one could simply club them under
    a common NSX transport zone. It's too early to comment on how transformers will
    evolve, so for the time being, it will be a watch and wait game for all of us.
    NSX is certainly an evolution in software-defined networking, and it has got all
    the bits and fragments needed to reach further heights, which will enormously
    help all types of businesses. I appreciate you all being part of this journey
    and I would encourage everyone to start testing and implementing this great solution.
    Let's be part of this game-changing software. Based on your technical background
    and the pace at which we can understand and learn a technology, I believe there
    will be a few questions, and I would appreciate if you all can reach out to me
    via LinkedIn. Rest assured I will ensure queries are addressed at the earliest
    opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with Introduction to troubleshooting followed by NSX
    manager,Controller and Data plane log collection and major focus points when things
    go wrong. Finally we ended this chapter with Future of NSX followed by few links
    for documentation reading.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lastly, as promised earlier, I''m posting all the articles that we all should
    read. You can trust me that the right way to climb the NSX ladder is by reading
    each and every document available; it will only multiply our knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: The CISCO NEXUS 9000 design guide, which talks about a few design scenarios
    with UCS servers. A great guide for physical network design understanding with
    NSX: [http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf](http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced networking services offered through vCloud Air. It is worth reading
    this document to know how NSX is helping vCloud Air customers and what services
    are offered by providing zero trust security in the public Cloud: [http://vcloud.vmware.com/service-offering/advanced-networking-services](http://vcloud.vmware.com/service-offering/advanced-networking-services)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VMware NSX design guide for vSphere is another knowledge hub for designing
    NSX in a vSphere environment: [http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf](http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should all go through this blog if we really want to keep ourselves updated
    with the technology. This is a network virtualization blog from VMware, and there
    is good amount of videos and use cases discussed: [http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI](http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware integrated OpenStack with NSX configuration guide. This document demands
    a little bit of VIO knowledge; however, readers will find out how NSX works in
    the VIO world: [https://communities.vmware.com/docs/DOC-30985](https://communities.vmware.com/docs/DOC-30985)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete guide to upgrading VCNS to NSX. This guide contains step-by-step
    instructions to upgrade all vCloud network security solutions to NSX: [https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf](https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I'm going to shout practice, practice, practice! Well, practice makes perfect,
    so keep practicing the free VMware HOL labs. I would highly recommend starting
    with vSphere distributed switch from A to Z labs before starting NSX labs: [http://labs.hol.vmware.com/HOL/catalogs/catalog/130](http://labs.hol.vmware.com/HOL/catalogs/catalog/130)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The document is a deployment guide for implementing VMware NSX with Brocade
    VCS: [http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html](http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
