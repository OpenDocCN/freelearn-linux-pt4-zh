- en: Chapter 8.  NSX Troubleshooting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章.  NSX故障排除
- en: 'Let me start this chapter with a famous quote from Antisthenes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从安提斯滕尼斯的名言开始本章：
- en: '*"Not to unlearn what you have learned is the most necessary kind of learning"*'
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“不忘记你已经学到的，才是最必要的学习。”*'
- en: I couldn't find a better quote than that for giving everyone a heads-up on how
    vital it is to ensure that we recollect what we have learned so far in previous
    chapters about how to approach a problem to see what the best solution is. For
    the best solution to also be the quickest, we truly need to know how to approach
    a scenario, where to start looking, what logs are useful, and lastly, when to
    engage the vendor for further troubleshooting. As we all know, our course is focused
    on NSX with vSphere. NSX is tightly integrated with vSphere.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我找不到比这更合适的名言来提醒大家，确保我们回顾前面章节中学习到的关于如何处理问题的知识，找出最佳解决方案是多么重要。为了让最佳解决方案也是最快的，我们真的需要知道如何处理一个场景，从哪里开始寻找，哪些日志有用，最后，何时需要联系供应商进一步排查。正如我们所知道的，我们的课程集中于NSX与vSphere的结合。NSX与vSphere紧密集成。
- en: 'Taking a real example, even a well-constructed building will not stand on a
    weak foundation. A bad vSphere design will have a direct impact on NSX components,
    no matter how good the NSX design is. This rule of thumb is the same for any VMware
    solution that runs on top of vSphere. In this chapter, we will cover the following
    topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个实际例子来说，即使是结构坚固的建筑也无法建立在薄弱的基础上。糟糕的vSphere设计将直接影响NSX组件，无论NSX设计多么优秀。这条经验法则适用于任何基于vSphere运行的VMware解决方案。在本章中，我们将讨论以下主题：
- en: NSX installation and registration issues
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX安装和注册问题
- en: The log collection process and steps
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志收集过程和步骤
- en: VXLAN troubleshooting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN故障排除
- en: NSX Manager installation and registration issues
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX Manager安装和注册问题
- en: 'Installing NSX Manager is one of the easiest tasks, and the bitter truth is
    that anyone who is familiar with vSphere OVA/OVF deployment can easily deploy
    an NSX Manager without any prior knowledge of NSX products. We know for sure,
    that in a production environment, no one will follow that method. However, I still
    want to educate you all about the importance of NSX installation. Let''s carefully
    go through the following points:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 安装NSX Manager是最简单的任务之一，残酷的事实是，任何熟悉vSphere OVA/OVF部署的人都可以轻松部署NSX Manager，而无需任何NSX产品的先验知识。我们确信，在生产环境中，没有人会采用这种方法。然而，我仍然想让大家了解NSX安装的重要性。让我们仔细阅读以下几点：
- en: There should not be any vCloud networking security (VCNS/vShield Manager) registered
    with the same vCenter when we are trying to register NSX Manager. If we find any
    such environments, we must ensure that we are unregistering one of the solutions;
    definitely VCNS/vShield, since that is an outdated solution compared with NSX
    Manager. That doesn't mean we can have two NSX Managers registered with the same
    vCenter Server. However, we can upgrade VCNS to NSX and I will be sharing the
    upgrade guide link in the chapter's final section.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们尝试注册NSX Manager时，不能与同一vCenter注册任何vCloud网络安全（VCNS/vShield Manager）。如果我们发现有这样的环境，我们必须确保取消注册其中一个解决方案；肯定是VCNS/vShield，因为与NSX
    Manager相比，它是一个过时的解决方案。这并不意味着我们可以在同一vCenter Server上注册两个NSX Manager。然而，我们可以将VCNS升级到NSX，升级指南的链接将在本章的最后部分分享。
- en: Never import any previously used NSX Manager instance to a new environment and
    register it as a solution with a new vCenter.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要将以前使用过的NSX Manager实例导入新环境并注册为新vCenter的解决方案。
- en: Always check if NSX Manager is registered with how many vSphere solutions. For
    example, we might have a **vCloudAutomation Center** (**VCAC**) and** vCloud Director** (**VCD**)
    registered with NSX Manager A, which is also registered with a vCenter Server
    environment. The reason why I'm more curious about such solutions is that careful
    planning and design is required not only for installation but also for uninstallation
    of NSX products during break fix time. Each solution's integration demands separate
    steps while unregistering NSX Manager.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终检查NSX Manager是否与多少个vSphere解决方案注册。例如，我们可能已经将**vCloudAutomation Center**（**VCAC**）和**vCloud
    Director**（**VCD**）注册到NSX Manager A，而NSX Manager A还与vCenter Server环境注册。我之所以对这些解决方案更感兴趣，是因为不仅在安装过程中需要仔细规划和设计，在故障修复时，NSX产品的卸载也需要特别的考虑。每个解决方案的集成都需要在取消注册NSX
    Manager时采取独立的步骤。
- en: Always take a backup of NSX Manager after initial deployment of the software.
    Never depend on the vSphere snapshot feature for this backup activity.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NSX Manager初次部署后，始终备份NSX Manager。绝不要依赖vSphere快照功能来进行此备份活动。
- en: NSX Manager can be treated as a normal vSphere virtual machine for troubleshooting
    any network-related issues. For example, we can migrate NSX Manager from one host
    to another host, or check the ESXTOP command to know Tx and Rx counts for isolating
    a network issue.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Manager可以作为普通的vSphere虚拟机来排查任何与网络相关的问题。例如，我们可以将NSX Manager从一个主机迁移到另一个主机，或者使用ESXTOP命令查看Tx和Rx计数，以便隔离网络问题。
- en: 'While registering with vCenter Server, we have two options:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注册vCenter Server时，我们有两个选项：
- en: '**Lookup service registration**: Lookup service registration is an optional
    feature for importing SSO users. However, if we are integrating with an SSO identity
    source, we need to follow all vendor-specific best practices for identity source
    availability. But, it''s worth remembering that if SSO is down except for login
    to NSX Manager, it won''t have any impact on NSX components and their features.'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查找服务注册**：查找服务注册是导入SSO用户的可选功能。然而，如果我们与SSO身份源集成，必须遵循所有供应商特定的最佳实践，以确保身份源的可用性。但值得记住的是，除登录到NSX
    Manager外，SSO出现故障对NSX组件及其功能没有任何影响。'
- en: '**vCenter Server Registration**: vCenter Server registration is the first and
    most critical integration. Hence, we need to ensure that we have proper connectivity
    and configuration for the following points:'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vCenter Server注册**：vCenter Server的注册是第一步，也是最关键的集成。因此，我们需要确保以下几点的连接性和配置正确：'
- en: DNS resolution should be configured between **NSX Manager** and **vCenter Server**.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX Manager**和**vCenter Server**之间应配置DNS解析。'
- en: '**NTP** should be configured properly; this point might be very familiar for
    most of the experts, but I will still reiterate it: The impact of wrong NTP is
    very high when we integrate the lookup service (SSO) and try to leverage SSO-based
    authentication.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NTP**应该正确配置；这个点对于大多数专家来说可能非常熟悉，但我还是要重申：错误的NTP配置在我们集成查找服务（SSO）并尝试利用基于SSO的认证时，影响非常大。'
- en: '**Firewall** ports should be opened between NSX Manager and vCenter Server.
    Always verify VMware **Knowledge Base** (**KB**) article for port requirements.
    The following link leads to a VMware KB article, which talks about all the port
    requirements:'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防火墙**端口应在NSX Manager和vCenter Server之间打开。始终检查VMware **知识库**（**KB**）文章以了解端口要求。以下链接指向一篇VMware
    KB文章，讨论了所有端口要求：'
- en: '`https://kb.vmware.com/selfservice/search.do?cmd=displayKC&docType=kc&docTypeID=DT_KB_1_1&externalId=2079386`'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`https://kb.vmware.com/selfservice/search.do?cmd=displayKC&docType=kc&docTypeID=DT_KB_1_1&externalId=2079386`'
- en: Ensure that we are using vCenter Server administrative user rights while registering
    with NSX Manager. We can certainly use the administrator@vsphere.local account
    to register NSX with vCenter, vCloud Director, and vRealize Automation products.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在注册到NSX Manager时，确保我们使用的是vCenter Server的管理员权限。我们当然可以使用administrator@vsphere.local账户将NSX注册到vCenter、vCloud
    Director和vRealize Automation产品中。
- en: Troubleshooting NSX Manager
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除NSX Manager
- en: Based on the situation, we may have to collect diagnostic information for NSX
    Manager for VMware Support. Keep the following steps handy for such scenarios.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据情况，我们可能需要为VMware支持收集NSX Manager的诊断信息。在此类情况下，请牢记以下步骤。
- en: Collecting NSX Manager logs via GUI
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过GUI收集NSX Manager日志
- en: 'The steps to collect NSX Manager logs via GUI are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过GUI收集NSX Manager日志的步骤如下：
- en: Log in to the ****NSX Manager**** virtual appliance through a web browser.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过网页浏览器登录到****NSX Manager****虚拟设备。
- en: In **NSX Manager Virtual Appliance Management**, click **Download Tech Support
    Log**.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**NSX Manager虚拟设备管理**中，点击**下载技术支持日志**。
- en: 'Click ****Download**** | ****Save**** . The following screenshot depicts the
    NSX Manager logs download:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击****下载**** | ****保存****。以下截图显示了NSX Manager日志下载：
- en: '![Collecting NSX Manager logs via GUI](img/image_08_001.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![通过GUI收集NSX Manager日志](img/image_08_001.jpg)'
- en: Collecting NSX Manager logs via CLI
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过CLI收集NSX Manager日志
- en: 'There might be instances when the NSX Manager GUI is not working and we might
    need to depend on the CLI for collecting logs. For CLI haters, there is no escape
    this time; we need to go through the following steps to capture NSX Manager logs:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现NSX Manager GUI无法正常工作，我们可能需要依赖CLI来收集日志。对于不喜欢CLI的用户，这次没有逃避的机会；我们需要通过以下步骤捕获NSX
    Manager日志：
- en: Log in to the **NSX Manager** virtual appliance through a SSH session.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过SSH会话登录到**NSX Manager**虚拟设备。
- en: Go to ****Enable Mode**** , by typing `enable`**.**
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输入`enable`进入****启用模式****。
- en: 'Issue the following command in **Enable Mode**, which will save the NSX Manager
    logs in a remote location based on the host name that we selected:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **启用模式** 下执行以下命令，命令会根据我们选择的主机名将 NSX Manager 日志保存到远程位置：
- en: '`export tech-support scp USERNAME@HOSTNAME:FILENAME`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`export tech-support scp USERNAME@HOSTNAME:FILENAME`'
- en: 'The following screenshot illustrates NSX CLI log capturing:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 NSX CLI 日志捕获：
- en: '![Collecting NSX Manager logs via CLI](img/image_08_002.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![通过 CLI 收集 NSX Manager 日志](img/image_08_002.jpg)'
- en: VMware Installation Bundle
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VMware 安装包
- en: 'Hypervisors are basically the backbone of network virtualization. Virtual machines
    are able to leverage NSX features primarily because the ESXi host is a network-virtualized
    host. One of the most critical pillars of an NSX installation is ESXi host preparation.
    If we don''t have the right modules running in the ESXi host, the whole purpose
    of leveraging NSX features will be defeated. Symptoms would be that we might not
    be able to install feature *X*, or we can configure feature *X*, but the functionality
    is impacted. Watch out for the following VIBs in the ESXi host:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化主机本质上是网络虚拟化的骨干。虚拟机能够利用 NSX 特性，主要是因为 ESXi 主机是一个网络虚拟化主机。NSX 安装的一个关键支柱是 ESXi
    主机的准备。如果 ESXi 主机上没有运行正确的模块，那么利用 NSX 特性的目的就会失效。可能的症状是我们可能无法安装特性 *X*，或者可以配置特性 *X*，但功能会受到影响。请注意以下在
    ESXi 主机中的 VIB：
- en: esx-vxlan
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: esx-vxlan
- en: esx-vsip
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: esx-vsip
- en: esx-dvfilter-switch-security (starting from NSX 6.2.0, esx-dvfilter-switch-security
    is part of esx-vxlan vibs)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: esx-dvfilter-switch-security（从 NSX 6.2.0 开始，esx-dvfilter-switch-security 成为
    esx-vxlan vibs 的一部分）
- en: 'This is the command to check if VIB is installed in ESXi host:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检查 VIB 是否已安装在 ESXi 主机中的命令：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Since these are VIBs, we can manually uninstall and install the same during
    break fix scenarios. But the real question is, who is pushing these VIBs? That's
    where I have seen the majority of issues. Behind the scenes, vCenter Server ESX
    Agent Managers (**EAM**) are responsible for installing these VIBs. So, first
    and foremost, the EAM service should be up and running. The following steps are
    useful for collecting EAM based upon the operating system and vCenter Server flavor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些是 VIB，我们可以在修复场景中手动卸载并重新安装相同的 VIB。但真正的问题是，谁在推送这些 VIB？我在这里看到的大多数问题都是出自这个原因。幕后，vCenter
    Server ESX Agent Managers (**EAM**) 负责安装这些 VIB。因此，首要任务是确保 EAM 服务正在运行。以下步骤对于根据操作系统和
    vCenter Server 版本收集 EAM 日志非常有用。
- en: EAM log location
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EAM 日志位置
- en: 'Following are the EAM log locations for respective vCenter Server and operating
    system versions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是各版本 vCenter Server 和操作系统对应的 EAM 日志位置：
- en: 'VMware vSphere 5.1.x/5.5.x (EAM is a part of the common Tomcat server):'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vSphere 5.1.x/5.5.x（EAM 是通用 Tomcat 服务器的一部分）：
- en: Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\VMware
    VirtualCenter\Logs\eam.log`
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\VMware
    VirtualCenter\Logs\eam.log`'
- en: 'Windows 2008: Same as Windows 2003, the VC log directory is located at `C:\ProgramData\VMware\VMware
    VirtualCenter\Logs\`'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Windows 2008: 与 Windows 2003 相同，VC 日志目录位于 `C:\ProgramData\VMware\VMware VirtualCenter\Logs\`'
- en: '**vCenter Server Virtual Appliance** (**VCVA**): `/storage/log/vmware/vpx/eam.log`'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vCenter Server 虚拟设备** (**VCVA**): `/storage/log/vmware/vpx/eam.log`'
- en: 'VMware vSphere 6.x (EAM is a standalone service and has embedded tcserver):'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vSphere 6.x（EAM 是独立服务，并嵌入了 tcserver）：
- en: Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\CIS\logs\eam\eam.log`
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Windows 2003: `C:\Documents and Settings\All Users\Application Data\VMware\CIS\logs\eam\eam.log`'
- en: Windows 2008: Same as Windows 2003, the VC log directory is located at `C:\ProgramData\VMware\VMware
    VirtualCenter\Logs\`
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Windows 2008: 与 Windows 2003 相同，VC 日志目录位于 `C:\ProgramData\VMware\VMware VirtualCenter\Logs\`'
- en: CloudVM: `/storage/log/vmware/eam/eam.log`
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CloudVM: `/storage/log/vmware/eam/eam.log`'
- en: 'I have seen a lot of issues, especially when a vCenter Server installation
    is a Windows-based installation, with EAM trying to use port `80` for downloading
    VIBs. At times, we might have other applications or services running in VC, which
    might be leveraging port `80` and will cause VIB download failures, so we have
    to change the default EAM ports. However, starting from VMware vSphere 6.0, VIB
    downloads over port `443` (instead of port `80`) are supported. This port is opened
    and closed dynamically. The intermediate devices (firewalls) between the ESXi
    hosts and vCenter Server must allow traffic using this port. With that, we will
    move on to our next topic: control plane and data plane log collection.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过很多问题，特别是当vCenter Server安装是基于Windows时，EAM尝试使用端口`80`下载VIB时。此时，VC中可能有其他应用程序或服务正在使用端口`80`，这会导致VIB下载失败，因此我们必须更改默认的EAM端口。然而，从VMware
    vSphere 6.0开始，支持通过端口`443`（而不是端口`80`）下载VIB。这个端口是动态开启和关闭的。ESXi主机与vCenter Server之间的中间设备（防火墙）必须允许通过该端口的流量。这样，我们将进入下一个主题：控制平面和数据平面日志收集。
- en: Control plane and data plane log collection
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制平面和数据平面日志收集
- en: 'Log collection is vital for proactive and root cause analysis. How many times
    have we ended up collecting the wrong set of logs or received feedback that we
    have to enable or increase certain logging levels to ensure that we have the right
    set of logs to analyze the root cause? Technically, that type of feedback is digestible.
    However, when it comes to production impact, it would be a disappointment to find
    that there is nothing conclusive, even after going through the logs. There is
    only one solution for this issue: we should know what logs need to be collected
    and most importantly, from which locality. First and foremost, we need to get
    some background knowledge on the following points:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 日志收集对于主动预警和根本原因分析至关重要。有多少次我们最终收集了错误的日志集，或者收到了反馈，说我们必须启用或提高某些日志级别，以确保我们收集了合适的日志来分析根本原因？从技术上讲，这类反馈是可以理解的。然而，当涉及到生产影响时，如果在查看日志后发现没有得出结论，那将是令人失望的。解决此问题的唯一方法是：我们应该清楚需要收集哪些日志，最重要的是，应该从哪个位置收集。首先，我们需要了解以下几个背景知识：
- en: Understanding the physical topology
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解物理拓扑
- en: 'Understanding the physical topology is not only important for overall NSX design
    and feature configuration, it is equally important to share effective feedback
    if there is a better way to approach the overall design. The following mentioned
    points are something that we need to keep handy:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理解物理拓扑不仅对于整体NSX设计和功能配置至关重要，还同样重要的是在发现有更好的设计方式时能够提供有效的反馈。以下是我们需要时刻注意的几个要点：
- en: Physical Network design - Spine-leaf/layer2 architecture
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理网络设计 - 脊叶/层2架构
- en: Existing firewall deployments and rules
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有的防火墙部署和规则
- en: Overall datacenter Routing and Switching topology
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整体数据中心路由和交换拓扑
- en: vSphere cluster design (placement in racks) and topology. In addition to that, how
    many clusters (single-site, multi-site), data centers and active-active or active-passive
    physical data center designs required currently or future configuration.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere集群设计（机架放置）和拓扑。除此之外，还需要了解当前或未来配置所需的集群数量（单站点、多站点）、数据中心以及主动-主动或主动-被动的物理数据中心设计。
- en: vSphere distributed Switch Uplink policy
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere分布式交换机上行链路策略
- en: 'The following are some recommendations based on the preceding points:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基于上述几点的建议：
- en: '**Physical Network design - Spine-leaf/layer2 architecture**: Spine/Leaf architecture
    is the best and most widely used connectivity now a days because of full mesh
    connectivity, less latency, high bandwidth, ECMP routing and most importantly
    easy expansion of network is achievable.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理网络设计 - 脊叶/层2架构**：脊叶架构是如今最好的也是最广泛使用的连接方式，因其具备完全网状连接、低延迟、高带宽、ECMP路由，最重要的是，网络扩展非常容易实现。'
- en: '**Existing firewall deployments and rules**: This is an important check primarily
    because of NSX Edge Firewall and Microsegmentation capabilities. My suggestion
    would be try to integrate vRealize Network-insight software to understand overall
    traffic growth in North-South and East-West direction and then decide what firewall
    policies to be configured at which points. In the closing section of the chapter
    i have given a small summary on vRealize Network-insight software.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有防火墙部署和规则**：这是一个重要的检查，主要是因为 NSX Edge 防火墙和微分段能力。我的建议是尝试集成 vRealize Network-insight
    软件，以便了解南北向和东西向流量的总体增长，然后决定在哪些点配置哪些防火墙策略。在本章的结尾部分，我对 vRealize Network-insight 软件做了简要总结。'
- en: '**Overall datacenter routing and switching topology**:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整体数据中心路由和交换拓扑**：'
- en: Here we are mainly focusing on routing protocols used in physical network -
    for example OSPF, BGP, ISIS.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们主要关注物理网络中使用的路由协议——例如 OSPF、BGP、ISIS。
- en: For example based on the AREA types configured for OSPF, assuming that upstream
    router is **Area Border Router (ABR)** we need to know what routes should be injected
    to Upstream router from NSX Edge and appropriate firewall policies for allowing
    the traffic.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，基于 OSPF 配置的区域类型，假设上游路由器是 **区域边界路由器（ABR）**，我们需要知道哪些路由应该从 NSX Edge 注入到上游路由器，以及允许流量的适当防火墙策略。
- en: In the Layer 2 side ideally ESXI, VXLAN traffic is the most important parameters
    that we need to know for assigning/configuring VLAN-ID (Management, VMotion, Storage,
    VXLAN).
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二层方面，理想情况下，ESXi 和 VXLAN 流量是我们需要了解的最重要参数，以便为 VLAN-ID（管理、VMotion、存储、VXLAN）分配/配置。
- en: '**vSphere cluster design (placement in racks) and topology**:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSphere 集群设计（机架中的部署位置）和拓扑**：'
- en: Cluster design should ideally be Management & Edge Cluster Separated from Compute
    Cluster with minimum of 4 Host in Management & Edge Cluster and maximum of 64
    host in Computer Cluster (only if environment is running vSphere 6.0)
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群设计应该理想地将管理和边缘集群与计算集群分开，管理和边缘集群至少有 4 台主机，而计算集群最多有 64 台主机（仅在环境运行 vSphere 6.0
    时）。
- en: For disaster recovery environment configured with SRM and NSX.It is recommended
    to maintain similar physical and virtual design in the DR site. Situations might
    demand workloads to be running in DR site for long time, hence we need to follow
    that strict rule.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于配置了 SRM 和 NSX 的灾难恢复环境，建议在灾难恢复站点保持类似的物理和虚拟设计。某些情况下，可能要求工作负载长时间运行在灾难恢复站点，因此我们需要遵循这一严格规则。
- en: '**vSphere distributed Switch Uplink policy**: This is the same topic that we
    have discussed in [Chapter 3](ch03.html "Chapter 3. NSX Manager Installation and
    Configuration"), *NSX Manager Installation and Configuration*:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSphere 分布式交换机上行链路策略**：这是我们在 [第 3 章](ch03.html "第 3 章. NSX 管理器安装与配置")中讨论过的相同主题，*NSX
    管理器安装与配置*：'
- en: Here we need to select and configure NIC teaming and failover policies. For
    example if we choose LACP configuration, we are limited with Single VTEP configuration,
    however route based on port and source MAC hash support multi VTEP configuration.
    Prevention is better that cure, it is recommended to load balance VXLAN traffic
    across all available uplinks rather than inviting performance issues.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们需要选择并配置 NIC 集群和故障转移策略。例如，如果选择 LACP 配置，我们将受到单一 VTEP 配置的限制，而基于端口和源 MAC 哈希的路由支持多
    VTEP 配置。防患于未然，建议通过所有可用上行链路负载均衡 VXLAN 流量，而不是因性能问题而遭遇麻烦。
- en: The idea behind these minor point discussion is to ensure all configurations/design
    aspects are taken care well in advance rather than spending time later to do configuration
    and performance tweaking.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些小点讨论背后的目的是确保所有配置/设计方面提前得到妥善处理，而不是后期花费时间进行配置和性能调优。
- en: NSX Controller log collection
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 控制器日志收集
- en: 'Controller is the real game changer component in the overall architecture of
    NSX. For the same reason, it remains a critical piece when it comes to troubleshooting.
    As we all know, controllers are deployed from NSX Manager in an **Open Virtualization
    Appliance** (**OVA**). In a worst-case scenario, even the deployment of controllers
    might fail, and that would be a showstopper for any NSX implementation. The majority
    of such failures happen for the following two reasons:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器是 NSX 整体架构中的真正变革性组件。正因如此，在故障排除时它仍然是一个至关重要的部分。正如我们所知，控制器是通过 **开放虚拟化设备**（**OVA**）从
    NSX 管理器部署的。在最坏的情况下，甚至控制器的部署可能会失败，这将成为任何 NSX 实施的拦路虎。这类故障大多数发生的原因有以下两个：
- en: DNS
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS
- en: NTP
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NTP
- en: 'There should be proper DNS/NTP configuration between ESXi hosts, vCenter Server,
    and NSX Manager for a successful deployment of NSX Controller. Apart from this
    point, a successful deployment of any virtual machine in vSphere certainly needs
    enough compute and storage capacity, and NSX Controller is no exception, primarily
    because these are virtual machines from an ESXi host perspective. For collecting
    NSX Controller logs, we need to complete the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ESXi 主机、vCenter Server 和 NSX Manager 之间应进行适当的 DNS/NTP 配置，以确保 NSX Controller
    部署成功。除此之外，vSphere 中任何虚拟机的成功部署都需要足够的计算和存储容量，NSX Controller 也不例外，主要因为这些都是从 ESXi
    主机的角度来看是虚拟机。对于收集 NSX Controller 日志，我们需要完成以下步骤：
- en: Firstly, we need to log in to vCenter Server using the vSphere web client.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要使用 vSphere Web 客户端登录到 vCenter Server。
- en: Click on **Networking and Security**.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**网络与安全**。
- en: Click **Installation** on the left-hand pane.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧面板上的**安装**。
- en: Under the **Manage** tab, select the controller you want to download logs from.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**管理**选项卡下，选择你想要下载日志的控制器。
- en: Click **Download Tech support logs**.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下载技术支持日志**。
- en: 'The following screenshot depicts the NSX Controller log collection process:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下截图展示了 NSX Controller 日志收集过程：
- en: '![NSX Controller log collection](img/image_08_003.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![NSX Controller 日志收集](img/image_08_003.jpg)'
- en: 'How about if the web client is down, or we don''t have access to the web client?
    What method we can follow to collect the logs? There are two options in those
    cases:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Web 客户端无法使用，或者我们无法访问 Web 客户端怎么办？在这种情况下，我们可以采用两种方法来收集日志：
- en: Using a vSphere client session, we can connect to vCenter Server or the ESXi
    host where the controller is running, and we can take a VM console session to
    controller to leverage the CLI command for log collection.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 vSphere 客户端会话，我们可以连接到运行控制器的 vCenter Server 或 ESXi 主机，并且我们可以通过虚拟机控制台会话访问控制器，利用
    CLI 命令进行日志收集。
- en: Take an SSH session directly to controller and execute a CLI command for log
    collection.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接通过 SSH 会话连接到控制器并执行 CLI 命令进行日志收集。
- en: Collecting NSX Controller logs using CLI steps
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CLI 步骤收集 NSX Controller 日志
- en: 'Firstly, log into the NSX Controller for which you want to gather logs using
    any of the previous steps, and execute the following command, as shown in the
    screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用之前的任何步骤登录到你想要收集日志的 NSX Controller，并执行如下命令，如截图所示：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Collecting NSX Controller logs using CLI steps](img/image_08_004.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CLI 步骤收集 NSX Controller 日志](img/image_08_004.jpg)'
- en: After the controller logs are captured, we can go ahead and copy the same set
    of logs to any machine that has got IP connectivity with the controller.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器日志收集完成后，我们可以将这组日志复制到任何与控制器具有 IP 连接的机器上。
- en: 'In the following example, I have copied the controller logs to one of the management
    ESXi host TMP locations:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我将控制器日志复制到了其中一台管理 ESXi 主机的 TMP 位置：
- en: '![Collecting NSX Controller logs using CLI steps](img/image_08_005.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![使用 CLI 步骤收集 NSX Controller 日志](img/image_08_005.jpg)'
- en: With that, we will move to NSX Edge and DLR log collection, and we will finish
    off with data plane log collection and a few important service statuses.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将转到 NSX Edge 和 DLR 日志收集，最后会结束于数据平面日志收集和一些重要的服务状态。
- en: The processes for collecting Edge and DLR logs are almost the same.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 收集 Edge 和 DLR 日志的过程几乎相同。
- en: Collecting Edge and Distributed Logical Router logs through the web client
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Web 客户端收集 Edge 和分布式逻辑路由器日志
- en: 'The following are the steps for collecting distributed logical router logs:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是收集分布式逻辑路由器日志的步骤：
- en: Firstly, log into vCenter Server using the vSphere web client.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用 vSphere Web 客户端登录到 vCenter Server。
- en: Click the **Networking & Security** icon.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**网络与安全**图标。
- en: Click **Edges** on the left-hand pane.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧面板上的**边缘**。
- en: On the right-hand pane, select the edge (**DLR/EDGE**) we want to download from.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右侧面板中，选择我们想要下载日志的边缘设备（**DLR/EDGE**）。
- en: Click **Actions** and select ****Download Tech support logs**** .
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**操作**并选择**下载技术支持日志**。
- en: 'In the following screenshot, we can see **Download Tech support log** highlighted
    for Distributed Logical Router, and I mentioned earlier that this is the same
    process as for collecting NSX Edge logs:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到**下载技术支持日志**已被高亮显示在分布式逻辑路由器（DLR）上，我之前提到过，这与收集 NSX Edge 日志的过程相同：
- en: '![Collecting Edge and Distributed Logical Router logs through the web client](img/image_08_006.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![通过 Web 客户端收集 Edge 和分布式逻辑路由器日志](img/image_08_006.jpg)'
- en: 'For collecting logs via CLI, we need to execute the following command by executing
    any of the following steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过 CLI 收集日志，我们需要执行以下命令，可以通过以下任一步骤来执行：
- en: Using a vSphere client session, we can connect to vCenter Server or the ESXi
    host where controller is running, and we can take a VM console session to controller
    to leverage the CLI command for log collection.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 vSphere 客户端会话，我们可以连接到 vCenter Server 或运行控制器的 ESXi 主机，并可以通过 VM 控制台会话连接到控制器，以利用
    CLI 命令进行日志收集。
- en: 'Take an SSH session directly to controller and execute the CLI command for
    log collection:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接通过 SSH 会话连接到控制器并执行 CLI 命令以收集日志：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have already discussed what EAM is and the role it plays in an NSX environment.
    Apart from that vSphere troubleshooting piece, we need to the status and logging
    level of two user world agents, which will be running in an NSX-prepared ESXi
    host.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了什么是 EAM 以及它在 NSX 环境中的作用。除了 vSphere 故障排除部分，我们还需要检查两个在 NSX 准备的 ESXi 主机上运行的用户世界代理的状态和日志级别。
- en: NSX user world agents
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 用户世界代理
- en: NSX Manager is responsible for deploying the NSX Controller cluster, ESXi hosts
    preparation by pushing **vSphere Installation Bundles** (**VIBs**) to enable VXLAN,
    distributed routing, distributed firewall, and a user world agent used to communicate
    at the control-plane level. The functionality of the user world agent is highly
    critical and any failures will have a direct impact on the control plane learning,
    which eventually affects data plane traffic. So, let's discuss these agents, along
    with basic health checks and log locations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NSX Manager 负责部署 NSX 控制器集群，推送 **vSphere 安装包**（**VIBs**）以启用 VXLAN、分布式路由、分布式防火墙，并且用于与控制平面级别通信的用户世界代理。用户世界代理的功能至关重要，任何故障都会直接影响控制平面的学习，最终影响数据平面流量。因此，让我们来讨论这些代理及其基本健康检查和日志位置。
- en: netcpa
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: netcpa
- en: 'It is a user world agent that communicates with NSX control plane, and the
    netcpa service should be up and running on the NSX-prepared ESXi host. If the
    functionality is impacted, we will certainly experience routing and switching
    issues in the NSX environment, and the ESXi host won''t learn new routes from
    the time the netcpa service was down. So, this is extremely important: creating
    the routes alone on an NSX Edge VM won''t do the trick; unless the netcpa service
    is up and running, ESXi host won''t learn those routes. Complete the following
    steps in order to check for netcpa-related issues:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它是与 NSX 控制平面通信的用户世界代理，netcpa 服务应在已准备的 NSX ESXi 主机上运行。如果功能受到影响，我们肯定会在 NSX 环境中遇到路由和交换问题，并且自从
    netcpa 服务停止运行以来，ESXi 主机将无法学习新路由。因此，这一点非常重要：仅在 NSX Edge 虚拟机上创建路由是无法解决问题的；除非 netcpa
    服务已正常运行，否则 ESXi 主机无法学习这些路由。请按以下步骤检查与 netcpa 相关的问题：
- en: Check if the netcpa service is running on the host (this needs to be checked
    on the host where we are experiencing network or control-plane related issues).
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 netcpa 服务是否在主机上运行（此操作需要在我们遇到网络或控制平面相关问题的主机上进行检查）。
- en: 'Use the following command to check the netcpa service:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令检查 netcpa 服务：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot depicts the netcpa agent''s status:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 netcpa 代理的状态：
- en: '![netcpa](img/image_08_007.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![netcpa](img/image_08_007.jpg)'
- en: 'Check if the netcpa configuration file is showing all the controllers. Use
    the following command to check controller details in the configuration file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 netcpa 配置文件是否显示所有控制器。使用以下命令检查配置文件中的控制器详情：
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following screenshot depicts the config file output, with controller IP
    and SSL certificate thumbprint information:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了配置文件输出，其中包括控制器 IP 和 SSL 证书指纹信息：
- en: '![netcpa](img/image_08_008.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![netcpa](img/image_08_008.jpg)'
- en: 'In the `/var/log/netcpa.log` file on the ESXi host, we can see the complete
    netcpa logs. The following screenshot depicts controller registration information,
    which is populated in netcpa logs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ESXi 主机的 `/var/log/netcpa.log` 文件中，我们可以查看完整的 netcpa 日志。以下截图显示了控制器注册信息，这些信息已被填充到
    netcpa 日志中：
- en: '![netcpa](img/image_08_009.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![netcpa](img/image_08_009.jpg)'
- en: 'Anytime we are facing issues with the netcpa service, I would strongly recommend
    restarting the service to confirm if that fixes the issue. To restart the netcpa
    service, we need to complete the following steps in order:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们遇到 netcpa 服务问题时，我强烈建议重启该服务，以确认是否能解决问题。要重启 netcpa 服务，我们需要按照以下步骤操作：
- en: Log in as root to the ESXi host through SSH or through the DCUI console.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 SSH 或通过 DCUI 控制台以 root 身份登录到 ESXi 主机。
- en: Run the `/etc/init.d/netcpad` restart command to restart the netcpa agent on
    the ESXi host.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`/etc/init.d/netcpad restart`命令以重启ESXi主机上的netcpa代理。
- en: Vsfwd
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vsfwd
- en: NSX distributed firewall is a hypervisor integrated firewall and apart from
    the point that the host should have a firewall `vib` installed, there should be
    a vsfwd daemon process up and running for proper message bus communication with
    NSX Manager.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: NSX分布式防火墙是一个集成于虚拟化平台的防火墙，除了主机需要安装防火墙`vib`外，还需要有一个vsfwd守护进程在运行，以确保与NSX管理器之间的消息总线通信正常。
- en: 'The following command and screenshot shows a stateful firewall status on the
    ESXi host:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令和截图显示了ESXi主机上的有状态防火墙状态：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Vsfwd](img/image_08_010.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Vsfwd](img/image_08_010.jpg)'
- en: 'To check the active message bus session with NSX Manager the following command
    and screenshot depicts an active session with NSX Manager (172.16.1.5):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查与NSX管理器的活动消息总线会话，可以使用以下命令和截图来查看与NSX管理器（172.16.1.5）的活动会话：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Vsfwd](img/image_08_011.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![Vsfwd](img/image_08_011.jpg)'
- en: A potential failure can happen if port `**5671**` is not opened between the
    ESXi host and NSX Manager.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果端口`**5671**`在ESXi主机和NSX管理器之间未打开，可能会发生潜在的故障。
- en: Vsfwd log location and collection process
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vsfwd日志位置和收集过程
- en: NSX Distributed Firewall is a new generation firewall in vSphere environment
    used primarily because of its ability to filter traffic at the virtual machine
    NIC level. Hence, it is important to understand the log collection and a few troubleshooting
    steps related to this feature. So, let's get started.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: NSX分布式防火墙是vSphere环境中的新一代防火墙，主要因其能够在虚拟机NIC级别过滤流量而广泛使用。因此，理解日志收集和相关的故障排除步骤非常重要。那么，让我们开始吧。
- en: 'Firstly, we need to start with the prerequisites to run **Distributed Firewall**
    (**DFW**). There is no need for log collection, even if the following requirements
    are not met. The prerequisites for running DFW are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要了解运行**分布式防火墙**（**DFW**）的前提条件。即使以下要求未满足，也无需进行日志收集。运行DFW的前提条件如下：
- en: VMware vCenter Server should be version 5.5 minimum
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vCenter Server的最低版本应为5.5
- en: VMware ESXi version should be at 5.1, 5.5, 6.0
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware ESXi版本应为5.1、5.5、6.0
- en: VMware NSX for vSphere 6.0 and later
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware NSX for vSphere 6.0及更高版本
- en: 'All logs related to vsfwd will be at the following location, and their representation
    is shown in the figure:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与vsfwd相关的日志都将位于以下位置，并且其表示方式如下图所示：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Vsfwd log location and collection process](img/image_08_012.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![Vsfwd日志位置和收集过程](img/image_08_012.jpg)'
- en: Collecting centralized logs from NSX Manager
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从NSX管理器收集集中日志
- en: 'Firstly, we need to log in to NSX Manager using the admin credentials and execute
    the following command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用管理员凭据登录到NSX管理器并执行以下命令：
- en: '[PRE8]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the introduction of NSX 6.2.3, VMware has come up with an export host-tech-support
    command, which can be executed on the NSX Manager to collect the following information.
    I strongly believe they will be adding more and more log collection options, since
    this is a centralized way of collecting the logs, but a lot depends upon the type
    of failure. If we encounter an NSX Manager failure scenario, centralized logging
    functionality is also impacted, hence it is important to understand the next plan
    for such scenarios, which is the whole purpose of me explaining the log collection
    process so far. Following is the list of logs that are included in the centralized
    logs as of now:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 随着NSX 6.2.3的推出，VMware推出了一个导出主机技术支持命令，可以在NSX管理器上执行以收集以下信息。我坚信他们将会增加更多的日志收集选项，因为这是集中式的日志收集方式，但很多情况取决于故障类型。如果遇到NSX管理器故障，集中式日志功能也会受到影响，因此了解这种情况的下一步计划非常重要，这也是我迄今为止解释日志收集过程的目的。以下是当前集中式日志中包含的日志列表：
- en: vmkernel and vsfwd log files
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vmkernel和vsfwd日志文件
- en: A list of filters
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器列表
- en: A list of dfw rules
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DFW规则列表
- en: A list of containers
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器列表
- en: Spoofguard details
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spoofguard详细信息
- en: Host-related information
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机相关信息
- en: ipdiscovery-related information
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ipdiscovery相关信息
- en: RMQ command outputs
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMQ命令输出
- en: Security group and services profile and instance details
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全组和服务配置文件及实例详细信息
- en: esxcli-related outputs
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: esxcli相关输出
- en: VXLAN troubleshooting
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VXLAN故障排除
- en: 'VXLAN is the overlay technology that is used in the VMware NSX environment
    when it comes to first-time testing and implementation, and most likely, we will
    end up with a few connectivity issues. Some of the common issues that we might
    face because of misconfiguration in both virtual and physical networks are as
    follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN 是 VMware NSX 环境中用于首次测试和实施的覆盖技术，最有可能出现一些连接问题。由于虚拟和物理网络配置错误，我们可能会遇到以下一些常见问题：
- en: Virtual machines have no network connectivity, either between other machines
    in the same VXLAN network, or no egress connection with the physical world
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟机没有网络连接，无论是同一 VXLAN 网络中的其他机器之间，还是没有与物理网络的出口连接。
- en: Frequent packet drops getting reported and the applications team facing poor
    performance issues
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常出现数据包丢失，并且应用团队面临性能差的问题。
- en: Virtual Machines have proper network connectivity on some of the ESXi hosts,
    but when placed on another set of hosts, there is no connectivity
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟机在一些 ESXi 主机上具有正常的网络连接，但当它们被放置在另一组主机上时，则没有网络连接。
- en: Normal ping tests are fine, but when checking with VXLAN packets, packet drops
    occur
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通的 ping 测试正常，但当检查 VXLAN 数据包时，会出现数据包丢失。
- en: Virtual machines in VLAN networks have proper network connectivity; however,
    VXLAN networks are not working
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VLAN 网络中的虚拟机具有正常的网络连接；然而，VXLAN 网络无法正常工作。
- en: The majority of network-related issues related to VXLAN will be around the preceding
    list of issues; however, only by applying our knowledge will we have a clear picture
    of what type of networks customers have and what type of network issues they are
    facing. So, let's get started with learning what the possible symptoms for such
    issues are and what the necessary actions to resolve the issue are.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VXLAN 相关的大多数网络问题将集中在上述问题列表中；然而，只有通过应用我们的知识，我们才能清楚地了解客户的网络类型以及他们面临的网络问题。因此，让我们开始了解这些问题的可能症状以及解决问题所需的必要行动。
- en: After implementing a VXLAN solution, I would strongly recommend to checking
    GUI-level PING and VXLAN tests between all the NSX-prepared ESXi hosts, which
    is the best way to confirm if initial requirements are met for sending a VXLAN
    packet from one hypervisor to another hypervisor.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施 VXLAN 解决方案后，我强烈建议在所有 NSX 配置好的 ESXi 主机之间检查 GUI 层面的 PING 和 VXLAN 测试，这是确认是否满足初步要求的最佳方式，这些初步要求是为了从一个虚拟化主机向另一个虚拟化主机发送
    VXLAN 数据包。
- en: 'We need to select one of the logical switches. Go to **Monitor** page, and
    there we will have two options:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择一个逻辑交换机。进入 **监控** 页面，我们将看到两个选项：
- en: '**Ping**'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ping**'
- en: '**Broadcast**'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广播**'
- en: 'The following screenshot shows the **Ping** and **Broadcast** test options:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 **Ping** 和 **广播** 测试选项：
- en: '![VXLAN troubleshooting](img/image_08_013.jpg)First, we will do a **Ping**
    test between the `172.16.1.94` and `172.16.1.96` ESXi hosts, and we will follow
    that with a VXLAN test. The following screenshot shows a successful Ping test:![VXLAN
    troubleshooting](img/image_08_014.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![VXLAN 故障排除](img/image_08_013.jpg)首先，我们将对 `172.16.1.94` 和 `172.16.1.96` 之间的
    ESXi 主机进行 **Ping** 测试，然后进行 VXLAN 测试。以下截图展示了成功的 Ping 测试：![VXLAN 故障排除](img/image_08_014.jpg)'
- en: 'The following screenshot shows a VXLAN test between the same ESXi host, and
    we can confirm that VXLAN packets sent from host `172.16.1.94` are successfully
    accepted by `172.16.1.96`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了同一 ESXi 主机之间的 VXLAN 测试，我们可以确认，从主机 `172.16.1.94` 发送的 VXLAN 数据包已被 `172.16.1.96`
    成功接收：
- en: '![VXLAN troubleshooting](img/image_08_015.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![VXLAN 故障排除](img/image_08_015.jpg)'
- en: 'In addition to the previous test, we can also perform a VXLAN ping test through
    an SSH session, and I have captured the output of this test between two ESXi hosts.
    The command to perform the test is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面的测试外，我们还可以通过 SSH 会话执行 VXLAN ping 测试，我已经捕获了在两台 ESXi 主机之间进行此测试的输出。执行此测试的命令如下：
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following screenshot depicts a VXLAN test performed through an SSH session
    to host 172.16.1.94:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了通过 SSH 会话对主机 172.16.1.94 进行的 VXLAN 测试：
- en: '![VXLAN troubleshooting](img/image_08_016.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![VXLAN 故障排除](img/image_08_016.jpg)'
- en: 'The preceding output is a clear indication that MTU is set properly in the
    VXLAN environment, so we are able to MTU more than 1500\. If there is an MTU misconfiguration
    issue, this test would fail, as shown in the following screenshot. I intentionally
    changed MTU in the distributed virtual switch from 1600 to 1500 to showcase the
    failure scenario:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出清楚地表明VXLAN环境中的MTU设置正确，因此我们能够支持大于1500的MTU。如果存在MTU配置错误问题，此测试将会失败，正如以下截图所示。我故意将分布式虚拟交换机中的MTU从1600改为1500，以展示失败的场景：
- en: '![VXLAN troubleshooting](img/image_08_017.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![VXLAN故障排除](img/image_08_017.jpg)'
- en: Network troubleshooting isn't complete without a packet capture. This is our
    final topic for this chapter, and I will showcase how to collect VXLAN packets.
    We will also take a quick walk-through to see what information is in the packet.
    Considering the knowledge we have gained so far, it should be a cakewalk for everyone.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 网络故障排除离不开数据包捕获。这是本章的最后一个话题，我将展示如何收集VXLAN数据包。我们还将快速浏览一下数据包中的信息。考虑到我们迄今为止获得的知识，对于每个人来说，这应该是轻而易举的。
- en: Packet capturing and analysis
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据包捕获与分析
- en: 'Starting from ESXi 5.5, the pktcap-uw tool is embedded inside the hypervisor.
    Some of you will be familiar with the tcpdump tool, which was already available
    in ESXi; pktcap is a replacement for the same. The prime reason for integrating
    the pktcap tool captures packets are every layer which is extremely essential
    in NSX world. So, we are no longer limited by capturing packets at the vmkernel
    layer. I have been a big fan of this tool starting from the vCloud networking
    and security days and I strongly believe most of us will like this tool. Before
    jumping into packet capturing, let''s be clear about the following points:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从ESXi 5.5开始，pktcap-uw工具被嵌入到虚拟化管理程序中。你们中的一些人可能已经熟悉tcpdump工具，它在ESXi中已有提供；pktcap是其替代工具。集成pktcap工具的主要原因是，它能够捕获每一层的所有数据包，这在NSX环境中至关重要。因此，我们不再仅限于捕获vmkernel层的数据包。从vCloud
    Networking和Security时代起，我就一直是这个工具的忠实粉丝，我坚信我们大多数人都会喜欢这个工具。在开始数据包捕获之前，让我们明确以下几点：
- en: '`Pktcap`, by default, collects only incoming packets, and it is unidirectional.
    So, if we want to capture both ingress and egress traffic, we need to add certain
    parameters. If not, the whole purpose of capturing the packet will be defeated.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，`Pktcap`只捕获入站数据包，且是单向的。因此，如果我们想要捕获入站和出站流量，我们需要添加一些特定的参数。如果不这样做，捕获数据包的目的就会落空。
- en: Traffic direction is mentioned as `-dir 0` for ingress packets.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量方向标记为`-dir 0`用于入站数据包。
- en: Traffic direction is mentioned as `-dir 1` for egress packets.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量方向标记为`-dir 1`用于出站数据包。
- en: We can capture a packet at the vmkernel, vmnic, and switch port levels (DVS).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在vmkernel、vmnic和交换机端口级别（DVS）捕获数据包。
- en: 'Detailed command syntax is available if we issue the following command in ESXi
    host:'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在ESXi主机上输入以下命令，可以查看详细的命令语法：
- en: '[PRE10]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Enough theory; let's get started by capturing the packet to analyze the VXLAN
    field. Before that, let me explain my lab setup and virtual machine details so
    that we can verify if those outputs match the captured packet details.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 够多的理论了；让我们通过捕获数据包来分析VXLAN字段。在此之前，先让我解释一下我的实验室设置和虚拟机详情，以便我们可以验证这些输出是否与捕获的数据包详情相匹配。
- en: Lab environment details
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验室环境详情
- en: In this lab, I have two vSphere clusters, and we have a VXLAN network 5001,
    which is stretched across these two clusters. Both clusters have their own distributed
    switch.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验室中，我有两个vSphere集群，并且有一个跨越这两个集群的VXLAN网络5001。两个集群都有各自的分布式交换机。
- en: The virtual machine IP in cluster A is `192.16.10.12`, with a VTEP IP of `172.16.1.32`
    running on ESXi `172.16.1.97`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 集群A中的虚拟机IP是`192.16.10.12`，其VTEP IP为`172.16.1.32`，运行在ESXi `172.16.1.97`上。
- en: The virtual machine IP in cluster B is `192.16.10.14`, with a VTEP IP of `172.16.1.33`
    running on ESXi `172.16.1.94`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 集群B中的虚拟机IP是`192.16.10.14`，其VTEP IP为`172.16.1.33`，运行在ESXi `172.16.1.94`上。
- en: VNIC packet capturing for egress traffic
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VNIC数据包捕获用于出站流量
- en: 'To start the packet capturing, we need to take an SSH session to host `172.16.1.97`
    and identify on which VNIC the VM is running:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始数据包捕获，我们需要通过SSH连接到主机`172.16.1.97`，并确定虚拟机运行在哪个VNIC上：
- en: Issue the `ESXTOP` command and press the *n* key to show the network parameters.
    The following screenshot shows the ESXTOP screen. We have identified that our
    source virtual machine 192.16.10.12 is running on `vmnic0`:![VNIC packet capturing
    for egress traffic](img/image_08_018.jpg)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `ESXTOP` 命令并按 *n* 键显示网络参数。以下截图展示了 ESXTOP 屏幕。我们已经识别出我们的源虚拟机 192.16.10.12 正在
    `vmnic0` 上运行：![出站流量的 VNIC 数据包捕获](img/image_08_018.jpg)
- en: 'Issue the following command for egress traffic capturing. Output is saved in
    the ESXi host **tmp** directory:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令以捕获出站流量。输出将保存到 ESXi 主机的 **tmp** 目录中：
- en: '[PRE11]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Initiate a `ping` request from the source virtual machine to the destination
    virtual machine, as shown in the following screenshot:![VNIC packet capturing
    for egress traffic](img/image_08_019.jpg)
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从源虚拟机发起 `ping` 请求到目标虚拟机，如下图所示：![出站流量的 VNIC 数据包捕获](img/image_08_019.jpg)
- en: Stop the packet capture after some time by pressing *Ctrl *+ *C* in an ESXi
    putty session and see the following output:![VNIC packet capturing for egress
    traffic](img/image_08_020.jpg)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一段时间后，通过按 *Ctrl + C* 停止数据包捕获，在 ESXi putty 会话中看到以下输出：![出站流量的 VNIC 数据包捕获](img/image_08_020.jpg)
- en: The saved packet `webAvxlan.pcap` needs to be imported to the Wireshark tool.
    I know for sure that everyone knows how to copy or download a file from the ESXi
    host.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存的数据包 `webAvxlan.pcap` 需要导入到 Wireshark 工具中。我相信大家都知道如何从 ESXi 主机复制或下载文件。
- en: Once we import the file to Wireshark, we will get the following output:![VNIC
    packet capturing for egress traffic](img/image_08_021.jpg)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件导入到 Wireshark 后，我们将获得以下输出：![出站流量的 VNIC 数据包捕获](img/image_08_021.jpg)
- en: Expand the **Virtual eXentsible Local Area Network** option, which will display
    the complete VXLAN header field as shown in the following screenshot:![VNIC packet
    capturing for egress traffic](img/image_08_022.jpg)
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开 **虚拟可扩展局域网（Virtual eXentsible Local Area Network）** 选项，将显示完整的 VXLAN 头字段，如下图所示：![出站流量的
    VNIC 数据包捕获](img/image_08_022.jpg)
- en: 'As we can see from the highlighted field, we got the following output, which
    matches perfectly with the lab environment details that we mentioned earlier:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从高亮区域可以看出，我们得到了以下输出，完美匹配我们之前提到的实验环境详情：
- en: The inner IP `Src` is `192.16.10.12` and `Dst` is `192.16.10.14` (two machines
    which we have tested the ping command)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部 IP `Src` 是 `192.16.10.12`，`Dst` 是 `192.16.10.14`（我们已测试过的两个 ping 命令的机器）
- en: The VXLAN network is `5001` (the logical switch where our virtual machines are
    running)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN 网络是 `5001`（我们的虚拟机所在的逻辑交换机）
- en: The VXLAN UDP port is `4789`
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN UDP 端口是 `4789`
- en: 'The outer IP `SRC: 172.16.1.32` and `DST: 172.16.1.33` (VXLAN tunnel endpoint
    IP address)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '外部 IP `SRC: 172.16.1.32` 和 `DST: 172.16.1.33`（VXLAN 隧道端点 IP 地址）'
- en: I strongly believe this is the most precise VXLAN output, which will help us
    in many scenarios, and pktcap-uw is a great tool, which is not used by many people,
    primarily because of lack of knowledge. With the captured output imported to the
    **Wireshark** tool, it gives us granular-level information on all the fields.
    So, keep these steps handy and I bet this will be useful in a production environment.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信这是最精确的 VXLAN 输出，它将帮助我们在许多场景中，而 pktcap-uw 是一个伟大的工具，虽然很多人没有使用它，主要是因为缺乏相关知识。通过将捕获的输出导入
    **Wireshark** 工具，它能提供所有字段的细节级信息。所以，保持这些步骤在手，我敢打赌在生产环境中它将非常有用。
- en: NSX upgrade checklist and planning order
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 升级检查清单和规划顺序
- en: 'It is always better to stop something from happening in the first instance
    than spend time repairing the damage after it has happened. Every product upgrade
    should follow a step-by-step process with proper planning, and an NSX upgrade
    is no exception. The following steps are the proper order that needs to be followed
    while planning to upgrade the NSX environment:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在初期就阻止某些事情发生总比事后修复损坏要好。每次产品升级都应该遵循一步步的流程并进行适当的规划，NSX 升级也不例外。以下步骤是升级 NSX 环境时需要遵循的正确顺序：
- en: Upgrade NSX Manager
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级 NSX Manager
- en: Upgrade NSX Controllers
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级 NSX 控制器
- en: Upgrade ESXi cluster prepared by NSX
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级由 NSX 准备的 ESXi 集群
- en: Upgrade Distributed Logical Router and Edge service gateway
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级分布式逻辑路由器和边缘服务网关
- en: Upgrade data security and guest introspection services
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级数据安全性和来宾内省服务
- en: First and foremost, we need to ensure the following pre-checks are met before
    doing any upgrade.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在进行任何升级之前，我们需要确保满足以下前置检查条件。
- en: 'The following is an NSX pre-upgrade checklist:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 NSX 升级前检查清单：
- en: Take backup for NSX Manager. In a cross-VC NSX environment, we need to take
    backup from all the NSX Managers and complete the following process for all NSX
    Managers.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a snapshot of all NSX Managers. This is only for additional protection
    in case normal backups are not available or corrupted due to human error or catastrophic
    events.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log in to NSX Manager and run show filesystems to show the `/dev/sda2` filesystem
    usage. If the filesystem usage is 100 percent, the upgrade process will certainly
    fail, and for such cases, we need to purge manager logs and NSX system commands
    and reboot the NSX Manager before starting with the upgrade.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Issue the following commands in NSX Manager to purge NSX logs and system commands:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: purge log manager
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: purge log system
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reboot the NSX Manager appliance for the log cleanup to take effect.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX data security should be uninstalled before upgrading NSX Manager.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure all controllers are connected and don't plan to deploy any new controllers
    during the existing controller upgrade phase.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Place the vSphere cluster ESXi host in maintenance mode, perform the upgrade,
    and reboot the host. Continue the same operation for the NSX-prepared ESXi host,
    so that new VIBS are pushed to all ESXi hosts, and finally, take the host out
    of maintenance mode.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from NSX Manager 6.2.3 onwards the default VXLAN port is `4789`. Before
    NSX 6.2.3, the default VXLAN UDP port number was `8472`. So, if we are planning
    to continue with the new VXLAN port `4789`, please ensure that this port is allowed
    in your firewall.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Edge and NSX Distributed Logical Router control VM can be upgraded in any
    order, and there is no repeated upgrade process for HA-enabled NSX Edge and control
    VM. Both appliances get upgraded at the same time.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we can go ahead and upgrade the guest introspection virtual machine
    and respective partner appliance.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the upgrade is complete, we should delete all snapshots taken for NSX Manager,
    after verifying all components and services are up and running.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts all operationally-impacted tasks, and non-impacted
    tasks and services during respective NSX component upgrade phases:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '| **NSX Components** | **Operational impact** | **Not impacted** |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| **NSX Manager** | NSX Manager GUI and API-related new tasks are blocked |
    Control plane and data plane continue to work |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| **NSX Controllers** | No modification accepted for logical networks and no
    new logical network creation will be accepted | Management plane and data plane
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| **vSphere Cluster** | No new VM provisioning will be accepted during this
    phase for that specific vSphere cluster. | Management plane, control plane, and
    data plane related tasks for other vSphere clusters will be working (if we are
    doing upgrades on one vSphere cluster at a time) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| **NSX Edge and Control VM** | All Edge and control VM services will be impacted
    during this operation. | Management plane, new Edges and control VM can be deployed.
    All existing configuration on old edges will be retained. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| **NSX边缘和控制虚拟机** | 在此操作期间，所有边缘和控制虚拟机服务将受到影响。 | 管理平面、新的边缘和控制虚拟机可以部署。所有旧边缘的现有配置将被保留。
    |'
- en: '| **Guest introspection and data security** | Virtual machines will be unprotected
    during this phase | All other NSX related tasks and services will be intact. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| **访客自我检查和数据安全** | 在此阶段，虚拟机将没有保护 | 所有其他与NSX相关的任务和服务将保持完整。 |'
- en: That concludes our final chapter, and I believe this network virtualization
    journey has been fantastic so far.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们最后一章的内容，我相信这段网络虚拟化的旅程到目前为止非常精彩。
- en: The future of NSX
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX的未来
- en: 'The current and future choice of IT will certainly be Software Defined Data
    Center, no doubt about it. One of the primary reasons I believe this product and
    technology is the cherry on top of the VMware product portfolio is the proven
    and larger ecosystem that VMware has, and the fact that customers can leverage
    NSX in private, public (vCloud Air), and with the mix of both, a truly hybrid
    cloud platform. There are millions of workloads, which are protected by vCloud
    Air disaster recovery data center, and behind the scenes, these are vSphere environments
    that are fully network-virtualized with NSX. It''s no mistake that Gartner has
    recently recognized the vCloud Air disaster recovery solution as one of the best
    in the public Cloud market. The VMware NSX approach is very simple: *Follow the
    virtual machines wherever they go: private, public, or hybrid will always remain
    secured*. From a cross-vendor perspective, there are definitely some hard decisions
    taken by VMware, especially with products such as NEXUS 1000V, which is no longer
    supported with VMware NSX. However, in a greenfield deployment scenario, there
    is only one question that might be raised while looking for an NSX solution: what
    is the cost and manpower involved in developing or remodeling existing applications
    if NSX is getting integrated? Does it demand an overall change in networking?
    Does it demand any specific models of switches or routers? We all have the answers
    now: NSX doesn''t demand any significant change in overall physical networking.
    Already a recognized leader in software defined networking, VMware has made another
    wise move by acquiring Arkin on June 13, 2016, and that will further simplify
    a lot of NSX operational tasks. Arkin cross-domain visibility will help customers
    to get a granular visibility on physical to overlay mapping and other security
    parameters, and with that approach and output, day-to-day operational tasks will
    be much more simplified. How many times have we received questions about how a
    virtual machine is connected to the network? The traditional way of checking that
    would be by taking multiple connections to multiple products to get an end-to-end
    connectivity view. I strongly believe tasks like this will be simplified with
    the integration of Arkin with NSX, and adding to that, it will give us precise
    information on what types of traffic we have in the data center, as well as overall
    traffic percentages for East-West and North-South, Internet traffic, as a few
    examples.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: VMware vRealize network insight (Arkin) is an intelligent security and operations
    management solution for the network, which provides 360-degree visibility across
    virtual and physical networks using network flow analytics. It is available for
    download from 1^(st) August 2016\. There was also a recent announcement from VMware
    regarding a new release of NSX multi hypervisor called NSX transformers, which
    supports hypervisors such as KVM and vSphere, and one could simply club them under
    a common NSX transport zone. It's too early to comment on how transformers will
    evolve, so for the time being, it will be a watch and wait game for all of us.
    NSX is certainly an evolution in software-defined networking, and it has got all
    the bits and fragments needed to reach further heights, which will enormously
    help all types of businesses. I appreciate you all being part of this journey
    and I would encourage everyone to start testing and implementing this great solution.
    Let's be part of this game-changing software. Based on your technical background
    and the pace at which we can understand and learn a technology, I believe there
    will be a few questions, and I would appreciate if you all can reach out to me
    via LinkedIn. Rest assured I will ensure queries are addressed at the earliest
    opportunity.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with Introduction to troubleshooting followed by NSX
    manager,Controller and Data plane log collection and major focus points when things
    go wrong. Finally we ended this chapter with Future of NSX followed by few links
    for documentation reading.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lastly, as promised earlier, I''m posting all the articles that we all should
    read. You can trust me that the right way to climb the NSX ladder is by reading
    each and every document available; it will only multiply our knowledge:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The CISCO NEXUS 9000 design guide, which talks about a few design scenarios
    with UCS servers. A great guide for physical network design understanding with
    NSX: [http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf](http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced networking services offered through vCloud Air. It is worth reading
    this document to know how NSX is helping vCloud Air customers and what services
    are offered by providing zero trust security in the public Cloud: [http://vcloud.vmware.com/service-offering/advanced-networking-services](http://vcloud.vmware.com/service-offering/advanced-networking-services)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VMware NSX design guide for vSphere is another knowledge hub for designing
    NSX in a vSphere environment: [http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf](http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should all go through this blog if we really want to keep ourselves updated
    with the technology. This is a network virtualization blog from VMware, and there
    is good amount of videos and use cases discussed: [http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI](http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware integrated OpenStack with NSX configuration guide. This document demands
    a little bit of VIO knowledge; however, readers will find out how NSX works in
    the VIO world: [https://communities.vmware.com/docs/DOC-30985](https://communities.vmware.com/docs/DOC-30985)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete guide to upgrading VCNS to NSX. This guide contains step-by-step
    instructions to upgrade all vCloud network security solutions to NSX: [https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf](https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I'm going to shout practice, practice, practice! Well, practice makes perfect,
    so keep practicing the free VMware HOL labs. I would highly recommend starting
    with vSphere distributed switch from A to Z labs before starting NSX labs: [http://labs.hol.vmware.com/HOL/catalogs/catalog/130](http://labs.hol.vmware.com/HOL/catalogs/catalog/130)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The document is a deployment guide for implementing VMware NSX with Brocade
    VCS: [http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html](http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
