- en: Chapter 4. NSX Virtual Networks and Logical Router
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scalability and flexibility are impressive features of Network Virtualization.
    On any IP network, any Switches/Routers, any physical network design NSX works
    flawlessly. With the emerging interest in overlay networks, there are different
    encapsulation technologies currently in the market. VXLAN, NVGRE, LISP are few
    in that list. In this chapter we will discuss on logical networks and how NSX
    simplifies datacenter routing and switching. Following topics will be the key
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: NSX logical switches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX virtual network creation - multicast, unicast, and hybrid replication modes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX virtual network best practices and deployment considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX logical router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX logical routing and bridging best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX logical switches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using VMware NSX virtual networks, we can create a logical network on top of
    any IP network. We have already discussed VXLAN fundamentals and the host installation
    process in the previous chapters. Now that we have a good understanding of the
    basics, it's time to move on with virtual network creation. Before we begin exploring,
    it is important to understand that each logical network is a separate broadcast
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Logical network prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Firstly, let''s have a look at the prerequisites for logical network creation:'
  prefs: []
  type: TYPE_NORMAL
- en: Host preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment ID (VNI) pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global transport zone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already discussed in detail how the underlying ESXi host is prepared
    from NSX Manager in [Chapter 3](ch03.html "Chapter 3. NSX Manager Installation
    and Configuration") , *NSX Manager Installation and Configuration*. Here, it is
    time to recollect that knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The hypervisor kernel modules enable ESXi hosts to support VXLAN, the logical
    switch, the distributed router, and the distributed firewall.
  prefs: []
  type: TYPE_NORMAL
- en: Segment ID (VNI) pool
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we know, the VXLAN network identifier is a 24-bit address that gets added
    to the VXLAN frame, which allows us to isolate each VXLAN network from another
    VXLAN network.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to configure the VNI pool
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here are the steps to configure the VNI pool:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Logical Network Preparation** tab, click the **Segment ID** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Edit** to open the **Edit Segment IDs and Multicast Address Allocation** dialog
    box and configure the given options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **OK** as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Steps to configure the VNI pool](img/image_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this example, we are not using multicast networks; rather, we want to leverage
    unicast networks. One of the classic examples of multicast VXLAN networks would
    be when a customer has existing VXLAN networks (created when he was using vCloud
    network security) and the management software was upgraded to NSX. For such a
    scenario, people stick with multicast mode VXLAN networks in NSX. Also, please
    note that we don't even need a controller in that case. Do not use 239.0.0.0/24
    or 239.128.0.0/24 as the multicast address range, because these networks are used
    for local subnet control, meaning that the physical switches flood all traffic
    that uses these addresses. The complete list is documented at [https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01](https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Transport zone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A transport zone is a boundary for a VNI. All clusters in the same transport
    zone share the same VNI. A transport zone can contain multiple clusters and a
    cluster can be part of multiple transport zones or, in other words, a host can
    be part of multiple transport zones. In the following screenshot, we have three
    clusters: **Cluster A**, **Cluster B**, and **Cluster C**. **Cluster A** and **Cluster
    B** are part of **Transport Zone A** and **Cluster C** is part of **Transport
    Zone B**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Transport zone](img/image_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Configuring a global transport zone
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following steps will help you to configure the transport zone:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Logical Network Preparation** tab, click **Transport Zones**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the green plus sign to open the **New Transport Zone** dialog box and
    configure the following options and click **OK**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the transport zone name in the **Name** text box.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In **Control Plane Mode**, select the **Unicast**, **Multicast**, or **Hybrid**
    button.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In **Select clusters to add**, select the check box for each of the vSphere
    clusters listed. Also, have a look at the distributed switch selection highlighted
    in red. We are following one of the NSX-DVS design best practices. Both the compute
    clusters are running on **Compute_VDS** and the management cluster is on **Mgmt_Edge_VDS**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once it is updated, verify that the transport zone appears in the transport
    zone list, with a control plane mode set to unicast, multicast, or hybrid based
    on the preceding selection. Refer to the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Configuring a global transport zone](img/image_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have now completed all the prerequisites for NSX logical networks. In this
    example, we have the following virtual machines already created in vSphere without
    any network connectivity. We will go ahead and create four logical networks and
    will connect to virtual machines and later we will test connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two web servers**: web-sv-01a and web-sv-02a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One DB server**: DB-sv-01a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application server**: app-sv-01a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating logical switches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In an old-fashioned vSphere environment, ideally, virtual machines will be
    connected to a preconfigured vSphere PortGroup with or without VLAN tagging. But
    now we are in the NSX world and let''s leverage an NSX logical switch for virtual
    machine connectivity. In the left navigation pane, select **Logical Switches**
    and in the center pane, click the green plus sign to open the **New Logical Switch**
    dialog box. Perform the following actions to configure the logical switch:'
  prefs: []
  type: TYPE_NORMAL
- en: Enter `App-Tier` in the **Name** text box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the **Transport Zone** selection is **Transport**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the **Control Plane Mode** selection is **Unicast**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **OK**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Creating logical switches](img/image_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the two options shown on the **New Logical Switch** screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '****Enable IP Discovery**** : Another great feature, which minimizes ARP flooding
    in a VXLAN network. When a virtual machine sends an ARP packet, the switch security
    module - which is nothing but a dvfilter module attached to VNIC - will query
    the NSX Controller to know if they have the MAC entry for that destination IP.
    Everyone knows that we have two chances in that case:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The controller has the MAC entry
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The controller doesn't have the MAC entry
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first condition, since the controller has the MAC entry, it will respond
    with the MAC address and, that way, ARP traffic is reduced. In the second condition,
    the controller responds with no MAC reply and the ARP will flood in the normal
    way.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Enable MAC Learning**** : When we enable MAC learning a VLAN/MAC pair table
    is maintained on each vNIC which is used by dvfilter data. This table will be
    intact whenever VM migrates from one host to another host with the help of dvfilter
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wait for the update to complete and confirm app-network appears with the status
    set to **Normal**. Repeat Steps 1-4 and create three more logical switches, and
    name them **Web-Tier**, **DB-Tier**, and **Transit** networks. The successful
    creation of logical switches will show us the same results when populated under
    logical switches, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating logical switches](img/image_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the preceding steps, we see four port groups created in the vSphere networking
    option with their respective VNI-ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '**5000**: **Transit** network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5001**: **Web_Tier** network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5002**: **App_Tier** network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5003**: **DB_Tier** network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Creating logical switches](img/B03244_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding replication modes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s discuss replication modes in more detail and later we will connect the
    logical switches to virtual machines. With the addition of an NSX Controller,
    the requirement for multicast protocol support on physical networks is completely
    removed for VXLAN. There are three replication modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '****Multicast**** : When multicast replication mode is chosen for a given logical
    switch, NSX relies on the Layer 2 and Layer 3 multicast capability of the data
    center physical network to ensure VXLAN encapsulated multi-destination traffic
    is sent to all the VTEPs. This mode is recommended only when you are upgrading
    from older VXLAN deployments (vCloud network security). It requires PIM/IGMP on
    a physical network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Unicast**** : The control plane is handled by the NSX Controller. All unicast
    traffic leverages head end replication. No multicast IP address or special network
    configuration is required. In unicast mode, the ESXi hosts in the NSX domain are
    divided into separate VTEP segments based on the IP subnet their VTEP interfaces
    belong to. There will be a UTEP selection for each segment to play the role of
    **Unicast Tunnel End Point** (**UTEP**). The UTEP is responsible for replicating
    multi-destination traffic received from the ESXi hypervisor hosting the VM sourcing
    the traffic and belonging to a different VTEP segment from all the ESXi hosts
    part of its segment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Hybrid**** : The optimized unicast mode. Offloads local traffic replication
    to a physical network (L2 multicast). This requires IGMP snooping on the first-hop
    switch, but does not require PIM. The first-hop switch handles traffic replication
    for the subnet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will consider the following screenshot as a network topology and will explain
    all three modes of replication, which will give us a precise picture of how replication
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding replication modes](img/image_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Firstly, let''s understand the configuration. The preceding screenshot shows
    us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There are two transport zones in this set-up, ****Transport Zone A**** and ****Transport
    Zone B****
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ****Distributed Virtual Switch** (**DVS**)** is part of both the transport
    zones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the virtual machines are connected to one common VXLAN network - ****VXLAN
    5001****
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will walk through all the modes one by one and also discuss their design
    decisions in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Unicast mode packet walk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s discuss a unicast mode VXLAN packet walk:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VM-A** generated **Broadcast, Unknown Unicast, Multicast** (**BUM**) traffic,
    which is typically Layer 2 traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ESXi-A** will do a local VTEP lookup and will learn that the packet needs
    to be locally replicated (same subnet), in this case **ESXi-B**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to that, the packet will also get remotely replicated. Since we
    have four hosts in a remote subnet (ESXi E, F, G, and H), to which host will it
    send the packet? This is one key differentiator between multicast mode VXLAN and
    unicast. In unicast mode, the packet will be sent to a proxy module called **UTEP**
    with a replicate-locally bit set. Why is it like that? The answer is very simple:
    since there is a replicate-locally bit set, UTEP will replicate the packet locally
    to one of the ESXi hosts that are part of same subnet. In this example, we have
    two subnets; based on the network topology, the process will be same for every
    subnet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design decisions for unicast mode VXLAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is not much to be discussed about unicast mode VXLAN design. However,
    it is important to know the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: We could simply stick with traditional IP network design and just ensure that
    the MTU is increased to 1,600.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's go back and read the VXLAN packet walk in unicast mode; what does it do
    in a nutshell? It replicates the packet locally and sends one copy to the remote
    subnet and again replicates it locally. Who does that replication? The ESXi host
    does all this intelligent work and of course, based on how big the environment
    is or how often we have **BUM** traffic, it will create a slight overhead on the
    hypervisor. So I would suggest unicast mode as the best way to start using VXLAN;
    however, it is not a great candidate for large environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In environments where customers have multicast limitation, unicast mode VXLAN
    is best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicast mode packet walk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whenever I used to explain about multicast VXLAN networks, it made me recollect
    the VMware **vCloud Networking and Security** (**vCNS**) solution days. Multicast
    mode VXLAN was the starting stage of VXLAN implementation in both virtualized
    vSphere environments and cloud environments running on vCloud director software.
    The solution was very powerful; however, physical network prerequisites were one
    of the difficult factors for all architects because it really defeats the purpose
    of saying NSX can be run on any IP network. The bitter truth is that IP networking
    demands some requirements for the technology to work flawlessly. With that said,
    let''s start with a packet walk:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VM-A** generates **BUM** traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The ESXi A** host encapsulates the packet with a VXLAN header (5001). Time
    to start guessing who it will send the packet to. Will it simply be broadcast?
    The Layer 2 frame is a broadcast frame encapsulated with a VXLAN header; however,
    the host would be sending it to one of the multicast groups. How will we ensure
    the multicast reaches only **ESXi B**, **E**, **F**, **G**, and **H** since we
    have a virtual machine running on the same VXLAN network? This is where a physical
    network requirement is a must. We need IGMP snoop for that; if not, that would
    be treated as an unknown multicast packet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **router** will perform an L3 multicast and will send it to a Layer 2 switch
    and the switch will again check the multicast group and will send it to the right
    host. Eventually, the VM running on the destination host will receive the packet
    after getting de-encapsulated by the VTEP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design decisions for multicast mode VXLAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As I mentioned, we certainly need to take care of physical network prerequirements
    in multicast mode VXLAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IGMP snoop** and **IP multicasting** are required in the switch and router
    throughout the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, one VXLAN segment to one multicast group is the recommended way to
    provide optimal multicast forwarding, which also demands an increase in multicast
    groups if we have large segments. Something which I have seen in cloud environments,
    wherein VXLAN networks are created on-the-fly and the cloud provider ensures enough
    multicast IP is available for 1:1 mapping; that way, a packet forwarded to one
    tenant won't be seen by other tenants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid mode packet walk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hybrid mode VXLAN is recommended for most large environments, primarily because
    of the simplicity and limited configuration changes that are demanded in the network.
    Let''s have a look at a hybrid mode VXLAN packet walk:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual Machine A** generates **BUM** traffic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ESXi A** host encapsulates the **L2 header** with **VXLAN header 5001** and
    will send it to a physical switch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, an encapsulated L2 header will be send it to a multicast group
    which is defined on the physical switch. I hope it makes more sense now. The physical
    switch will deliver the packet to destination ESXi host part of that multicast
    group, in this case, ESXi B, C, D. In addition to that, ESXi A will send a `Locally_Replicate_BIT`
    set packet to a remote subnet. This packet will be received by a proxy module
    called **Multicast Tunnel End Point** (**MTEP**). Again, it is a straightforward
    answer, since there is a replicate locally bit set, MTEP (ESXi host) will replicate
    the packet locally to one of the ESXi host that are part of same subnet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MTEP will again send the packet to a physical switch and the physical switch
    will deliver the packet to all the host part of same multicast group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Design decisions for hybrid mode VXLAN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hybrid mode VXLAN being one of most widely used replication modes, I believe
    all of us will be interested to know the key design decisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IGMP snoop** is required to be configured on a physical switch throughout
    the VXLAN network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP multicast is not required in the physical router throughout the network.
    I'm not sure if I'm safe enough to say that, because replication modes can be
    selected per logical switch, which means we can deploy a logical switch in unicast,
    multicast, or hybrid mode. What if we are deploying logical switch A in multicast
    and logical switch B in hybrid mode in the same VXLAN domain? It demands IP multicasting
    in physical networking. But again, we don't need IP multicasting explicitly for
    hybrid mode VXLAN networking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is strongly recommended to define an **IGMP Querier** for each VLAN to ensure
    successful L2 multicast delivery and avoid non-deterministic behavior. In order
    for IGMP, and thus IGMP snooping, to function, a multicast router must exist on
    the network and generate IGMP queries. The tables created for snooping (holding
    the member ports for each multicast group) are associated with the querier. I
    believe we have a strong foundation in VXLAN and its replication modes; let's
    move on to the connectivity of logical switches and virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting virtual machines to logical switches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since we have already created logical switches, let''s go ahead and connect
    logical switches to the following virtual machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two web servers**: web-sv-01a and web-sv-02a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One DB server**: DB-sv-01a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application server**: app-sv-01a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us see how to connect the logical switches:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the **vSphere Web Client** home icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **vSphere Web Client** home tab, click **Inventories** | **Networking
    & Security**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left navigation pane, select **Logical Switches**. In the center pane,
    select the **App-Tier** logical switch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Add Virtual Machines** icon, or select **Add VM** from the **Actions**
    drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **App-Tier**,add virtual machines dialog box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the filter list, select the **app-01a** check boxes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Next**. In the **Select VNICs** list, select the **Network Adapter 1
    (VM Network)** check box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Next** as shown in the following screenshot:![Connecting virtual machines
    to logical switches](img/B03244_04_08-1024x516.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Finish**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat Steps 1-7 and connect both the web servers (web-sv-01a, web-sv-02a) and
    DB server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing connectivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we know that our three-tier application, web, app, and DB, is connected
    to logical switches, let''s do some basic testing to confirm their connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, go ahead and power on those machines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **vSphere Web Client** home icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **vSphere Web Client** home tab, click the **Inventories** | **VMs and
    Templates** icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expand the VMs and templates inventory tree and power on each of the following
    virtual machines found in the discovered virtual machine folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: web-sv-01a
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: web-sv-02a
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: app-sv-01a
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: db-sv-01a
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Testing connectivity](img/image_04_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: To power on a virtual machine, select the virtual machine in the inventory,
    then select **Power On** from the **Actions** drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the machines are powered on, we will go ahead and record their IP address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`web-01a`: `172.16.10.11`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`web-02a`: `172.16.10.12`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`app-01a`: `172.16.20.11`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DB-01a`: `172.16.30.11`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We now do a simple ping test between `web-01a` and `app-01a` as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing connectivity](img/image_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Why do we have 100% packet loss when we ping `web-01a (172.16.10.11)` and `app-01a(172.16.20.11)`?
    Will a logical switch perform a Layer 3 routing? Definitely not. The traditional
    way of performing routing for such networks would be through a physical router,
    which is nothing but going all the way out of the rack, gets it routed to the
    right destination. Let's not do that legacy routing in this case, we would leverage
    NSX logical router capability.
  prefs: []
  type: TYPE_NORMAL
- en: The Distributed Logical Router
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The whole purpose of routing is to process the packets between two different
    IP networks. Let''s discuss the fundamentals of routing before getting into logical
    routers. Every router will build a routing table, which will have information
    about **destination network**, **next hop router**, **metrics, and administrative
    distance**. There are two methods of building a routing table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static routing**: Static routing is manually created and updated by a network
    administrator. Based on the network topology, we will be in need of configuring
    a static route on each and every router for end-to-end network connectivity. Even
    though this gives full control over the routing, it would be an extremely tedious
    job to configure routes on a large network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic routing**: Dynamic routing is created and updated by a routing protocol
    running on a router; **Routing Information Protocol** (**RIP**) and **Open Shortest
    Path First** (**OSPF**) are some examples. Dynamic routing protocols are intelligent
    enough to choose a better path whenever there is a change in routing infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The VMware NSX Distributed Logical Router supports static routes, OSPF, ISIS,
    and BGP routing protocols. It is important to know that dynamic routing protocols
    are supported only on external interface (uplink) of the **Distributed Logical
    Router** (**DLR**). The DLR allows an ESXi hypervisor to locally do routing intelligence,
    through which we can optimize East-West data plane traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Distributed Logical Router
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A DLR is a virtual appliance which has the control plane intelligence and it
    relies on NSX Controllers to push the routing updates to the ESXi kernel modules.
  prefs: []
  type: TYPE_NORMAL
- en: Procedure for deploying a logical router
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s walk through the step-by-step configuration of a Distributed Logical
    Router:'
  prefs: []
  type: TYPE_NORMAL
- en: In the vSphere web client, navigate to **Home** | **Networking & Security**
    | **NSX Edges**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Select the appropriate NSX Manager on which to make your changes. If you are
    creating a universal logical router, you must select the primary NSX Manager.
    We will be discussing about Primary/Secondary NSX manager concepts in  [Chapter
    7](ch07.html "Chapter 7. NSX Cross vCenter") , *NSX Cross vCenter*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the type of router you wish to add; in this case, we would add **logical
    router**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Logical (Distributed) Router** to add a logical router local to the
    selected NSX Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we haven't discussed the cross-vCenter NSX environment, we won't leverage
    a universal logical distributed router in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Type a name for the device. This name appears in your **vCenter inventory**.
    The name should be unique across all logical routers within a single tenant. Optionally,
    you can also enter a hostname. This name appears in the CLI. If you do not specify
    the hostname, the edge ID, which gets created automatically, is displayed in the
    CLI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Deploy Edge Appliance** option is selected by default. An edge appliance
    (also called a logical router virtual appliance) is required for dynamic routing
    and the logical router appliance's firewall, which applies to logical router pings,
    SSH access, and dynamic routing traffic. You can deselect the **Deploy Edge Appliance**
    option if you require only static routes, and do not want to deploy an edge appliance.
    You cannot add an edge appliance to the logical router after the logical router
    has been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Enable High Availability** option is not selected by default. Select
    the **Enable High Availability** check box to enable and configure high availability.
    High Availability is required if you are planning to do dynamic routing. I want
    everyone to think from a cloud provider perspective: if your tenant is requesting
    the High Availability feature, how do you satisfy that requirement? NSX Edge replicates
    the configuration of the primary appliance for the standby appliance and ensures
    that the two HA NSX Edge virtual machines are not on the same ESXi host even after
    you use DRS and vMotion.  Two virtual machines are deployed on vCenter in the
    same resource pool and data store as the appliance you configured. Local link
    IPs are assigned to HA virtual machines in the NSX Edge HA so that they can communicate
    with each other. But remember that instead of one control VM, we have two control
    VMs running now so definitely it will consume twice the compute resource.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows NSX DLR-VM deployment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/image_04_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Type and retype a password for the logical router. The password must be 12-255
    characters and must contain the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At least one uppercase letter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At least one lowercase letter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At least one number
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: At least one special character
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable **SSH** and set the log level (optional). By default, **SSH** is disabled.
    If you do not enable SSH, you can still access the logical router by opening the
    virtual appliance console. Enabling SSH here causes the SSH process to run on
    the logical router virtual appliance, but you will also need to adjust the logical
    router firewall configuration manually to allow SSH access to the logical router's
    protocol address. The protocol address is configured when you configure dynamic
    routing on the logical router. By default, the log level is emergency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: On logical routers, only IPv4 addressing is supported.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Configure the interfaces. Under Configure interfaces, add four **logical interfaces**
    (**LIFs**) to the logical router:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uplink connected to **Transit-Network-01** logical switch with an IP of 192.168.10.2/29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal connected to **Web-Tier-01 Logical Switch** with IP 172.16.10.1/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal connected to **App-Tier-01 Logical Switch** with IP 172.16.20.1/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal connected to **DB-Tier-01 Logical Switch** with IP 172.16.30.1/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the **Add Interface** screen:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/image_04_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Configure interfaces of this NSX Edge: Internal interfaces are for connections
    to logical switches that allow VM-to-VM (East-West) communication. Internal interfaces
    are created on the logical router virtual appliance and we call them LIF. Uplink
    interfaces are for North-South communication. A logical router uplink interface
    can be connected to an NSX Edge services gateway, third-party router VM, or a
    VLAN-backed dvPortgroup to make the logical router connection to a physical router
    directly. You must have at least one uplink interface for dynamic routing to work.
    Uplink interfaces are created as vNICs on the logical router virtual appliance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We can add, remove, and modify interfaces after a logical router is deployed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the DLR configuration that we have performed
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/image_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have successfully deployed a DLR and configured with logical interfaces,
    we would expect the DLR to perform basic routing functionality for web, app, and
    DB machines to communicate with each other, which was not possible earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the three-tier application architecture without
    routing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/image_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go ahead and perform a quick ping test between `web-01a (172.16.10.11)`
    and `app (172.16.20.11)`. As we can see from the following screenshot, web servers
    and application servers are able to communicate each other since we have a Distributer
    Logical Router, which does the routing in this case. The first ping result is
    before adding the Distributed Logical Router:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/image_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have discussed the **Distributed Logical Router** (**DLR**), which
    allows ESXi hypervisor to locally do routing intelligence through which we can
    optimize East-West data plane traffic. But I know we are very keen to view the
    DLR routing table in an ESXi host. Let's focus on the following screenshot to
    know the network topology.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the three-tier application architecture with
    DLR connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/B03244_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following questions might come to our mind:'
  prefs: []
  type: TYPE_NORMAL
- en: How many networks do we have?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 172.16.10.0/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 172.16.20.0/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 172.16.30.0/24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 192.168.10.0/29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are the networks directly connected to the router?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, they are connected to the router.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will go ahead and SSH to one of the ESXi hosts to check the logical router
    instance, MAC, ARP, and routing tables, which will certainly give granular-level
    details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will display the logical router instance as shown in
    the following screenshot. You can see the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VDR Name` is `default+edge-19`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of Lifs` is `4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember we connected four logical networks to the distributed router? Hence
    the count is `4`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of Routes` is `4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we have connected four logical networks, the router is aware of those
    directly connected networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/B03244_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following command will verify the network routes discovered by DLR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The logical router routing table is pushed by the NSX Controller to the ESXi
    host and it will be consistent across all the ESXi hosts. You will see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/B03244_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now log in to the controller CLI to view the logical router state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/B03244_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The other command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'All four logical switches (VXLAN 5000, 5001, 5002, and 5003, which we have
    connected to the logical router) are displaying in the following output with their
    respective interface IP, which would be the default gateway for web, app, and
    DB machines. Again, the idea here is to showcase the power of NSX CLI commands,
    which give granular-level information and are extremely useful when troubleshooting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Procedure for deploying a logical router](img/B03244_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding logical interfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I''m pretty sure we now have a good understanding of how distributed routing
    works in the NSX environment. Again, NSX DLR is not limited between VXLAN networks;
    we can certainly leverage the routing functionality between VXLAN and VLAN networks.
    Let''s discuss logical interfaces in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: If the Distributed Logical Router connects to a vSphere distributed switch port
    group, the interface is called a **VLAN LIF**. VLAN LIFs make use of **Designated
    Instance** (**DI**) for resolving ARP queries. The NSX Controller randomly selects
    one of the ESXi hosts as the designated instance to ease the ARP traffic so that
    any ARP traffic for that subnet will be handled by one of the ESXi hosts and every
    other ESXi host is also aware of where DI is running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Distributed Logical Router connects to a logical switch, the interface
    is called a **VXLAN LIF**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A LIF can either be an uplink or internal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple LIFs can be configured on one Distributed Logical Router instance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ARP table is maintained for each LIF.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each LIF has assigned an IP address representing the default IP gateway for
    the logical network it connects to and a vMAC address. The IP address is unique
    for each LIF, whereas the same virtual MAC is assigned to all the defined LIFs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can configure up to 999 interfaces, with a maximum of eight uplinks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The routing table can be populated in multiple ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly connected
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Static routes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: OSPF
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: BGP
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Route redistribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss dynamic routing and route redistribution during [Chapter 5](ch05.html
    "Chapter 5.  NSX Edge Services") , *NSX Edge Services*, which will give us a clear
    view on how tenants access public networks (North-South connectivity).
  prefs: []
  type: TYPE_NORMAL
- en: Logical router deployment considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Distributed Logical Router** (**DLR**) deployment is highly critical in NSX
    environments. Let''s check a few critical decision factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that controllers are up-and-running before deploying a logical router.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't deploy a logical router during controller deployment. This is not limited
    to DLR deployments; it applies for all NSX features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a logical router is to be connected to VLAN dvPortgroups, ensure that all
    hypervisor hosts with a logical router appliance installed can reach each other
    on UDP port 6999 for logical router VLAN-based ARP proxy to work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logical router interfaces should not be created on two different distributed
    port groups (dvPortgroups) with the same VLAN ID if the two networks are in the
    same vSphere distributed switch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with VMware NSX for vSphere 6.2, the L2 bridging feature can now participate
    in distributed logical routing. The VXLAN network to which the bridge instance
    is connected will be used to connect the routing instance and the bridge instance.
    This was unsupported in earlier releases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLR interfaces don't support trunking; however, each DLR interface can be connected
    to NSX Edge sub-interfaces which support trunking. But we are limited with leveraging
    IP-Sec, L2-VPN, BGP (dynamic routing), DHCP, and DNAT features on sub-interfaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,000 logical interfaces are supported on DLR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DLR doesn't support **virtual routing and forwarding** (**VRF**). For true network
    multitenancy, we need to deploy a unique DLR which can be connected to the same
    or different NSX Edges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equal Cost Multi Path** (**ECMP**) is supported in DLR; however, state full
    firewalls are not supported, primarily because of asymmetric routing behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: We will discuss ECMP and asymmetric routing in the next chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The DLR control VM should not deployed in the compute cluster. If the host fails,
    both the data plane and control plane will be impacted at the same time if the
    control VM is also residing on same ESXi host. So the right place to deploy the
    DLR control VM will be either on the management cluster or if we have a separate
    vSphere Edge cluster, that is the best option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 2 bridges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logical network to physical network access might be required due to multiple
    reasons in a NSX environment:'
  prefs: []
  type: TYPE_NORMAL
- en: During **Physical to Virtual** (**P2V**) migrations where changing IP addresses
    is not an option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending virtual services in the logical switch to external devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending physical network services to virtual machines in logical switches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing existing physical network and security resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since Layer 2 bridging is a NSX Edge Distributed Logical Router functionality,
    the L2 bridge runs on the same host on which the edge logical router control virtual
    machine is running. Bridging is entirely done at kernel level, as it was for Distributed
    Logical Routing. A special dvPort type called a **sink port** is used to steer
    packets to the bridge. In the following screenshot, we have a VXLAN environment
    wherein virtual machines in VXLAN network 5006 need to communicate with a physical
    site, which is in VLAN-100:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Layer 2 bridges](img/B03244_04_21-1024x617.jpg)'
  prefs: []
  type: TYPE_IMG
- en: NSX Layer 2 bridging
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an L2 bridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s have a look at the deployment of a Layer 2 bridge:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the vSphere Web Client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Networking & Security** and then click **NSX Edges**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click a logical router.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Manage** and then click **Bridging**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the add icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type a name for the bridge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the logical switch that you want to create a bridge for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the distributed virtual port group to which you want to bridge the logical
    switch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following example, we are bridging logical switch **bRANCH** with **Mgmt_Edge_VDS**
    port group which is VLAN enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying an L2 bridge](img/B03244_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Design considerations for the L2 bridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like any other NSX components, L2 bridging is an equally important design decision
    factor. The following are the key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Bridging VLAN-ID 0 is not supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple bridges are supported per logical router; however, we cannot have more
    than one bridge instance active per VXLAN-VLAN pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridging cannot be used for VLAN-VLAN connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridging is not a data center interconnect technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting from NSX 6.2, DLR interfaces can be connected to a VXLAN network that
    is bridged with a VLAN network. Earlier versions don't have this feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don't mix a DLR and a next hop router (NSX Edge) on the same host; host failure
    will have a direct impact on both the devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though Layer 2 bridging is a great feature, remember that all ARP resolutions
    are done explicitly by a bridge instance module running on the same host wherein
    we have deployed the logical router. For the same reason, running too many bridge
    instances and that too if they all are on the same host in addition to that if
    we have UTEPs and MTEPs running on the same host there would be certainly a performance
    impact. So try to run bridge instances on separate hosts as much as we can or,
    in another words, distribute logical router deployment across the management cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with an introduction to the NSX Distributed Logical
    Router and discussed VXLAN replication modes and a few packet walks. Later, we
    covered a few key design decisions while deploying a DLR. We also discussed the
    Layer 2 bridging feature of DLRs and we moved on to important design decisions
    that need to be noted while leveraging bridging functionality.
  prefs: []
  type: TYPE_NORMAL
- en: There are exciting times ahead as we discuss more and more features and their
    functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss NSX Edge routing and we will establish
    connectivity with the DLR with a dynamic routing protocol.
  prefs: []
  type: TYPE_NORMAL
