<html><head></head><body>
<p id="filepos660330" class="calibre_"><span class="calibre1"><span class="bold">Chapter 9. OpenStack Integration with Containers and CoreOS</span></span></p><p class="calibre_8">OpenStack is<a/> an open source cloud operating system for managing public and private clouds. It is a pretty mature technology that is supported by the majority of the vendors and is used in a wide variety of production deployments. Running CoreOS in the OpenStack environment will give OpenStack users a Container-based Micro OS to deploy their distributed applications. Having Container orchestration integrated with OpenStack gives OpenStack users a single management solution to manage VMs and Containers. There are multiple projects ongoing in OpenStack currently to integrate Container management and Container networking with OpenStack.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">An overview of OpenStack</li><li value="2" class="calibre_13">Running CoreOS in OpenStack</li><li value="3" class="calibre_13">Options to run Containers in OpenStack—the Nova Docker driver, Heat Docker plugin, and Magnum</li><li value="4" class="calibre_13">Container networking using OpenStack Kuryr and Neutron</li></ul><div class="mbp_pagebreak" id="calibre_pb_197"/>


<p id="filepos661772" class="calibre_14"><span class="calibre1"><span class="bold">An overview of OpenStack</span></span></p><p class="calibre_8">Just like an <a/>OS for a desktop or server manages the resources associated with it, a cloud OS manages the resources associated with the cloud. Major cloud resources are compute, storage, and network. Compute includes servers and hypervisors associated with the servers that allows VM creation. Storage includes the local storage, <span class="bold">Storage Area Network</span> (<span class="bold">SAN</span>), and <a/>object storage. </p><p class="calibre_8">Network includes vlans, firewalls, load balancers, and routers. A cloud OS is also responsible for other infrastructure-related items such as image management, authentication, security, billing, and so on. A cloud OS also provides some automated characteristics such as elasticity, a self service provisioning model, and others. Currently, the most popular open source cloud OS in the market is OpenStack. OpenStack has a lot of momentum going <a/>for it along with a great industry backing.</p><p class="calibre_8">The following are some key OpenStack <a/>services:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Nova: Compute</li><li value="2" class="calibre_13">Swift: Object storage</li><li value="3" class="calibre_13">Cinder: Block storage</li><li value="4" class="calibre_13">Neutron: Networking</li><li value="5" class="calibre_13">Glance: Image management</li><li value="6" class="calibre_13">Keystone: Authentication</li><li value="7" class="calibre_13">Heat: Orchestration</li><li value="8" class="calibre_13">Ceilometer: Metering</li><li value="9" class="calibre_13">Horizon: Web interface</li></ul><p class="calibre_8">OpenStack<a/> can be downloaded from <a href="https://wiki.openstack.org/wiki/Get_OpenStack">https://wiki.openstack.org/wiki/Get_OpenStack</a>. It is pretty complex to install OpenStack as there are multiple components involved. Similar to Linux distributions provided by Linux vendors, there are multiple vendors offering OpenStack distributions. The best way to try out OpenStack is using <a/>Devstack (<a href="http://devstack.org/">http://devstack.org/</a>). Devstack offers a scripted approach to install and can be installed on a laptop or VM. Devstack can be used to create a single-node cluster or multi-node cluster.</p><div class="mbp_pagebreak" id="calibre_pb_198"/>


<p id="filepos664403" class="calibre_"><span class="calibre1"><span class="bold">CoreOS on OpenStack</span></span></p><p class="calibre_8">CoreOS <a/>can be run as a VM on OpenStack. CoreOS OpenStack images are available for alpha, beta, and stable versions.</p><p class="calibre_8">Here, I have described the procedure to install CoreOS on OpenStack running in the Devstack environment. The procedure is based on the CoreOS OpenStack documentation (<a href="https://coreos.com/os/docs/latest/booting-on-openstack.html">https://coreos.com/os/docs/latest/booting-on-openstack.html</a>).</p><p class="calibre_8">The following is a summary of the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Get OpenStack Kilo running in Devstack. In my case, I installed Devstack in the Ubuntu 14.04 VM.</li><li value="2" class="calibre_13">Set up the keys for authentication and a security group for SSH access.</li><li value="3" class="calibre_13">Set up external network access and DNS for the VM. This is necessary as the CoreOS nodes need to discover each other using the token service.</li><li value="4" class="calibre_13">Download the appropriate CoreOS image and upload to OpenStack using the Glance service.</li><li value="5" class="calibre_13">Get a discovery token and update it in the user data configuration file.</li><a/><li value="6" class="calibre_13">Start CoreOS instances using custom user data specifying necessary services to be started and the number of instances to be started.</li></ol><div class="mbp_pagebreak" id="calibre_pb_199"/>


<p id="filepos666132" class="calibre_14"><span class="calibre3"><span class="bold">Get OpenStack Kilo running in Devstack</span></span></p><p class="calibre_8">The following blog covers the procedure in detail:</p><p class="calibre_8"><a href="https://sreeninet.wordpress.com/2015/02/21/openstack-juno-install-using-devstack/">https://sreeninet.wordpress.com/2015/02/21/openstack-juno-install-using-devstack/</a>
</p><p class="calibre_8">This is the <tt class="calibre2">local.conf</tt> file that I used:</p><p class="calibre_8"><tt class="calibre2">[[local|localrc]]<br class="calibre4"/>DEST=/opt/stack<br class="calibre4"/><br class="calibre4"/># Logging<br class="calibre4"/>LOGFILE=$DEST/logs/stack.sh.log<br class="calibre4"/>VERBOSE=True<br class="calibre4"/>SCREEN_LOGDIR=$DEST/logs/screen<br class="calibre4"/>OFFLINE=True<br class="calibre4"/><br class="calibre4"/># HOST<br class="calibre4"/>#EDITME<br class="calibre4"/>HOST_IP=&lt;EDITME&gt;<br class="calibre4"/><br class="calibre4"/># Networking<br class="calibre4"/>FIXED_RANGE=10.0.0.0/24<br class="calibre4"/>disable_service n-net<br class="calibre4"/>enable_service q-svc<br class="calibre4"/>enable_service q-agt<br class="calibre4"/>enable_service q-dhcp<br class="calibre4"/>enable_service q-meta<br class="calibre4"/>enable_service q-l3<br class="calibre4"/>#ml2<br class="calibre4"/>Q_PLUGIN=ml2<br class="calibre4"/>Q_AGENT=openvswitch<br class="calibre4"/># vxlan<br class="calibre4"/>Q_ML2_TENANT_NETWORK_TYPE=vxlan<br class="calibre4"/><br class="calibre4"/># Credentials<br class="calibre4"/>ADMIN_PASSWORD=openstack<br class="calibre4"/>MYSQL_PASSWORD=openstack<br class="calibre4"/>RABBIT_PASSWORD=openstack<br class="calibre4"/>SERVICE_PASSWORD=openstack<br class="calibre4"/>SERVICE_TOKEN=tokentoken<br class="calibre4"/><br class="calibre4"/>#scheduler<br class="calibre4"/>enable_service n-sch<br class="calibre4"/>SCHEDULER=nova.scheduler.chance.ChanceScheduler<br class="calibre4"/><br class="calibre4"/>#vnc<br class="calibre4"/>enable_service n-novnc<br class="calibre4"/>enable_service n-cauth</tt></p><div class="mbp_pagebreak" id="calibre_pb_200"/>


<p id="filepos667768" class="calibre_9"><span class="calibre3"><span class="bold">Setting up keys and a security group</span></span></p><p class="calibre_8">The following <a/>are the commands that I used to create a keypair and to expose port SSH and ICMP port of the VM:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">nova keypair-add heattest &gt; ~/Downloads/heattest.pem</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">nova secgroup-add-rule default tcp 1 65535 0.0.0.0/0</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><div class="mbp_pagebreak" id="calibre_pb_201"/>


<p id="filepos668410" class="calibre_9"><span class="calibre3"><span class="bold">Setting up external network access</span></span></p><p class="calibre_8">The first command sets up the NAT rule for VM external access and the second command sets up a DNS server:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">neutron subnet-update  &lt;subnet&gt; --dns-nameservers list=true &lt;dns address&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">(Find <tt class="calibre2">&lt;subnet&gt;</tt> using <tt class="calibre2">nova subnet-list</tt> and <tt class="calibre2">&lt;dns address&gt;</tt> from the running host machine).</p><div class="mbp_pagebreak" id="calibre_pb_202"/>


<p id="filepos669160" class="calibre_9"><span class="calibre3"><span class="bold">Download the CoreOS image and upload to Glance</span></span></p><p class="calibre_8">The following command is used to download the latest alpha image and upload to OpenStack glance:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">wget http://alpha.release.core-os.net/amd64-usr/current/coreos_production_openstack_image.img.bz2</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">bunzip2 coreos_production_openstack_image.img.bz2</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">glance image-create --name CoreOS \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  --container-format bare \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  --disk-format qcow2 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  --file coreos_production_openstack_image.img \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  --is-public True</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following<a/> is the <tt class="calibre2">glance image-list</tt> output and we can see the CoreOS image uploaded to Glance:</p><p class="calibre_9"><img src="images/00231.jpg" class="calibre_371"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_203"/>


<p id="filepos670391" class="calibre_9"><span class="calibre3"><span class="bold">Updating the user data to be used for CoreOS</span></span></p><p class="calibre_8">I had some issues using the default user data to start CoreOS because there were issues with CoreOS determining the system IP. I raised a case (<a href="https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4">https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4</a>) and the CoreOS team provided a sample user data where IP addresses are determined using a script inside the user data.</p><p class="calibre_8">The following is the user data that I used:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/><br class="calibre4"/>write_files:<br class="calibre4"/>  - path: /tmp/ip.sh<br class="calibre4"/>    permissions: 0755<br class="calibre4"/>    content: |<br class="calibre4"/>      #!/bin/sh<br class="calibre4"/>      get_ipv4() {<br class="calibre4"/>          IFACE="${1}"<br class="calibre4"/><br class="calibre4"/>          local ip<br class="calibre4"/>          while [ -z "${ip}" ]; do<br class="calibre4"/>              ip=$(ip -4 -o addr show dev "${IFACE}" scope global | gawk '{split ($4, out, "/"); print out[1]}')<br class="calibre4"/>              sleep .1<br class="calibre4"/>          done<br class="calibre4"/><br class="calibre4"/>          echo "${ip}"<br class="calibre4"/>      }<br class="calibre4"/>      echo "IPV4_PUBLIC=$(get_ipv4 eth0)" &gt; /run/metadata<br class="calibre4"/>      echo "IPV4_PRIVATE=$(get_ipv4 eth0)" &gt;&gt; /run/metadata<br class="calibre4"/><br class="calibre4"/>coreos:<br class="calibre4"/>  units:<br class="calibre4"/>    - name: populate-ips.service<br class="calibre4"/>      command: start<br class="calibre4"/>      runtime: true<br class="calibre4"/>      content: |<br class="calibre4"/>        [Service]<br class="calibre4"/>        Type=oneshot<br class="calibre4"/>        ExecStart=/tmp/ip.sh<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>      runtime: true<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: custom.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Unit]<br class="calibre4"/>            Requires=populate-ips.service<br class="calibre4"/>            After=populate-ips.service<br class="calibre4"/><br class="calibre4"/>            [Service]<br class="calibre4"/>            EnvironmentFile=/run/metadata<br class="calibre4"/>            ExecStart=<br class="calibre4"/>            ExecStart=/usr/bin/etcd2 --initial-advertise-peer-urls=http://${IPV4_PRIVATE}:2380 --listen-peer-urls=http://${IPV4_PRIVATE}:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://${IPV4_PUBLIC}:2379 --discovery=https://discovery.etcd.io/0cbf57ced1c56ac028af8ce7e32264ba<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The preceding user <a/>data does the following:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The <tt class="calibre2">populate-ips.service</tt> unit file is used to update the IP address. It reads the IP manually and updates <tt class="calibre2">/run/metadata</tt> with the IP address.</li><li value="2" class="calibre_13">The discovery token is updated so that nodes can discover each other.</li><li value="3" class="calibre_13">Etcd2 service is started using the IP address set in <tt class="calibre2">/run/metadata</tt>.</li><li value="4" class="calibre_13">Fleet service is started using fleet unit file.</li></ul><p class="calibre_8">The following <a/>command is used to start two CoreOS instances using the preceding user data:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">nova boot \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--user-data ./user-data1.yaml \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--image 8ae5223c-1742-47bf-9bb3-873374e61a64 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--key-name heattest \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--flavor m1.coreos \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--num-instances 2 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">--security-groups default coreos</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: For the CoreOS instance, I have used a custom flavor <tt class="calibre2">m1.coreos</tt> with 1 vcpu, 2 GB memory, and 10 GB hard disk. If these resource requirements are not met, instance creation will fail.</p><p class="calibre_8">Let's look at the list of VMs. We can see the two CoreOS instances in the following image:</p><p class="calibre_9"><img src="images/00232.jpg" class="calibre_71"/></p><p class="calibre_8">
</p><p class="calibre_8">The following command shows the CoreOS version running in OpenStack:</p><p class="calibre_9"><img src="images/00234.jpg" class="calibre_372"/></p><p class="calibre_8">
</p><p class="calibre_8">The following command shows the etcd member list:</p><p class="calibre_9"><img src="images/00235.jpg" class="calibre_373"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>command shows the fleet machines showing the two CoreOS nodes:</p><p class="calibre_9"><img src="images/00237.jpg" class="calibre_374"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_204"/>


<p id="filepos675994" class="calibre_"><span class="calibre1"><span class="bold">OpenStack and Containers</span></span></p><p class="calibre_8">Even though <a/>OpenStack has supported VMs and baremetal for quite some time, Containers are pretty new to OpenStack. The initial focus in OpenStack was to extend VM Orchestration to also manage Containers. The Nova Docker driver and Heat Docker plugin are examples of this. This was not widely adopted as some of the Container functionality was missing in this approach. The OpenStack Magnum project addresses some of the limitations and manages Containers as a first-class citizen like a VM.</p><div class="mbp_pagebreak" id="calibre_pb_205"/>


<p id="filepos676665" class="calibre_9"><span class="calibre3"><span class="bold">The Nova Docker driver</span></span></p><p class="calibre_8">Nova<a/> typically manages VMs. In this approach, the Nova driver is extended to spawn Docker Containers.</p><p class="calibre_8">The following diagram describes the <a/>architecture:</p><p class="calibre_9"><img src="images/00238.jpg" class="calibre_375"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>are some notes on the architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Nova is configured to use the Nova Docker driver for Containers</li><li value="2" class="calibre_13">The Nova Docker driver talks to the Docker daemon using the REST API</li><li value="3" class="calibre_13">Docker images are imported to Glance and the Nova Docker driver uses these images to spawn Containers</li></ul><p class="calibre_8">The Nova Docker driver is not present in the mainstream OpenStack installation and has to be installed separately.</p><p id="filepos677897" class="calibre_9"><span class="calibre3"><span class="bold">Installing the Nova Driver</span></span></p><p class="calibre_8">In the following <a/>example, we will cover the installation and usage of the Nova Docker driver to create Containers.</p><p class="calibre_8">The following is a summary of the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">You need to have a Ubuntu 14.04 VM.</li><li value="2" class="calibre_13">Install Docker.</li><li value="3" class="calibre_13">Install the Nova docker plugin.</li><li value="4" class="calibre_13">Do the stacking of Devstack.</li><li value="5" class="calibre_13">Install nova-docker rootwrap filters.</li><li value="6" class="calibre_13">Create Docker images and export to Glance.</li><li value="7" class="calibre_13">Spawn Docker containers from Nova.</li><a/></ol><div class="mbp_pagebreak" id="calibre_pb_206"/>


<p id="filepos678965" class="calibre_14"><span class="calibre3"><span class="bold">Installing Docker</span></span></p><p class="calibre_8">The following<a/> is the Docker version running in my system after the Docker installation:</p><p class="calibre_9"><img src="images/00240.jpg" class="calibre_376"/></p><p class="calibre_8">
</p><p id="filepos679355" class="calibre_9"><span class="calibre3"><span class="bold">Install the Nova Docker plugin</span></span></p><p class="calibre_8">Use the following command to install the plugin:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">git clone -b stable/kilo https://github.com/stackforge/nova-docker.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">cd nova-docker</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo pip install .</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the Docker driver version after installation:</p><p class="calibre_9"><img src="images/00241.jpg" class="calibre_377"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_207"/>


<p id="filepos680125" class="calibre_9"><span class="calibre3"><span class="bold">The Devstack installation</span></span></p><p class="calibre_8">I have used a <a/>stable Kilo release with the following <tt class="calibre2">local.conf</tt>. This sets up Nova to use the Docker driver:</p><p class="calibre_8"><tt class="calibre2">[[local|localrc]]<br class="calibre4"/># HOST<br class="calibre4"/>HOST_IP=&lt;EDITME&gt;<br class="calibre4"/><br class="calibre4"/>ADMIN_PASSWORD=openstack<br class="calibre4"/>DATABASE_PASSWORD=$ADMIN_PASSWORD<br class="calibre4"/>RABBIT_PASSWORD=$ADMIN_PASSWORD<br class="calibre4"/>SERVICE_PASSWORD=$ADMIN_PASSWORD<br class="calibre4"/>SERVICE_TOKEN=super-secret-admin-token<br class="calibre4"/>VIRT_DRIVER=novadocker.virt.docker.DockerDriver<br class="calibre4"/><br class="calibre4"/># Logging<br class="calibre4"/>VERBOSE=True<br class="calibre4"/>DEST=$HOME/stack<br class="calibre4"/>SCREEN_LOGDIR=$DEST/logs/screen<br class="calibre4"/>SERVICE_DIR=$DEST/status<br class="calibre4"/>DATA_DIR=$DEST/data<br class="calibre4"/>LOGFILE=$DEST/logs/stack.sh.log<br class="calibre4"/>LOGDIR=$DEST/logs<br class="calibre4"/>OFFLINE=false<br class="calibre4"/><br class="calibre4"/># Networking<br class="calibre4"/>FIXED_RANGE=10.0.0.0/24<br class="calibre4"/><br class="calibre4"/># This enables Neutron<br class="calibre4"/>disable_service n-net<br class="calibre4"/>enable_service q-svc<br class="calibre4"/>enable_service q-agt<br class="calibre4"/>enable_service q-dhcp<br class="calibre4"/>enable_service q-l3<br class="calibre4"/>enable_service q-meta<br class="calibre4"/><br class="calibre4"/># Introduce glance to docker images<br class="calibre4"/>[[post-config|$GLANCE_API_CONF]]<br class="calibre4"/>[DEFAULT]<br class="calibre4"/>container_formats=ami,ari,aki,bare,ovf,ova,docker<br class="calibre4"/><br class="calibre4"/># Configure nova to use the nova-docker driver<br class="calibre4"/>[[post-config|$NOVA_CONF]]<br class="calibre4"/>[DEFAULT]<br class="calibre4"/>compute_driver=novadocker.virt.docker.DockerDriver</tt></p><p class="calibre_8">For installing <a/>the nova-docker rootwrap filters run the following command:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo cp nova-docker/etc/nova/rootwrap.d/docker.filters \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  /etc/nova/rootwrap.d/</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">For uploading the Docker image to Glance run the following command:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker save nginx |  glance image-create --is-public=True --container-format=docker --disk-format=raw --name nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the Glance image list; we can see the nginx container image:</p><p class="calibre_9"><img src="images/00243.jpg" class="calibre_81"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's <a/>create the nginx container:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">nova boot --flavor m1.small --image nginx nginxtest</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the Nova instances:</p><p class="calibre_9"><img src="images/00244.jpg" class="calibre_378"/></p><p class="calibre_8">
</p><p class="calibre_8">We can also see the running Container using the Docker native command:</p><p class="calibre_9"><img src="images/00246.jpg" class="calibre_145"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_208"/>


<p id="filepos683434" class="calibre_9"><span class="calibre3"><span class="bold">The Heat Docker plugin</span></span></p><p class="calibre_8">The <a/>following are some of the items that the Nova Docker driver cannot do currently:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Passing environment variables</li><li value="2" class="calibre_13">Linking containers</li><li value="3" class="calibre_13">Specifying volumes</li><li value="4" class="calibre_13">Orchestrating and scheduling the containers</li></ul><p class="calibre_8">These missing functionalities are important and unique for Containers. The Heat Docker plugin solves these problems partially, except for the orchestration part.</p><p class="calibre_8">The following diagram shows the Heat Docker orchestration <a/>architecture:</p><p class="calibre_9"><img src="images/00247.jpg" class="calibre_379"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some notes on the architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Heat uses the Heat Docker plugin to talk to Docker. The Docker plugin uses the REST API to talk to the Docker engine.</li><li value="2" class="calibre_13">There is no direct interaction of Heat with the Docker registry.</li><li value="3" class="calibre_13">Using the Heat orchestration script, we can use all the features of the Docker engine. The disadvantage of this approach is that there is no direct integration of Docker with other OpenStack modules.</li></ul><p id="filepos685253" class="calibre_14"><span class="calibre3"><span class="bold">Installing the Heat plugin</span></span></p><p class="calibre_8">I used the <a/>procedure at <a href="https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/">https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/</a> and <a href="https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat">https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat</a> to do the OpenStack Heat Docker plugin integration with OpenStack Icehouse.</p><p class="calibre_8">Using the Heat <a/>plugin, we can spawn Docker containers either in the localhost or VM created by OpenStack.</p><p class="calibre_8">I have used a Ubuntu 14.04 VM with Icehouse installed using Devstack. I used the procedure in the preceding links to install the Heat Docker plugin.</p><p class="calibre_8">The following command output shows that the Heat plugin is successfully installed in the localhost:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">$ heat resource-type-list | grep Docker</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">| DockerInc::Docker::Container     </span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is a heat template file to spawn the nginx container in the localhost:</p><p class="calibre_8"><tt class="calibre2">heat_template_version: 2013-05-23<br class="calibre4"/>description: &gt;<br class="calibre4"/>  Heat template to deploy Docker containers to an existing host<br class="calibre4"/>resources:<br class="calibre4"/>  nginx-01:<br class="calibre4"/>    type: DockerInc::Docker::Container<br class="calibre4"/>    properties:<br class="calibre4"/>      image: nginx<br class="calibre4"/>      docker_endpoint: 'tcp://192.168.56.102:2376'</tt></p><p class="calibre_8">We have specified the endpoint as the localhost IP address and Docker engine port number.</p><p class="calibre_8">The following command is used to create the Container using the preceding heat template:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">heat stack-create -f ~/heat/docker_temp.yml nginxheat1</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows that the heat stack installation is complete:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">$ heat stack-list</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">+--------------------------------------+---------------+-----------------+----------------------+</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">| id                                   | stack_name    | stack_status    | creation_time        |</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">+--------------------------------------+---------------+-----------------+----------------------+</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">| d878d8c1-ce17-4f29-9203-febd37bd8b7d | nginxheat1    | CREATE_COMPLETE | 2015-06-14T13:27:54Z |</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">+--------------------------------------+---------------+-----------------+----------------------</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows the successful running container in the localhost:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">$ docker -H :2376 ps</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">624ff5de9240        nginx:latest        "nginx -g 'daemon of   2 minutes ago       Up 2 minutes        80/tcp, 443/tcp     trusting_pasteur   </span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">We can use the Heat plugin approach to run Containers on OpenStack VMs by changing the endpoint IP address from the localhost to the VM's IP address.</p><div class="mbp_pagebreak" id="calibre_pb_209"/>


<p id="filepos689417" class="calibre_9"><span class="calibre3"><span class="bold">Magnum</span></span></p><p class="calibre_8">With Nova Driver <a/>and Heat Orchestration, Containers were not a first-class citizen in OpenStack and Container specifics were not easy to manage with these approaches. Magnum is a generic Container management solution being developed in OpenStack to manage Docker as well as other Container technologies. Magnum supports Kubernetes, Docker Swarm, and Mesos for Orchestration currently. Other orchestration solutions will be added in the future. Magnum supports Docker Containers currently. The architecture allows it to support other Container runtime such as Rkt in the future. Magnum is still in the early stages and is available as a beta feature in the OpenStack Liberty release.</p><p id="filepos690231" class="calibre_9"><span class="calibre3"><span class="bold">The Magnum architecture</span></span></p><p class="calibre_8">The <a/>following diagram shows the different layers in Magnum:</p><p class="calibre_9"><img src="images/00249.jpg" class="calibre_380"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>are some notes on the Magnum architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Magnum client talks to the Magnum API server, which in turn talks to the Magnum conductor. The Magnum conductor is responsible for interacting with Kubernetes, Docker Swarm, and Heat.</li><li value="2" class="calibre_13">Heat takes care of interacting with other OpenStack modules such as Nova, Neutron, Keystone, and Glance.</li><li value="3" class="calibre_13">Nova is used to create nodes in the Bay and they can run different Micro OSes such as CoreOS and Atomic.</li></ul><p class="calibre_8">OpenStack Magnum <a/>uses the following constructs:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Bay model</span>: This is a cluster definition that describes properties of the cluster, such as the node flavor, node OS, and orchestration engine to be used. The following is an example bay model template that uses the node flavor as <tt class="calibre2">m1.small</tt>, fedora atomic as the base OS for the node, and Kubernetes as the orchestration engine:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">magnum baymodel-create --name k8sbaymodel \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --image-id fedora-21-atomic-5 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --keypair-id testkey \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --external-network-id public \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --dns-nameserver 8.8.8.8 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --flavor-id m1.small \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --docker-volume-size 5 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --network-driver flannel \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">                       --coe kubernetes</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31"><span class="bold">Bay</span>: Bays are instantiated based on the bay model with the number of nodes necessary in Bay.</li><a/><li value="3" class="calibre_13"><span class="bold">Nodes, Pods, and Containers</span>: Nodes are the individual VM instances. Pods are a collection of containers that share common properties and are scheduled together. Containers run within a Pod.</li><a/></ul><p class="calibre_8">The following diagram shows the relationship between the Bay model, <span class="bold">Bay</span>, <span class="bold">Node</span>, <span class="bold">Pod</span>, and <span class="bold">Container</span>:</p><p class="calibre_9"><img src="images/00250.jpg" class="calibre_381"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the <a/>advantages of using OpenStack Magnum versus a native orchestration solution such as Kubernetes:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">For customers who are already using OpenStack, this provides an integrated solution.</li><li value="2" class="calibre_13">OpenStack provides multitenancy at all layers. This can be extended for Containers as well.</li><li value="3" class="calibre_13">OpenStack Magnum allows interaction with other OpenStack modules such as Neutron, Keystone, Glance, Swift, and Cinder. Some of these integrations are planned for the future.</li><li value="4" class="calibre_13">VMs and Containers have different purposes and most likely, they will coexist. OpenStack with the Magnum project provides you with an orchestration solution covering both VMs and Containers and this makes it very attractive.</li></ul><p id="filepos694665" class="calibre_14"><span class="calibre3"><span class="bold">Installing Magnum</span></span></p><p class="calibre_8">Magnum can <a/>be installed using the procedure at <a href="https://github.com/openstack/magnum/blob/master/doc/source/dev/quickstart.rst">https://github.com/openstack/magnum/blob/master/doc/source/dev/quickstart.rst</a>. The following is a summary of the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create the OpenStack development environment with Devstack enabling the Magnum service.</li><li value="2" class="calibre_13">By default, the Fedora Atomic image gets downloaded to Glance as part of the Devstack installation. If the CoreOS image is necessary, we need to download it manually to Glance.</li><li value="3" class="calibre_13">Create a Bay model. A Bay model is like a template with a specific set of parameters using which multiple bays can be created. In the Bay model, we can specify the Bay type (currently supported Bay types are Kubernetes and Swarm), base image type (currently supported base images are Fedora Atomic and CoreOS), networking model (Flannel), instance size, and so on.</li><li value="4" class="calibre_13">Create a Bay using the Bay model as a template. While creating a Bay, we can specify the number of nodes that need to be created. Node is a VM on top of which the base image is installed.</li><li value="5" class="calibre_13">Deploy Containers using either Kubernetes or Swarm on top of the created Bay. Kubernetes or Swarm will take care of scheduling the Containers among the different nodes in the Bay.<p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: It is recommended that you avoid running Magnum in a VM. It is necessary to have a beefy machine as each Fedora instance requires at least 1 or 2 GB of RAM and 8 GB of hard disk space.</p></li><a/></ol><div class="mbp_pagebreak" id="calibre_pb_210"/>


<p id="filepos696738" class="calibre_"><span class="calibre1"><span class="bold">Container networking using OpenStack Kuryr</span></span></p><p class="calibre_8">In this section, we <a/>will cover how Container networking can be done with OpenStack Neutron using the OpenStack Kuryr project.</p><div class="mbp_pagebreak" id="calibre_pb_211"/>


<p id="filepos697062" class="calibre_9"><span class="calibre3"><span class="bold">OpenStack Neutron</span></span></p><p class="calibre_8">OpenStack Neutron provides the networking functionality for OpenStack clusters. The following <a/>are some properties of OpenStack Neutron:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Neutron provides networking as an API service with backends or plugins doing the implementation</li><li value="2" class="calibre_13">Neutron can be used for baremetal networking as well as VM networking</li><li value="3" class="calibre_13">Basic Neutron constructs are Neutron network, Port, Subnet, and Router</li><li value="4" class="calibre_13">Common Neutron backends are OVS, OVN, and Linux bridge</li><li value="5" class="calibre_13">Neutron also provides advanced networking services such as load balancing as a service, Firewall as a service, Routing as a service, and VPN as a service</li></ul><div class="mbp_pagebreak" id="calibre_pb_212"/>


<p id="filepos698155" class="calibre_14"><span class="calibre3"><span class="bold">Containers and networking</span></span></p><p class="calibre_8">We covered the<a/> details of Container networking in the earlier chapters. Some of the common technologies used were Flannel, Docker Libnetwork, Weave, and Calico. Most of these technologies use the Overlay network to provide Container networking.</p><div class="mbp_pagebreak" id="calibre_pb_213"/>


<p id="filepos698581" class="calibre_9"><span class="calibre3"><span class="bold">OpenStack Kuryr</span></span></p><p class="calibre_8">The goal <a/>of OpenStack Kuryr is to use Neutron to provide Container networking. Considering that Neutron is a mature technology, Kuryr aims to leverage the Neutron effort and make it easy for OpenStack users to adopt the Container technology. Kuryr is not a networking technology by itself; it aims to act as a bridge between Container networking and VM networking and enhancing Neutron to provide missing Container networking pieces.</p><p class="calibre_8">The following diagram shows you how Docker can be used with Neutron and where Kuryr fits in:</p><p class="calibre_9"><img src="images/00253.jpg" class="calibre_382"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some notes on the Kuryr <a/>architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Kuryr is implemented as the Docker libnetwork plugin. Container networking calls are mapped by Kuryr to appropriate Neutron API calls.</li><li value="2" class="calibre_13">Neutron uses OVN, Midonet, and Dragonflow as backends to implement the Neutron calls.</li></ul><p class="calibre_8">The following <a/>are some advantages of OpenStack Kuryr:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It provides a common networking solution for both VMs and Containers.</li><li value="2" class="calibre_13">With Magnum and Kuryr together, Containers and VMs can have a common Orchestration.</li><li value="3" class="calibre_13">Considering that the Neutron technology is already mature, Containers can leverage all the Neutron functionalities.</li><li value="4" class="calibre_13">With default Container networking, there is a double encapsulation problem when Containers are deployed over a VM. Container networking does the first level of encapsulation and VM networking does the next level of encapsulation. This can cause performance overhead. With Kuryr, the double encapsulation problem can be avoided because Containers and VMs share the same network.</li><li value="5" class="calibre_13">Kuryr can integrate well with other OpenStack components to provide a complete Container solution with built-in multitenant support.</li></ul><p class="calibre_8">The following <a/>table shows the mapping between the Neutron and Libnetwork abstraction:</p><table border="1" valign="top" class="calibre_16"><tr valign="top" class="calibre_17"><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Neutron</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Libnetwork</span></p></th></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Neutron network</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Network</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Port</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Endpoint</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Subnet</p></td><td valign="top" class="calibre_19"><p class="calibre_8">IPAM</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Plugin API (plug/unplug)</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Plugin API (Join/leave)</p></td></tr></table><p class="calibre_8">The following diagram shows you how Kuryr can provide a common networking solution for Containers, VMs, and bare metal:</p><p class="calibre_9"><img src="images/00254.jpg" class="calibre_383"/></p><p class="calibre_8">
</p><p class="calibre_8">The following image <a/>shows you where Kuryr fits in with Magnum and Container orchestration projects:</p><p class="calibre_9"><img src="images/00256.jpg" class="calibre_384"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_214"/>


<p id="filepos703130" class="calibre_9"><span class="calibre3"><span class="bold">The current state and roadmap of Kuryr</span></span></p><p class="calibre_8">The Kuryr project <a/>is pretty new, and the Mitaka release will be the first OpenStack release with Kuryr support. The following are the ongoing and <a/>future work items with Kuryr:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Adding missing Container features to Neutron, such as Port forwarding, resource tagging, and service discovery.</li><li value="2" class="calibre_13">Handling the nested container issue by integrating VM and Container networking.</li><li value="3" class="calibre_13">Better integration with OpenStack Magnum and Kolla projects.</li><li value="4" class="calibre_13">Current integration is focused on Docker. There are integration plans with the Kubernetes networking model.</li></ul><div class="mbp_pagebreak" id="calibre_pb_215"/>


<p id="filepos704135" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered how Containers and CoreOS integrate with OpenStack. As CoreOS allows only applications running as Containers inside it, the OpenStack integration with CoreOS becomes more useful if OpenStack supports Container Orchestration. Even though the Nova driver and Heat plugin add Container support in OpenStack, the Magnum project seems like the correct solution treating Containers as a first-class citizen in OpenStack. We also covered how OpenStack Neutron can be used to provide Container networking using the Kuryr project. OpenStack Container integration is relatively new and there is still a lot of work ongoing to complete this integration. Managing VMs and Containers using single orchestration software gives tighter integration and eases the management and debugging capabilities. In the next chapter, we will cover CoreOS troubleshooting and debugging.</p><div class="mbp_pagebreak" id="calibre_pb_216"/>


<p id="filepos705163" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Magnum: <a href="https://wiki.openstack.org/wiki/Magnum">https://wiki.openstack.org/wiki/Magnum</a></li><li value="2" class="calibre_13">Magnum developer quick start: <a href="https://github.com/openstack/magnum/blob/master/doc/source/dev/dev-quickstart.rst">https://github.com/openstack/magnum/blob/master/doc/source/dev/dev-quickstart.rst</a></li><li value="3" class="calibre_13">CoreOS on OpenStack: <a href="https://coreos.com/os/docs/latest/booting-on-openstack.html">https://coreos.com/os/docs/latest/booting-on-openstack.html</a></li><li value="4" class="calibre_13">The OpenStack Docker driver: <a href="https://wiki.openstack.org/wiki/Docker">https://wiki.openstack.org/wiki/Docker</a></li><li value="5" class="calibre_13">Installing Nova-docker with OpenStack: <a href="http://blog.oddbit.com/2015/02/11/installing-novadocker-with-devstack/">http://blog.oddbit.com/2015/02/11/installing-novadocker-with-devstack/</a></li><li value="6" class="calibre_13">OpenStack and Docker driver: <a href="https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-1/">https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-1/</a></li><li value="7" class="calibre_13">OpenStack and Docker with Heat and Magnum: <a href="https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/">https://sreeninet.wordpress.com/2015/06/14/openstack-and-docker-part-2/</a></li><li value="8" class="calibre_13">The OpenStack Heat plugin for Docker: <a href="https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat">https://github.com/MarouenMechtri/Docker-containers-deployment-with-OpenStack-Heat</a></li><li value="9" class="calibre_13">OpenStack Kuryr: <a href="https://github.com/openstack/kuryr">https://github.com/openstack/kuryr</a></li><li value="10" class="calibre_13">OpenStack Kuryr background: <a href="https://galsagie.github.io/sdn/openstack/docker/kuryr/neutron/2015/08/24/kuryr-part1/">https://galsagie.github.io/sdn/openstack/docker/kuryr/neutron/2015/08/24/kuryr-part1/</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_217"/>


<p id="filepos707582" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Private Cloud Dream Stack - OpenStack + CoreOS + Kubernetes: <a href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/private-cloud-dream-stack-openstack-coreos-kubernetes">https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/private-cloud-dream-stack-openstack-coreos-kubernetes</a></li><li value="2" class="calibre_13">Magnum OpenStack presentations: <a href="https://www.youtube.com/watch?v=BM6nFH7G8Vc">https://www.youtube.com/watch?v=BM6nFH7G8Vc</a> and <a href="https://www.youtube.com/watch?v=_ZbebTIaS7M">https://www.youtube.com/watch?v=_ZbebTIaS7M</a></li><li value="3" class="calibre_13">Kuryr OpenStack presentations: <a href="https://www.openstack.org/summit/tokyo-2015/videos/presentation/connecting-the-dots-with-neutron-unifying-network-virtualization-between-containers-and-vms">https://www.openstack.org/summit/tokyo-2015/videos/presentation/connecting-the-dots-with-neutron-unifying-network-virtualization-between-containers-and-vms</a> and <a href="https://www.youtube.com/watch?v=crVi30bgOt0">https://www.youtube.com/watch?v=crVi30bgOt0</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_218"/>
</body></html>