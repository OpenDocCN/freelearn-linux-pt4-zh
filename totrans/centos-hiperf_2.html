<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;2.&#xA0;Installing Cluster Services and Configuring Network Components"><div class="book" id="H5A42-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Installing Cluster Services and Configuring Network Components</h1></div></div></div><p class="calibre8">In this chapter, you will learn how to set up and configure the basic required network infrastructure and also the clustering components that we installed in the previous chapter.</p><p class="calibre8">In addition to this, we will review the basic and important concepts of firewalling and Internet protocols, and we will explain how to add the firewall rules that will allow communication between the nodes and the proper operation of the clustering services on each node.</p><p class="calibre8">If your native language is any other than English, you must have taken an English class or taught yourself (as I did) before being able to read this book. The same thing happens when two people who do not speak the same language want to communicate with each other. At least one of them needs to know the language of the other, or the two of them need to agree on a common idiom in order to be able to understand each other.</p><p class="calibre8">In networking, the equivalent of languages in the above<a id="id62" class="calibre1"/> analogy is called <span class="strong"><strong class="calibre2">protocols</strong></span>. In order to enable data transmission between two machines, there must be a logical way for them to be able to speak to each other. This is at the very heart of the Internet protocol suite, also<a id="id63" class="calibre1"/> known as the <span class="strong"><strong class="calibre2">Internet model</strong></span>, which provides a set of communication protocols or rules.</p><p class="calibre8">It is precisely this set of protocols that make data transmission possible in networks such as the Internet. Later in this chapter, we will explain the protocols and network ports that participate in the communication inside a cluster.</p></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Installing Cluster Services and Configuring Network Components">
<div class="book" title="Configuring and starting clustering services"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec12" class="calibre1"/>Configuring and starting clustering services</h1></div></div></div><p class="calibre8">Having <a id="id64" class="calibre1"/>reviewed the key networking concepts that were <a id="id65" class="calibre1"/>outlined earlier, we are now ready to start describing the cluster services.</p></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Installing Cluster Services and Configuring Network Components">
<div class="book" title="Configuring and starting clustering services">
<div class="book" title="Starting and enabling clustering services"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec13" class="calibre1"/>Starting and enabling clustering services</h2></div></div></div><p class="calibre8">You will recall <a id="id66" class="calibre1"/>from the previous chapter that we installed the following clustering components:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Pacemaker</strong></span>: This<a id="id67" class="calibre1"/> is the cluster resource manager</li><li class="listitem"><span class="strong"><strong class="calibre2">Corosync</strong></span>: This<a id="id68" class="calibre1"/> is the messaging service</li><li class="listitem"><span class="strong"><strong class="calibre2">PCS</strong></span>: This is<a id="id69" class="calibre1"/> the synchronization and configuration tool</li></ul></div><p class="calibre8">As you can probably guess from the preceding list, these components should run as daemons, a special type of process that runs in the background without the need of direct intervention or control of an administrator. Although we installed the necessary packages in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>, we did not start the cluster resource manager or the messaging services. So, we now need to start them manually for the first time and enable them to run automatically on startup during the next system boot.</p><p class="calibre8">We will start by configuring <code class="email">pacemaker</code> and <code class="email">corosync</code> first and save PCS for later in this chapter.</p><p class="calibre8">As shown in the following screenshot, these daemons (also known as units in systemd-based systems) are inactive when you first boot both nodes (and are not automatically started on reboot) after performing all the tasks outlined in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>. You can check their current running status using the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl status pacemaker</strong></span>
<span class="strong"><strong class="calibre2">systemctl status corosync</strong></span></pre></div><div class="mediaobject"><img src="../images/00018.jpeg" alt="Starting and enabling clustering services" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In order to<a id="id70" class="calibre1"/> start corosync and pacemaker on each node and enable both services to start automatically during system boot, first create the corosync configuration file by making a copy of the example file, which came with the installation package. As opposed to the pacemaker and PCS, corosync does not create the configuration file automatically for you:</p><p class="calibre8">To create the corosync configuration file, do:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf</strong></span></pre></div><p class="calibre8">And then restart and enable the services by running the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl start pacemaker</strong></span>
<span class="strong"><strong class="calibre2">systemctl enable corosync</strong></span>
<span class="strong"><strong class="calibre2">systemctl enable pacemaker</strong></span></pre></div><p class="calibre8">In the preceding commands, note that we are not starting corosync manually, as it will launch on its own when pacemaker is started. It is important to note that on systemd-based systems, enabling a service is not the same as starting it. A unit may be enabled but not started, or the other way around. As shown in the following code, enabling a service involves creating a symlink to the unit's configuration file, which among other things specifies the actions to be taken on system boot and shutdown.</p><p class="calibre8">Perform the following operations on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# systemctl enable pacemaker</strong></span>
<span class="strong"><strong class="calibre2">ln -s '/usr/lib/systemd/system/pacemaker.service' '/etc/systemd/system/multi-user.target.wants/pacemaker.service'</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]# systemctl enable corosync</strong></span>
<span class="strong"><strong class="calibre2">ln -s '/usr/lib/systemd/system/corosync.service' '/etc/systemd/system/multi-user.target.wants/corosync.service'</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">Finally, before we can configure the cluster at a later stage, we need to perform the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Start and enable the PCS daemon (<code class="email">pcsd</code>), which will be in charge of keeping the corosync configuration synced on <code class="email">node01</code> and <code class="email">node02</code>. In order for the <code class="email">pcsd</code> daemon to work as expected, corosync and pacemaker must have been started previously. Note that when you use the <code class="email">systemctl</code> tool to manage services in a systemd-based system, you can omit the trailing <code class="email">.service</code> after the daemon name (or use it if you want, as indicated in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>). Start and enable the PCS daemon with:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl start pcsd</strong></span>
<span class="strong"><strong class="calibre2">systemctl enable pcsd</strong></span></pre></div></li><li class="listitem" value="2"><a id="id71" class="calibre1"/>Now set the password for the hacluster Linux account, which was created automatically when PCS was installed. This account is used by the PCS daemon to set up communication between nodes, and is best managed when the password is identical on both nodes. To set the password for hacluster, type the following command and assign the same password on both nodes:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">passwd hacluster</strong></span></pre></div></li></ol><div class="calibre14"/></div></div></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Installing Cluster Services and Configuring Network Components">
<div class="book" title="Configuring and starting clustering services">
<div class="book" title="Troubleshooting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec14" class="calibre1"/>Troubleshooting</h2></div></div></div><p class="calibre8">Under normal circumstances, starting pacemaker should start corosync automatically. You can check<a id="id72" class="calibre1"/> corosync's status with the <code class="email">systemctl status corosync</code> command. If for some reason that is not the case, you can still run the following command to manually start the messaging service:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl start corosync</strong></span></pre></div><p class="calibre8">Should any of the preceding commands return an error, running <code class="email">systemctl -l status unit</code>, where <code class="email">unit</code> is either corosync or pacemaker, will return a detailed status about the respective service.</p><p class="calibre8">Here is another useful troubleshooting command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">journalctl -xn</strong></span></pre></div><p class="calibre8">This will query the systemd journal (systemd's own log) and return verbose messages about the last events.</p><p class="calibre8">Both of these commands will provide helpful information as to what went wrong, and point you in the right direction to solve the problem.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip02" class="calibre1"/>Tip</h3><p class="calibre8">You can<a id="id73" class="calibre1"/> read more about the systemd<a id="id74" class="calibre1"/> journal in its man page, <span class="strong"><em class="calibre9">man journalctl</em></span>, or in the online version, which is available at <a class="calibre1" href="http://www.freedesktop.org/software/systemd/man/journalctl.html">http://www.freedesktop.org/software/systemd/man/journalctl.html</a>.</p></div></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Security fundamentals"><div class="book" id="I3QM2-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec13" class="calibre1"/>Security fundamentals</h1></div></div></div><p class="calibre8">At this point, we are ready to discuss network security to only allow the proper network traffic between the<a id="id75" class="calibre1"/> nodes. During the initial setup and while performing your first tests, you may want to disable the firewall and SELinux (which is described later in this chapter) and then go through both of them at a later stage—it is up to you depending on your grade of familiarity with them at this point.</p></div>

<div class="book" title="Security fundamentals">
<div class="book" title="Letting in and letting out"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec15" class="calibre1"/>Letting in and letting out</h2></div></div></div><p class="calibre8">After having<a id="id76" class="calibre1"/> started and enabled the services mentioned earlier, we are ready to take a closer look at the network processes involved in the cluster configuration and maintenance. With the help of the <code class="email">netstat</code> command, a tool included in the <code class="email">net-tools</code> package for CentOS 7, we will print the current listening network ports and verify that corosync is running and listening for connections. Before doing so, you will need to install the net-tools package, as it is not included in the minimal CentOS 7 setup, using the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">yum –y install net-tools &amp;&amp; netstat -npltu | grep -i corosync</strong></span></pre></div><p class="calibre8">As we can see in the following screenshot, Corosync is listening on the <span class="strong"><strong class="calibre2">UDP</strong></span> ports <code class="email">5404</code> and <code class="email">5405</code> of the loopback interface (<code class="email">127.0.0.1</code>) and on the port <code class="email">5405</code> of the multicast address (which is set to <code class="email">239.255.1.1</code> by default and provides a logical way to identify this group of nodes):</p><div class="mediaobject"><img src="../images/00019.jpeg" alt="Letting in and letting out" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note03" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre2">User Datagram Protocol</strong></span> (UDP) is one of the core members of the Internet model. This<a id="id77" class="calibre1"/> protocol allows applications to send messages (also<a id="id78" class="calibre1"/> known as <span class="strong"><strong class="calibre2">datagrams</strong></span>) to hosts in a network in order to set up paths for data transmission without performing full handshakes (or a successful connection between two hosts in a network). Additionally, UDP does not include error checking and correction in a network communication (these checks are performed at the destination application itself).</p><p class="calibre8">The <span class="strong"><strong class="calibre2">Transmission Control Protocol</strong></span> (<span class="strong"><strong class="calibre2">TCP</strong></span>) is another core protocol of the Internet <a id="id79" class="calibre1"/>model. As opposed to UDP, it provides error, delivery, ordering, and duplicates checking of data streams between computers in a network. Several well-known application layer protocols (such as HTTP, SMTP, and SSH, to name a few) are encapsulated in TCP.</p><p class="calibre8"><span class="strong"><strong class="calibre2">Internet Group Management Protocol</strong></span> (<span class="strong"><strong class="calibre2">IGMP</strong></span>) is the communication protocol used by<a id="id80" class="calibre1"/> network devices (whether they can be either hosts or routers) to establish multicast data transmissions, which allows one host on the network to send datagrams to several other systems that are interested in receiving the source content.</p></div><p class="calibre8">Before we <a id="id81" class="calibre1"/>proceed further, we will need to allow traffic through the firewall on each node. By default, the ports named in the following list are the default ports where these services will listen after being started, as we previously did. Specifically, in both nodes, we need to perform the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Open the network ports needed by <code class="email">corosync</code> (<span class="strong"><strong class="calibre2">UDP</strong></span> ports <span class="strong"><strong class="calibre2">5404</strong></span> and <span class="strong"><strong class="calibre2">5405</strong></span>) and PCS (usually <span class="strong"><strong class="calibre2">TCP</strong></span> <span class="strong"><strong class="calibre2">2224</strong></span>) using the following commands:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -m state --state NEW -p udp -m multiport --dports 5404,5405 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">iptables -I INPUT -p tcp -m state --state NEW --dport 2224 -j ACCEPT</strong></span></pre></div><div class="note" title="Note"><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre8">Note that the use of <code class="email">-m</code> multiport allows you to combine a number of different ports in one rule instead of having to write several rules that are almost identical. This results in fewer rules and easier maintenance of <code class="email">iptables</code>.</p></div></li><li class="listitem" value="2">Allow IGMP and multicast traffic using the following commands:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -p igmp -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">iptables -I INPUT -m addrtype --dst-type MULTICAST -j ACCEPT</strong></span></pre></div></li><li class="listitem" value="3">Change the default <code class="email">iptables</code> policy for the <code class="email">INPUT</code> chain to <code class="email">DROP</code>. Thus, any packet that does not comply with the rules that we just added will be dropped. Note that, as opposed to the <code class="email">REJECT</code> policy, <code class="email">DROP</code> does not send any response whatsoever to the calling client, just "radio silence" while actively dropping the packets:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -P INPUT DROP</strong></span></pre></div></li><li class="listitem" value="4">After adding the necessary rules, our firewall configuration looks as shown in the<a id="id82" class="calibre1"/> following code, where we can clearly see that besides the rules that we added in the two previous steps, there are others that were initialized by default when we started and enabled <code class="email">iptables</code>, as explained in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>. Run the following command to list the firewall rules along with their corresponding numbers:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# iptables -L -v --line-numbers</strong></span>
<span class="strong"><strong class="calibre2">Chain INPUT (policy DROP 0 packets, 0 bytes)</strong></span>
<span class="strong"><strong class="calibre2">num   pkts bytes target     prot opt in     out     source               destination</strong></span>
<span class="strong"><strong class="calibre2">1      423 48645 ACCEPT     all  --  any    any     anywhere             anywhere             ADDRTYPE match dst-type MULTICAST</strong></span>
<span class="strong"><strong class="calibre2">2        0     0 ACCEPT     igmp --  any    any     anywhere             anywhere</strong></span>
<span class="strong"><strong class="calibre2">3        0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere             state NEW tcp dpt:efi-mg</strong></span>
<span class="strong"><strong class="calibre2">4     1200  124K ACCEPT     udp  --  any    any     anywhere             anywhere             state NEW multiport dports hpoms-dps-lstn,netsupport</strong></span>
<span class="strong"><strong class="calibre2">5       86  7152 ACCEPT     all  --  any    any     anywhere             anywhere             state RELATED,ESTABLISHED</strong></span>
<span class="strong"><strong class="calibre2">6        0     0 ACCEPT     icmp --  any    any     anywhere             anywhere</strong></span>
<span class="strong"><strong class="calibre2">7      387 41151 ACCEPT     all  --  lo     any     anywhere             anywhere</strong></span>
<span class="strong"><strong class="calibre2">8        0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere             state NEW tcp dpt:ssh</strong></span>
<span class="strong"><strong class="calibre2">9       65 10405 REJECT     all  --  any    any     anywhere             anywhere             reject-with icmp-host-prohibited</strong></span>

<span class="strong"><strong class="calibre2">Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)</strong></span>
<span class="strong"><strong class="calibre2">num   pkts bytes target     prot opt in     out     source               destination</strong></span>
<span class="strong"><strong class="calibre2">1        0     0 REJECT     all  --  any    any     anywhere             anywhere             reject-with icmp-host-prohibited</strong></span>

<span class="strong"><strong class="calibre2">Chain OUTPUT (policy ACCEPT 1149 packets, 127K bytes)</strong></span>
<span class="strong"><strong class="calibre2">num   pkts bytes target     prot opt in     out     source               destination</strong></span></pre></div></li><li class="listitem" value="5">If the<a id="id83" class="calibre1"/> last default rule in the <code class="email">INPUT</code> chain implements a <code class="email">REJECT</code> procedure for non-compliant packets, we will delete it because we already took care of that need by changing the default policy for the chain:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -D INPUT [rule number]</strong></span></pre></div></li><li class="listitem" value="6">Finally, we must save the firewall rules for persistency across boots. As shown in the following screenshot, this consists of saving the changes to <code class="email">/etc/sysconfig/iptables</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">service iptables save</strong></span></pre></div><div class="mediaobject"><img src="../images/00020.jpeg" alt="Letting in and letting out" class="calibre10"/></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div><p class="calibre8">If we inspect the <code class="email">/etc/sysconfig/iptables</code> file with our preferred text editor or pager, we will realize that it presents the same firewall rules in a format that is somewhat easier to read, as shown in the following code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node02 ~]# cat /etc/sysconfig/iptables</strong></span>
<span class="strong"><strong class="calibre2"># Generated by iptables-save v1.4.21 on Sat Dec  5 10:09:24 2015</strong></span>
<span class="strong"><strong class="calibre2">*filter</strong></span>
<span class="strong"><strong class="calibre2">:INPUT DROP [0:0]</strong></span>
<span class="strong"><strong class="calibre2">:FORWARD ACCEPT [0:0]</strong></span>
<span class="strong"><strong class="calibre2">:OUTPUT ACCEPT [263:28048]</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -m addrtype --dst-type MULTICAST -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -p igmp -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -p tcp -m state --state NEW -m tcp --dport 2224 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -p icmp -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -i lo -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">-A INPUT -j REJECT --reject-with icmp-host-prohibited</strong></span>
<span class="strong"><strong class="calibre2">-A FORWARD -j REJECT --reject-with icmp-host-prohibited</strong></span>
<span class="strong"><strong class="calibre2">COMMIT</strong></span>
<span class="strong"><strong class="calibre2"># Completed on Sat Dec  5 10:09:24 2015</strong></span></pre></div><p class="calibre8">Next, you will also need to edit the <code class="email">/etc/sysconfig/iptables-config</code> file to indicate that firewall rules should be persistent on system shutdown and reboot. Note that these lines already exist in the file and need to be changed. As a precaution, you may want to back up the existing file before making the change:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cp /etc/sysconfig/iptables-config /etc/sysconfig/iptables-config.orig</strong></span></pre></div><p class="calibre8">Now, open <code class="email">/etc/sysconfig/iptables-config</code> with your preferred text editor and ensure<a id="id84" class="calibre1"/> that the indicated lines read as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">IPTABLES_SAVE_ON_STOP="yes"</strong></span>
<span class="strong"><strong class="calibre2">IPTABLES_SAVE_ON_RESTART="yes"</strong></span></pre></div><p class="calibre8">As usual, do not forget to restart <code class="email">iptables</code> (<code class="email">systemctl restart iptables</code>) in order to apply changes.</p><p class="calibre8">CentOS 7, just like the previous versions of the distribution, comes with built-in <span class="strong"><strong class="calibre2">SELinux</strong></span> (<span class="strong"><strong class="calibre2">Security Enhanced Linux</strong></span>) support. This provides native, flexible access control functionality<a id="id85" class="calibre1"/> for the operating system based on the kernel itself. You may well be wondering what to do with SELinux policies at this stage. The current settings, which can be displayed with the <code class="email">sestatus</code> and <code class="email">getenforce</code> commands, and are shown in the following screenshot, will do for the time being:</p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Letting in and letting out" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In simple terms, we will leave the default mode set to <code class="email">enforcing</code> for security purposes. This should not cause any issues further down the road, but if it does, feel free to set the mode to <code class="email">permissive</code> with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">setenforce 0</strong></span></pre></div><p class="calibre8">The<a id="id86" class="calibre1"/> preceding command will enable warnings and log errors to help you troubleshoot issues while the server is still running. In case you need to troubleshoot issues and you suspect that SELinux may be causing them, you should look in <code class="email">/var/log/audit/audit.log</code>. SELinux log messages, which are labeled with the AVC keyword, are written to that file via <code class="email">auditd</code>, the Linux auditing system, which is started by default. Otherwise, these messages are written to <code class="email">/var/log/messages</code>.</p><p class="calibre8">Now, before you tackle the next heading, don't forget to repeat the same operations and save the changes on the other node as well!</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Getting acquainted with PCS"><div class="book" id="J2B82-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec14" class="calibre1"/>Getting acquainted with PCS</h1></div></div></div><p class="calibre8">We are getting closer to actually setting up the cluster. Before diving into that task, we need to become familiar with PCS—the core component of our cluster—so to speak, which will be<a id="id87" class="calibre1"/> used to control and configure pacemaker and corosync. To begin doing that, we can just run PCS without arguments, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs</strong></span></pre></div><p class="calibre8">This returns the output shown in the following screenshot, which provides a short explanation of each option and command available in PCS:</p><div class="mediaobject"><img src="../images/00022.jpeg" alt="Getting acquainted with PCS" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We are interested in the <span class="strong"><strong class="calibre2">Commands</strong></span> section, where the actual categories of clustering that can be<a id="id88" class="calibre1"/> managed through this tool are listed, along with a brief description of their usage. Each of them supports several capabilities, which can be shown by appending the word <span class="strong"><strong class="calibre2">help</strong></span> to <code class="email">pcs [category]</code>. For example, let's' see the functionality that is provided by the <code class="email">cluster</code> command (which by the way, we will use shortly):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster help</strong></span>
<span class="strong"><strong class="calibre2">Usage: pcs cluster [commands]...</strong></span>
<span class="strong"><strong class="calibre2">Configure cluster for use with pacemaker</strong></span>

<span class="strong"><strong class="calibre2">Commands:</strong></span>
<span class="strong"><strong class="calibre2">  auth [node] [...] [-u username] [-p password] [--force] [--local]</strong></span>
<span class="strong"><strong class="calibre2">     Authenticate pcs to pcsd on nodes specified, or on all nodes</strong></span>
<span class="strong"><strong class="calibre2">     configured in corosync.conf if no nodes are specified (authorization</strong></span>
<span class="strong"><strong class="calibre2">     tokens are stored in ~/.pcs/tokens or /var/lib/pcsd/tokens for root).</strong></span>
<span class="strong"><strong class="calibre2">     By default all nodes are also authenticated to each other, using</strong></span>
<span class="strong"><strong class="calibre2">     --local only authenticates the local node (and does not authenticate</strong></span>
<span class="strong"><strong class="calibre2">     the remote nodes with each other).  Using --force forces</strong></span>
<span class="strong"><strong class="calibre2">     re-authentication to occur.</strong></span>

<span class="strong"><strong class="calibre2">  setup [--start] [--local] [--enable] --name &lt;cluster name&gt; &lt;node1[,node1-altaddr]&gt;</strong></span>
<span class="strong"><strong class="calibre2">         [node2[,node2-altaddr]] [..] [--transport &lt;udpu|udp&gt;] [--rrpmode active|passive]</strong></span>
<span class="strong"><strong class="calibre2">         [--addr0 &lt;addr/net&gt; [[[--mcast0 &lt;address&gt;] [--mcastport0 &lt;port&gt;]</strong></span>
<span class="strong"><strong class="calibre2">                         [--ttl0 &lt;ttl&gt;]] | [--broadcast0]]</strong></span>
<span class="strong"><strong class="calibre2">         [--addr1 &lt;addr/net&gt; [[[--mcast1 &lt;address&gt;] [--mcastport1 &lt;port&gt;]</strong></span>
<span class="strong"><strong class="calibre2">                         [--ttl1 &lt;ttl&gt;]] | [--broadcast1]]]]</strong></span>
<span class="strong"><strong class="calibre2">         [--wait_for_all=&lt;0|1&gt;] [--auto_tie_breaker=&lt;0|1&gt;]</strong></span>
<span class="strong"><strong class="calibre2">         [--last_man_standing=&lt;0|1&gt; [--last_man_standing_window=&lt;time in ms&gt;]]</strong></span>
<span class="strong"><strong class="calibre2">         [--ipv6] [--token &lt;timeout&gt;] [--join &lt;timeout&gt;]</strong></span>
<span class="strong"><strong class="calibre2">         [--consensus &lt;timeout&gt;] [--miss_count_const &lt;count&gt;]</strong></span>
<span class="strong"><strong class="calibre2">         [--fail_recv_const &lt;failures&gt;]</strong></span>
<span class="strong"><strong class="calibre2">     Configure corosync and sync configuration out to listed nodes</strong></span>
<span class="strong"><strong class="calibre2">     --local will only perform changes on the local node</strong></span>
<span class="strong"><strong class="calibre2">     --start will also start the cluster on the specified nodes</strong></span>
<span class="strong"><strong class="calibre2">     --enable will enable corosync and pacemaker on node startup</strong></span>
<span class="strong"><strong class="calibre2">     --transport allows specification of corosync transport (default: udpu)</strong></span>
<span class="strong"><strong class="calibre2">     The --wait_for_all, --auto_tie_breaker, --last_man_standing,</strong></span>
<span class="strong"><strong class="calibre2">     --last_man_standing_window options are all documented in corosync's'</strong></span>
<span class="strong"><strong class="calibre2">     votequorum(5) man page.</strong></span>
<span class="strong"><strong class="calibre2">     --ipv6 will configure corosync to use ipv6 (instead of ipv4)</strong></span>
<span class="strong"><strong class="calibre2">     --token &lt;timeout&gt; sets time in milliseconds until a token loss is</strong></span>
<span class="strong"><strong class="calibre2">         declared after not receiving a token (default 1000 ms)</strong></span>
<span class="strong"><strong class="calibre2">     --join &lt;timeout&gt; sets time in milliseconds to wait for join mesages</strong></span>
<span class="strong"><strong class="calibre2">         (default 50 ms)</strong></span>
<span class="strong"><strong class="calibre2">     --consensus &lt;timeout&gt; sets time in milliseconds to wait for consensus</strong></span>
<span class="strong"><strong class="calibre2">         to be achieved before starting a new round of membership configuration</strong></span>
<span class="strong"><strong class="calibre2">         (default 1200 ms)</strong></span>
<span class="strong"><strong class="calibre2">     --miss_count_const &lt;count&gt; sets the maximum number of times on</strong></span>
<span class="strong"><strong class="calibre2">         receipt of a token a message is checked for retransmission before</strong></span>
<span class="strong"><strong class="calibre2">         a retransmission occurs (default 5 messages)</strong></span>
<span class="strong"><strong class="calibre2">     --fail_recv_const &lt;failures&gt; specifies how many rotations of the token</strong></span>
<span class="strong"><strong class="calibre2">         without receiving any messages when messages should be received</strong></span>
<span class="strong"><strong class="calibre2">         may occur before a new configuration is formed (default 2500 failures)</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note05" class="calibre1"/>Note</h3><p class="calibre8">Note that the output is truncated for brevity.</p></div><p class="calibre8">You will <a id="id89" class="calibre1"/>often find yourself examining the documentation, so you should consider seriously becoming acquainted with the help.</p></div>

<div class="book" title="Getting acquainted with PCS">
<div class="book" title="Managing authentication and creating the cluster"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec16" class="calibre1"/>Managing authentication and creating the cluster</h2></div></div></div><p class="calibre8">We are now<a id="id90" class="calibre1"/> ready to authenticate PCS to the <code class="email">pcsd</code> service <a id="id91" class="calibre1"/>on the nodes specified in the command line. By default, all nodes are authenticated to each other and thus PCS can talk to itself from one cluster member to the rest.</p><p class="calibre8">This is precisely where the hacluster user (of which we changed the password earlier) comes in handy, as it is the account that is used for this purpose. The generic syntax for PCS to perform this step in a cluster with <code class="email">N</code> nodes is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster auth member1 member 2 … memberN</strong></span></pre></div><p class="calibre8">In our current setup with two nodes, setting up authentication means:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster auth node01 node02</strong></span></pre></div><p class="calibre8">We will be prompted to enter the username and password that will be used for authentication, as discussed earlier, and fortunately for us, this process does not need to be repeated as we can now control the cluster from any of the nodes. This procedure is exemplified in the following screenshot (where we set up the authentication for <code class="email">pcs</code> from <code class="email">node01</code>), and later when we create the cluster itself issuing the command in <code class="email">node02</code>, from where the <code class="email">/etc/corosync/corosync.conf</code> file is synchronized to the other node:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Managing authentication and creating the cluster" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To create the <a id="id92" class="calibre1"/>cluster using the specified nodes, type (on one node only, after successfully trying the password as illustrated in the preceding screenshot) the<a id="id93" class="calibre1"/> following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster setup --name MyCluster node01 node02</strong></span></pre></div><p class="calibre8">Here, <code class="email">MyCluster</code> is the name we have chosen for our cluster (and you may want to change it according to your liking). Next, press <span class="strong"><em class="calibre9">Enter</em></span> and verify the output. Note that it is this command that creates the cluster configuration file in <code class="email">/etc/corosync/corosync.conf</code> on both nodes.</p><p class="calibre8">If you created the <code class="email">corosync.conf</code> file using the sample configuration file as instructed earlier in this chapter (in order to start pacemaker and corosync), you will have to use the <code class="email">--force</code> option to overwrite that file with the current settings of the newly created cluster:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# pcs cluster setup --name MyCluster node01 node02</strong></span>
<span class="strong"><strong class="calibre2">Error: /etc/corosync/corosync.conf already exists, use --force to overwrite</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]# pcs cluster setup --name MyCluster node01 node02 --force</strong></span>
<span class="strong"><strong class="calibre2">Shutting down pacemaker/corosync services...</strong></span>
<span class="strong"><strong class="calibre2">Redirecting to /bin/systemctl stop  pacemaker.service</strong></span>
<span class="strong"><strong class="calibre2">Redirecting to /bin/systemctl stop  corosync.service</strong></span>
<span class="strong"><strong class="calibre2">Killing any remaining services...</strong></span>
<span class="strong"><strong class="calibre2">Removing all cluster configuration files...</strong></span>
<span class="strong"><strong class="calibre2">node01: Succeeded</strong></span>
<span class="strong"><strong class="calibre2">node02: Succeeded</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note06" class="calibre1"/>Note</h3><p class="calibre8">If you get the following error message while trying to set up the <code class="email">pcs</code> authentication. Ensure that <code class="email">pcsd</code> is running (and enabled) on <code class="email">nodeXX</code>, and try again:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">Error: Unable to communicate with nodeXX</strong></span></pre></div></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note07" class="calibre1"/>Note</h3><p class="calibre8">(Here, <code class="email">XX</code> is the node number)</p></div><p class="calibre8">At this point, the <code class="email">/etc/corosync/corosync.conf</code> file in <code class="email">node02</code> should be identical to the same file in <code class="email">node01</code>, as can be seen in the output of the following <code class="email">diff</code> command, when run<a id="id94" class="calibre1"/> from either node. An empty output indicates<a id="id95" class="calibre1"/> that the corosync configuration file has been correctly synced from one node to the other:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">diff /etc/corosync/corosync.conf &lt;(ssh node02 'cat /etc/corosync/corosync.conf')</strong></span></pre></div><p class="calibre8">The next step consists of actually starting the cluster by issuing the command (again, on one node only):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster start --all</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note08" class="calibre1"/>Note</h3><p class="calibre8">The command that is used to start the cluster (<code class="email">pcs cluster start</code>) deserves further clarification:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">  start [--all] [node] [...]</strong></span>
<span class="strong"><strong class="calibre2">     Start corosync &amp; pacemaker on specified node(s), if a node is not</strong></span>
<span class="strong"><strong class="calibre2">     specified then corosync &amp; pacemaker are started on the local node.</strong></span>
<span class="strong"><strong class="calibre2">     If --all is specified then corosync &amp; pacemaker are started on all</strong></span>
<span class="strong"><strong class="calibre2">     nodes.</strong></span></pre></div></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note09" class="calibre1"/>Note</h3><p class="calibre8">There will be times when you want to start the cluster on a specific node. In that case, you will name such a node instead of using the <code class="email">--all</code> flag.</p></div><p class="calibre8">The output to the preceding command should be as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster start --all</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]# pcs cluster start --all</strong></span>
<span class="strong"><strong class="calibre2">node01: Starting Cluster...</strong></span>
<span class="strong"><strong class="calibre2">node02: Starting Cluster...</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">Once the cluster has been started, you can check its status from any of the nodes (remember that PCS makes it possible for you to manage the cluster from any node):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 log]# pcs status cluster</strong></span>
<span class="strong"><strong class="calibre2">Cluster Status:</strong></span>
<span class="strong"><strong class="calibre2"> Last updated: Sat Dec  5 11:59:14 2015         Last change: Sat Dec  5 11:53:01 2015 by root via cibadmin on node01</strong></span>
<span class="strong"><strong class="calibre2"> Stack: corosync</strong></span>
<span class="strong"><strong class="calibre2"> Current DC: node02 (version 1.1.13-a14efad) - partition with quorum</strong></span>
<span class="strong"><strong class="calibre2"> 2 nodes and 0 resources configured</strong></span>
<span class="strong"><strong class="calibre2"> Online: [ node01 node02 ]</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 log]#or just pcs status:</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 log]# pcs status</strong></span>
<span class="strong"><strong class="calibre2">Cluster name: MyCluster</strong></span>
<span class="strong"><strong class="calibre2">WARNING: no stonith devices and stonith-enabled is not false</strong></span>
<span class="strong"><strong class="calibre2">Last updated: Sat Dec  5 11:55:43 2015          Last change: Sat Dec  5 11:53:01 2015 by root via cibadmin on node01</strong></span>
<span class="strong"><strong class="calibre2">Stack: corosync</strong></span>
<span class="strong"><strong class="calibre2">Current DC: node02 (version 1.1.13-a14efad) - partition with quorum</strong></span>
<span class="strong"><strong class="calibre2">2 nodes and 0 resources configured</strong></span>

<span class="strong"><strong class="calibre2">Online: [ node01 node02 ]</strong></span>

<span class="strong"><strong class="calibre2">Full list of resources:</strong></span>


<span class="strong"><strong class="calibre2">PCSD Status:</strong></span>
<span class="strong"><strong class="calibre2">  node01: Online</strong></span>
<span class="strong"><strong class="calibre2">  node02: Online</strong></span>

<span class="strong"><strong class="calibre2">Daemon Status:</strong></span>
<span class="strong"><strong class="calibre2">  corosync: active/disabled</strong></span>
<span class="strong"><strong class="calibre2">  pacemaker: active/disabled</strong></span>
<span class="strong"><strong class="calibre2">  pcsd: active/enabled</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note10" class="calibre1"/>Note</h3><p class="calibre8">The <code class="email">pcs status</code> command provides more detailed information, including the status <a id="id96" class="calibre1"/>of services and resources. It is possible<a id="id97" class="calibre1"/> that you notice that one of the nodes is <code class="email">OFFLINE</code>, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">Online: [ node01 ]</strong></span>
<span class="strong"><strong class="calibre2">OFFLINE: [ node02 ]</strong></span></pre></div></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note11" class="calibre1"/>Note</h3><p class="calibre8">In this case, ensure that both pacemaker and corosync are enabled (as indicated after the <code class="email">Daemon status: line</code>) and started on the node that's marked as <code class="email">OFFLINE</code>, and then perform <code class="email">pcs status</code> again.</p><p class="calibre8">Another issue you may encounter is having one or more of the nodes in an unclean state. While that is not common, resyncing the nodes by stopping and restarting the cluster on both nodes will fix it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster stop</strong></span>
<span class="strong"><strong class="calibre2">pcs cluster start</strong></span></pre></div></div><p class="calibre8">The node that is marked as <span class="strong"><strong class="calibre2">DC</strong></span>, that is, <span class="strong"><strong class="calibre2">Designated Controller</strong></span>, is the node where the cluster was<a id="id98" class="calibre1"/> originally started and from where the cluster-related commands will be typically issued. If for some reason, the current DC fails, a new designated controller is chosen automatically from the remaining nodes. You can see which node is the current DC with:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status | grep -i dc</strong></span></pre></div><p class="calibre8">To see the current DC in your cluster, do:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# pcs status | grep -i dc</strong></span>
<span class="strong"><strong class="calibre2">Current DC: node02 (version 1.1.13-a14efad) - partition with quorum</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">You <a id="id99" class="calibre1"/>will <a id="id100" class="calibre1"/>also want to check on each node individually:</p><p class="calibre8">The <code class="email">pcs status nodes</code> command allows you to view all information about the cluster and its configured resources:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# pcs status nodes</strong></span>
<span class="strong"><strong class="calibre2">Pacemaker Nodes:</strong></span>
<span class="strong"><strong class="calibre2">Online: node01 node02</strong></span>
<span class="strong"><strong class="calibre2">Standby:</strong></span>
<span class="strong"><strong class="calibre2">Offline:</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">The <code class="email">corosync-cmapctl</code> command is another tool for accessing the cluster's object database, where you will be able to view the properties and configuration of each node. Since the output of <code class="email">corosync-cmapctl</code> command is rather lengthy, you may want to filter by a chosen keyword, such as <code class="email">members</code> or <code class="email">cluster_name</code>, for example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# corosync-cmapctl | grep -Ei 'cluster'_name|members'</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.1.config_version (u64) = 0</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(192.168.0.2)</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.1.join_count (u32) = 1</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.1.status (str) = joined</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.2.config_version (u64) = 0</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(192.168.0.3)</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.2.join_count (u32) = 1</strong></span>
<span class="strong"><strong class="calibre2">runtime.totem.pg.mrp.srp.members.2.status (str) = joined</strong></span>
<span class="strong"><strong class="calibre2">totem.cluster_name (str) = MyCluster</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">As you<a id="id101" class="calibre1"/> can see, the preceding output allows you to see<a id="id102" class="calibre1"/> the name of your cluster, the IP address, and the status of each member.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Setting up a virtual IP for the cluster" id="K0RQ1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec15" class="calibre1"/>Setting up a virtual IP for the cluster</h1></div></div></div><p class="calibre8">As mentioned in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>, since a cluster is by definition<a id="id103" class="calibre1"/> a group of computers (which we have been<a id="id104" class="calibre1"/> referring to as nodes or members) that work together so that the set is seen as a single system from the outside, we need to ensure that end users and clients see it that way.</p><p class="calibre8">For this reason, the last thing that we will do in this chapter is configure a virtual IP, which is the address that external clients will use to connect to our cluster. Note that in an ordinary, non-cluster environment, you can use tools, such as <code class="email">ifconfig</code> to configure a virtual IP for your system.</p><p class="calibre8">However, in our case, we will use nothing more and nothing less than PCS and perform two operations at once:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Creating the IPv4 address</li><li class="listitem">Assigning it to the cluster as a whole</li></ul></div></div>

<div class="book" title="Setting up a virtual IP for the cluster" id="K0RQ1-1d2c6d19b9d242db82da724021b51ea0">
<div class="book" title="Adding a virtual IP as a cluster resource"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec17" class="calibre1"/>Adding a virtual IP as a cluster resource</h2></div></div></div><p class="calibre8">Since a<a id="id105" class="calibre1"/> virtual IP is what is called a <span class="strong"><strong class="calibre2">cluster resource</strong></span>, we <a id="id106" class="calibre1"/>will use <code class="email">pcs resource help</code> to look<a id="id107" class="calibre1"/> for information on to how to create it. You will need, in advance, to pick an IP address that is not being used in your LAN to assign to the virtual IP resource. After the virtual IP is initialized, you can ping it as usual to confirm its availability.</p><p class="calibre8">To create the virtual IP named <code class="email">virtual_ip</code> with the address <code class="email">192.168.0.4/24</code>, monitored everything 30 seconds on <code class="email">enp0s3</code>, run the following command on either node:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource create virtual_ip ocf:heartbeat:IPaddr2 ip=192.168.0.4 cidr_netmask=24 nic=enp0s3 op monitor interval=30s</strong></span></pre></div><p class="calibre8">Up to this point, the virtual IP resource will show as stopped in the output of <code class="email">pcs cluster status</code> or <code class="email">pcs status</code> until a later stage when we will disable STONITH (which is <a id="id108" class="calibre1"/>a cluster feature that is explained in the <a id="id109" class="calibre1"/>next section).</p></div></div>

<div class="book" title="Setting up a virtual IP for the cluster" id="K0RQ1-1d2c6d19b9d242db82da724021b51ea0">
<div class="book" title="Viewing the status of the virtual IP"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec18" class="calibre1"/>Viewing the status of the virtual IP</h2></div></div></div><p class="calibre8">To view<a id="id110" class="calibre1"/> the current status of cluster resources use the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status resources</strong></span></pre></div><p class="calibre8">In case the newly created virtual IP is not started automatically, you will want to perform a more thorough check, including a verbose output of the configuration used by the running cluster as provided by <code class="email">crm_verify</code>, a tool that is part of the pacemaker cluster resource manager:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# crm_verify -L -V</strong></span>
<span class="strong"><strong class="calibre2">   error: unpack_resources:     Resource start-up disabled since no STONITH resources have been defined</strong></span>
<span class="strong"><strong class="calibre2">   error: unpack_resources:     Either configure some or disable STONITH with the stonith-enabled option</strong></span>
<span class="strong"><strong class="calibre2">   error: unpack_resources:     NOTE: Clusters with shared data need STONITH to ensure data integrity</strong></span>
<span class="strong"><strong class="calibre2">Errors found during check: config not valid</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note12" class="calibre1"/>Note</h3><p class="calibre8"><span class="strong"><strong class="calibre2">STONITH</strong></span>, an acronym for <span class="strong"><strong class="calibre2">Shoot The Other Node In The Head</strong></span>, represents a cluster <a id="id111" class="calibre1"/>feature that prevents nodes in a high-availability cluster from becoming active at the same time, and thus serving the same content.</p></div><p class="calibre8">As the preceding error message indicates, clusters with shared data need STONITH to ensure data integrity. However, we will defer the appropriate discussion for this feature for the next chapter, and we will disable it for the time being in order to be able to show how the virtual IP is started and becomes accessible. On the other hand, when <code class="email">crm_verify –L –V</code> does not return any output, it means that the configuration is valid and free from errors.</p><p class="calibre8">Go ahead and disable STONITH but keep in mind that we will return to this in the next chapter:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs property set stonith-enabled=false</strong></span></pre></div><p class="calibre8">Next, check the cluster status again.</p><p class="calibre8">The <a id="id112" class="calibre1"/>resource should now show as started when you query the cluster status. You can check the resource availability by pinging it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">ping -c 4 192.168.0.4</strong></span></pre></div><p class="calibre8">If the ping operation returns a warning that some packets were not delivered to destination, refer to <code class="email">/var/log/pacemaker.log</code> or <code class="email">/var/log/cluster/corosync.log</code> for information on what could have failed.</p></div></div>
<div class="book" title="Summary" id="KVCC1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec16" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, you learned how to set up and configure the basic required network infrastructure and also the clustering components that we installed in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>. Having reviewed the concepts associated with security, firewalling, and Internet protocols, we were able to add the firewall rules that will allow the communication of each node with each other and the proper operation of the clustering services on each box.</p><p class="calibre8">We will use the tools discussed in this article throughout the rest of this book, not only to check on the status of the cluster or the individual nodes, but also as a troubleshooting technique in case things don't go as expected.</p></div></body></html>