<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Implementing iSCSI SANs" id="aid-UGI01"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Implementing iSCSI SANs</h1></div></div></div><p>A really big change that you may notice on RHEL 7 compared with earlier releases is that the iSCSI target service is now part of the kernel. This is part and parcel of the move to version 3.x.x of the Linux kernel that we see in the latest Enterprise offering from Raleigh, NC. The management and configuration of the iSCSI server or target is completely revamped. You will soon learn how to share disks and partitions with devices on your network. In doing so, we will look at how to configure the following prerequisites:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The iSCSI target (server)</li><li class="listitem">Logical volumes with LVM</li><li class="listitem">Installing the <code class="literal">targetd</code> service and <code class="literal">targetcli</code> tools</li><li class="listitem">Managing iSCSI targets using <code class="literal">targetcli</code></li><li class="listitem">The iSCSI initiator client</li></ul></div><div class="section" title="The iSCSI target (server)"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/>The iSCSI target (server)</h1></div></div></div><p>The iSCSI target is a <a id="id136" class="indexterm"/>software that makes disk space available on a network. This service shares disks rather than filesystems and establishes a <span class="strong"><strong>Storage Area Network</strong></span> (<span class="strong"><strong>SAN</strong></span>). This SAN storage can be used so that servers can share same disks, a situation <a id="id137" class="indexterm"/>that is often required where other network services are clustered and need access to shared disks on the network. It's usual that only one server will have access to each shared disk at any one time. The iSCSI target can share complete disks, but it's often more efficient to share the exact space required by the client through logical volumes implemented with LVMs. As well as sharing block devices, it's also possible to create files and share the file space as disks through the target server.</p><p>The disk IO passes through standard network connections to iSCSI servers. So, the faster the network connection, the better the storage performance. Although iSCSI will work on 1 GB Ethernet networks, 10 GB Ethernet is preferred for Enterprise usage. Having said that, for home or small office use, you will find that 1 GB network speed should be fine (especially if you can define a separate network segment to isolate the iSCSI traffic from the rest of your network). The <a id="id138" class="indexterm"/>default TCP port used by the target is <code class="literal">3260</code>.</p></div></div>
<div class="section" title="Managing logical volumes with LVM"><div class="titlepage" id="aid-VF2I2"><div><div><h1 class="title"><a id="ch04lvl1sec27"/>Managing logical volumes with LVM</h1></div></div></div><p>Although <a id="id139" class="indexterm"/>we can share the entire disk space or disk partitions, it really makes sense to share just the disk space that a client service requires. So, for example, if a web server requires 20 GB of space for web files, we can share just <a id="id140" class="indexterm"/>that absolute space, rather than the whole disk that may be terabytes in size. To do so, we will create logical volumes and share these as block devices. To implement LVMs, we create three objects:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Physical volumes</strong></span>: This represents the raw disk space as disk partitions. When we use <a id="id141" class="indexterm"/>partitions, the partition type should be set to <code class="literal">Linux LVM</code> with an ID of <code class="literal">8E</code> using the <code class="literal">fdisk</code> partitioning tool.</li><li class="listitem"><span class="strong"><strong>Volume groups</strong></span>: This aggregates physical volumes together so that the disk space <a id="id142" class="indexterm"/>can be consumed to logical volumes.</li><li class="listitem"><span class="strong"><strong>Logical volumes</strong></span>: This represents the block device that can be shared. It consumes space <a id="id143" class="indexterm"/>that is allocated from volume groups.</li></ul></div><p>On the demonstration RHEL 7.1 system that we will use for this course, I have three disks attached currently. We can use part of the space on the third drive for the LVM system. We will start by partitioning the third disk (currently unpartitioned) so that we can use some elements of this disk for LVM and other elements for other filesystems.</p><div class="section" title="Partitioning the disk"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec26"/>Partitioning the disk</h2></div></div></div><p>Using the <code class="literal">fdisk</code> <a id="id144" class="indexterm"/>command, we can partition the disk as required. We will use a single extended partition and create logical partitions therein. This is purely to allow many partitions that we can use here and in later chapters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo fdisk /dev/sdc</strong></span>
</pre></div><p>Take care with the device name that you use!</p><p>When you run the <code class="literal">fdisk</code> command, you will be presented with a menu. The <code class="literal">m</code> command can be used to see menu choices, but we can use <code class="literal">n</code> to create a new partition and then <code class="literal">e</code> to create an extended partition. We will enter to accept the defaults for the partition number and the start and end sectors. We will set the extended partition to use the complete disk.</p><p>Now, we will use <code class="literal">n</code> again to create another new partition; this time we will choose <code class="literal">l</code> for logical reasons. The partition number will default to <code class="literal">5</code>, so <code class="literal">/dev/sdc5</code> in my case3. We can accept the default starting sector, but we will limit the size to 200 M with <code class="literal">+200M</code> as the ending sector. The following screenshot illustrates this setting:</p><div class="mediaobject"><img src="../Images/image00225.jpeg" alt="Partitioning the disk"/></div><p style="clear:both; height: 1em;"> </p><p>With the settings entered <a id="id145" class="indexterm"/>and still within the interactive <code class="literal">fdisk</code> command, we can use the <code class="literal">t</code> option to set a type. By default, this will be set to <code class="literal">83</code>. When you enter <code class="literal">t,</code> you will be asked for the partition number, which will default to <code class="literal">5</code>. To type the partition code, we will use <code class="literal">8e</code> for LVM. Subsequently, we will use <code class="literal">p</code> to print the configuration and then <code class="literal">w</code> to save the changes and exit the program.</p></div><div class="section" title="Creating the physical volume"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Creating the physical volume</h2></div></div></div><p>So far, we have <a id="id146" class="indexterm"/>created a partition for LVM to use, but this is not part of any <a id="id147" class="indexterm"/>LVM system yet. To mark it as available, we will use the <code class="literal">pvcreate</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo pvcreate /dev/sdc5</strong></span>
</pre></div><p>To display the LVM physical volume on a system, you can use either the <code class="literal">pvs</code> command or the <code class="literal">pvscan</code> command as the root user. The output of <code class="literal">sudo pvscan</code> is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00226.jpeg" alt="Creating the physical volume"/></div><p style="clear:both; height: 1em;"> </p><p>The output shows that we already have LVM in use on this system because this is the default on RHEL and many other systems. The new <code class="literal">PV</code> is shown as <code class="literal">/dev/sdc5</code>, but without any <a id="id148" class="indexterm"/>membership of a <span class="strong"><strong>volume group</strong></span> (<span class="strong"><strong>VG</strong></span>).</p></div><div class="section" title="Creating the volume group"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec28"/>Creating the volume group</h2></div></div></div><p>As we already <a id="id149" class="indexterm"/>have a volume group in place, we will use that group and extend it to include the new PV using the <code class="literal">vgextend</code> command. We want to keep this volume <a id="id150" class="indexterm"/>group separate and solely for space to be shared with the iSCSI Target. For this reason, we will create a new volume group with the <code class="literal">vgcreate</code> command, as shown in the following code example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vgcreate iscsi /dev/sdc5</strong></span>
</pre></div><p>Using this command, we will create a new volume group called <code class="literal">iscsi</code> and use <code class="literal">/dev/sdc5</code> PV.</p><p>Similar to physical volumes, we can use <code class="literal">vgscan</code> or <code class="literal">vgs</code> to display information on the volume groups that are available. The output from <code class="literal">sudo vgs</code> is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00227.jpeg" alt="Creating the volume group"/></div><p style="clear:both; height: 1em;"> </p><p>From the preceding output, we can see that our newly created <code class="literal">VG</code> named <code class="literal">iscsi</code> has a single <code class="literal">PV</code> connected to it, but as yet, no logical volumes (<code class="literal">LV</code>), which we will create next.</p></div><div class="section" title="Creating logical volumes"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Creating logical volumes</h2></div></div></div><p>
<span class="strong"><strong>Logical volumes</strong></span> (<span class="strong"><strong>LVs</strong></span>) are block device units that we can use locally or (in our case) share via iSCSI. We <a id="id151" class="indexterm"/>create LVs using the <code class="literal">lvcreate</code> command. An <a id="id152" class="indexterm"/>example is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo lvcreate -n web -L 100M iscsi</strong></span>
</pre></div><p>As is normal, we will not use all of the available space and just use the space requested by the web team for their new web volume. The <code class="literal">-L</code> option sets the size we allocate. We will allocate <code class="literal">100M</code>; <code class="literal">-n</code> sets the name to web in this case, whereas the <code class="literal">VG</code> name is appended to the end of the command string.</p><p>This command will create a block device in the <code class="literal">/dev</code> directory, but this device is usually accessed via symbolic links. The following two symbolic links will be created:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">/dev/mapper/iscsi-web</code></li><li class="listitem"><code class="literal">/dev/iscsi/web</code></li></ul></div><p>In our case, these link to the <code class="literal">/dev/dm-2</code> block device. On your system, the actual block device name will depend on how many existing LVs you have. This is why the OS uses symbolic links because this name is determinable, whereas the actual block device name is not so determinable.</p><p>If you can detect a pattern here, you will realize that, in order to display information about LVs on a system, we can use <code class="literal">lvs</code> or <code class="literal">lvscan</code>. The output of the <code class="literal">sudo lvscan</code> command is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00228.jpeg" alt="Creating logical volumes"/></div><p style="clear:both; height: 1em;"> </p><p>At this stage, we have a working block device that we can use with the iSCSI target service to share between <a id="id153" class="indexterm"/>servers on the network. We will now look at how to <a id="id154" class="indexterm"/>configure the iSCSI target on RHEL 7.1.</p></div></div>
<div class="section" title="Installing the targetd service and targetcli tools" id="aid-10DJ41"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Installing the targetd service and targetcli tools</h1></div></div></div><p>To manage the <a id="id155" class="indexterm"/>kernel-based iSCSI Target service on RHEL 7, we will <a id="id156" class="indexterm"/>need to install the <code class="literal">targetd</code> and <code class="literal">targetcli</code> package, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo yum install targetd targetcli</strong></span>
</pre></div><p>From the output, we can see that additional packages are installed; however, it's more interesting to see how Python is used as a major tool to manage iSCSI. The following screenshot is an extract from the command line output:</p><div class="mediaobject"><img src="../Images/image00229.jpeg" alt="Installing the targetd service and targetcli tools"/></div><p style="clear:both; height: 1em;"> </p><p>Although the iSCSI target runs as part of the kernel, the <code class="literal">targetd</code> package provides a service. This service is used to load the iSCSI target configuration. This is all that <code class="literal">targetd</code> does, so we <a id="id157" class="indexterm"/>never need to start this service as such; just ensure that <a id="id158" class="indexterm"/>
<code class="literal">targetd</code> is enabled for autostart, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo systemctl enable targetd</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="tip14"/>Tip</h3><p>Once the system startup enables the <code class="literal">targetd</code> service, it ensures that the <code class="literal">targetcli restoreconfig</code> command is executed. It also ensures that the current configuration is loaded on boot.</p></div><p>We have now installed management tools for the <code class="literal">targetcli</code> iSCSI target and the <code class="literal">targetd</code> service, which provides a mechanism to enable the configuration to be read at boot time. We will now move on to how to configure the target with <code class="literal">targetcli</code>.</p></div>
<div class="section" title="Managing iSCSI targets with targetcli" id="aid-11C3M1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Managing iSCSI targets with targetcli</h1></div></div></div><p>The <a id="id159" class="indexterm"/>
<code class="literal">targetcli</code> command is a shell to <a id="id160" class="indexterm"/>view, edit, save, and load the iSCSI target configuration. When you look at the configuration, you will see that <code class="literal">targetcli</code> provides a hierarchical structure in a similar way to a filesystem.</p><p>To invoke the <code class="literal">targetcli</code> shell, we will run this command as root using <code class="literal">sudo</code>. You will see that on the first run of the command, a preferences file is created. This is illustrated in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00230.jpeg" alt="Managing iSCSI targets with targetcli"/></div><p style="clear:both; height: 1em;"> </p><p>As you can see in the preceding output, you can enter <code class="literal">help</code> to display a list of commands that can be <a id="id161" class="indexterm"/>entered. To view the available <a id="id162" class="indexterm"/>configuration objects, we can use the <code class="literal">ls</code> command. The output is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00231.jpeg" alt="Managing iSCSI targets with targetcli"/></div><p style="clear:both; height: 1em;"> </p><p>We will work with <code class="literal">backstores</code> objects to start with so that we can add it to the LVM block device in the configuration in addition to the <code class="literal">fileio</code> backstore. As the name suggests, this <a id="id163" class="indexterm"/>will be a file within the <a id="id164" class="indexterm"/>filesystem; we can share this to a network as a virtual disk.</p><div class="section" title="Creating storage backstores"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Creating storage backstores</h2></div></div></div><p>We will work <a id="id165" class="indexterm"/>from the root of the <code class="literal">targetcli</code> configuration; this should be exactly where we are, but we can always use the <code class="literal">pwd</code> command to display our working directory. If required, we can change it to the root of the configuration with <code class="literal">cd /</code>.</p><div class="note" title="Note"><h3 class="title"><a id="tip15"/>Tip</h3><p>While using the <code class="literal">targetcli</code> command, we can use <span class="emphasis"><em>CTRL</em></span> + <span class="emphasis"><em>L</em></span> to clear the screen as we would in Bash, but most importantly, the <span class="emphasis"><em>Tab</em></span> key completion works, so we do not need to type the complete name or path to objects and properties.</p></div><p>To create a new <code class="literal">block</code>, back store on the LVM LV that we created earlier in this section. If we recall, this was <code class="literal">/dev/iscsi/web</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; backstore/block/ create web_lv /dev/iscsi/web</strong></span>
</pre></div><p>This will create the block backstore with a name called <code class="literal">web_lv</code>. Using the <code class="literal">ls</code> command again will list the additional object within the hierarchy. In the following screenshot, we see the creation of the backstore and the subsequent listing:</p><div class="mediaobject"><img src="../Images/image00232.jpeg" alt="Creating storage backstores"/></div><p style="clear:both; height: 1em;"> </p><p>We will also add a new backstore called <code class="literal">fileio</code>. The creation of the new backstore is similar to the root of the configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; backstores/fileio create file_store /tmp/fs 100M</strong></span>
</pre></div><p>This command will create the backstore and the physical file, which we will use as a virtual disk. If the file already exists, we omit the size parameter. Both these objects will show in the listing if we choose to use the <code class="literal">ls</code> command again.</p><p>Other backstore types include <code class="literal">pscsi</code> and <code class="literal">ramdisk</code>. These represent <code class="literal">Passthrough SCSI</code> connections that <a id="id166" class="indexterm"/>refer to physical iSCSI devices and <code class="literal">Memory Based Disks</code>, which, as with <code class="literal">fileio</code>, can be created on the fly using <code class="literal">targetcli</code>.</p></div></div>
<div class="section" title="Creating iSCSI targets" id="aid-12AK81"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Creating iSCSI targets</h1></div></div></div><p>The iSCSI objects that we <a id="id167" class="indexterm"/>see in the main list represents iSCSI targets and their properties. Firstly, we will create a simple iSCSI target with default names. We can then delete this object and see how to create our own target with the correct naming convention:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; iscsi/ create</strong></span>
</pre></div><p>This will create an iSCSI target and listen on the TCP port <code class="literal">3260</code>. There will not be any LUNS or backstores connected, and the <span class="strong"><strong>IQN</strong></span> (<span class="strong"><strong>iSCSI Qualified Name</strong></span>) will be system generated. We <a id="id168" class="indexterm"/>can always add the backstore, but most likely, we want to use our own name. So, in this case, we will delete the object. The IQN on my system was generated as <code class="literal">iqn.2003-01.org.linux-iscsi.redhat7.x8664:sn.ce1ebea336a2</code>, but do not forget that we can use the <span class="emphasis"><em>Tab</em></span> key completion. So, we do not need to write the complete name while deleting or editing it. The following command displays this, but it may wrap when displayed or printed so that it is executed as a single line of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; iscsi/ delete iqn.2003-01.org.linux-iscsi.redhat7.x8664:sn.ce1ebea336a2</strong></span>
</pre></div><p>We will now create an iSCSI target by supplying a custom IQN. To perform this, we create the object as before, but this time, specify the name that is usually written to contain the date and the reversed DNS name. The following command is an example that we will use in this book:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; iscsi/ create iqn.2015-01.com.tup.rhel7:web</strong></span>
</pre></div><p>IQN starts with <code class="literal">iqn</code>, which is followed by the year and month it was created and the reverse DNS name. We can add the description of the target with the <code class="literal">:web</code> at the end, indicating that this is a target for the web server.</p><p>We can filter what is displayed using the <code class="literal">ls</code> command by adding the object hierarchy that we want to list. For example, to list targets, we will use the <code class="literal">ls iscsi</code> command.</p><p>The output of this command is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00233.jpeg" alt="Creating iSCSI targets"/></div><p style="clear:both; height: 1em;"> </p><p>Now we have our <a id="id169" class="indexterm"/>customized name for the target, but we still have to add the LUNS or logical units to make the <span class="strong"><strong>SAN</strong></span> (<span class="strong"><strong>Storage Area Network</strong></span>) effective.</p><div class="section" title="Adding LUNS to the iSCSI target"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Adding LUNS to the iSCSI target</h2></div></div></div><p>Staying with the <a id="id170" class="indexterm"/>
<code class="literal">targetcli</code> shell, we will now move on to our target and <span class="strong"><strong>TPG</strong></span> (<span class="strong"><strong>Target Portal Group</strong></span>) object. Similar to the filesystem, this is achieved <a id="id171" class="indexterm"/>using the <code class="literal">cd</code> command, as shown in the following <a id="id172" class="indexterm"/>command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/&gt; cd iscsi/iqn.2015-01.com.tup.rhel:web/tpg1/</strong></span>
</pre></div><p>We can run <code class="literal">ls</code> from here, but the content was included in the previous listing that we ran from the root of the configuration. We have one portal that listens on all IPv4 interfaces on the TCP port <code class="literal">3260</code>. Currently, we have no <code class="literal">acls</code> or <code class="literal">luns</code>. To add a LUN, we will use the following command, which will utilize the LVM block backstore:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/iscsi/iqn.20...rhel:web/tpg1&gt; luns/ create /backstores/block/web_lv</strong></span>
</pre></div><p>This will have an additional side effect of activating a backstore. This can be seen by listing the <code class="literal">/backstores</code> object. The command and output are shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00234.jpeg" alt="Adding LUNS to the iSCSI target"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Adding ACLS"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Adding ACLS</h2></div></div></div><p>We are not required <a id="id173" class="indexterm"/>to add ACLS, but often we only want a single host, perhaps a virtual cluster device in order to access the LUN. If there is no ACL, we will need to set a property so that the LUN does not default to read only.</p><p>To create an ACL, we limit the access from LUN to a given initiator name or names that we mention in <span class="strong"><strong>Access Control List</strong></span> (<span class="strong"><strong>ACL</strong></span>). The initiator is the iSCSI client and will have a unique client IQN configured on the initiator in the <code class="literal">/etc/iscsi/initiatorname.iscsi</code> file. If this file is not present, you will need to install the <code class="literal">iscsi-initiator-utils</code> package. The filename used to configure the initiator name will be consistent for Linux clients, but will differ for other operating systems. To add an ACL, we will remain with the current configuration hierarchy: <code class="literal">/iscsi/iqn….:web/tpg1</code> and issue the following command, again written as a single line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/iscsi/iqn.20...rhel:web/tpg1&gt; acls/ create iqn.2015-01.com.tup.web:web</strong></span>
</pre></div><p>This ACL restricts access to the initiator listed within the ACL. Be careful if you ever change the initiator name because the ACL will also need to be updated. The initiator is the iSCSI client.</p><p>Using the <code class="literal">ls</code> command from this location in the configuration hierarchy, we see the output similar to the following screenshot, which also includes the command to create the ACL:</p><div class="mediaobject"><img src="../Images/image00235.jpeg" alt="Adding ACLS"/></div><p style="clear:both; height: 1em;"> </p><p>If you do not add an ACL, the LUN will be read only. If you require the LUN to be writable, you will need to use the following command in order to set the required attribute:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/iscsi/iqn.20...rhel:web/tpg1&gt; set attribute demo_mode_write_protect=0</strong></span>
</pre></div><p>The iSCSI target is now configured. Exiting <code class="literal">targetcli</code> should save this configuration, but you may feel safer to manually save your changes.</p><p>To do this, return to the root of the configuration and enter the <code class="literal">saveconfig</code> command, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/iscsi/iqn.20...rhel:web/tpg1&gt; cd /</strong></span>
<span class="strong"><strong>/&gt; saveconfig</strong></span>
<span class="strong"><strong>/&gt; exit</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="tip16"/>Tip</h3><p>The <code class="literal">targetd</code> service that we enabled earlier in this chapter runs the <code class="literal">restoreconfig</code> command from <code class="literal">targetcli</code>. This is used to load the configuration when the system boots.</p></div><p>With the configuration saved, we <a id="id174" class="indexterm"/>can migrate to the client in order to look at the iSCSI Initiator and see the disk sharing at work on our SAN.</p></div></div>
<div class="section" title="Working with the iSCSI Initiator" id="aid-1394Q1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Working with the iSCSI Initiator</h1></div></div></div><p>The iSCSI Initiator or <a id="id175" class="indexterm"/>client on RHEL 7 is installed with the <code class="literal">iscsi-initiator-utils</code> package; you can verify that this is installed on your system using the <code class="literal">yum</code> command, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ yum list iscsi-initiator-utils</strong></span>
</pre></div><p>If it's listed as <code class="literal">Installed</code>, all well and good, but if it's listed as <code class="literal">Available</code>, you will need to install it.</p><p>For the purpose of this exercise, we will use a separate RHEL 7 system as our initiator and connect it to the existing target. We will need to edit the <code class="literal">/etc/iscsi/initiatorname.iscsi</code> file on the new RHEL 7 system to ensure that the name is set to match the name we added to the ACL in the earlier section of this chapter; we can display this using the <code class="literal">cat</code> command, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00236.jpeg" alt="Working with the iSCSI Initiator"/></div><p style="clear:both; height: 1em;"> </p><p>We will use the main client tool: <code class="literal">iscsiadm</code>. This was installed with the previously mentioned package. To discover iSCSI LUNS on the target, we will use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo iscsiadm --mode discovery --type sendtargets \</strong></span>
<span class="strong"><strong>--portal 192.168.40.3 --discover</strong></span>
</pre></div><p>The output should be similar to the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>192.168.40.3:3260,1 iqn.2015-01.com.tup.rhel7:web</strong></span>
</pre></div><p>Now, we have seen that we can connect to the iSCSI target and have it sent us the configured LUNS. We should now connect to this LUN and use the same command with the following options:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo iscsiadm --mode node \ </strong></span>
<span class="strong"><strong>--targetname iqn.2015-01.com.tup.rhel7:web \ </strong></span>
<span class="strong"><strong>--portal 192.168.40.3 --login</strong></span>
</pre></div><p>The following <a id="id176" class="indexterm"/>screenshot shows the command and output:</p><div class="mediaobject"><img src="../Images/image00237.jpeg" alt="Working with the iSCSI Initiator"/></div><p style="clear:both; height: 1em;"> </p><p>To the initiator, the shared LUN is now a disk. We can partition and format this disk in a normal manner. We will use <code class="literal">lsblk</code> to list the various connected block devices. On this system, we can see that it connects as <code class="literal">/dev/sdc</code> and matches the <code class="literal">100M</code> size that we assigned, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00238.jpeg" alt="Working with the iSCSI Initiator"/></div><p style="clear:both; height: 1em;"> </p><p>Using the traditional <code class="literal">fdisk</code> or <code class="literal">parted</code> commands, we can create a partition and then format it to be used locally on this system. As we used <code class="literal">fdisk</code> previously in the chapter to create the partition for LVM interactively, we will see how to manage this from the command line directly with <code class="literal">parted</code>.</p><p>The command will need a disk label in order to create the partition table. This can be set to <code class="literal">msdos</code> or <code class="literal">gpt</code>. The <code class="literal">fdisk</code> command creates the <code class="literal">msdos</code> label automatically, but this is because it can only work with traditional <code class="literal">msdos</code> partition tables. Parted can work with <code class="literal">msdos</code> and <code class="literal">gpt</code> (GUID partition tables). The <code class="literal">parted</code> command also allows partitions to be created either interactively or directly from the command line and hence, is scriptable. There is an added complication here, that is, the sectors to start a new partition are not shown. So, we need to figure out the optimal starting sector.</p><p>Once you know this for a disk of a given type, this will be the same for similar disks.</p><p>To establish the starting sector on a disk, we will read the values from two files: <code class="literal">/sys/block/sdc/queue/optimal_io_size</code> and divide this by <code class="literal">/sys/block/sdc/queue/physical_block_size</code>.</p><p>On the demonstration system, this relates to <span class="emphasis"><em>4194304 / 512 = 8192</em></span>; values from files can be read with the <code class="literal">cat</code> command as a standard user. Once we are aware of the optimal alignment details, we can label the disk and create the partition with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo parted /dev/sdc mklabel msdos</strong></span>
<span class="strong"><strong>$ sudo parted /dev/sdc mkpart primary 8192s 100%</strong></span>
</pre></div><p>We create a single partition: <code class="literal">/dev/sdc1</code>. When we start with the optimal starting sector, this partition uses 100 percent of the used disk space.</p><p>With this in place, we <a id="id177" class="indexterm"/>can format the partition with the filesystem of our choice and mount it in a normal manner. The <code class="literal">iscsid</code> background service is enabled, but it only runs when required. On reboot, the connection will be remade to the remote iSCSI Storage server so that the <code class="literal">/dev/sdc1</code> partition will persist on the client. This will happen as long as the default setting is not changed on the initiator. You should check the <code class="literal">/etc/iscsi/iscsid.conf</code> file and ensure that the setting is done as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node.startup = automatic</strong></span>
</pre></div><p>With this in place, which is the default on RHEL 7, the <code class="literal">iscsid</code> service will reconnect on startup.</p></div>
<div class="section" title="Summary" id="aid-147LC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Summary</h1></div></div></div><p>In this chapter, we have seen how ready RHEL 7 is for the Enterprise network to act as a SAN server using the new kernel-based iSCSI target server. The management of the server is now made through Python-based tools, such as <code class="literal">targetcli</code>, and the <code class="literal">targetd</code> service is there to load the configuration at boot. We often provide disk storage on demand from logical volumes. We also looked at how to use three components of LVM to make this happen: physical volumes, volume groups, and logical volumes.</p><p>With our storage created and shared, we looked at the second RHEL 7 system and how to connect it as an iSCSI Initiator to utilize this shared storage on the iSCSI target. This was managed initially using <code class="literal">iscsiadm</code>, but the connections are persisted through the <code class="literal">iscsid</code> service.</p><p>In the next chapter, we will take a look at the <span class="strong"><strong>BTRFS</strong></span> (<span class="strong"><strong>Better File System</strong></span>), which makes its first appearance on RHEL with version 7. I am sure that you will be impressed with what is on offer with this filesystem.</p></div></body></html>