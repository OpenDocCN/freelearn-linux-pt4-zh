<html><head></head><body>
<p id="filepos342077" class="calibre_"><span class="calibre1"><span class="bold">Chapter 5. CoreOS Networking and Flannel Internals</span></span></p><p class="calibre_8">Microservices increased the need to have lots of containers and also connectivity between containers across hosts. It is necessary to have a robust Container networking scheme to achieve this goal. This chapter will cover the basics of Container networking with a focus on how CoreOS does Container networking with Flannel. Docker networking and other related container networking technologies will also be covered. The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Container networking basics</li><li value="2" class="calibre_13">Flannel internals</li><li value="3" class="calibre_13">A CoreOS Flannel cluster using Vagrant, AWS, and GCE</li><li value="4" class="calibre_13">Docker networking and experimental Docker networking</li><li value="5" class="calibre_13">Docker networking using Weave and Calico</li><li value="6" class="calibre_13">Kubernetes networking</li></ul><div class="mbp_pagebreak" id="calibre_pb_122"/>


<p id="filepos343357" class="calibre_14"><span class="calibre1"><span class="bold">Container networking basics</span></span></p><p class="calibre_8">The <a/>following are the reasons why we need Container networking:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Containers need to talk to the external world.</li><li value="2" class="calibre_13">Containers should be reachable from the external world so that the external world can use the services that Containers provide.</li><li value="3" class="calibre_13">Containers need to talk to the host machine. An example can be sharing volumes.</li><li value="4" class="calibre_13">There should be inter-container connectivity in the same host and across hosts. An example is a WordPress container in one host talking to a MySQL container in another host.</li></ul><p class="calibre_8">Multiple solutions are <a/>currently available to interconnect Containers. These solutions are pretty new and actively under development. Docker, until release 1.8, did not have a native solution to interconnect Containers across hosts. Docker release 1.9 introduced a Libnetwork-based solution to interconnect containers across hosts as well as perform service discovery. CoreOS is using Flannel for container networking in CoreOS clusters. There are projects such as Weave and Calico that are developing Container networking solutions, and they plan to be a networking container plugin for any Container runtime such as Docker or Rkt.</p><div class="mbp_pagebreak" id="calibre_pb_123"/>


<p id="filepos344984" class="calibre_"><span class="calibre1"><span class="bold">Flannel</span></span></p><p class="calibre_8">Flannel<a/> is an open source project that provides a Container networking solution for CoreOS clusters. Flannel can also be used for non-CoreOS clusters. Kubernetes uses Flannel to set up networking between the Kubernetes pods. Flannel allocates a separate subnet for every host where a Container runs, and the Containers in this host get allocated an individual IP address from the host subnet. An overlay network is set up between each host that allows Containers on different hosts to talk to each other. In <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span> we provided an overview of the Flannel control and data path. This section will delve into the Flannel internals.</p><div class="mbp_pagebreak" id="calibre_pb_124"/>


<p id="filepos345829" class="calibre_9"><span class="calibre3"><span class="bold">Manual installation</span></span></p><p class="calibre_8">Flannel can be installed <a/>manually or using the <tt class="calibre2">systemd</tt> unit, <tt class="calibre2">flanneld.service</tt>. The <a/>following command will install Flannel in the CoreOS node using a container to build the flannel binary. The flanneld Flannel binary will be available in <tt class="calibre2">/home/core/flannel/bin</tt> after executing the following commands:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">git clone https://github.com/coreos/flannel.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -v /home/core/flannel:/opt/flannel -i -t google/golang /bin/bash -c "cd /opt/flannel &amp;&amp; ./build"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the Flannel version after we build flannel in our CoreOS node:</p><p class="calibre_9"><img src="images/00132.jpg" class="calibre_186"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_125"/>


<p id="filepos346911" class="calibre_9"><span class="calibre3"><span class="bold">Installation using flanneld.service</span></span></p><p class="calibre_8">Flannel<a/> is not installed by default in CoreOS. This<a/> is done to keep the CoreOS image size <a/>to a minimum. Docker requires flannel to configure the network and flannel requires Docker to download the flannel container. To avoid this chicken-and-egg problem, <tt class="calibre2">early-docker.service</tt> is started by default in CoreOS, whose primary purpose is to download the flannel container and start it. A regular <tt class="calibre2">docker.service</tt> starts the Docker daemon with the flannel network.</p><p class="calibre_8">The following figure shows you the sequence in <tt class="calibre2">flanneld.service</tt>, where the early Docker daemon starts the flannel container, which, in turn starts <tt class="calibre2">docker.service</tt> with the subnet created by flannel:</p><p class="calibre_9"><img src="images/00136.jpg" class="calibre_187"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the<a/> relevant section of <tt class="calibre2">flanneld.service</tt> that<a/> downloads the flannel container from <a/>the Quay repository:</p><p class="calibre_9"><img src="images/00285.jpg" class="calibre_188"/></p><p class="calibre_8">
</p><p class="calibre_8">The following<a/> output shows the early docker's running <a/>containers. Early-docker will manage <a/>Flannel only:</p><p class="calibre_9"><img src="images/00142.jpg" class="calibre_189"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the relevant section of <tt class="calibre2">flanneld.service</tt> that updates the docker options to use the subnet created by flannel:</p><p class="calibre_9"><img src="images/00146.jpg" class="calibre_190"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the content of <tt class="calibre2">flannel_docker_opts.env</tt>—in my case—after flannel was started. The address, <tt class="calibre2">10.1.60.1/24</tt>, is chosen by this CoreOS node for its containers:</p><p class="calibre_9"><img src="images/00042.jpg" class="calibre_191"/></p><p class="calibre_8">
</p><p class="calibre_8">Docker will be <a/>started as part of <tt class="calibre2">docker.service</tt>, as<a/> shown in the following screenshot, with the <a/>preceding environment file:</p><p class="calibre_9"><img src="images/00151.jpg" class="calibre_192"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_126"/>


<p id="filepos349801" class="calibre_9"><span class="calibre3"><span class="bold">Control path</span></span></p><p class="calibre_8">There is no central controller<a/> in flannel, and it uses etcd for internode communication. Each<a/> node in the CoreOS cluster runs a flannel agent and they communicate with each other using etcd.</p><p class="calibre_8">As part of starting the Flannel service, we specify the Flannel subnet that can be used by the individual nodes in the network. This subnet is registered with etcd so that every CoreOS node in the cluster can see it. Each node in the network picks a particular subnet range and registers atomically with etcd.</p><p class="calibre_8">The following is the relevant section of <tt class="calibre2">cloud-config</tt> that starts <tt class="calibre2">flanneld.service</tt> along with specifying the configuration for Flannel. Here, we have specified the subnet to be used for flannel as <tt class="calibre2">10.1.0.0/16</tt> along with the encapsulation type as <tt class="calibre2">vxlan</tt>:</p><p class="calibre_9"><img src="images/00155.jpg" class="calibre_97"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding configuration <a/>will create the following etcd key as seen in the node. This <a/>shows that <tt class="calibre2">10.1.0.0/16</tt> is allocated for flannel to be used across the CoreOS cluster and that the encapsulation type is <tt class="calibre2">vxlan</tt>:</p><p class="calibre_9"><img src="images/00309.jpg" class="calibre_193"/></p><p class="calibre_8">
</p><p class="calibre_8">Once each node gets a subnet, containers started in this node will get an IP address from the IP address pool allocated to the node. The following is the etcd subnet allocation per node. As we can see, all the subnets are in the <tt class="calibre2">10.1.0.0/16</tt> range that was configured earlier with etcd and with a 24-bit mask. The subnet length per host can also be controlled as a flannel configuration option:</p><p class="calibre_9"><img src="images/00161.jpg" class="calibre_194"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at <tt class="calibre2">ifconfig</tt> of the Flannel interface created in this node. The IP address is in the address range of <tt class="calibre2">10.1.0.0/16</tt>:</p><p class="calibre_9"><img src="images/00165.jpg" class="calibre_124"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_127"/>


<p id="filepos352423" class="calibre_9"><span class="calibre3"><span class="bold">Data path</span></span></p><p class="calibre_8">Flannel uses the<a/> Linux bridge to encapsulate the packets using an overlay protocol <a/>specified in the Flannel configuration. This allows for connectivity between containers in the same host as well as across hosts.</p><p class="calibre_8">The following are the major backends currently supported by Flannel and specified in the JSON configuration file. The JSON configuration file can be specified in the <span class="italic">Flannel</span> section of <tt class="calibre2">cloud-config</tt>:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">UDP</span>: In UDP encapsulation, packets from containers are encapsulated in UDP with the default port number <tt class="calibre2">8285</tt>. We can change the port number if needed.</li><a/><li value="2" class="calibre_13"><span class="bold">VXLAN</span>: From an encapsulation overhead perspective, VXLAN is efficient when compared to UDP. By default, port <tt class="calibre2">8472</tt> is used for VXLAN encapsulation. If we want to use an IANA-allocated VXLAN port, we need to specify the port field as <tt class="calibre2">4789</tt>.</li><a/><li value="3" class="calibre_13"><span class="bold">AWS-VPC</span>: This is applicable to using Flannel in the AWS VPC cloud. Instead of encapsulating the packets using an overlay, this approach uses a VPC route table to communicate across containers. AWS limits each VPC route table entry to 50, so this can become a problem with bigger clusters.</li><a/></ul><p class="calibre_8">The following is an example of specifying the AWS type in the flannel configuration:</p><p class="calibre_9"><img src="images/00168.jpg" class="calibre_36"/></p><p class="calibre_8">
</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">GCE</span>: This is applicable to using Flannel in the GCE cloud. Instead of encapsulating the packets using an overlay, this approach uses the GCE route table to communicate across containers. GCE limits each VPC route table entry to <tt class="calibre2">100</tt>, so this can become a problem with bigger clusters.</li><a/></ul><p class="calibre_8">The following is an example of specifying the GCE type in the Flannel configuration:</p><p class="calibre_9"><img src="images/00171.jpg" class="calibre_195"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's create containers in two different hosts with a VXLAN encapsulation and check whether the connectivity is fine. The following example uses a Vagrant CoreOS cluster with the Flannel service enabled.</p><p class="calibre_8">Configuration in Host1:</p><p class="calibre_8">Let's start a <tt class="calibre2">busybox</tt> container:</p><p class="calibre_9"><img src="images/00175.jpg" class="calibre_196"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's check the IP address allotted to the container. This IP address comes from the IP pool allocated to this CoreOS node by the flannel agent. <tt class="calibre2">10.1.19.0/24</tt> was allocated to <tt class="calibre2">host1</tt> and this container got the <tt class="calibre2">10.1.19.2</tt> address:</p><p class="calibre_9"><img src="images/00177.jpg" class="calibre_197"/></p><p class="calibre_8">
</p><p class="calibre_8">Configuration in Host2:</p><p class="calibre_8">Let's start a <tt class="calibre2">busybox</tt> container:</p><p class="calibre_9"><img src="images/00180.jpg" class="calibre_198"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's check the IP <a/>address allotted to this container. This IP address comes from<a/> the IP pool allocated to this CoreOS node by the flannel agent. <tt class="calibre2">10.1.1.0/24</tt> was allocated to <tt class="calibre2">host2</tt> and this container got the <tt class="calibre2">10.1.1.2</tt> address:</p><p class="calibre_9"><img src="images/00184.jpg" class="calibre_199"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows you the ping being successful between container 1 and container 2. This ping packet is travelling across the two CoreOS nodes and is encapsulated using VXLAN:</p><p class="calibre_9"><img src="images/00326.jpg" class="calibre_200"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_128"/>


<p id="filepos357237" class="calibre_9"><span class="calibre3"><span class="bold">Flannel as a CNI plugin</span></span></p><p class="calibre_8">As <a/>explained in <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>, APPC defines a <a/>Container specification that any Container runtime can use. For Container networking, APPC defines a <span class="bold">Container Network Interface</span> (<span class="bold">CNI</span>) specification. With CNI, the Container networking functionality can be implemented as a plugin. CNI expects plugins to support APIs with a set of parameters and the implementation is left to the plugin. The plugin implements APIs like adding a container to a network and removing container from the network with a defined parameter list. </p><p class="calibre_8">This allows the implementation of network plugins by different vendors and also the reuse of plugins across different Container runtimes. The following figure shows the relationship between the <a/>
<span class="bold">RKT</span> container runtime, <a/>
<span class="bold">CNI</span> layer, and <a/>
<span class="bold">Plugin like Flannel</span>. The <span class="bold">IPAM Plugin</span>
<a/> is used to allocate an IP address to the containers and this is nested inside the initial networking plugin:</p><p class="calibre_9"><img src="images/00328.jpg" class="calibre_201"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_129"/>


<p id="filepos358646" class="calibre_9"><span class="calibre3"><span class="bold">Setting up a three-node Vagrant CoreOS cluster with Flannel and Docker</span></span></p><p class="calibre_8">The following <a/>example sets up a<a/> three-node Vagrant <a/>CoreOS cluster with<a/> the <tt class="calibre2">etcd</tt>, <tt class="calibre2">fleet</tt>, and <tt class="calibre2">flannel</tt> services turned on by default. In this example, <tt class="calibre2">vxlan</tt> is used for encapsulation. The following is the <tt class="calibre2">cloud-config</tt> used for this:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: &lt;update this&gt;<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}'<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The <a/>following are the steps<a/> for this:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Clone the CoreOS Vagrant repository.</li><a/><li value="2" class="calibre_13">Change the instance count to three in <tt class="calibre2">config.rb</tt>.</li><a/><li value="3" class="calibre_13">Update the discovery token in the <tt class="calibre2">cloud-config</tt> user data.</li><li value="4" class="calibre_13">Perform <tt class="calibre2">vagrant up</tt> to start the cluster.</li></ol><p class="calibre_8">For more details on the steps, refer to <a href="index_split_048.html#filepos153225">Chapter 2</a>, <span class="italic">Setting up the CoreOS Lab</span>. We can test the container connectivity by starting <tt class="calibre2">busybox</tt> containers in both the hosts and checking that the ping is working between the two Containers.</p><div class="mbp_pagebreak" id="calibre_pb_130"/>


<p id="filepos361244" class="calibre_9"><span class="calibre3"><span class="bold">Setting up a three-node CoreOS cluster with Flannel and RKT</span></span></p><p class="calibre_8">Here, we <a/>will <a/>set up <a/>a three-node <a/>CoreOS cluster with RKT containers using the Flannel CNI networking plugin to set up the networking. This example will allow RKT containers across hosts to communicate with each other.</p><p class="calibre_8">The following is the <tt class="calibre2">cloud-config</tt> used:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: &lt;update token&gt;<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "network": "10.1.0.0/16" }'<br class="calibre4"/>      command: start<br class="calibre4"/># Rkt configuration<br class="calibre4"/>write_files:<br class="calibre4"/>  - path: "/etc/rkt/net.d/10-containernet.conf"<br class="calibre4"/>    permissions: "0644"<br class="calibre4"/>    owner: "root"<br class="calibre4"/>    content: |<br class="calibre4"/>      {<br class="calibre4"/>        "name": "containernet",<br class="calibre4"/>        "type": "flannel"<br class="calibre4"/>      }</tt></p><p class="calibre_8">The <tt class="calibre2">/etc/rkt/net.d/10-containernet.conf</tt> file sets up the CNI plugin type as Flannel and RKT<a/> containers use this.</p><p class="calibre_8">The following <a/>are the steps for this:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Clone the CoreOS Vagrant repository.</li><a/><li value="2" class="calibre_13">Change the instance count to three in <tt class="calibre2">config.rb</tt>.</li><a/><li value="3" class="calibre_13">Update the discovery token in the <tt class="calibre2">cloud-config</tt> user data.</li><li value="4" class="calibre_13">Perform <tt class="calibre2">vagrant up</tt> to start the cluster.</li></ol><p class="calibre_8">Let's start a <tt class="calibre2">busybox</tt> container in <tt class="calibre2">node1</tt>:</p><p class="calibre_9"><img src="images/00329.jpg" class="calibre_202"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">ifconfig</tt> output in <tt class="calibre2">busybox node1</tt> is as follows:</p><p class="calibre_9"><img src="images/00331.jpg" class="calibre_203"/></p><p class="calibre_8">
</p><p class="calibre_8">Start a <tt class="calibre2">busybox</tt> container in <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00332.jpg" class="calibre_204"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">ifconfig</tt> output <a/>in <tt class="calibre2">busybox node2</tt> is as<a/> follows:</p><p class="calibre_9"><img src="images/00334.jpg" class="calibre_205"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>screenshot shows you the successful <a/>ping output across containers:</p><p class="calibre_9"><img src="images/00336.jpg" class="calibre_206"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: <tt class="calibre2">Docker.service</tt> should not be started with RKT containers as the Docker bridge uses the same address that is allocated to Flannel for Docker container communication. Active work is going on to support running both Docker and RKT containers using Flannel. Some discussion on this topic can be found at <a href="https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc">https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc</a>.</p><div class="mbp_pagebreak" id="calibre_pb_131"/>


<p id="filepos365996" class="calibre_9"><span class="calibre3"><span class="bold">An AWS cluster using Flannel</span></span></p><p class="calibre_8">Flannel<a/> can be used to provide Container networking<a/> between CoreOS nodes in the AWS cloud. In the following two examples, we will create a three-node CoreOS cluster in AWS using Flannel with VXLAN and Flannel with AWS VPC networking. These examples<a/> are<a/> based on the procedure described at <a href="https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/">https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/</a>.</p><p id="filepos366610" class="calibre_9"><span class="calibre3"><span class="bold">An AWS cluster using VXLAN networking</span></span></p><p class="calibre_8">The following are <a/>the prerequisities <a/>for this:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a token for the three-node cluster from the discovery token service.</li><li value="2" class="calibre_13">Set up a security group exposing the ports <tt class="calibre2">ssh</tt>, <tt class="calibre2">icmp</tt>, <tt class="calibre2">2379</tt>, <tt class="calibre2">2380</tt>, and <tt class="calibre2">8472</tt>. <tt class="calibre2">8472</tt> is used for VXLAN encapsulation.</li><li value="3" class="calibre_13">Determine the AMI image ID using this link (<a href="https://coreos.com/os/docs/latest/booting-on-ec2.html">https://coreos.com/os/docs/latest/booting-on-ec2.html</a>) based on your AWS Zone, and update the channel based on your AWS zone and update channel. For the following example, we will use ami-150c1425, which is the latest 815 alpha image.</li><a/></ol><p class="calibre_8">Create <tt class="calibre2">cloud-config-flannel-vxlan.yaml</tt> with the same content used for the Vagrant CoreOS cluster with Flannel and Docker, as specified in the previous section.</p><p class="calibre_8">Use the following AWS CLI to start the three-node cluster:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos " --user-data</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">We can test connectivity across containers using two <tt class="calibre2">busybox</tt> containers in two CoreOS nodes as specified in the previous sections.</p><p id="filepos368384" class="calibre_9"><span class="calibre3"><span class="bold">An AWS cluster using AWS-VPC</span></span></p><p class="calibre_8">AWS VPC<a/> provides you with an option to create <a/>custom networking for the instances created in AWS. With AWS VPC, we can create subnets and route tables and configure custom IP addresses for the instances.</p><p class="calibre_8">Flannel supports the encapsulation type, <tt class="calibre2">aws-vpc</tt>. When using this option, Flannel updates the VPC route table to route between instances by creating a custom route table per VPC based on the container IP addresses allocated to the individual node. From a data path perspective, there is no encapsulation such as UDP or VXLAN that's used. Instead, AWS VPC takes care of routing the packets to the appropriate instance using the route table configured by Flannel.</p><p class="calibre_8">The following are the steps to create the cluster:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a discovery token for the three-node cluster.</li><li value="2" class="calibre_13">Set up a security group exposing the ports <tt class="calibre2">ssh</tt>, <tt class="calibre2">icmp</tt>, <tt class="calibre2">2379</tt>, and <tt class="calibre2">2380</tt>.</li><li value="3" class="calibre_13">Determine the AMI image ID using this link (<a href="https://coreos.com/os/docs/latest/booting-on-ec2.html">https://coreos.com/os/docs/latest/booting-on-ec2.html</a>). For the following example, we will use <tt class="calibre2">ami-150c1425</tt>, which is the latest 815 alpha image.</li><a/><li value="4" class="calibre_13">Create a VPC using the VPC wizard with a single public subnet. The following diagram shows you the VPC created from the AWS console:<p class="calibre_"><img src="images/00338.jpg" class="calibre_207"/></p><p class="calibre_8">
</p></li><a/><li value="5" class="calibre_13">Create an IAM policy, <tt class="calibre2">demo-policy</tt>, from the AWS console. This policy allows the instance to modify routing tables:<p class="calibre_8"><tt class="calibre2">{<br class="calibre4"/>    "Version": "2012-10-17",<br class="calibre4"/>    "Statement": [<br class="calibre4"/>    {<br class="calibre4"/>            "Effect": "Allow",<br class="calibre4"/>            "Action": [<br class="calibre4"/>                "ec2:CreateRoute",<br class="calibre4"/>                "ec2:DeleteRoute",<br class="calibre4"/>                "ec2:ReplaceRoute"<br class="calibre4"/>            ],<br class="calibre4"/>            "Resource": [<br class="calibre4"/>                "*"<br class="calibre4"/>            ]<br class="calibre4"/>    },<br class="calibre4"/>    {<br class="calibre4"/>            "Effect": "Allow",<br class="calibre4"/>            "Action": [<br class="calibre4"/>                "ec2:DescribeRouteTables",<br class="calibre4"/>                "ec2:DescribeInstances"<br class="calibre4"/>            ],<br class="calibre4"/>            "Resource": "*"<br class="calibre4"/>    }<br class="calibre4"/>    ]<br class="calibre4"/>}</tt></p></li><li value="6" class="calibre_31">Create an IAM role, <tt class="calibre2">demo-role</tt>, and associate <tt class="calibre2">demo-policy</tt> created in the preceding code with this role.</li><a/><li value="7" class="calibre_13">Create <tt class="calibre2">cloud-config-flannel-aws.yaml</tt> with the following content. We will use the type as <tt class="calibre2">aws-vpc</tt>, as shown in the following code:<p class="calibre_8"><tt class="calibre2">Cloud-config-flannel-aws.yaml:<br class="calibre4"/>#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: &lt;your token&gt;<br class="calibre4"/>    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" , "Backend": {"Type": "aws-vpc"}}'<br class="calibre4"/>      command: start</tt></p></li></ol><p class="calibre_8">Create a three-node CoreOS cluster with a security group, IAM role, <tt class="calibre2">vpcid/subnetid</tt>, <tt class="calibre2">security group</tt>, and <tt class="calibre2">cloud-config</tt> file as follows:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">aws ec2 run-instances --image-id ami-150c1425 --subnet subnet-a58fc5c0 --associate-public-ip-address --iam-instance-profile Name=demo-role --count 3 --security-group-ids sg-f22cb296 --instance-type t2.micro --key-name "smakam-oregon"  --user-data file://cloud-config-flannel-aws.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: It is necessary to disable the source and destination checks to allow traffic from containers as the IP address for the containers is allocated by flannel and not by AWS. To do this, we need to go to each instance in the AWS console and select <span class="bold">Networking</span> | <span class="bold">change source/dest check</span> | <span class="bold">disable</span>.</p><p class="calibre_8">Looking at <a/>the <tt class="calibre2">etcdctl</tt> output in one of the CoreOS nodes, we <a/>can see the following subnets allocated to each node of the three-node cluster.</p><p class="calibre_9"><img src="images/00339.jpg" class="calibre_208"/></p><p class="calibre_8">
</p><p class="calibre_8">Flannel will go ahead and update the VPC route table to route the preceding subnets based on the instance ID on which the subnets are present. If we check the VPC route table, we can see the following routes, which match the networks created by Flannel:</p><p class="calibre_9"><img src="images/00343.jpg" class="calibre_209"/></p><p class="calibre_8">
</p><p class="calibre_8">At this point, we<a/> can test connectivity across containers <a/>using two <tt class="calibre2">busybox</tt> containers in two CoreOS nodes, as specified in the previous sections.</p><div class="mbp_pagebreak" id="calibre_pb_132"/>


<p id="filepos375094" class="calibre_9"><span class="calibre3"><span class="bold">A GCE cluster using Flannel</span></span></p><p class="calibre_8">Flannel can be<a/> used to provide Container networking <a/>between CoreOS nodes in the GCE cloud. In the following two examples, we will create a three-node CoreOS cluster using Flannel with VXLAN and Flannel with GCE networking. These examples are based on the procedure described at <a href="https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/">https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/</a>.</p><p id="filepos375682" class="calibre_9"><span class="calibre3"><span class="bold">GCE cluster using VXLAN networking</span></span></p><p class="calibre_8">The<a/> following are the prerequisities <a/>for this:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a token for the three-node cluster from the discovery token service.</li><li value="2" class="calibre_13">Set up a security group exposing the ports <tt class="calibre2">ssh</tt>, <tt class="calibre2">icmp</tt>, <tt class="calibre2">2379</tt>, <tt class="calibre2">2380</tt>, and <tt class="calibre2">8472</tt>. <tt class="calibre2">8472</tt> is used for VXLAN encapsulation.</li><li value="3" class="calibre_13">Determine the AMI image ID using this link (<a href="https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html">https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html</a>). We will use alpha image 815 for the following example.</li></ol><p class="calibre_8">Create <tt class="calibre2">cloud-config-flannel-vxlan.yaml</tt> with the same content that was used for the Vagrant CoreOS cluster with Flannel and Docker specified in the previous section.</p><p class="calibre_8">The following command can be used to set up a three-node CoreOS cluster in GCE with Flannel and VXLAN encapsulation:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --tags coreos --metadata-from-file user-data=cloud-config-flannel-vxlan.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">A ping test<a/> across containers in different hosts<a/> can be done to verify that the Flannel control and data path is working fine.</p><p id="filepos377572" class="calibre_9"><span class="calibre3"><span class="bold">A GCE cluster using GCE networking</span></span></p><p class="calibre_8">Similar to AWS VPC, the <a/>Google cloud also has its cloud <a/>networking service that provides you with the capability to create custom subnets, routes, and IP addresses.</p><p class="calibre_8">The following are the steps to create a three-node CoreOS cluster using flannel and GCE networking:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a token for the three-node cluster from the discovery token service.</li><li value="2" class="calibre_13">Create a custom network, <tt class="calibre2">customnet</tt>, with firewall rules allowing TCP ports <tt class="calibre2">2379</tt> and <tt class="calibre2">2380</tt>. The following is the custom network that I created with subnet <tt class="calibre2">10.10.0.0/16</tt>:<p class="calibre_"><img src="images/00344.jpg" class="calibre_210"/></p><p class="calibre_8">
</p></li><li value="3" class="calibre_13">Create <tt class="calibre2">cloud-config-flannel-gce.yaml</tt> with the following content. Use the Flannel type as <tt class="calibre2">gce</tt>:<p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: &lt;yourtoken&gt;<br class="calibre4"/>    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" , "Backend": {"Type": "gce"}}'<br class="calibre4"/>      command: start</tt></p></li><li value="4" class="calibre_31">Create three CoreOS instances with a <tt class="calibre2">customnet</tt> network, IP forwarding turned on, and scope for the instance to modify the route table:<p class="calibre_8"><tt class="calibre2"><span class="bold">gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --network customnet --can-ip-forward --scopes compute-rw --metadata-from-file user-data=cloud-config-flannel-gce.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><p class="calibre_8">The following are the Flannel networks for containers created by each node:</p><p class="calibre_9"><img src="images/00346.jpg" class="calibre_211"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the routing table in GCE. As shown by the following output, Flannel has updated the GCE route table for the container networks:</p><p class="calibre_9"><img src="images/00348.jpg" class="calibre_212"/></p><p class="calibre_8">
</p><p class="calibre_8">At this point, we should have Container connectivity across nodes.</p><div class="mbp_pagebreak" id="calibre_pb_133"/>


<p id="filepos381251" class="calibre_9"><span class="calibre3"><span class="bold">Experimental multitenant networking</span></span></p><p class="calibre_8">By default, Flannel creates a single<a/> network, and all the nodes can communicate with each other over the single network. This poses a security risk when there are multiple tenants using the same network. One approach to achieve multitenant networking is using multiple instances of flannel managing each tenant. This can get cumbersome to set up. As of version 0.5.3, Flannel has introduced multinetworking in the experimental mode, where a single Flannel daemon can manage multinetworks with isolation. When there are multiple tenants using the cluster, a multinetwork mode would help in isolating each tenant's traffic.</p><p class="calibre_8">The following are the steps for this:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create subnet configurations for multiple tenants. This can be done by reserving a subnet pool in <tt class="calibre2">etcd</tt>. The following example sets up three networks, <tt class="calibre2">blue</tt>, <tt class="calibre2">green</tt>, and <tt class="calibre2">red</tt>, each having a different subnet:<p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl set /coreos.com/network/blue/config  '{ "Network": "10.1.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 } }'</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcdctl set /coreos.com/network/green/config '{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 2 } }'</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcdctl set /coreos.com/network/red/config   '{ "Network": "10.3.0.0/16", "Backend": { "Type": "vxlan", "VNI": 3 } }'</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Start the Flannel agent with the networks that this Flannel agent needs to be part of. This will take care of reserving the IP pool per node per network. In this example, we have started the flannel agent to be part of all three networks, <tt class="calibre2">blue</tt>, <tt class="calibre2">green</tt>, and <tt class="calibre2">red</tt>:<p class="calibre_8"><tt class="calibre2"><span class="bold">sudo flanneld --networks=blue,green,red &amp;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><p class="calibre_8">Flannel picked three subnet ranges for the three networks, as shown in the following screenshot. <tt class="calibre2">10.1.87.0/24</tt> is allocated for the <tt class="calibre2">blue</tt> network, <tt class="calibre2">10.2.4.0/24</tt> is allocated for the <tt class="calibre2">green</tt> network, and <tt class="calibre2">10.3.93.0/24</tt> is allocated for the <tt class="calibre2">red</tt> network:</p><p class="calibre_9"><img src="images/00350.jpg" class="calibre_213"/></p><p class="calibre_8">
</p><p class="calibre_8">Under <tt class="calibre2">/run/flannel</tt>, multiple networks can be seen, as follows:</p><p class="calibre_9"><img src="images/00351.jpg" class="calibre_214"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can start the <a/>Docker or Rkt container with the appropriate tenant network created. At this point, there is no automatic integration of <tt class="calibre2">flanneld.service</tt> with multinetworks; this has to be done manually.</p><p class="calibre_8">The following link <a/>is a related Google discussion on this topic:</p><p class="calibre_8"><a href="https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4">https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4</a>
</p><div class="mbp_pagebreak" id="calibre_pb_134"/>


<p id="filepos384864" class="calibre_9"><span class="calibre3"><span class="bold">Experimental client-server networking</span></span></p><p class="calibre_8">In the default <a/>Flannel mode, there is a flannel agent in each node, and the backend data is maintained in <tt class="calibre2">etcd</tt>. This keeps Flannel stateless. In this mode, there is a requirement for each Flannel node to run <tt class="calibre2">etcd</tt>. Flannel client-server mode is useful in the following scenarios:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Only the master node runs <tt class="calibre2">etcd</tt> and Worker nodes need not run <tt class="calibre2">etcd</tt>. This is useful from both the performance and security perspectives.</li><li value="2" class="calibre_13">When using other backends such as AWS with flannel, it's necessary to store the AWS key, and when using the client-server model, the key can be present in the master node only; this is again important from a security perspective.</li></ul><p class="calibre_8">Flannel client-server feature is currently in experimental mode as of Flannel version 0.5.3.</p><p class="calibre_8">The following figure describes the interconnection between different components for the Flannel client-server networking:</p><p class="calibre_9"><img src="images/00353.jpg" class="calibre_215"/></p><p class="calibre_8">
</p><p class="calibre_8">If necessary, we can use secure (HTTPS) means of communication both from the flanneld server to the etcd as well as between the flanneld client and server.</p><p id="filepos386540" class="calibre_9"><span class="calibre3"><span class="bold">Setting up client-server Flannel networking</span></span></p><p class="calibre_8">Let's start with a <a/>three-node CoreOS cluster without Flannel running on any node. Start the <tt class="calibre2">flanneld</tt> server and client in <tt class="calibre2">node1</tt> and client in <tt class="calibre2">node2</tt> and <tt class="calibre2">node3</tt>.</p><p class="calibre_8">Start flannel server as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00354.jpg" class="calibre_216"/></p><p class="calibre_8">
</p><p class="calibre_8">Start flannel client as shown in the following screenshot:</p><p class="calibre_8">It is necessary to specify the interface with <tt class="calibre2">eth1</tt> as an argument as <tt class="calibre2">eth0</tt> is used as the NAT interface and is common across all nodes, eth1 is unique across nodes:</p><p class="calibre_9"><img src="images/00357.jpg" class="calibre_217"/></p><p class="calibre_8">
</p><p class="calibre_8">After starting the client<a/> in <tt class="calibre2">node2</tt> and <tt class="calibre2">node3</tt>, let's look at the <tt class="calibre2">etcd</tt> output in <tt class="calibre2">node1</tt> showing the three subnets acquired by three CoreOS nodes:</p><p class="calibre_9"><img src="images/00308.jpg" class="calibre_218"/></p><p class="calibre_8">
</p><p class="calibre_8">To start <tt class="calibre2">docker.service</tt> manually, we first need to create <tt class="calibre2">flannel_docker_opts.env</tt> as follows:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">/usr/bin/docker run --net=host --rm -v /run:/run \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  quay.io/coreos/flannel:0.5.3 \</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env –i</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following image is the created <tt class="calibre2">flannel_docker_opts.env</tt>:</p><p class="calibre_9"><img src="images/00469.jpg" class="calibre_219"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can<a/> start <tt class="calibre2">docker.service</tt>, which uses environment variables in <tt class="calibre2">flannel_docker_opts.env</tt>.</p><p class="calibre_8">Start <tt class="calibre2">docker.service</tt>:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">sudo systemctl start docker.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">As we can see, the docker bridge gets the IP address in the range allocated to this node:</p><p class="calibre_9"><img src="images/00052.jpg" class="calibre_124"/></p><p class="calibre_8">
</p><p class="calibre_8">This feature is currently experimental. There are plans to add a server failover feature in future.</p><div class="mbp_pagebreak" id="calibre_pb_135"/>


<p id="filepos389598" class="calibre_"><span class="calibre1"><span class="bold">Docker networking</span></span></p><p class="calibre_8">The following is the<a/> Docker networking model to interconnect containers in a single host:</p><p class="calibre_9"><img src="images/00312.jpg" class="calibre_220"/></p><p class="calibre_8">
</p><p class="calibre_8">Each Container <a/>resides in its own networking namespace and uses a Linux bridge on the host machine to talk to each other. More details on <a/>Docker networking options can be found at <a href="https://docs.docker.com/engine/userguide/networking/dockernetworks/">https://docs.docker.com/engine/userguide/networking/dockernetworks/</a>. The following are the networking options available as of Docker release 1.9:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">--net=bridge</tt>: This is the default option that Docker provides, where containers connect to the Linux <tt class="calibre2">docker</tt> bridge using a veth pair.</li><a/><li value="2" class="calibre_13"><tt class="calibre2">--net=host</tt>: In this option, there is no new network namespace created for the container, and the container shares the same network namespace as the host machine.</li><a/><li value="3" class="calibre_13"><tt class="calibre2">--net= (the container name or ID)</tt>: In this option, the new container shares the same network namespace as the specified container in the <tt class="calibre2">net</tt> option. (For example: <tt class="calibre2">sudo docker run -ti –name=ubuntu2 –net=container:ubuntu1 ubuntu:14.04 /bin/bash</tt>. Here, the <tt class="calibre2">ubuntu2</tt> container shares the same network namespace as the <tt class="calibre2">ubuntu1</tt> container.)</li><a/><li value="4" class="calibre_13"><tt class="calibre2">--net=none</tt>: In this option, the container does not get allocated a new network namespace. Only the loopback interface is created in this case. This option is useful in scenarios where we want to create our own networking options for the container or where there is no need for connectivity.</li><a/><li value="5" class="calibre_13"><tt class="calibre2">--net=overlay</tt>: This option was added in Docker release 1.9 to support overlay networking that allows Containers across hosts to be able to talk to each other.</li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_136"/>


<p id="filepos392034" class="calibre_14"><span class="calibre3"><span class="bold">Docker experimental networking</span></span></p><p class="calibre_8">As of Docker release 1.8, Docker did not have a native solution to connect Containers across hosts. With the Docker experimental release, we can connect Containers across hosts using the<a/> Docker native solution as well as external networking plugins to connect Containers across hosts.</p><p class="calibre_8">The following figure illustrates this:</p><p class="calibre_9"><img src="images/00447.jpg" class="calibre_221"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some notes on the<a/> Docker libnetwork solution:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Docker runtime was previously integrated with the networking module and there was no way to separate them. Libnetwork is the new networking library that provides the networking functionality and is separated from Core Docker. Docker 1.7 release has already included the libnetwork and is backward-compatible from the end user's perspective.</li><li value="2" class="calibre_13">Drivers implement the APIs provided by libnetwork. Docker is leaning towards a plugin approach for major functionalities such as Networking, Storage, and Orchestration where Docker provides a native solution that can be substituted with technologies from other vendors as long as they implement the APIs provided by the common library. In this case, Bridge and Overlay are the native Docker networking drivers and remote drivers can be implemented by a third party. There are already many remote drivers available, such as Weave and Calico.</li><a/></ul><p class="calibre_8">Docker experimental<a/> networking has the following concepts:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Docker container attaches to the network using the endpoint or service.</li><li value="2" class="calibre_13">Multiple endpoints share a network. In other words, only endpoints located in the same network can talk to each other.</li><li value="3" class="calibre_13">When creating the network, the network driver can be mentioned. This can be a Docker-provided driver, such as Overlay, or an external driver, such as Weave and Calico.</li><li value="4" class="calibre_13">Libnetwork provides service discovery, where Containers can discover other endpoints in the same network. There is a plan in the future to make service discovery a plugin. Services can talk to each other using the service name rather than the IP address. Currently, Consul is used for service discovery; this might change later.</li><li value="5" class="calibre_13">Shared storage such as <tt class="calibre2">etcd</tt> or <tt class="calibre2">consul</tt> is used to determine the nodes that are part of the same cluster.</li></ul><p id="filepos395154" class="calibre_14"><span class="calibre3"><span class="bold">A multinetwork use case</span></span></p><p class="calibre_8">With the <a/>latest Docker networking enhancements, Containers can be part of multiple networks and only Containers in the same network can talk to each other. To illustrate these concepts, let's take a look at the following example:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Set up two nginx containers and one HAProxy Container in the backend network, <tt class="calibre2">be</tt>.</li><li value="2" class="calibre_13">Add the HAProxy Container in the frontend network, <tt class="calibre2">fe</tt>, as well.</li><li value="3" class="calibre_13">Connect to the HAProxy Container using the busybox Container in the frontend network, <tt class="calibre2">fe</tt>. As the busybox Container is in the <tt class="calibre2">fe</tt> network and nginx Containers are in the <tt class="calibre2">be</tt> network, they cannot talk to each other directly.</li><li value="4" class="calibre_13">The Haproxy Container will load balance the web connection between the two nginx backend Containers.</li></ol><p class="calibre_8">The following are the command details:</p><p class="calibre_8">Create the <tt class="calibre2">fe</tt> and <tt class="calibre2">be</tt> networks:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker network create be</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker network create fe</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Create two nginx containers in the <tt class="calibre2">be</tt> network:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --name nginx1 --net be -v ~/haproxy/nginx1.html:/usr/share/nginx/html/index.html -d nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --name nginx2 --net be -v ~/haproxy/nginx2.html:/usr/share/nginx/html/index.html -d nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Create<a/>
<tt class="calibre2">haproxy</tt> in the <tt class="calibre2">be</tt> network:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name haproxy --net be -v ~/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg haproxy</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Attach <tt class="calibre2">haproxy</tt> to the <tt class="calibre2">fe</tt> network:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker network connect fe haproxy</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Create a busybox container in the <tt class="calibre2">fe</tt> network accessing the <tt class="calibre2">haproxy</tt> web page:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -it --rm --net fe busybox wget -qO- haproxy/index.html</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If we try running the busybox container multiple times, it will switch between <tt class="calibre2">nginx1</tt> and <tt class="calibre2">nginx2</tt> web server outputs.</p><p id="filepos398170" class="calibre_9"><span class="calibre3"><span class="bold">The Docker overlay driver</span></span></p><p class="calibre_8">The following <a/>example shows you how to do multihost container connectivity using the Docker experimental overlay driver. I have used a Ubuntu VM for the following example and not CoreOS because the experimental docker overlay driver needs a new kernel release, which is not yet available in CoreOS.</p><p class="calibre_8">The following figure illustrates the use case that is being tried in this example:</p><p class="calibre_9"><img src="images/00314.jpg" class="calibre_222"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is a <a/>summary of the steps:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Create two hosts with experimental Docker installed.</li><li value="2" class="calibre_13">Install Consul on both the hosts with one of the hosts acting as the consul server. Consul is needed to store common data that is used for inter-container communication.</li><li value="3" class="calibre_13">Start Docker with Consul as the key store mechanism on both hosts.</li><li value="4" class="calibre_13">Create containers with different endpoints on both hosts sharing the same network.</li></ul><p class="calibre_8">The first step is to create 2 host machines with experimental Docker installed.</p><p class="calibre_8">The following set of commands creates two Docker hosts using docker-machine. We have used docker-machine with a custom ISO image for experimental Docker:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Install Consul on both nodes. The following command shows you how to download and install consul:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl -OL https://dl.bintray.com/mitchellh/consul/0.5.2_linux_amd64.zip</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">unzip 0.5.2_linux_amd64.zip</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo mv consul /usr/local/bin/</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the consul <a/>server and docker daemon with the consul keystore in <tt class="calibre2">node1</tt>:</p><p class="calibre_8">The following set of commands starts the consul server and Docker daemon with the consul agent in <tt class="calibre2">node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Docker-machine ssh dev1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">consul agent -server -bootstrap -data-dir /tmp/consul -bind=192.168.99.100 &amp;</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the consul agent and Docker daemon with the consul keystore in <tt class="calibre2">node2</tt>:</p><p class="calibre_8">The following set of commands starts the consul agent and Docker daemon with the consul agent in <tt class="calibre2">node2</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Docker-machine ssh dev2</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">consul agent -data-dir /tmp/consul -bind 192.168.99.101 &amp;</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">consul join 192.168.99.100 &amp;</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1 --label=com.docker.network.driver.overlay.neighbor_ip=192.168.99.100</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the container with the <tt class="calibre2">svc1</tt> service, <tt class="calibre2">dev</tt> network, and <tt class="calibre2">overlay</tt> driver in <tt class="calibre2">node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -i -t --publish-service=svc1.dev.overlay busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the container with the <tt class="calibre2">svc2</tt> service, <tt class="calibre2">dev</tt> network, and <tt class="calibre2">overlay</tt> driver in node2:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -i -t --publish-service=svc2.dev.overlay busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">As we can see, we are able to ping <tt class="calibre2">svc1</tt> and <tt class="calibre2">svc2</tt> from <tt class="calibre2">node1</tt> successfully:</p><p class="calibre_9"><img src="images/00491.jpg" class="calibre_223"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: The overlay driver needs the Linux kernel version 3.16 or higher.</p><p id="filepos403418" class="calibre_9"><span class="calibre3"><span class="bold">The external networking calico plugin</span></span></p><p class="calibre_8">In this example, we will illustrate how to do Container networking using Calico as a plugin to the Docker libnetwork. This support was available originally in experimental networking and later in the Docker 1.9 release. More details about the Calico networking approach are mentioned in the following Calico networking section. This example is based on <a href="https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md">https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md</a>. To set up a CoreOS Vagrant cluster for Calico, we can use the procedure in <a href="https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md">https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md</a>.</p><p class="calibre_8">After setting up a Vagrant CoreOS cluster, we can see the two nodes of the CoreOS cluster. We should make sure that <tt class="calibre2">etcd</tt> is running successfully, as shown in the following output:</p><p class="calibre_9"><img src="images/00318.jpg" class="calibre_224"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the steps to get Calico working with Docker as a networking plugin:</p><p class="calibre_8">Start Calico in both nodes with the libnetwork option:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo calicoctl node --libnetwork</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">We should see the following Docker containers in both nodes:</p><p class="calibre_9"><img src="images/00028.jpg" class="calibre_225"/></p><p class="calibre_8">
</p><p class="calibre_8">Create the <tt class="calibre2">net1</tt> network with Calico driver:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker network create --driver=calico --subnet=192.168.0.0/24 net1</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This gets replicated to all the nodes in the cluster. The following is the network list in <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00320.jpg" class="calibre_226"/></p><p class="calibre_8">
</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Create container 1 in <tt class="calibre2">node1</tt> with the <tt class="calibre2">net1</tt> network:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --net net1 --name workload-A -tid busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Create container 2 in <tt class="calibre2">node2</tt> with the <tt class="calibre2">net1</tt> network:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --net net1 --name workload-B -tid busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ul><p class="calibre_8">Now, we can ping the two containers as follows:</p><p class="calibre_9"><img src="images/00050.jpg" class="calibre_227"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_137"/>


<p id="filepos406979" class="calibre_9"><span class="calibre3"><span class="bold">The Docker 1.9 update</span></span></p><p class="calibre_8">Docker 1.9 got released at the end of October 2015 that transitioned the experimental networking into production. There could be minor modifications necessary to the Docker networking examples in this chapter, which were tried with the Docker 1.8 experimental networking version.</p><p class="calibre_8">With Docker 1.9, multihost networking is integrated with Docker Swarm and Compose. This allows us to orchestrate a multicontainer application spread between multiple hosts with a single command and the multi-host Container networking will be handled automatically.</p><div class="mbp_pagebreak" id="calibre_pb_138"/>


<p id="filepos407723" class="calibre_"><span class="calibre1"><span class="bold">Other Container networking technologies</span></span></p><p class="calibre_8">Weave and Calico are open source projects, and they develop Container networking technologies for Docker. Kubernetes is a Container orchestration open source project and it has specific networking requirements and implementations for Containers. There are also other projects such as Cisco Contiv (<a href="https://github.com/contiv/netplugin">https://github.com/contiv/netplugin</a>) that is targeted at Container networking. Container technologies like Weave, Calico and Contiv have plans to integrate with Rkt Container runtime in the future.</p><div class="mbp_pagebreak" id="calibre_pb_139"/>


<p id="filepos408457" class="calibre_9"><span class="calibre3"><span class="bold">Weave networking</span></span></p><p class="calibre_8">Weaveworks has developed a solution to provide Container networking. The following are some details of their solution:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Weave creates a Weave bridge as well as a Weave router in the host machine.</li><li value="2" class="calibre_13">The Weave router establishes both TCP and UDP connections across hosts to other Weave routers. A TCP connection is used for discovery- and protocol-related exchange. UDP is used for data encapsulation. Encryption can be done if necessary.</li><li value="3" class="calibre_13">The Weave bridge is configured to sniff the packets that need to be sent across hosts and redirected to the Weave router. For local switching, the Weave router is not used.</li><li value="4" class="calibre_13">Weave's Weavenet product provides you with container connectivity. They also have Weavescope that provides container visibility and Weaverun that provides service discovery and load balancing.</li><li value="5" class="calibre_13">Weave is also available as a Docker plugin integrated with the Docker release 1.9.</li></ul><p class="calibre_8">The following figure illustrates the solution from Weave:</p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00068.jpg" class="calibre_228"/></p><p class="calibre_8">
</p><p class="calibre_8">
</p><p class="calibre_8">To run Weave on CoreOS, I used cloud-config from <a href="https://github.com/lukebond/coreos-vagrant-weave">https://github.com/lukebond/coreos-vagrant-weave</a>. In the following example, we will create containers in two CoreOS nodes and use Weave to communicate with each other. In this example, we have not used the Docker Weave plugin but used environment variables to communicate between Docker and Weave.</p><p class="calibre_8">The following are the steps to create a Weave cluster:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Clone the repository (<tt class="calibre2">git clone https://github.com/lukebond/coreos-vagrant-weave.git</tt>).</li><li value="2" class="calibre_13">Change the number of instances in <tt class="calibre2">config.rb</tt> to <tt class="calibre2">3</tt>.</li><li value="3" class="calibre_13">Get a new discovery token for node count 3 and update it in the user data.</li><li value="4" class="calibre_13">Perform <tt class="calibre2">vagrant up</tt> to start the cluster.</li></ol><p class="calibre_8">The <tt class="calibre2">cloud-config</tt> file takes care of downloading Weave agents in each node and starting them.</p><p class="calibre_8">The following <a/>section of the service file <a/>downloads the Weave container:</p><p class="calibre_9"><img src="images/00421.jpg" class="calibre_229"/></p><p class="calibre_8">
</p><p class="calibre_8">The following section of the service file starts the Weave container:</p><p class="calibre_9"><img src="images/00340.jpg" class="calibre_230"/></p><p class="calibre_8">
</p><p class="calibre_8">On each of the nodes, we can see the following Weave containers started:</p><p class="calibre_9"><img src="images/00278.jpg" class="calibre_140"/></p><p class="calibre_8">
</p><p class="calibre_8">Before starting application containers, we need to set the environment variables so that Weave can intercept Docker commands and create their own networking. As part of starting Weave in <tt class="calibre2">Weave.service</tt>, environment variables have already been set up. The following command in the node shows this:</p><p class="calibre_9"><img src="images/00292.jpg" class="calibre_231"/></p><p class="calibre_8">
</p><p class="calibre_8">Source the Weave environment as follows:</p><p class="calibre_9"><img src="images/00366.jpg" class="calibre_232"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's start busybox containers in two CoreOS nodes:</p><p class="calibre_9"><img src="images/00315.jpg" class="calibre_233"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the Weave interface created in the busybox container of CoreOS node1:</p><p class="calibre_9"><img src="images/00325.jpg" class="calibre_234"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the <a/>Weave interface created in the<a/> busybox container of CoreOS node2:</p><p class="calibre_9"><img src="images/00374.jpg" class="calibre_235"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can successfully ping between the two containers. As part of Docker 1.9, Weave is available as a Docker networking plugin and this makes configuration much easier.</p><div class="mbp_pagebreak" id="calibre_pb_140"/>


<p id="filepos414130" class="calibre_9"><span class="calibre3"><span class="bold">Calico networking</span></span></p><p class="calibre_8">Calico provides you with a<a/> Container networking solution for Docker<a/> similar to Weave. The following are some details of Calico's implementation:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Calico provides container networking directly at L3 without using overlay technologies</li><li value="2" class="calibre_13">Calico uses BGP for route distribution</li><li value="3" class="calibre_13">There are two components of Calico: BIRD, which is used for route distribution and FELIX, which is an agent in each node that does discovery and routing</li><li value="4" class="calibre_13">Calico is also available as a Docker networking plugin integrated with Docker release 1.9</li></ul><p class="calibre_8">The following figure illustrates the Calico data path:</p><p class="calibre_9"><img src="images/00365.jpg" class="calibre_236"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_141"/>


<p id="filepos415366" class="calibre_9"><span class="calibre3"><span class="bold">Setting up Calico with CoreOS</span></span></p><p class="calibre_8">I <a/>followed<a/> the<a/> procedure at <a href="https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md">https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md</a> to set up a two-node CoreOS cluster.</p><p class="calibre_8">The first step is checking out the repository:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">git clone https://github.com/projectcalico/calico-docker.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">There are three approaches described by Calico for Docker networking:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Powerstrip: Calico uses an HTTP proxy to listen to Docker calls and set up networking.</li><li value="2" class="calibre_13">Default networking: Docker Containers are set up with no networking. Using Calico, network endpoints are added and networking is set up.</li><li value="3" class="calibre_13">Libnetwork: Calico is integrated with Docker libnetwork as of Docker release 1.9. This will be the long-term solution.</li></ul><p class="calibre_8">In the following <a/>example, we have used the default networking <a/>approach to set up Container connectivity using Calico.</p><p class="calibre_8">The following are the steps needed to set up the default networking option with Calico:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Start calicoctl in all the nodes.</li><li value="2" class="calibre_13">Start the containers with the <tt class="calibre2">--no-net</tt> option.</li><li value="3" class="calibre_13">Attach the calico network specifying the IP address to each container.</li><li value="4" class="calibre_13">Create a policy profile. Profiles set up the policy that allows containers to talk to each other.</li><li value="5" class="calibre_13">Attach profiles to the container.</li></ol><p class="calibre_8">The following commands set up a container in <tt class="calibre2">node1</tt> and <tt class="calibre2">node2</tt> and establish a policy that allows containers to talk to each other.</p><p class="calibre_8">Execute the following commands on <tt class="calibre2">node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --net=none --name workload-A -tid busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo calicoctl container add workload-A 192.168.0.1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">calicoctl profile add PROF_A_B</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">calicoctl container workload-A profile append PROF_A_B</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This starts the docker container, attaches the calico endpoint, and applies the profile to allow Container connectivity.</p><p class="calibre_8">Execute the following commands on <tt class="calibre2">node2</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --net=none --name workload-B -tid busybox</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo calicoctl container add workload-B 192.168.0.2</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">calicoctl container workload-B profile append PROF_A_B</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This starts the docker container, attaches the calico endpoint, and applies the same profile as in the preceding commands to allow Container connectivity.</p><p class="calibre_8">Now, we can test<a/> intercontainer <a/>connectivity:</p><p class="calibre_9"><img src="images/00383.jpg" class="calibre_237"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_142"/>


<p id="filepos419476" class="calibre_9"><span class="calibre3"><span class="bold">Kubernetes networking</span></span></p><p class="calibre_8">Kubernetes is a Container <a/>orchestration service. Kubernetes is an open source <a/>project that's primarily driven by Google. We will discuss about Kubernetes in the next chapter on Container orchestration. In this chapter, we will cover some of the Kubernetes networking basics.</p><p class="calibre_8">The following are some details as to how Kubernetes does the networking of containers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Kubernetes has a concept called Pods, which is a collection of closely-tied containers. For example, a service and its logging service can be a single pod. Another example of a pod could be a service and sidekick service that checks the health of the main service. A single pod with its associated containers is always scheduled on one machine.</li><li value="2" class="calibre_13">Each pod gets an IP address. All containers within a pod share the same IP address.</li><li value="3" class="calibre_13">Containers within a pod share the same network namespace. For containers within a pod to communicate, they can use a regular process-based communication.</li><li value="4" class="calibre_13">Pods can communicate with each other using a cloud networking VPC-based approach or container networking solution such as Flannel, Weave, or Calico.</li><li value="5" class="calibre_13">As pods are ephemeral, Kubernetes has a unit called service. Each service has an associated virtual IP address and proxy agent running on the nodes' load balancers and directs traffic to the right pod.</li><a/></ul><p class="calibre_8">The following is an illustration of how Pods and Containers are related and how they communicate:</p><p class="calibre_9"><img src="images/00403.jpg" class="calibre_238"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_143"/>


<p id="filepos421633" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered different Container networking technologies with a focus on Container networking in CoreOS. There are many companies trying to solve this Container networking problem. CNI and Flannel have become the default for CoreOS and Libnetwork has become the default for Docker. Having standards and pluggable networking architecture is good for the industry as this allows interoperability. Container networking is still in the early stages, and it will take some time for the technologies to mature in this area. In the next chapter, we will discuss about CoreOS storage management.</p><div class="mbp_pagebreak" id="calibre_pb_144"/>


<p id="filepos422377" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Flannel docs: <a href="https://coreos.com/flannel/docs/latest/">https://coreos.com/flannel/docs/latest/</a></li><li value="2" class="calibre_13">Flannel GitHub page: <a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a></li><li value="3" class="calibre_13">CNI spec: <a href="https://github.com/appc/cni/blob/master/SPEC.md">https://github.com/appc/cni/blob/master/SPEC.md</a></li><li value="4" class="calibre_13">Flannel with AWS and GCE: <a href="https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/">https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/</a></li><li value="5" class="calibre_13">Weaveworks: <a href="https://github.com/weaveworks/weave">https://github.com/weaveworks/weave</a></li><li value="6" class="calibre_13">Libnetwork: <a href="https://github.com/docker/libnetwork">https://github.com/docker/libnetwork</a></li><li value="7" class="calibre_13">Docker experimental: <a href="https://github.com/docker/docker/tree/master/experimental">https://github.com/docker/docker/tree/master/experimental</a></li><li value="8" class="calibre_13">Calico: <a href="https://github.com/projectcalico/calico-docker">https://github.com/projectcalico/calico-docker</a></li><li value="9" class="calibre_13">Kubernetes: <a href="http://kubernetes.io/">http://kubernetes.io/</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_145"/>


<p id="filepos424081" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Flannel CoreOS Fest presentation: <a href="https://www.youtube.com/watch?v=_HYeSaGtEYw">https://www.youtube.com/watch?v=_HYeSaGtEYw</a></li><li value="2" class="calibre_13">The Calico and Weave presentation: <a href="https://giantswarm.io/events/2015-04-20-docker-coreos/">https://giantswarm.io/events/2015-04-20-docker-coreos/</a></li><li value="3" class="calibre_13">Contiv netplugin: <a href="https://github.com/contiv/netplugin">https://github.com/contiv/netplugin</a></li><li value="4" class="calibre_13">Kubernetes networking: <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md">https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_146"/>
</body></html>