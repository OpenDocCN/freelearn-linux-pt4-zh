- en: Kernel Tuning for KVM Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对 KVM 性能的内核调优
- en: 'In this chapter, we are going to cover the following performance tuning recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下性能调优方法：
- en: Tuning the kernel for low I/O latency
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对低 I/O 延迟调优内核
- en: Memory tuning for KVM guests
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KVM 客户机的内存调优
- en: CPU performance options
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU 性能选项
- en: NUMA tuning with libvirt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 libvirt 进行 NUMA 调优
- en: Tuning the kernel for network performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对网络性能调优内核
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this chapter, we are going to explore various configuration options and tools
    that can help improve the performance of the host OS and the KVM instances running
    on it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨一些配置选项和工具，这些可以帮助提升宿主操作系统及其上运行的 KVM 实例的性能。
- en: 'When running KVM virtual machines, it''s important to understand that from
    the host perspective, they are regular processes. We can see that KVM guests are
    Linux processes by examining the process tree on the hypervisor:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 KVM 虚拟机时，重要的是要理解，从宿主的角度来看，它们是普通的进程。我们可以通过检查虚拟化管理程序上的进程树，看到 KVM 客户机是 Linux
    进程：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The virtual CPUs allocated to the KVM guests are Linux threads, managed by
    the host scheduler:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 分配给 KVM 客户机的虚拟 CPU 是 Linux 线程，由宿主调度程序管理：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Depending on the type of I/O scheduler, the `libvirt` network driver, and memory
    configuration, the performance of the virtual machine can vary greatly. Before
    making any changes to the earlier-mentioned components, it is important to understand
    the type of work the guest OS will be performing. Tuning the host and guest OS
    for memory-intensive work will be different from I/O or CPU bound loads.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 I/O 调度程序的类型、`libvirt` 网络驱动程序以及内存配置的不同，虚拟机的性能可能会有很大差异。在对上述组件进行任何更改之前，了解客户操作系统将执行的工作类型非常重要。为内存密集型工作调优宿主机和客户操作系统与为
    I/O 或 CPU 密集型负载调优是不同的。
- en: Because all KVM instances are just regular Linux processes, the QEMU driver
    can apply any of the following **Control Group** (**cgroup**) controllers: `cpuset`,
    `cpu`, `memory`, `blkio`, and device controllers. Using the cgroup controllers
    provides more granular control over the allowed CPU, memory, and I/O resources,
    as we are going to see in more detail in the following recipes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因为所有 KVM 实例只是普通的 Linux 进程，所以 QEMU 驱动程序可以应用以下任何 **控制组**（**cgroup**）控制器：`cpuset`、`cpu`、`memory`、`blkio`
    和设备控制器。使用 cgroup 控制器可以对允许的 CPU、内存和 I/O 资源进行更细粒度的控制，正如我们在接下来的方法中将更详细地看到的那样。
- en: Perhaps the most important point when tuning and optimizing any system is to
    establish the performance baseline prior to making any adjustments. Start by measuring
    the baseline performance of a subsystem, such as memory or I/O, make small incremental
    adjustments, then measure the impact of the changes again. Repeat as necessary
    until a desired effect is reached.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在调优和优化任何系统时，也许最重要的一点是，在进行任何调整之前，首先要确定性能基准。通过测量子系统（如内存或 I/O）的基准性能，进行小幅增量调整，然后再次测量这些变化的影响，必要时重复此过程，直到达到预期效果。
- en: The recipes in this chapter are meant to give the reader a starting point for
    what can be tuned on the host and the virtual machines to improve performance,
    or account for the side effects of running different workloads on the same host/VM
    and the effects of multitenancy. All resources should be adjusted based on the
    type of workload, hardware setup, and other variables.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的方法旨在为读者提供一个起点，帮助了解可以在宿主机和虚拟机上进行哪些调优，以提高性能，或考虑在同一宿主机/虚拟机上运行不同工作负载以及多租户环境的副作用。所有资源的调整应根据工作负载类型、硬件设置和其他变量来进行。
- en: Tuning the kernel for low I/O latency
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对低 I/O 延迟调优内核
- en: In this recipe, we are going to cover some of the disk performance optimization
    techniques by selecting an I/O scheduler and tuning the block I/O using Linux
    control groups, for the virtual guest and the host.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一些通过选择 I/O 调度程序和使用 Linux 控制组调优块 I/O 来优化磁盘性能的技巧，适用于虚拟客体和宿主机。
- en: 'There are three I/O schedulers to choose from on the host OS and in the KVM
    instance:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在宿主操作系统和 KVM 实例中，有三种 I/O 调度程序可以选择：
- en: '`noop`: This is one of the simplest kernel schedulers; it works by inserting
    all incoming I/O requests into a simple **FIFO** (**First In**, **First Out**)
    queue. This scheduler is useful when the host OS should not attempt to reorder
    I/O requests when multiple virtual machines are running.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noop`：这是最简单的内核调度程序之一，它通过将所有传入的 I/O 请求插入一个简单的 **FIFO**（**先进先出**）队列来工作。当宿主操作系统在运行多个虚拟机时不应该尝试重新排序
    I/O 请求时，这个调度程序非常有用。'
- en: '`deadline`: This scheduler imposes a deadline on all I/O operations to prevent
    starvation of requests, giving priority to read requests, due to processes usually
    blocking on read operations.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deadline`：该调度器对所有I/O操作施加一个截止期限，以防止请求饿死，并优先考虑读请求，因为进程通常会在读操作上阻塞。'
- en: '`cfq`: The main goal of **Completely Fair Queuing** (**CFQ**) is to maximize
    the overall CPU utilization while allowing better interactive performance.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cfq`：**完全公平队列**（**CFQ**）的主要目标是最大化整体CPU利用率，同时提供更好的交互性能。'
- en: Selecting the right I/O scheduler on the host and guests greatly depends on
    the workloads and the underlying hardware storage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机和客户机上选择正确的I/O调度器很大程度上取决于工作负载和底层硬件存储。
- en: As a general rule, selecting the `noop` scheduler for the guest OS allows the
    host hypervisor to better optimize the I/O requests, because it is aware of all
    requests coming from the virtual guests. However, if the underlying storage for
    the KVM machines is iSCSI volumes or any other remote storage such as GlusterFS,
    using the deadline scheduler, might yield better results.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，为客户机操作系统选择`noop`调度器允许主机虚拟化管理程序更好地优化I/O请求，因为它能感知来自虚拟客户机的所有请求。然而，如果KVM虚拟机的底层存储是iSCSI卷或其他远程存储（如GlusterFS），使用deadline调度器可能会获得更好的结果。
- en: On most modern Linux kernels, the `deadline` scheduler is the default, and it
    might be sufficient for hosts running multiple KVM virtual machines. As with any
    system tuning, testing is required when changing the schedulers on the host and
    guest OS.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现代Linux内核中，`deadline`调度器是默认的，它可能足以满足运行多个KVM虚拟机的主机需求。和任何系统调优一样，修改主机和客机操作系统的调度器时需要进行测试。
- en: Getting ready
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we are going to need the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南需要以下内容：
- en: An Ubuntu host, with libvirt and QEMU installed and configured
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台安装并配置了libvirt和QEMU的Ubuntu主机
- en: A running KVM virtual machine
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正在运行的KVM虚拟机
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何实现的...
- en: 'To change the I/O scheduler on the host and the KVM instance and set an I/O
    weight, perform the following steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改主机和KVM实例上的I/O调度器并设置I/O权重，请执行以下步骤：
- en: 'On the host OS, list the I/O scheduler currently in use, substituting the block
    device with whatever is appropriate for your system:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主机操作系统上，列出当前使用的I/O调度器，替换为适合您系统的块设备：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Change the I/O scheduler on demand and ensure it is in use by running:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按需更改I/O调度器并确保它正在使用，可以运行以下命令：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To make the change persistent across server restarts, add the following line
    to the GRUB default configuration and update:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使更改在服务器重启后仍然有效，请将以下行添加到GRUB默认配置文件中并更新：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For the KVM instance, set up the noop I/O scheduler persistently:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于KVM实例，持久化设置noop I/O调度器：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Set a weight of 100 for the KVM instance using the `blkio` cgroup controller:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`blkio` cgroup控制器为KVM实例设置权重为100：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Find the `cgroup` directory hierarchy on the host:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找主机上的`cgroup`目录层级：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Ensure that the cgroup for the KVM instance contains the weight that we set
    up earlier on the `blkio` controller:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保KVM实例的cgroup包含我们之前在`blkio`控制器上设置的权重：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For a detailed explanation on how Linux cgroups work, refer to the *Containerization
    with LXC* book from Packt publishing at [https://www.packtpub.com/virtualization-and-cloud/containerization-lxc](https://www.packtpub.com/virtualization-and-cloud/containerization-lxc).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Linux cgroup工作原理的详细解释，请参考Packt出版的*《LXC容器化》*一书，链接：[https://www.packtpub.com/virtualization-and-cloud/containerization-lxc](https://www.packtpub.com/virtualization-and-cloud/containerization-lxc)。
- en: How it works...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We can see what I/O scheduler the kernel is currently using by examining the `scheduler`
    file in the `/sys` virtual filesystem. In step 1, we see that it's the `cfq` scheduler.
    We then proceed to change the I/O scheduler on the running system in step 2\.
    Please keep in mind that changing the scheduler on demand like that will not persist
    server restarts. In steps 3 and 4, we modify the GRUB configuration which will
    append the new scheduler information to the kernel boot instructions. Restarting
    the server or the virtual machine will now select the new I/O scheduler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查`/sys`虚拟文件系统中的`scheduler`文件来查看内核当前使用的I/O调度器。在第1步中，我们看到它是`cfq`调度器。接下来，在第2步中我们将修改正在运行的系统的I/O调度器。请记住，像这样按需更改调度器不会在服务器重启后保持有效。在第3步和第4步中，我们修改GRUB配置文件，将新的调度器信息添加到内核启动指令中。现在重启服务器或虚拟机时将会选择新的I/O调度器。
- en: If running multiple virtual machines on the same host, it might be useful to
    give more I/O priority to some of them based on certain criteria, such as time
    of day and VM workload. In step 5, we use the `blkio` cgroup controller to set
    a weight for the KVM guest. Lower weight will give better I/O priority. In steps
    6 and 7, we can see that the correct `cgroup` hierarchy has been created and the `blkio.weight`
    file contains the new weight we set with the `virsh` command.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在同一主机上运行多个虚拟机，根据特定标准（如时间段和虚拟机工作负载）为其中一些虚拟机分配更高的 I/O 优先级可能会很有用。在第 5 步中，我们使用
    `blkio` cgroup 控制器为 KVM 客户机设置一个权重。较低的权重将获得更好的 I/O 优先级。在第 6 步和第 7 步中，我们可以看到正确的
    `cgroup` 层次结构已经创建，并且 `blkio.weight` 文件包含了我们通过 `virsh` 命令设置的新权重。
- en: Memory tuning for KVM guests
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KVM 客户机的内存调优
- en: When it comes to memory tuning of KVM guests there are few options available,
    depending on the workload of the virtual machine. One such option is Linux HugePages.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 KVM 客户机的内存调优方面，根据虚拟机的工作负载，提供的选项很少。一个这样的选项是 Linux 的 HugePages。
- en: Most Linux hosts by default address memory in 4 KB segments, named pages. However,
    the kernel is capable of using larger page sizes. Using HugePages (pages bigger
    than 4 KB) may improve performance by increasing the CPU cache hits against the
    transaction **Lookaside Buffer** (**TLB**). The TLB is a memory cache that stores
    recent translations of virtual memory to physical addresses for quick retrieval.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Linux 主机默认按 4 KB 的段（称为页面）来分配内存。然而，内核可以使用更大的页面大小。使用 HugePages（大于 4 KB 的页面）可以通过增加
    CPU 缓存命中率来提高性能，从而加速事务的**旁路缓冲区**（**TLB**）的访问。TLB 是一种内存缓存，用于存储虚拟内存到物理地址的最近转换，以便快速检索。
- en: In this recipe, we are going to enable and set HugePages on the hypervisor and
    the KVM guest, then examine the tuning options that the `virsh` command provides.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将启用并设置虚拟化主机和 KVM 客户机上的 HugePages，然后检查 `virsh` 命令提供的调优选项。
- en: Getting ready
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we are going to need the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将需要以下内容：
- en: An Ubuntu host, with libvirt and QEMU installed and configured
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个安装并配置了 libvirt 和 QEMU 的 Ubuntu 主机
- en: A running KVM virtual machine
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正在运行的 KVM 虚拟机
- en: How to do it...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To enable and set HugePages on the hypervisor and the KVM guest and use the
    `virsh` command to set various memory options, follow these steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要在虚拟化主机和 KVM 客户机上启用和设置 HugePages，并使用 `virsh` 命令设置各种内存选项，请按照以下步骤操作：
- en: 'Check the current HugePages settings on the host OS:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查主机操作系统上当前的 HugePages 设置：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Connect to the KVM guest and check the current HugePages settings:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到 KVM 客户机并检查当前的 HugePages 设置：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Increase the size of the pool of HugePages from 0 to 25000 on the hypervisor and
    verify the following:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将虚拟化主机上 HugePages 池的大小从 0 增加到 25000，并验证以下内容：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Check whether the hypervisor CPU supports 2 MB and 1 GB HugePages sizes:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查虚拟化主机 CPU 是否支持 2 MB 和 1 GB 的 HugePages 大小：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set 1 GB HugePages size by editing the default GRUB configuration and rebooting:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编辑默认的 GRUB 配置并重启，设置 1 GB 的 HugePages 大小：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Install the HugePages package:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 HugePages 包：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Check the current HugePages size:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查当前的 HugePages 大小：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Enable HugePages support for KVM:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 KVM 启用 HugePages 支持：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Mount the HugeTable virtual filesystem on the host OS:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主机操作系统上挂载 HugeTable 虚拟文件系统：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Edit the configuration for the KVM guest and enable HugePages:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 KVM 客户机的配置并启用 HugePages：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you see the following error when starting the KVM instance: `error: internal
    error: hugetlbfs filesystem is not mounted or disabled by administrator config`, make
    sure that the HugePages virtual filesystem was mounted successfully in step 9.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '如果在启动 KVM 实例时看到以下错误：`error: internal error: hugetlbfs filesystem is not mounted
    or disabled by administrator config`，请确保在第 9 步中成功挂载了 HugePages 虚拟文件系统。'
- en: 'If you see the following error when starting the KVM instance: `error: internal
    error: process exited while connecting to monitor: file_ram_alloc: can''t mmap
    RAM pages: Cannot allocate memory`, you need to increase the HugePages pool in
    step 3.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '如果在启动 KVM 实例时看到以下错误：`error: internal error: process exited while connecting
    to monitor: file_ram_alloc: can''t mmap RAM pages: Cannot allocate memory`，则需要在第
    3 步中增加 HugePages 池的大小。'
- en: 'Update the memory hard limit for the KVM instance and verify, as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 KVM 实例的内存硬限制并进行验证，方法如下：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Libvirt and KVM support and take advantage of HugePages. Please be aware that
    not every workload will benefit of having pages larger than the default. Instances
    running databases and memory bound KVM instances are good use cases. As always,
    before enabling this feature, measure the performance of your application inside
    the virtual machine to ensure that it will benefit from HugePages.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Libvirt 和 KVM 支持并利用 HugePages。请注意，并非每个工作负载都能从大于默认大小的页面中受益。运行数据库和内存绑定的 KVM 实例是良好的使用场景。像往常一样，在启用此功能之前，请先在虚拟机内测量应用程序的性能，以确保它能从
    HugePages 中受益。
- en: In this recipe, we enabled HugePages on the host and the guest OS and set a
    hard limit on the usable memory for the guest. Let's go through the steps in more
    detail.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们在主机和客户操作系统上启用了 HugePages，并为客户操作系统设置了可用内存的硬性限制。让我们更详细地了解这些步骤。
- en: In steps 1 and 2, we check the current state of HugePages. From the output,
    we can see that there's no HugePages pool currently allocated, indicated by the `HugePages_Total`field
    and the current size of the HugePages of 2 MB.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 和 2 中，我们检查 HugePages 的当前状态。从输出中我们可以看到，目前没有分配 HugePages 池，`HugePages_Total`
    字段表明当前 HugePages 的大小为 2 MB。
- en: In step 3, we increase the HugePages pool size to 25000\. The change is on demand
    and will not persist server reboot. To make it persistent, you can add it to the `/etc/sysctl.conf`
    file.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们将 HugePages 池的大小增加到 25000。此更改是按需的，并且不会在服务器重启后保持。如果希望其持久化，可以将其添加到 `/etc/sysctl.conf`
    文件中。
- en: In order to use the HugePages feature, we need to ensure that the CPU of the
    host server has hardware support for it, as indicated by the `pse` and `pdpe1`
    flags, as shown in step 4.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 HugePages 特性，我们需要确保主机服务器的 CPU 支持此功能，正如在步骤 4 中显示的 `pse` 和 `pdpe1` 标志所示。
- en: In step 5, we configure the GRUB bootloader to start the kernel with HugePages
    support and a set size of 1 GB.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 5 中，我们配置 GRUB 启动加载程序以启动支持 HugePages 的内核，并设置大小为 1 GB。
- en: Although we can work directly with the files exposed by the `/proc` virtual
    filesystem, in step 6, we install the HugePages package, which provides a few
    useful userspace tools to list and manage various memory settings. We use the
    `hugeadm` command in step 7 to list the size of the HugePages pool.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以直接操作 `/proc` 虚拟文件系统暴露的文件，但在步骤 6 中，我们安装了 HugePages 包，它提供了一些有用的用户空间工具，用于列出和管理各种内存设置。在步骤
    7 中，我们使用 `hugeadm` 命令列出了 HugePages 池的大小。
- en: To enable HugePages support for KVM, we update the `/etc/default/qemu-kvm` file
    in step 8, mount the virtual filesystem for it in step 9, and finally reconfigure
    the KVM virtual machine to use HugePages by adding the `<hugepages/>` stanza for
    the `<memoryBacking>` object.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用 KVM 的 HugePages 支持，我们在步骤 8 中更新了 `/etc/default/qemu-kvm` 文件，在步骤 9 中挂载了虚拟文件系统，最后通过为
    `<memoryBacking>` 对象添加 `<hugepages/>` 字段重新配置 KVM 虚拟机以使用 HugePages。
- en: Libvirt provides a convenient way to manage the amount of allocated memory for
    the KVM guests. In step 11, we set a hard limit of 2 GB for the `kvm1` virtual
    machine.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Libvirt 提供了一种方便的方式来管理 KVM 客户机分配的内存量。在步骤 11 中，我们为 `kvm1` 虚拟机设置了 2 GB 的硬性限制。
- en: CPU performance options
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU 性能选项
- en: There are a few methods to control CPU allocation and the available CPU cycles for
    KVM machines-using cgroups and the libvirt-provided CPU pinning and affinity functions,
    we are going to explore in this recipe. CPU affinity is a scheduler property that
    connects a process to a given set of CPUs on the host OS.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以控制 CPU 分配以及 KVM 虚拟机的可用 CPU 周期——通过使用 cgroups 和 libvirt 提供的 CPU 钉扎和亲和性功能，我们将在本配方中探讨这些内容。CPU
    亲和性是一种调度器属性，它将进程与主机操作系统上给定的一组 CPU 连接。
- en: When provisioning virtual machines with libvirt, the default behavior is to
    provision the guests on any available CPU cores. In some cases, **Non-Uniform
    Memory Access** (**NUMA**) is a good example of when we need to designate a core
    per KVM instance (as we are going to see in the next recipe), that it's better
    to assign the virtual machine to a specified CPU core. Since each KVM virtual
    machine is a kernel process (`qemu-system-x86_64` more specifically in our examples),
    we can do this using tools such as `taskset` or the `virsh` command. We can also
    use the cgroups CPU subsystem to manage CPU cycle allocation, which provides more
    granular control over CPU resource utilization per virtual machine.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 libvirt 配置虚拟机时，默认行为是将客户机分配到任何可用的 CPU 核心。在某些情况下，**非均匀内存访问**（**NUMA**）就是一个需要为每个
    KVM 实例指定一个核心的好例子（正如我们将在下一个食谱中看到的那样）。因此，为虚拟机分配指定的 CPU 核心会更好。由于每个 KVM 虚拟机都是一个内核进程（在我们的示例中，更具体地说是
    `qemu-system-x86_64`），我们可以使用 `taskset` 或 `virsh` 命令等工具来实现这一点。我们还可以使用 cgroups 的
    CPU 子系统来管理 CPU 周期分配，从而对每个虚拟机的 CPU 资源利用进行更细粒度的控制。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we are going to need the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中，我们需要以下内容：
- en: An Ubuntu host, with libvirt and QEMU installed and configured
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装并配置了 libvirt 和 QEMU 的 Ubuntu 主机
- en: A running KVM virtual machine
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行中的 KVM 虚拟机
- en: How to do it...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To pin a KVM virtual machine to a specific CPU and to change the CPU shares,
    perform the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 KVM 虚拟机固定到特定 CPU 并更改 CPU 共享，请执行以下操作：
- en: 'Obtain information about the available CPU cores on the hypervisor:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取关于主机上可用 CPU 核心的信息：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Get information about the CPU allocation for the KVM guest:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取关于 KVM 客户机的 CPU 分配信息：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Pin the KVM instance CPU (`VCPU: 0`) to the first hypervisor CPU (`CPU: 0`)
    and display the result:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将 KVM 实例的 CPU（`VCPU: 0`）固定到第一个虚拟机 CPU（`CPU: 0`）并显示结果：'
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'List the share of runtime that is assigned to a KVM instance:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出分配给 KVM 实例的运行时共享数：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Modify the current CPU weight of a running virtual machine:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改运行中虚拟机的当前 CPU 权重：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Check the CPU shares in the CPU cgroups subsystem:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查 CPU cgroups 子系统中的 CPU 共享：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Examine the updated XML instance definition:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查更新后的 XML 实例定义：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We begin by gathering information about the CPU resources available on the hypervisor.
    From the output in step 1, we can see that the host OS has 40 CPUs on one socket.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先收集关于主机上可用的 CPU 资源信息。从步骤 1 的输出中，我们可以看到主机操作系统在一个插槽上有 40 个 CPU。
- en: 'In step 2, we collect information about the virtual machine CPU and its affinity
    with the host CPUs. In this example, the KVM guest has one virtual CPU, denoted
    by the `VCPU: 0` record and affinity to all 40 hypervisor processors, as indicated
    by the `CPU Affinity: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy` field.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '在步骤 2 中，我们收集虚拟机 CPU 及其与主机 CPU 的亲和性信息。在这个示例中，KVM 客户机有一个虚拟 CPU，通过 `VCPU: 0` 记录表示，并且与所有
    40 个虚拟机处理器相关联，如 `CPU Affinity: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy` 字段所示。'
- en: 'In step 3, we pin/bind the virtual CPU to the first physical processor on the
    hypervisor. Note the change in the affinity output: `CPU Affinity: y---------------------------------------`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在步骤 3 中，我们将虚拟 CPU 固定/绑定到主机上的第一个物理处理器。注意亲和性输出的变化：`CPU Affinity: y---------------------------------------`。'
- en: From the output of the `virsh` command in step 4, we can observe that the CPU
    shares allocated to the KVM instance are set to 1024\. This value is a ratio,
    meaning that if another guest has 512 shares, it will have twice fewer CPU runtime
    than that of an instance with 1024 shares. We reduce that value in step 5.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从步骤 4 中 `virsh` 命令的输出中，我们可以观察到分配给 KVM 实例的 CPU 共享数被设置为 1024。这个值是一个比率，意味着如果另一个客户机的共享数为
    512，它将比一个共享数为 1024 的实例少一倍的 CPU 运行时间。我们将在步骤 5 中减少该值。
- en: In steps 6 and 7, we confirm that the CPU shares were correctly set in the CPU
    cgroup subsystem on the host OS. As we mentioned earlier, CPU shares are configured
    using cgroups and can be adjusted directly or by the provided libvirt functionality,
    by means of the `virsh` command.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 6 和 7 中，我们确认了 CPU 共享在主机操作系统的 CPU cgroup 子系统中正确设置。如前所述，CPU 共享是通过 cgroups
    配置的，可以通过 `virsh` 命令直接调整，或者通过 libvirt 提供的功能进行调整。
- en: NUMA tuning with libvirt
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 libvirt 进行 NUMA 调优
- en: NUMA is a technology that allows the system memory to be divided into zones,
    also named nodes. The NUMA nodes are then allocated to particular CPUs or sockets.
    In contrast to the traditional monolithic memory approach, where each CPU/core
    can access all the memory regardless of its locality, usually resulting in larger
    latencies, NUMA bound processes can access memory that is local to the CPU they
    are being executed on. In most cases, this is much faster than the memory connected
    to the remote CPUs on the system.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: NUMA 是一种技术，它允许将系统内存分为多个区域，也称为节点。NUMA 节点随后分配给特定的 CPU 或插槽。与传统的单一内存访问方式不同，在该方式下每个
    CPU/核心可以访问所有内存而不考虑其局部性，通常会导致较大的延迟，NUMA 限制的进程可以访问本地 CPU 执行的内存。通常，这比访问连接到远程 CPU
    的内存要快得多。
- en: 'Libvirt uses the `libnuma` library to enable NUMA functionality for virtual
    machines, as we can see here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Libvirt 使用 `libnuma` 库为虚拟机启用 NUMA 功能，如下所示：
- en: '[PRE27]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Libvirt NUMA supports the following memory allocation policies to place virtual
    machines to NUMA nodes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Libvirt NUMA 支持以下内存分配策略来将虚拟机分配到 NUMA 节点：
- en: '**strict**: The placement will fail if the memory cannot be allocated on the
    target node'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strict**: 如果内存无法分配到目标节点，分配将失败'
- en: '**interleave**: Memory pages are allocated in a round-robin fashion'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**interleave**: 内存页面以轮询方式分配'
- en: '**preferred**: This policy allows the hypervisor to provide memory from other
    nodes in case there''s not enough memory available from the specified nodes'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**preferred**: 该策略允许虚拟化主机在指定节点的内存不足时从其他节点提供内存'
- en: In this recipe, we are going to enable NUMA access for a KVM instance and explore
    its impact on the overall system performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将启用 KVM 实例的 NUMA 访问，并探索其对整体系统性能的影响。
- en: Getting ready
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'For this recipe, we are going to need the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将需要以下内容：
- en: An Ubuntu host, with libvirt and QEMU installed and configured
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台安装并配置了 libvirt 和 QEMU 的 Ubuntu 主机
- en: A running KVM virtual machine
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台正在运行的 KVM 虚拟机
- en: The `numastat` utility
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numastat` 工具'
- en: How to do it...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'To enable a KVM virtual machine to run on a given NUMA node and CPU using the
    strictNUMA policy, perform the following steps:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用严格 NUMA 策略使 KVM 虚拟机在特定的 NUMA 节点和 CPU 上运行，请执行以下步骤：
- en: 'Install the `numactl` package and check the hardware configuration of the hypervisor:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `numactl` 包并检查虚拟化主机的硬件配置：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Display the current NUMA placement for the KVM guest:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示当前 KVM 客户机的 NUMA 布局：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Edit the XML instance definition, set the memory mode to strict, and select
    the second NUMA node (indexing starts from 0, so the second NUMA node is labeled
    as 1), then restart the guest:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 XML 实例定义，将内存模式设置为严格，并选择第二个 NUMA 节点（索引从 0 开始，因此第二个 NUMA 节点标记为 1），然后重启客户机：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Get the NUMA parameters for the KVM instance:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取 KVM 实例的 NUMA 参数：
- en: '[PRE31]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Print the current virtual CPU affinity:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印当前虚拟 CPU 的亲和性：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Print the NUMA node placement for the KVM instance:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 KVM 实例的 NUMA 节点布局：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'We start by examining the NUMA setup on the host OS. From the output of the
    `numactl` command in step 1, we can observe that the hypervisor has two NUMA nodes:
    node 0 and node 1\. Each node manages a list of CPUs. In this case, NUMA node
    1 contains CPUs from 10 to 19 and from 30 to 39 and contains 64 GB of memory.
    This means that 64 GB of RAM is going to be local to those CPUs and access to
    the memory from those CPUs is going to be much faster than from CPUs that are
    part of node 0\. To improve memory access latencies for a KVM guest, we need to
    pin the virtual CPUs assigned to the virtual machine to CPUs that are a part of
    the same NUMA node.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先检查主机操作系统上的 NUMA 设置。从步骤 1 中 `numactl` 命令的输出中，我们可以观察到虚拟化主机有两个 NUMA 节点：节点 0
    和节点 1。每个节点管理一组 CPU。在这种情况下，NUMA 节点 1 包含 CPU 10 到 19 和 CPU 30 到 39，并拥有 64 GB 的内存。这意味着
    64 GB 的 RAM 将本地化到这些 CPU 上，并且从这些 CPU 访问内存的速度要比从节点 0 的 CPU 访问内存的速度快得多。为了改善 KVM 客户机的内存访问延迟，我们需要将分配给虚拟机的虚拟
    CPU 固定到属于同一 NUMA 节点的 CPU 上。
- en: In step 2, we can see that the KVM instance uses memory from both NUMA nodes,
    which is not ideal.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 2 中，我们可以看到 KVM 实例使用来自两个 NUMA 节点的内存，这并不理想。
- en: In step 3, we edit the guest XML definition and pin the guest on the 10th and
    11th CPUs, which are a part of the NUMA node 1, using the `cpuset='10-11'` parameter.
    We also specify the strict NUMA node and the second NUMA node with the `<memory
    mode='strict' nodeset='1'/>` parameter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3中，我们编辑了虚拟机的XML定义，并使用`cpuset='10-11'`参数将虚拟机固定在第10和第11个CPU上，这些CPU属于NUMA节点1。我们还使用`<memory
    mode='strict' nodeset='1'/>`参数指定了严格的NUMA节点以及第二个NUMA节点。
- en: After restarting the instance, in step 4, we confirm that the KVM guest is now
    running using the strict NUMA mode on node 1\. We also confirm that the CPU pinning
    is indeed what we specified in step 5\. Note that the CPU affinity is flagged
    on the 10th and 11th elements of the CPU affinity element.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在重启实例后，在步骤4中，我们确认KVM虚拟机现在正在使用严格的NUMA模式在节点1上运行。我们还确认CPU绑定确实符合步骤5中指定的内容。请注意，CPU亲和性已在CPU亲和性元素的第10和第11个元素上标记。
- en: From the output in step 6, we can see that the KVM guest is now using memory
    only from the NUMA node 1 as desired.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从步骤6的输出中，我们可以看到KVM虚拟机现在仅使用NUMA节点1的内存，符合预期。
- en: If you run a memory intensive application before and after the NUMA adjustment
    and test, you will most likely see significant performance gains when accessing
    large amounts of memory inside the KVM guest, thanks to the CPU and memory locality
    that NUMA provides.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在NUMA调整前后运行一个内存密集型应用并进行测试，你很可能会看到，当访问KVM虚拟机内的大量内存时，得益于NUMA提供的CPU和内存本地性，性能有显著提升。
- en: There is more...
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'In this recipe, we saw examples on how to manually assign a KVM process to
    a NUMA node by editing the XML definition of the guest. Some Linux distributions
    such as RHEL/CentOS 7 and Ubuntu 16.04 provide the `numad` (NUMA daemon) service,
    which aims at automatically balancing processes between NUMA nodes by monitoring
    the current memory topology:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们展示了如何通过编辑虚拟机的XML定义手动将KVM进程分配到NUMA节点。像RHEL/CentOS 7和Ubuntu 16.04这样的某些Linux发行版提供了`numad`（NUMA守护进程）服务，旨在通过监控当前的内存拓扑，自动平衡NUMA节点之间的进程：
- en: 'To install the service on Ubuntu 16.04, run:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Ubuntu 16.04上安装该服务，请运行：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To start the service, execute the following code:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动该服务，请执行以下代码：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To manage a specific KVM guest with `numad`, pass the process ID of the KVM
    instance:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用`numad`管理特定的KVM虚拟机，传递KVM实例的进程ID：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The service will log any NUMA rebalancing attempts:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该服务将记录任何NUMA重平衡尝试：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `numad` service can be helpful on OpenStack compute nodes, where manual
    NUMA balancing may be too involving.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`numad`服务在OpenStack计算节点中非常有用，手动进行NUMA平衡可能过于繁琐。'
- en: Tuning the kernel for network performance
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调优内核以提升网络性能
- en: Most modern Linux kernels ship sufficiently tuned for various network workloads.
    Some distributions provide predefined tuning services (a good example is `tuned` for
    Red Hat/CentOS), which include a set of profiles based on the server role.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代Linux内核已针对各种网络工作负载进行了足够的调优。一些发行版提供预定义的调优服务（例如，Red Hat/CentOS的`tuned`），包括基于服务器角色的配置文件集合。
- en: 'Let''s go over the steps taken during data transmission and reception, on a
    typical Linux host, before we delve into how to tune the hypervisor:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨如何调优虚拟化管理程序之前，让我们回顾一下典型Linux主机在数据传输和接收过程中所执行的步骤：
- en: The application first writes the data to a socket, which in turn is put in the
    transmit buffer.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序首先将数据写入套接字，然后这些数据被放入发送缓冲区。
- en: The kernel encapsulates the data into a **Protocol Data Unit** (**PDU**).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内核将数据封装成**协议数据单元**（**PDU**）。
- en: The PDU is then moved onto the per-device transmit queue.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，PDU被移至每个设备的发送队列。
- en: The **Network Interface Cards** (**NIC**) driver then pops the PDU from the
    transmit queue and copies it to the NIC.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网络接口卡**（**NIC**）驱动程序将从发送队列中取出PDU并将其复制到NIC。'
- en: The NIC sends the data and raises a hardware interrupt.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NIC发送数据并触发硬件中断。
- en: On the other end of the communication channel, the NIC receives the frame, copies
    it on the receive buffer, and raises hard interrupt.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通信通道的另一端，NIC接收帧，将其复制到接收缓冲区，并触发硬中断。
- en: The kernel in turn handles the interrupt and raises a soft interrupt to process
    the packet.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内核则处理该中断并触发软中断来处理数据包。
- en: Finally, the kernel handles the soft interrupt and moves the packet up the TCP/IP
    stack for decapsulation, and puts it in a receive buffer for a process to read
    from.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，内核处理软中断，将数据包上送至TCP/IP栈进行解封装，并将其放入接收缓冲区，供进程读取。
- en: In this recipe, we are going to examine a few best practices for tuning the
    Linux kernel, usually resulting in better network performance, on multitenant
    KVM hosts.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探讨一些调整Linux内核的最佳实践，这通常能在多租户KVM主机上提高网络性能。
- en: Please make sure that you establish a baseline before making any configuration
    changes, by measuring the host performance first. Make small incremental changes,
    then measure the impact again.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在进行任何配置更改之前，通过先测量主机性能来建立基线。进行小范围的增量更改后，再次测量其影响。
- en: The examples in this recipe are not meant to be copied/pasted without prior
    understanding of the possible positive or negative impact they might make. Use
    the examples presented as a guide as to what can be tuned-the actual values must
    be carefully considered, based on the server type and the entire environment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中的示例并非旨在直接复制粘贴，而是用于帮助理解可能的正面或负面影响。在使用这些示例时，请作为调优指南参考——实际值必须根据服务器类型和整个环境仔细考虑。
- en: Getting ready
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we are going to need the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本教程，我们将需要以下内容：
- en: An Ubuntu host, with libvirt and QEMU installed and configured
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台已安装并配置了libvirt和QEMU的Ubuntu主机
- en: A running KVM virtual machine
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正在运行的KVM虚拟机
- en: How to do it...
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To tune the kernel for better network performance, execute the following steps
    (for more information on what the kernel tunables are, read the *How it works...*
    section):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整内核以获得更好的网络性能，请执行以下步骤（有关内核可调项的更多信息，请阅读*工作原理...*部分）：
- en: 'Increase the max TCP send and receive socket buffer size:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加最大TCP发送和接收套接字缓冲区大小：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Increase the TCP buffer limits: min, default, and max number of bytes. Set
    max to 16 MB for 1 GE NIC, and 32 M or 54 M for 10 GE NIC:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加TCP缓冲区限制：最小、默认和最大字节数。对于1 GE网卡，将最大值设置为16 MB，对于10 GE网卡，设置为32 MB或54 MB：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Ensure that TCP window scaling is enabled:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保启用TCP窗口扩展：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To help increase TCP throughput with 1 GB NICs or larger, increase the length
    of the transmit queue of the network interface. For paths with more than 50 ms
    RTT, a value of 5000-10000 is recommended:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高1 GB网卡或更大网卡的TCP吞吐量，请增加网络接口的发送队列长度。对于往返时延（RTT）大于50 ms的路径，建议设置为5000-10000：
- en: '[PRE41]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Reduce the `tcp_fin_timeout` value:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少`tcp_fin_timeout`值：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Reduce the `tcp_keepalive_intvl` value:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低`tcp_keepalive_intvl`值：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Enable fast recycling of `TIME_WAIT` sockets. The default value is 0 (disabled):'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`TIME_WAIT`套接字的快速回收。默认值为0（禁用）：
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Enable the reusing of sockets in  the `TIME_WAIT` state for new connections.
    The default value is 0 (disabled):'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用`TIME_WAIT`状态下套接字的重用，以便为新连接使用。默认值为0（禁用）：
- en: '[PRE45]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Starting with kernel version 2.6.13, Linux supports pluggable congestion control
    algorithms. The congestion control algorithm used is set using the `sysctl` variable
    `net.ipv4.tcp_congestion_control`, which is set to bic/cubic by default on Ubuntu.
    To get a list of congestion control algorithms that are available in your kernel
    (if you are running 2.6.20 or higher), run the following:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从内核版本2.6.13开始，Linux支持可插拔的拥塞控制算法。所使用的拥塞控制算法通过`sysctl`变量`net.ipv4.tcp_congestion_control`设置，Ubuntu的默认值为bic/cubic。要查看内核中可用的拥塞控制算法列表（如果您运行的是2.6.20或更高版本），请运行以下命令：
- en: '[PRE46]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To enable more pluggable congestion control algorithms, load the kernel modules:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启用更多可插拔的拥塞控制算法，请加载内核模块：
- en: '[PRE47]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For long, fast paths, it is usually better to use cubic or htcp algorithms.
    Cubic is the default for a number of Linux distributions, but if it is not the
    default on your system, you can do the following:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于长且快速的路径，通常最好使用cubic或htcp算法。Cubic是许多Linux发行版的默认算法，但如果在您的系统中不是默认值，您可以执行以下操作：
- en: '[PRE48]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If the hypervisor is overwhelmed with SYN connections, the following options
    might help in reducing the impact:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果虚拟机监控程序（Hypervisor）被SYN连接淹没，以下选项可能有助于减少影响：
- en: '[PRE49]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Having a sufficient number of available file descriptors is quite important,
    since pretty much everything on Linux is a file. Each network connection uses
    a file descriptor/socket. To check your current max and available file descriptors, run
    the following code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有足够的可用文件描述符非常重要，因为Linux上的几乎所有东西都是文件。每个网络连接都使用一个文件描述符/套接字。要检查当前的最大和可用文件描述符，请运行以下代码：
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To increase the max file descriptors, execute the following:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要增加最大文件描述符，请执行以下操作：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If your hypervisor is using stateful iptable rules, the `nf_conntrack` kernel
    module might run out of memory for connection tracking and an error will be logged:
    `nf_conntrack: table full, dropping packet`. In order to raise that limit and
    therefore allocate more memory, you need to calculate how much RAM each connection
    uses. You can get that information from the proc file `/proc/slabinfo`. The `nf_conntrack`
    entry shows the active entries, how big each object is, and how many fit in a
    slab (each slab fits in one or more kernel page, usually 4 K if not using HugePages).
    Accounting for the overhead of the kernel page size, you can see from the `slabinfo`
    that each `nf_conntrack` object takes about 316 bytes (this will differ on different
    systems). So to track 1 M connections, you''ll need to allocate roughly 316 MB
    of memory:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果你的虚拟机监控程序使用有状态的iptables规则，`nf_conntrack`内核模块可能会因连接跟踪耗尽内存而导致错误日志记录：`nf_conntrack:
    table full, dropping packet`。为了提高这个限制并分配更多内存，你需要计算每个连接占用多少RAM。你可以通过`/proc/slabinfo`文件获取此信息。`nf_conntrack`条目展示了活动条目、每个对象的大小以及每个slab中可以容纳多少个对象（每个slab通常会占用一个或多个内核页，通常是4K，如果没有使用HugePages的话）。考虑到内核页的开销，你可以从`slabinfo`中看到每个`nf_conntrack`对象大约占用316字节（不同系统上可能有所不同）。因此，要跟踪100万个连接，你大约需要分配316
    MB的内存：'
- en: '[PRE52]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: How it works...
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we increase the maximum send and receive socket buffers. This will
    allocate more memory to the TCP stack, but on servers with a large amount of memory
    and many TCP connections, it will ensure that the buffer sizes will be sufficient.
    A good starting point for selecting the default values is the **Bandwidth Delay
    Product **(**BDP**) based on a measured delay, for example, multiply the bandwidth
    of the link to the average round trip time to some host.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们增加了最大发送和接收套接字缓冲区的大小。这将为TCP栈分配更多内存，但在内存较大且有大量TCP连接的服务器上，确保缓冲区大小足够。选择默认值的一个好起点是**带宽延迟积（BDP）**，它是基于测得的延迟，例如，将链路带宽与某个主机的平均往返时间相乘。
- en: In step 2, we increase the min, default, and max number of bytes used by TCP
    to regulate send buffer sizes. TCP dynamically adjusts the size of the send buffer
    from the default values.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们增加了TCP用于调节发送缓冲区大小的最小、默认和最大字节数。TCP会根据默认值动态调整发送缓冲区的大小。
- en: In step 3, we make sure that window scaling is enabled. TCP window scaling automatically
    increases the receive window size.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们确保启用了窗口缩放。TCP窗口缩放会自动增大接收窗口的大小。
- en: For more information on window scaling, please refer to[ https://en.wikipedia.org/wiki/TCP_window_scale_option](https://en.wikipedia.org/wiki/TCP_window_scale_option).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 有关窗口缩放的更多信息，请参考[https://en.wikipedia.org/wiki/TCP_window_scale_option](https://en.wikipedia.org/wiki/TCP_window_scale_option)。
- en: In step 5, we reduce the `tcp_fin_timeout` value which specifies how many seconds
    to wait for a final FIN packet before the socket is forcibly closed. In steps
    6 and 7, we reduce the number of seconds between TCP keep-alive probes and fast
    recycling of sockets in the `TIME_WAIT` state.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们减少了`tcp_fin_timeout`值，该值指定在强制关闭套接字之前等待最后一个FIN包的秒数。在第6和第7步中，我们减少了TCP保持活动探测的时间间隔，并加速了`TIME_WAIT`状态下套接字的回收。
- en: 'As a refresher, the following diagram shows the various TCP states a connection
    can be in:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助回忆，以下图示展示了连接可能处于的各种TCP状态：
- en: '![](img/00013.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: TCP state diagram
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: TCP状态图
- en: In step 8, we enable the reuse of sockets in the `TIME_WAIT` state only for
    new connections. On hosts with large numbers of KVM instances, this might have
    a significant impact on how fast new connections can be established.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8步中，我们仅对新连接启用`TIME_WAIT`状态下的套接字重用。在有大量KVM实例的主机上，这可能会显著影响新连接的建立速度。
- en: In steps 9 and 10, we enable various congestion control algorithms. The choice
    of congestion control algorithms is selected when the kernel is built. In step
    11, we select the cubic algorithm, in which the window is a cubic function of
    time since the last congestion event, with the inflection point set to the window
    prior to that event.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9和第10步中，我们启用了各种拥塞控制算法。选择哪种拥塞控制算法是在内核构建时确定的。在第11步中，我们选择了cubic算法，其中窗口是自上次拥塞事件以来时间的立方函数，拐点设置为该事件发生前的窗口大小。
- en: For more information about network congestion-avoidance algorithms, please refer
    to [https://en.wikipedia.org/wiki/TCP_congestion_control](https://en.wikipedia.org/wiki/TCP_congestion_control).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于网络拥塞避免算法的信息，请参考[https://en.wikipedia.org/wiki/TCP_congestion_control](https://en.wikipedia.org/wiki/TCP_congestion_control)。
- en: On systems experiencing an overwhelming amount of SYN requests, adjusting the
    maximum number of queued connection requests that have still not received an acknowledgement
    from the connecting client, using the `tcp_max_syn_backlog` and `tcp_synack_retries`
    options, might help. We do that in step 12.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在遇到大量SYN请求的系统中，通过调整尚未收到连接客户端确认的排队连接请求的最大数量，可以使用`tcp_max_syn_backlog`和`tcp_synack_retries`选项进行优化。这一操作我们在第12步完成。
- en: In steps 13 and 14, we increase the maximum number of file descriptors on the
    system. This helps when a large number of network connections are present because
    each connection requires a file descriptor.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13和第14步中，我们增加了系统上的最大文件描述符数量。当系统中有大量网络连接时，这有助于处理，因为每个连接都需要一个文件描述符。
- en: In the last step, we have the `nf_conntrack_max` option. This is useful if we
    are tracking connections on the hypervisor using the `nf_conntrack` kernel module.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们有`nf_conntrack_max`选项。如果我们使用`nf_conntrack`内核模块在虚拟机监控器上跟踪连接，这个选项是非常有用的。
