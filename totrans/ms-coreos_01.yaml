- en: Chapter 1. CoreOS Overview
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS is a Container-optimized Linux-based operating system to deploy a distributed
    application across a cluster of nodes. Along with providing a secure operating
    system, CoreOS provides services such as `etcd` and `fleet` that simplify the
    Container-based distributed application deployment. This chapter will provide
    you with an overview of Microservices and distributed application development
    concepts along with the basics of CoreOS, Containers, and Docker. Microservices
    is a software application development style where applications are composed of
    small, independent services talking to each other with APIs. After going through
    the chapter, you will be able to appreciate the role of CoreOS and Containers
    in the Microservices architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed application development—an overview and components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparison of currently available minimalist Container-optimized OSes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers—technology and advantages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker—architecture and advantages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS—architecture and components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of CoreOS components—`systemd`, `etcd`, `fleet`, `flannel`, and
    `rkt`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker versus Rkt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A workflow for distributed application development with Docker, Rkt, and CoreOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed application development
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed application development involves designing and coding a microservice-based
    application rather than creating a monolithic application. Each standalone service
    in the microservice-based application can be created as a Container. Distributed
    applications existed even before Containers were available. Containers provide
    the additional benefit of isolation and portability to each individual service
    in the distributed application. The following diagram shows you an example of
    a microservice-based application spanning multiple hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Components of distributed application development
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the primary components of distributed application development.
    This assumes that individual services of the distributed application are created
    as Containers:'
  prefs: []
  type: TYPE_NORMAL
- en: Applications or microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud infrastructure—public (AWS, GCE, and Digital Ocean) or private.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base OS—CoreOS, Atomic, Rancher OS, and others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed data store and service discovery—`etcd`, `consul`, and `Zookeeper`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancer—NGINX and HAProxy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime—Docker, Rkt, and LXC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container orchestration—Fleet, Kubernetes, Mesos, and Docker Swarm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage—local or distributed storage. Some examples are GlusterFS and Ceph for
    cluster storage and AWS EBS for cloud storage. Flocker's upcoming storage driver
    plugin promises to work across different storage mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking—using cloud-based networking such as AWS VPC, CoreOS Flannel, or
    Docker networking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miscellaneous—Container monitoring (cadvisor, Sysdig, and Newrelic) and Logging
    (Spout and Logentries).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An update strategy to update microservices, such as a rolling upgrade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages and disadvantages
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some advantages of distributed application development:'
  prefs: []
  type: TYPE_NORMAL
- en: Application developers of each microservice can work independently. If necessary,
    different microservices can even have their own programming language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application component reuse becomes high. Different unrelated projects can use
    the same microservice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each individual service can be horizontally scaled. CPU and memory usage for
    each microservice can be tuned appropriately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure can be treated like cattle rather than a pet, and it is not necessary
    to differentiate between each individual infrastructure component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications can be deployed in-house or on a public, private, or hybrid cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some problems associated with the microservices approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of microservices to manage can become huge and this makes it complex
    to manage the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging can become difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining integrity and consistency is difficult so services must be designed
    to handle failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools are constantly changing, so there is a need to stay updated with current
    technologies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A minimalist Container-optimized OS
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a new OS category for developing distributed applications that has
    become popular in recent years. Traditional Linux-based OSes were bulky for Container
    deployment and did not natively provide the services that Containers need. The
    following are some common characteristics of a Container-optimized OS:'
  prefs: []
  type: TYPE_NORMAL
- en: The OS needs to be bare-minimal and fast to bootup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should have an automated update strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application development should be done using Containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundancy and clustering should be built-in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following table captures the comparison of features of four common Container-optimized
    OSes. Other OSes such as VMWare Photon and Mesos DCOS have not been included.
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | CoreOS | Rancher OS | Atomic | Ubuntu snappy |'
  prefs: []
  type: TYPE_TB
- en: '| Company | CoreOS | Rancher Labs | Red Hat | Canonical |'
  prefs: []
  type: TYPE_TB
- en: '| Containers | Docker and Rkt | Docker | Docker | Snappy packages and Docker
    |'
  prefs: []
  type: TYPE_TB
- en: '| Maturity | First release in 2013, relatively mature | First release in early
    2015, pretty new | First release in early 2015, pretty new | First release in
    early 2015, pretty new |'
  prefs: []
  type: TYPE_TB
- en: '| Service management | Systemd and Fleet | System docker manages system services
    and user docker manages user containers | Systemd | Systemd and Upstart |'
  prefs: []
  type: TYPE_TB
- en: '| Tools | Etcd, fleet, and flannel | Rancher has tools for service discovery,
    load balancing, dns, storage, and networking | Flannel and other RedHat tools
    | Ubuntu tools |'
  prefs: []
  type: TYPE_TB
- en: '| Orchestration | Kubernetes and Tectonic | Rancher''s own orchestration and
    Kubernetes | Kubernetes. Atomic app, and Nulecule also used | Kubernetes and any
    other orchestration tool |'
  prefs: []
  type: TYPE_TB
- en: '| Update | Automatic, uses A and B partitions | Automatic | Automatic, uses
    rpm-os-tree | Automatic |'
  prefs: []
  type: TYPE_TB
- en: '| Registry | Docker hub and Quay | Docker hub | Docker hub | Docker hub |'
  prefs: []
  type: TYPE_TB
- en: '| Debugging | Toolbox | Rancher''s own tools and external tools | RedHat tools
    | Ubuntu debug tools |'
  prefs: []
  type: TYPE_TB
- en: '| Security | SELinux can be turned on | There is a plan to add SELinux and
    AppArmor support | SELinux enabled by default, additional security | AppArmor
    security profile can be used |'
  prefs: []
  type: TYPE_TB
- en: Containers
  prefs: []
  type: TYPE_NORMAL
- en: Containers do virtualization at the OS level while VMs do virtualization at
    the hardware level. Containers in a single host share the same kernel. As Containers
    are lightweight, hundreds of containers can run on a single host. In a microservices-based
    design, the approach taken is to split a single application into multiple small
    independent components and run each component as a Container. LXC, Docker, and
    Rkt are examples of Container runtime implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Technology
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the two critical Linux kernel technologies that are used
    in Containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Namespaces: They virtualize processes, networks, filesystems, users, and so
    on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'cgroups: They limit the usage of the CPU, memory, and I/O per group of processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some significant advantages of Containers:'
  prefs: []
  type: TYPE_NORMAL
- en: Each container is isolated from other Containers. There is no issue of shared
    package management, shared libraries, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to a VM, Containers have smaller footprints and are faster to load
    and run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide an efficient usage of computing power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can work seamlessly across dev, test, and production. This makes Containers
    DevOps-friendly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of Docker architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker is a Container runtime implementation. Even though Containers were available
    for quite a long time, Docker revolutionized Container technology by making it
    easier to use. The following image shows you the main components of Docker (the
    Docker engine, Docker CLI, Docker REST, and Docker hub) and how they interact
    with each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00192.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following are some details on the Docker architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker daemon runs in every host where Docker is installed and started.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker uses Linux kernel container facilities such as namespaces and cgroups
    through the libcontainer library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker client can run in the host machine or externally and it communicates
    with the Docker daemon using the REST interface. There is also a CLI interface
    that the Docker client provides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker hub is the repository for Docker images. Both private and public
    images can be hosted in the Docker hub repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dockerfile is used to create container images. The following is a sample Dockerfile
    that is used to create a Container that starts the Apache web service exposing
    port `80` to the outside world:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00194.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: The Docker platform as of release 1.9 includes orchestration tools such as Swarm,
    Compose, Kitematic, and Machine as well as native networking and storage solutions.
    Docker follows a batteries-included pluggable approach for orchestration, storage,
    and networking where a native Docker solution can be swapped with vendor plugins.
    For example, Weave can be used as an external networking plugin, Flocker can be
    used as an external storage plugin, and Kubernetes can be used as an external
    orchestration plugin. These external plugins can replace the native Docker solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of Docker
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some significant advantages of Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker has revolutionized Container packaging and tools around Containers and
    this has helped both application developers and infrastructure administrators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easier to deploy and upgrade individual containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is more suitable for the microservices architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works great across all Linux distributions as long as the kernel version
    is greater than or equal to 3.10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Union filesystem makes it faster to download and keep different versions
    of container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container management tools such as Dockerfile, Docker engine CLI, Machine, Compose,
    and Swarm make it easy to manage containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker provides an easy way to share Container images using public and private
    registry services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS belongs to the minimalist Container-optimized OS category. CoreOS is
    the first OS in this category and many new OSes have appeared recently in the
    same category. CoreOS's mission is to improve the security and reliability of
    the Internet. CoreOS is a pioneer in this space and its first alpha release was
    in July 2013\. A lot of developments have happened in the past two years in the
    area of networking, distributed storage, container runtime, authentication, and
    security. CoreOS is used by PaaS providers (such as Dokku and Deis), Web application
    development companies, and many enterprise and service providers developing distributed
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Properties
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the key CoreOS properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel is very small and fast to bootup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The base OS and all services are open sourced. Services can also be used standalone
    in non-CoreOS systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No package management is provided by the OS. Libraries and packages are part
    of the application developed using Containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It enables secure, large server clusters that can be used for distributed application
    development.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is based on principles from the Google Chrome OS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime, SSH, and kernel are the primary components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every process is managed by systemd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd, fleet, and flannel are all controller units running on top of the kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports both Docker and Rkt Container runtime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic updates are provided with A and B partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Quay registry service can be used to store public and private Container
    images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS release channels (stable, beta, and alpha) are used to control the release
    cycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commercial products include the Coreupdate service (part of the commercially
    managed and enterprise CoreOS), Quay enterprise, and Tectonic (CoreOS + Kubernetes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It currently runs on x86 processors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some significant advantages of CoreOS:'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel auto-update feature protects the kernel from security vulnerabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CoreOS memory footprint is very small.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The management of CoreOS machines is done at the cluster level rather than at
    an individual machine level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides service-level (using systemd) and node-level (using fleet) redundancy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quay provides you with a private and public Container repository. The repository
    can be used for both Docker and Rkt containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet is used for basic service orchestration and Kubernetes is used for application
    service orchestration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is supported by all major cloud providers such as AWS, GCE, Azure, and DigitalOcean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majority of CoreOS components are open sourced and the customer can choose the
    combination of tools that is necessary for their specific application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supported platforms
  prefs: []
  type: TYPE_NORMAL
- en: The following are the official and community-supported CoreOS platforms. This
    is not an exhaustive list.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For exhaustive list of CoreOS supported platforms, please refer to this link
    ([https://coreos.com/os/docs/latest/](https://coreos.com/os/docs/latest/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The platforms that are officially supported are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud platforms such as AWS, GCE, Microsoft Azure, DigitalOcean, and OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bare metal with PXE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vagrant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The platforms that are community-supported are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: CloudStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS components
  prefs: []
  type: TYPE_NORMAL
- en: The following are the CoreOS core components and CoreOS ecosystem. The ecosystem
    can become pretty large if automation, management, and monitoring tools are included.
    These have not been included here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Core components: Kernel, systemd, etcd, fleet, flannel, and rkt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS ecosystem: Docker and Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows you the different layers in the CoreOS architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00197.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'CoreOS uses the latest Linux kernel in its distribution. The following screenshot
    shows the Linux kernel version running in the CoreOS stable release 766.3.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Systemd
  prefs: []
  type: TYPE_NORMAL
- en: 'Systemd is an init system used by CoreOS to start, stop, and manage processes.
    SysVinit is one of the oldest init systems. The following are some of the common
    init systems used in the Unix world:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Systemd: CoreOS and RedHat'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upstart: Ubuntu'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Supervisord: The Python world'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some of the common functionality performed by an init system:'
  prefs: []
  type: TYPE_NORMAL
- en: It is the first process to start
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It controls the ordering and execution of all the user processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes care of restarting processes if they die or hang
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes care of process ownership and resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are some specifics of systemd:'
  prefs: []
  type: TYPE_NORMAL
- en: Every process in systemd runs in one cgroup and this includes forked processes.
    If the systemd service is killed, all the processes associated with the service,
    including forked processes, are killed. This also provides you with a nice way
    to control resource usage. If we run a Container in systemd, we can control the
    resource usage even if the container contains multiple processes. Additionally,
    systemd takes care of restarting containers that die if we specify the `restart`
    option in systemd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systemd units are run and controlled on a single machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are some systemd unit types—service, socket, device, and mount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Service` type is the most common type and is used to define a service with
    its dependencies. The `Socket` type is used to expose services to the external
    world. For example, `docker.service` exposes external connectivity to the Docker
    engine through `docker.socket`. Sockets can also be used to export logs to external
    machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `systemctl` CLI can be used to control Systemd units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systemd units
  prefs: []
  type: TYPE_NORMAL
- en: The following are some important systemd units in a CoreOS system.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd2.service
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example `etcd2.service` unit file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00201.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some details about the etcd2 service unit file:'
  prefs: []
  type: TYPE_NORMAL
- en: All units have the `[Unit]` and `[Install]` sections. There is a type-specific
    section such as `[Service]` for service units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Conflicts` option notifies that either `etcd` or `etcd2` can run, but not
    both.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Environment` option specifies the environment variables to be used by `etcd2`.
    The `%m` unit specifier allows the machine ID to be taken automatically based
    on where the service is running.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ExecStart` option specifies the executable to be run.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Restart` option specifies whether the service can be restarted. The `Restartsec`
    option specifies the time interval after which the service should be restarted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LimitNoFILE` specifies the file count limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `WantedBy` option in the `Install` section specifies the group to which
    this service belongs. The grouping mechanism allows systemd to start up groups
    of processes at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fleet.service
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of the `fleet.service` unit file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00203.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding unit file, we can see two dependencies for `fleet.service.`
    `etcd.Service` and `etcd2.service` are specified as dependencies as Fleet depends
    on them to communicate between fleet agents in different nodes. The `fleet.socket`
    socket unit is also specified as a dependency as it is used by external clients
    to talk to Fleet.
  prefs: []
  type: TYPE_NORMAL
- en: Docker.service
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker service consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Docker.service`: This starts the Docker daemon'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Docker.socket`: This allows communication with the Docker daemon from the
    CoreOS node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Docker-tcp.socket`: This allows communication with the Docker daemon from
    external hosts with port `2375` as the listening port'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following `docker.service` unit file starts the Docker daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00205.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following `docker.socket` unit file starts the local socket stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00208.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the example code
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this b
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `docker-tcp.socket` unit file sets up a listening socket for
    remote client communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00212.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `docker ps` command uses `docker.socket` and `docker -H tcp://127.0.0.1:2375
    ps` uses `docker-tcp.socket` unit to communicate with the Docker daemon running
    in the local system.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure to start a simple systemd service
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start a simple `hello1.service` unit that runs a Docker busybox container,
    as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00215.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the steps to start `hello1.service`:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy `hello1.service` as sudo to `/etc/systemd/system`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enable the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo systemctl enable /etc/systemd/system/hello1.service`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Start `hello1.service`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo systemctl start hello1.service`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This creates the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '`core@core-01 /etc/systemd/system/multi-user.target.wants $ ls -la``lrwxrwxrwx 1 root root   34 Aug 12 13:25 hello1.service -> /etc/systemd/system/hello1.service`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can see the status of `hello1.service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00219.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, we can see that the service is in the active state.
    At the end, we can also see `stdout` where the echo output is logged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the running Docker containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00223.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When starting Docker Containers with systemd, it is necessary to avoid using
    the `-d` option as it prevents the Container process to be monitored by systemd.
    More details can be found at [https://coreos.com/os/docs/latest/getting-started-with-docker.html](https://coreos.com/os/docs/latest/getting-started-with-docker.html).
  prefs: []
  type: TYPE_NORMAL
- en: Demonstrating systemd HA
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `hello1.service` created, we specified two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Restart=always RestartSec=30s`'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the service should be restarted after 30 seconds in case the
    service exits for some reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s stop the Docker `hello1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00226.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Service gets restarted automatically after 30 seconds, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00229.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows you that the `hello1` container is running again.
    From the Container status output, we can see that the container is up only for
    a minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00233.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also confirm the service restarted from the systemd logs associated
    with that service. In the following output, we can see that the service exited
    and restarted after 30 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00236.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Etcd
  prefs: []
  type: TYPE_NORMAL
- en: Etcd is a distributed key-value store used by all machines in the CoreOS cluster
    to read/write and exchange data. Etcd uses the Raft consensus algorithm ([https://raft.github.io/](https://raft.github.io/))
    to maintain a highly available cluster. Etcd is used to share configuration and
    monitoring data across CoreOS machines and for doing service discovery. All other
    CoreOS services such as Fleet and Flannel use etcd as a distributed database.
    Etcd can also be used as a standalone outside CoreOS. In fact, many complex distributed
    application projects such as Kubernetes and Cloudfoundry use etcd for their distributed
    key-value store. The `etcdctl` utility is the CLI frontend for etcd.
  prefs: []
  type: TYPE_NORMAL
- en: The following are two sample use cases of etcd.
  prefs: []
  type: TYPE_NORMAL
- en: 'Service discovery: Service discovery can be used to communicate service connectivity
    details across containers. Let''s take an example WordPress application with a
    WordPress application container and MySQL database container. If one of the machines
    has a database container and wants to communicate its service IP address and port
    number, it can use etcd to write the relevant key and data; the WordPress container
    in another host can use the key value to write to the appropriate database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration sharing: The Fleet master talks to Fleet agents using etcd to
    decide which node in the cluster will execute the Fleet service unit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd discovery
  prefs: []
  type: TYPE_NORMAL
- en: The members in the cluster discover themselves using either a static approach
    or dynamic approach. In the static approach, we need to mention the IP addresses
    of all the neighbors statically in every node of the cluster. In the dynamic approach,
    we use the discovery token approach where we get a distributed token from a central
    etcd server and use this in all members of the cluster so that the members can
    discover each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get a distributed token as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`curl https://discovery.etcd.io/new?size=<size>`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of getting a discovery token for a cluster size
    of three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00239.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The discovery token feature is hosted by CoreOS and is implemented as an etcd
    cluster as well.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster size
  prefs: []
  type: TYPE_NORMAL
- en: It is preferable to have an odd-sized etcd cluster as it gives a better failure
    tolerance. The following table shows the majority count and failure tolerance
    for common cluster sizes up to five. With a cluster size of two, we cannot determine
    majority.
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster size | Majority | Failure tolerance |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: The Majority count tells us the number of nodes that is necessary to have a
    working cluster, and failure tolerance tells us the number of nodes that can fail
    and still keep the cluster operational.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd cluster details
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Etcd member list in a 3 node CoreOS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00242.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are three members that are part of the etcd cluster with
    their machine ID, machine name, IP address, and port numbers used for etcd server-to-server
    and client-to-server communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows you the etcd cluster health:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00245.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that all three members of the etcd cluster are healthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows you etcd statistics with the cluster leader:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00248.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the member ID matches with the leader ID, `41419684c778c117`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows you etcd statistics with the cluster member:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00251.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simple set and get operations using etcd
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will set the `/message1` key to the `Book1` value
    and then later retrieve the value of the `/message1` key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00255.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fleet
  prefs: []
  type: TYPE_NORMAL
- en: Fleet is a cluster manager/scheduler that controls service creation at the cluster
    level. Like systemd being the init system for a node, Fleet serves as the init
    system for a cluster. Fleet uses etcd for internode communication.
  prefs: []
  type: TYPE_NORMAL
- en: The Fleet architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows you the components of the Fleet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fleet uses master, slave model with Fleet Engine playing master role and Fleet
    agent playing slave role. Fleet engine is responsible for scheduling Fleet units
    and Fleet agent is responsible for executing the units as well as reporting the
    status back to the Fleet engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One master engine is elected among the CoreOS cluster using etcd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the user starts a Fleet service, each agent bids for that service. Fleet
    uses a very simple `least-loaded` scheduling algorithm to schedule the unit to
    the appropriate node. Fleet units also consist of metadata that is useful to control
    where the unit runs with respect to the node property as well as based on other
    services running on that particular node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Fleet agent processes the unit and gives it to systemd for execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any node dies, a new Fleet engine is elected and the scheduled units in that
    node are rescheduled to a new node. Systemd provides HA at the node level; Fleet
    provides HA at the cluster level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering that CoreOS and Google are working closely on the Kubernetes project,
    a common question that comes up is the role of Fleet if Kubernetes is going to
    do container orchestration. Fleet is typically used for the orchestration of critical
    system services using systemd while Kubernetes is used for application container
    orchestration. Kubernetes is composed of multiple services such as the kubelet
    server, API server, scheduler, and replication controller and they all run as
    Fleet units. For smaller deployments, Fleet can also be used for application orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: A Fleet scheduling example
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a three-node CoreOS cluster with some metadata present for
    each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00054.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A global unit example
  prefs: []
  type: TYPE_NORMAL
- en: A global unit executes the same service unit on all the nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample `helloglobal.service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=My Service After=docker.service [Service] TimeoutStartSec=0
    ExecStartPre=-/usr/bin/docker kill hello ExecStartPre=-/usr/bin/docker rm hello
    ExecStartPre=/usr/bin/docker pull busybox ExecStart=/usr/bin/docker run --name hello busybox /bin/sh -c "while true; do echo Hello World; sleep 1; done"
    ExecStop=/usr/bin/docker stop hello [X-Fleet] Global=true`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the unit as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00057.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the same service is started on all three nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00059.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Scheduling based on metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we have a three-node CoreOS cluster with the following metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: Node1 (compute=web, rack=rack1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node2 (compute=web, rack=rack2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node3 (compute=db, rack=rack3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have used the `compute` metadata to identity the type of machine as `web`
    or `db`. We have used the `rack` metadata to identify the rack number. Fleet metadata
    for a node can be specified in the Fleet section of the `cloud-config`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start a web service and database service with each having its corresponding
    metadata and see where they get scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=Apache web server service After=etcd.service After=docker.service  [Service]
    TimeoutStartSec=0 KillMode=none EnvironmentFile=/etc/environment ExecStartPre=-/usr/bin/docker kill nginx
    ExecStartPre=-/usr/bin/docker rm nginx ExecStartPre=/usr/bin/docker pull nginx
    ExecStart=/usr/bin/docker run --name nginx -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx
    ExecStop=/usr/bin/docker stop nginx  [X-Fleet] MachineMetadata=compute=web`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the database service:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=Redis DB service After=etcd.service After=docker.service  [Service]
    TimeoutStartSec=0 KillMode=none EnvironmentFile=/etc/environment ExecStartPre=-/usr/bin/docker kill redis
    ExecStartPre=-/usr/bin/docker rm redis ExecStartPre=/usr/bin/docker pull redis
    ExecStart=/usr/bin/docker run --name redis redis ExecStop=/usr/bin/docker stop redis  [X-Fleet]
    MachineMetadata=compute=db`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the services using Fleet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, `nginxweb.service` got started on `Node1` and `nginxdb.service`
    got started on `Node3`. This is because `Node1` and `Node2` were of the `web`
    type and `Node3` was of the `db` type.
  prefs: []
  type: TYPE_NORMAL
- en: Fleet HA
  prefs: []
  type: TYPE_NORMAL
- en: When any of the nodes has an issue and does not respond, Fleet automatically
    takes care of scheduling the service units to the next appropriate machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding example, let''s reboot `Node1`, which has `nginxweb.service`.
    The service gets scheduled to `Node2` and not to `Node3` because Node2 has the
    `web` metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, we can see that `nginxweb.service` is rescheduled to
    `Node2` and that `Node1` is not visible in the Fleet cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Flannel
  prefs: []
  type: TYPE_NORMAL
- en: Flannel uses an Overlay network to allow Containers across different hosts to
    talk to each other. Flannel is not part of the base CoreOS image. This is done
    to keep the CoreOS image size minimal. When Flannel is started, the flannel container
    image is retrieved from the Container image repository. The Docker daemon is typically
    started after the Flannel service so that containers can get the IP address assigned
    by Flannel. This represents a chicken-and-egg problem as Docker is necessary to
    download the Flannel image. The CoreOS team has solved this problem by running
    a master Docker service whose only purpose is to download the Flannel container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows you how Flannel agents in each node communicate using
    etcd:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some Flannel internals:'
  prefs: []
  type: TYPE_NORMAL
- en: Flannel runs without a central server and uses etcd for communication between
    the nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As part of starting Flannel, we need to supply a configuration file that contains
    the IP subnet to be used for the cluster as well as the backend protocol method
    (such as UDP and VXLAN). The following is a sample configuration that specifies
    the subnet range and backend protocol as UDP:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00071.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Each node in the cluster requests an IP address range for containers created
    in that host and registers this IP range with etcd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As every node in the cluster knows the IP address range allocated for every
    other node, it knows how to reach containers created on any node in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When containers are created, containers get an IP address in the range allocated
    to the node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When Containers need to talk across hosts, Flannel does the encapsulation based
    on the backend encapsulation protocol chosen. Flannel, in the destination node,
    de-encapsulates the packet and hands it over to the Container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By not using port-based mapping to talk across containers, Flannel simplifies
    Container-to-Container communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows the data path for Container-to-Container communication
    using Flannel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A Flannel service unit
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a flannel service unit where we set the IP range
    for the flannel network as `10.1.0.0/16`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a three-node etcd cluster, the following is a sample output that shows the
    Container IP address range picked by each node. Each node requests an IP range
    with a 24-bit mask. `10.1.19.0/24` is picked by node A, `10.1.3.0/24` is picked
    by node B, and `10.1.62.0/24` is picked by node C:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Rkt
  prefs: []
  type: TYPE_NORMAL
- en: 'Rkt is the Container runtime developed by CoreOS. Rkt does not have a daemon
    and is managed by systemd. Rkt uses the Application Container image (ACI) image
    format, which is according to the APPC specification ([https://github.com/appc/spec](https://github.com/appc/spec)).
    Rkt''s execution is split into three stages. This approach was taken so that some
    of the stages can be replaced by a different implementation if needed. Following
    are details on the three stages of Rkt execution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 0:'
  prefs: []
  type: TYPE_NORMAL
- en: This is the first stage of Container execution. This stage does image discovery,
    retrieval and sets up filesystem for stages 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 1:'
  prefs: []
  type: TYPE_NORMAL
- en: This stage sets up the execution environment for containers like Container namespace,
    cgroups using the filesystem setup by stage 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stage 2:'
  prefs: []
  type: TYPE_NORMAL
- en: This stage executes the Container using execution environment setup by stage
    1 and filesystem setup by stage 0.
  prefs: []
  type: TYPE_NORMAL
- en: As of release 0.10.0, Rkt is still under active development and is not ready
    for production.
  prefs: []
  type: TYPE_NORMAL
- en: The CoreOS cluster architecture
  prefs: []
  type: TYPE_NORMAL
- en: Nodes in the CoreOS cluster are used to run critical CoreOS services such as
    etcd, fleet, Docker, systemd, flannel, and journald as well as application containers.
    It is important to avoid using the same host to run critical services as well
    as application containers so that there is no resource contention for critical
    services. This kind of scheduling can be achieved using the Fleet metadata to
    separate the core machines and worker machines. The following are two cluster
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The development cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a development cluster with three CoreOS nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To try out CoreOS and etcd, we can start with a single-node cluster. With this
    approach, there is no need to have dynamic discovery of cluster members. Once
    this works fine, we can expand the cluster size to three or five to achieve redundancy.
    The static or dynamic discovery approach can be used to discover CoreOS members.
    As CoreOS critical services and application containers run in the same cluster,
    there could be resource contention in this approach.
  prefs: []
  type: TYPE_NORMAL
- en: The production cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a production cluster with a three-node master cluster
    and five-node worker cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can have a three or five-node master cluster to run critical CoreOS services
    and then have a dynamic worker cluster to run application Containers. The master
    cluster will run etcd, fleet, and other critical services. In worker nodes, etcd
    will be set up to proxy to master nodes so that worker nodes can use master nodes
    for etcd communication. Fleet, in worker nodes, will also be set up to use etcd
    in master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Docker versus Rkt
  prefs: []
  type: TYPE_NORMAL
- en: As this is a controversial topic, I will try to give a neutral stand here.
  prefs: []
  type: TYPE_NORMAL
- en: History
  prefs: []
  type: TYPE_NORMAL
- en: 'CoreOS team started the Rkt project because of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Container interoperability issue needed to be addressed since Docker runtime
    was not fully following the Container manifest specification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting Docker to run under systemd had some issues because of Docker running
    as the daemon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container image discovery and image signing required improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security model for Containers needed to be improved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APPC versus OCI
  prefs: []
  type: TYPE_NORMAL
- en: APPC ([https://github.com/appc/spec](https://github.com/appc/spec)) and OCI
    ([https://github.com/opencontainers/specs](https://github.com/opencontainers/specs))
    define Container standards.
  prefs: []
  type: TYPE_NORMAL
- en: 'The APPC specification is primarily driven by CoreOS along with a few other
    community members. The APPC specification defines the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image format: Packaging and signing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runtime: How to execute the Container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naming and Sharing: Automatic discovery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APPC is implemented by Rkt, Kurma, Jetpack, and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'OCI ([https://www.opencontainers.org/](https://www.opencontainers.org/)) is
    an open container initiative project started in April 2015 and has members from
    all major companies including Docker and CoreOS. Runc is an implementation of
    OCI. The following image shows you how APPC, OCI, Docker, and Rkt are related:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The current status
  prefs: []
  type: TYPE_NORMAL
- en: Based on the latest developments, there is consensus among the community to
    having a common container specification called the Open Container Specification.
    Anyone can develop a Container runtime based on this specification. This will
    allow Container images to be interoperable. Docker, Rkt, and Odin are examples
    of Container runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The original APPC container specification proposed by CoreOS covers four different
    elements of container management—packaging, signing, naming (sharing the container
    with others), and runtime. As per the latest CoreOS blog update ([https://coreos.com/blog/making-sense-of-standards.html](https://coreos.com/blog/making-sense-of-standards.html)),
    APPC and OCI will intersect only on runtime and APPC will continue to focus on
    image format, signing, and distribution. Runc is an implementation of OCI and
    Docker uses Runc.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between Docker and Rkt
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some differences between Docker and Rkt Container runtimes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker uses LibContainer APIs to access the Linux kernel Container functionality
    while Rkt uses the Systemd-nspawn API to access the Linux kernel Container functionality.
    The following image illustrates this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00098.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Docker requires a daemon to manage Container images, remote APIs, and Container
    processes. Rkt is daemonless and Container resources are managed by systemd. This
    makes Rkt integrate better with init systems such as systemd and upstart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker has a complete platform to manage containers such as Machine, Compose,
    and Swarm. CoreOS will use some of its own tools such as Flannel for the Networking
    and combines it with tools such as Kubernetes for Orchestration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker is pretty mature and production-ready as compared to Rkt. As of the Rkt
    release 0.10.0, Rkt is not yet ready for production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the Container image registry, Docker has the Docker hub and Rkt has Quay.
    Quay also has Docker images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS is planning to support both Docker and Rkt and users will have a choice
    to use the corresponding Container runtime for their applications.
  prefs: []
  type: TYPE_NORMAL
- en: A workflow for distributed application development with Docker and CoreOS
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a typical workflow to develop microservices using Docker and
    CoreOS:'
  prefs: []
  type: TYPE_NORMAL
- en: Select applications that need to be containerized. This could be greenfield
    or legacy applications. For legacy applications, reverse engineering might be
    required to split the monolithic application and containerize the individual components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a Dockerfile for each microservice. The Dockerfile defines how to create
    the Container image from the base image. Dockerfile itself could be source-controlled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the stateless and stateful pieces of the application. For stateful applications,
    a storage strategy needs to be decided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices need to talk to each other and some of the services should be
    reachable externally. Assuming that basic network connectivity between services
    is available, services can talk to each other either statically by defining a
    service name to IP address and port number mapping or by using service discovery
    where services can dynamically discover and talk to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker container images need to be stored in a private or public repository
    so that they can be shared among development, QA, and production teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application can be deployed in a private or public cloud. An appropriate
    infrastructure has to be selected based on the business need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the CoreOS cluster size and cluster architecture. It's better to make
    infrastructure dynamically scalable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write CoreOS unit files for basic services such as etcd, fleet, and flannel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finalize a storage strategy—local versus distributed versus cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For orchestration of smaller applications, fleet can be used. For complex applications,
    the Kubernetes kind of Orchestration solution will be necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For production clusters, appropriate monitoring, logging, and upgrading strategies
    also need to be worked out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of CoreOS, Containers, and Docker and
    how they help in distributed application development and deployment. These technologies
    are under active development and will revolutionize and create a new software
    development and distribution model. We will explore each individual topic in detail
    in the following chapters. In the next chapter, we will cover how to set up the
    CoreOS development environment in Vagrant as well as in a public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: 'APPC specification: [https://github.com/appc/spec/blob/master/SPEC.md](https://github.com/appc/spec/blob/master/SPEC.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OCI specification: [https://github.com/opencontainers/specs](https://github.com/opencontainers/specs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS documentation: [https://coreos.com/docs/](https://coreos.com/docs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker documentation: [https://docs.docker.com/](https://docs.docker.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  prefs: []
  type: TYPE_NORMAL
- en: 'A blog on the minimalist operating system: [https://blog.docker.com/2015/02/the-new-minimalist-operating-systems/](https://blog.docker.com/2015/02/the-new-minimalist-operating-systems/)
    and [https://blog.codeship.com/container-os-comparison/](https://blog.codeship.com/container-os-comparison/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Container basics: [http://www.slideshare.net/jpetazzo/anatomy-of-a-container-namespaces-cgroups-some-filesystem-magic-linuxcon](http://www.slideshare.net/jpetazzo/anatomy-of-a-container-namespaces-cgroups-some-filesystem-magic-linuxcon)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An introduction to Docker: [https://www.youtube.com/watch?v=Q5POuMHxW-0](https://www.youtube.com/watch?v=Q5POuMHxW-0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesos overview: [https://www.youtube.com/watch?v=gVGZHzRjvo0](https://www.youtube.com/watch?v=gVGZHzRjvo0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CoreOS presentation: [http://www.slideshare.net/RichardLister/core-os](http://www.slideshare.net/RichardLister/core-os)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DigitalOcean CoreOS tutorials: [https://www.digitalocean.com/community/tags/coreos?type=tutorials](https://www.digitalocean.com/community/tags/coreos?type=tutorials)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microservices'' characteristics: [http://martinfowler.com/articles/microservices.html](http://martinfowler.com/articles/microservices.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Docker daemon issue: [http://www.ibuildthecloud.com/blog/2014/12/03/is-docker-fundamentally-flawed/](http://www.ibuildthecloud.com/blog/2014/12/03/is-docker-fundamentally-flawed/)
    and [https://github.com/ibuildthecloud/systemd-docker](https://github.com/ibuildthecloud/systemd-docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
