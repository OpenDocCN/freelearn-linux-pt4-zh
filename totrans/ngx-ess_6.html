<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Performance Tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Performance Tuning</h1></div></div></div><p>Performance tuning is the improvement of system performance. In our context, it is the performance<a id="id320" class="indexterm"/> of an entire web service or an individual web server. The need for such activity arises when there is a real or anticipated performance problem, such as excessive response latency, insufficient upload or download rate, lack of system scalability, or excessive use of computer system resources for seemingly low service usage.</p><p>In this chapter, we will look at a number of topics that deal with performance problems using features of Nginx. Each section explains when and how a solution is applicable; that is, what kind of performance problems it addresses.</p><p>In this chapter you will learn about:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to optimize static file retrieval</li><li class="listitem" style="list-style-type: disc">How to set up response compression</li><li class="listitem" style="list-style-type: disc">How to optimize data buffer allocation</li><li class="listitem" style="list-style-type: disc">How to accelerate SSL by enabling session caching</li><li class="listitem" style="list-style-type: disc">How to optimize worker process allocation on multi-core systems</li></ul></div><div class="section" title="Optimizing static file retrieval"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec31"/>Optimizing static file retrieval</h1></div></div></div><p>Static file <a id="id321" class="indexterm"/>retrieval performance directly affects visitors' perceived website performance. This happens because web pages usually contain numerous references to dependent resources. These resources need to be quickly retrieved before the entire page can be rendered. The faster the web server can start returning a static file (lower latency) and the higher the parallelism of retrieval, the higher the perceived performance of the website.</p><p>When the latency is the driving factor, it is important that files are returned predominantly from the main memory, as it has much lower latency compared to hard drives.</p><p>Fortunately, the operating system already takes very good care of that through filesystem cache. You only need to stimulate cache usage by specifying some advisory parameters and eliminating waste:</p><div class="informalexample"><pre class="programlisting">location /css {
    sendfile on;
    sendfile_max_chunk 1M;
    [...]
}</pre></div><p>By default, Nginx reads the content of a file into the user space before sending to the client. This is suboptimal and can be avoided by using the <code class="literal">sendfile()</code> system call if it is available. The <code class="literal">sendfile()</code> function implements a zero-copy transfer strategy by copying data from <a id="id322" class="indexterm"/>one file descriptor to another bypassing user space.</p><p>We enable <code class="literal">sendfile()</code> by specifying the <code class="literal">sendfile on</code> parameter in code. We limit the maximum amount of data that <code class="literal">sendfile()</code> can send in one invocation to 1 MB using the <code class="literal">sendfile_max_chunk</code> directive. In this way, we prevent a single fast connection from occupying the whole worker process.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>Response body filters such as the <code class="literal">.gzip</code> compressor require response data in the user space. They cannot be combined with a zero-copy strategy and consequently with <code class="literal">sendfile()</code>. When enabled, they cancel the effect of <code class="literal">sendfile()</code>.</p></div></div><p>The preceding configuration is optimized for latency. Compare it to the example from the <span class="emphasis"><em>Setting up Nginx to serve static data</em></span> section in <a class="link" href="ch02.html" title="Chapter 2. Managing Nginx">Chapter 2</a>, <span class="emphasis"><em>Managing Nginx</em></span>. You will see that the <code class="literal">tcp_nopush</code> directive is gone. The <code class="literal">off</code> state of this option will make network utilization a bit less efficient, but will deliver data—including the HTTP header—to the client as soon as possible.</p><p>With <code class="literal">tcp_nopush</code> set to <code class="literal">on</code>, the first packet of the response will be sent as soon as the chunk of data is obtained by <code class="literal">sendfile()</code>.</p><p>Another aspect of static file retrieval is large file download. In this case, the startup time is not as important as the download throughput or, in other words, the download speed that a server can attain while returning a large file. Caching stops being desirable for large files. Nginx reads them sequentially, so cache hits are much less likely for them. Cached segments of a large file would therefore simply pollute the cache.</p><p>On Linux, caching can be bypassed by using Direct I/O. With Direct I/O enabled, the operating system translates read offsets into the underlying block device addresses, and queues<a id="id323" class="indexterm"/> read requests directly into the underlying block device queue. The following configuration shows how to enable Direct I/O:</p><div class="informalexample"><pre class="programlisting">location /media {
    sendfile off;
    directio 4k;
    output_buffers 1 256k;
    [...]
}</pre></div><p>The <code class="literal">directio</code> directive takes a single argument that specifies the minimum size a file must have in order to be read with Direct I/O. In addition to specifying <code class="literal">direction</code>, we extend the output buffer using the <code class="literal">output_buffers</code> directive in order to increase system call efficiency.</p><p>Note that Direct I/O blocks the worker processes during reads. This reduces parallelism and <a id="id324" class="indexterm"/>throughput of file retrieval. To avoid blocking and increase parallelism, you can enable <span class="strong"><strong>Asynchronous I/O</strong></span> (<span class="strong"><strong>AIO</strong></span>):</p><div class="informalexample"><pre class="programlisting">location /media {
    sendfile off;
    aio on;
    directio 4k;
    output_buffers 1 256k;
    [...]
}</pre></div><p>On Linux, AIO is available as of kernel version 2.6.22 and it is non-blocking only in combination with Direct I/O. AIO and Direct I/O can be combined with <code class="literal">sendfile()</code>:</p><div class="informalexample"><pre class="programlisting">location /media {
    sendfile on;
    aio on;
    directio 4k;
    output_buffers 1 256k;
    [...]
}</pre></div><p>In this case, files smaller than the size specified in <code class="literal">directio</code> will be send using <code class="literal">sendfile()</code>, or else with AIO plus Direct I/O.</p><p>As of Nginx version 1.7.11, you can delegate file read operations to a pool of threads. This makes perfect sense if you are not limited by memory or CPU resources. As threads do not require Direct I/O, enabling them on large files will lead to aggressive caching:</p><div class="informalexample"><pre class="programlisting">location /media {
    sendfile on;
    aio threads;
    [...]
}</pre></div><p>Threads <a id="id325" class="indexterm"/>are not compiled by default (at the moment of writing this chapter), so you have to enable them using the with-threads configuration switch. In addition to that, threads can work only with <code class="literal">epoll</code>, <code class="literal">kqueue</code>, and <code class="literal">eventport</code> event processing methods.</p><p>With threads, both higher parallelism and caching can be attained without blocking the worker process, although threads and communication between threads require some additional resources.</p></div></div>
<div class="section" title="Enabling response compression"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec32"/>Enabling response compression</h1></div></div></div><p>Performance<a id="id326" class="indexterm"/> of your website can be improved by enabling response compression using GZIP. Compression reduces the size of a response body, reduces the bandwidth required to transfer the response data, and ultimately makes sure the resources of your website are delivered to the client side sooner.</p><p>Compression can be enabled using the <code class="literal">gzip</code> directive:</p><div class="informalexample"><pre class="programlisting">location / {
    gzip on;
    [...]
}</pre></div><p>This directive is valid in the <code class="literal">http</code>, <code class="literal">server</code>, <code class="literal">location</code>, and <code class="literal">if</code> sections. Specifying <code class="literal">off</code> as the first argument of this directive disables compression in the corresponding location if it was enabled in outer sections.</p><p>By default, only documents with MIME type <span class="emphasis"><em>text/HTML</em></span> are compressed. To enable compression for other types of documents, use the <code class="literal">gzip_types</code> directive:</p><div class="informalexample"><pre class="programlisting">location / {
    gzip on;
    gzip_types text/html text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript;
    [...]
}</pre></div><p>The preceding <a id="id327" class="indexterm"/>configuration enables compression for MIME types that hypertext documents, cascading style sheets, and JavaScript files appear to be in. These are the types of documents that benefit most from the compression, as text files and source code files—if they are large enough—usually contain a lot of entropy.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>Archives, images, and movies are not suitable for compression, as they are usually already compressed. Executable files are less suitable for compression, but can benefit from it in some cases.</p></div></div><p>It makes sense to disable compression for small documents, as compression efficiency might not be worth the efforts—or even worse—might be negative. In Nginx, you can implement compression using the <code class="literal">gzip_min_length</code> directive. This directive specifies the minimum length a document must be in order to be eligible for compression:</p><div class="informalexample"><pre class="programlisting">location / {
    gzip on;
    gzip_min_length 512;
    [...]
}</pre></div><p>With the preceding configuration, all documents smaller than 512 bytes will not be compressed. The length information that is used to apply this restriction is extracted from the Content-Length response header. If no such header is present, the response will be compressed regardless of its length.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>Response compression comes at a cost: it is CPU-intensive. You need to consider that in your capacity planning and system design. If CPU utilization becomes a bottleneck, try reducing the compression level using the <code class="literal">gzip_comp_level</code> directive.</p></div></div><p>The following table lists some other directives that affect the behavior of compression:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Function</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_disable &lt;regex&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>If the User-Agent field of a<a id="id328" class="indexterm"/> request matches the specified regular expression, the compression for that request will be disabled.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_comp_level &lt;level&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the GZIP <a id="id329" class="indexterm"/>compression level to use. The lowest is 1 and the highest is 9. These values correspond to options -1 … -9 of the <code class="literal">gzip</code> command.</p>
</td></tr></tbody></table></div><p>The<a id="id330" class="indexterm"/> preceding directives can help you fine-tune the response compression in your system.</p><p>The efficiency of response body compression can be monitored via the <code class="literal">$gzip_ratio</code> variable. This variable indicates the attained compression ratio equal to the ratio of the size of the original response body to the size of the compressed one.</p><p>The value of this variable can be written to the log file and later extracted and picked up by your monitoring system. Consider the following example:</p><div class="informalexample"><pre class="programlisting">http {
    log_format gzip '$remote_addr - $remote_user [$time_local] $status '
        '"$request" $body_bytes_sent "$http_referer" '
        '"$http_user_agent" "$host" $gzip_ratio';

    server {
        [...]
        access_log  /var/log/nginx/access_log gzip;
        [...]
    }
}</pre></div><p>The preceding configuration creates a log file format named <code class="literal">gzip</code> and uses this format to log HTTP requests in one of the virtual hosts. The last field in the log file will indicate the attained compression ratio.</p></div>
<div class="section" title="Optimizing buffer allocation"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec33"/>Optimizing buffer allocation</h1></div></div></div><p>Nginx <a id="id331" class="indexterm"/>uses buffers to store request and response data at various stages. Optimal buffer allocation can help you spare memory consumption and reduce CPU usage. The following table lists directives that control buffer allocation and the stages they are applied to:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Function</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">client_body_buffer_size &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the size of the<a id="id332" class="indexterm"/> buffer that is used to receive the request body from the client.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">output_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the <a id="id333" class="indexterm"/>number and the size of buffers that are used to send the response body to the client in case no acceleration is used.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">gzip_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the <a id="id334" class="indexterm"/>number and the size of the buffers that are used to compress the response body.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the <a id="id335" class="indexterm"/>number and the size of the buffers that are used to receive the response body from a proxied server. This directive makes sense only if buffering is enabled.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">fastcgi_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies<a id="id336" class="indexterm"/> the number and the size of the buffers that are used to receive the response body from a FastCGI server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">uwcgi_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the <a id="id337" class="indexterm"/>number and the size of the buffers that are used to receive the response body from a UWCGI server.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">scgi_buffers &lt;number&gt; &lt;size&gt;</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the<a id="id338" class="indexterm"/> number and the size of the buffers that are used to receive the response body from a SCGI server.</p>
</td></tr></tbody></table></div><p>As you can see, most of the directives take two arguments: a number and a size. The number argument specifies the maximum number of buffers that can be allocated per request. The size argument specifies the size of each buffer.</p><div class="mediaobject"><img src="graphics/B04282_06_01.jpg" alt="Optimizing buffer allocation"/></div><p>The<a id="id339" class="indexterm"/> preceding figure illustrates how buffers are allocated for a data stream. Part <span class="strong"><strong>a</strong></span> shows what happens when an input data stream is shorter than the buffer size specified in the directives above. The data stream occupies the entire buffer even though the space for the whole buffer is allocated from the heap. Part <span class="strong"><strong>b</strong></span> shows a data stream that is longer than a single buffer, but shorter than the longest allowed chain of buffers. As you can see, if the buffers are used in the most efficient way, some of them will be fully used and the last one might be only partially used. Part <span class="strong"><strong>c</strong></span> shows a data stream that is much longer than the longest chain of buffers allowed. Nginx tries to fill all available buffers with input data and flushes them once the data is sent. After that, empty buffers wait until more input data becomes available.</p><p>New buffers<a id="id340" class="indexterm"/> are allocated as long as there are no free buffers at hand and input data is available. Once the maximum number of buffers is allocated, Nginx waits until used buffers are emptied and reuses them. This makes sure that no matter how long the data stream, it will not consume more memory per request (the number of buffers multiplied by the size) than specified by the corresponding directive.</p><p>The smaller the buffers, the higher the allocation overhead. Nginx needs to spend more CPU cycles to allocate and free buffers. The larger the buffers, the larger memory consumption overhead. If a response occupies only a fraction of a buffer, the remainder of the buffer is not used—even though the entire buffer has to be allocated from the heap.</p><p>The minimum portion of the configuration that the buffer size directives can be applied to is a location. This means that if mixtures of large and small responses share the same location, the combined buffer usage pattern will vary.</p><p>Static files are read into buffers controlled by the <code class="literal">output_buffers</code> directive unless <code class="literal">sendfile</code> is set to <code class="literal">on</code>. For static files, multiple output buffers don't make much sense, as they are filled in the blocking mode anyway (this means a buffer cannot be emptied while the other one is being filled). However, larger buffers lead to lower system call rate. Consider the following example:</p><div class="informalexample"><pre class="programlisting">location /media {
    output_buffers 1 256k;
    [...]
}</pre></div><p>If the output buffer size is too large without threads or AIO, it can lead to long blocking reads that will affect worker process responsiveness.</p><p>When a response body is pipelined from a proxied server, FastCGI, UWCGI, or SCGI server, Nginx is able to read data into one part of the buffers and simultaneously send the other part to the client. This makes the most sense for long replies.</p><p>Assume you tuned your TCP stack before reading this chapter. The total size of a buffer chain is then connected to the kernel socket's read and write buffer sizes. On Linux, the maximum size of a kernel socket read buffer can be examined using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat /proc/sys/net/core/rmem_max</strong></span>
</pre></div><p>While the maximum size of a kernel socket write buffer can be examined using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat /proc/sys/net/core/wmem_max</strong></span>
</pre></div><p>These settings can be changed using the <code class="literal">sysctl</code> command or via <code class="literal">/etc/sysctl.conf</code> at system startup.</p><p>In my case, both of them are set to <code class="literal">163840</code> (160 KB). This is low for a real system, but let's use it<a id="id341" class="indexterm"/> as an example. This number is the maximum amount of data Nginx can read from or write to a socket in one system call without the socket being suspended. With reads and writes going asynchronously, we need a buffer space no less than the sum of <code class="literal">rmem_max</code> and <code class="literal">wmem_max</code> for optimal system call rate.</p><p>Assume that the preceding Nginx proxies long files with <code class="literal">rmem_max</code> and <code class="literal">wmem_max</code> settings. The following configuration must yield the lowest system call rate with the minimum amount of memory per request in the most extreme case:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_buffers 8 40k;
}</pre></div><p>The same considerations apply to the <code class="literal">fastcgi_buffers</code>, <code class="literal">uwcgi_buffers</code>, and <code class="literal">scgi_buffers</code> directives.</p><p>For short response bodies, the buffer size has to be a bit larger than the predominant size of a response. In this case, all replies will fit into one buffer—only one allocation per request will be needed.</p><p>For the preceding setup, assume that most of the replies fit 128 KB, while some span up to dozens of megabytes. The optimal buffer configuration will be somewhere between <code class="literal">proxy_buffers 2 160k</code> and <code class="literal">proxy_buffers 4 80k</code>.</p><p>In the case of response body compression, the size of the GZIP buffer chain must be downscaled by the average compression ratio. For the preceding setup, assume that the average compression ratio is 3.4. The following configuration must yield the lowest system call rate with a minimal amount of memory per request in presence of response body compression:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_buffers 8 40k;
    gzip on;
    gzip_buffers 4 25k;
}</pre></div><p>In the preceding configuration we make sure that in the most extreme case, if half of the proxy buffers are being used for reception, the other half is ready for compression. GZIP buffers<a id="id342" class="indexterm"/> are configured in a way that makes sure that the compressor output for half of the uncompressed data occupies half of the output buffers, while the other half of the buffers with compressed data are sent to the client.</p></div>
<div class="section" title="Enabling SSL session reuse"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Enabling SSL session reuse</h1></div></div></div><p>An SSL<a id="id343" class="indexterm"/> session is started by a handshake procedure that involves multiple round trips (see the following figure). The client and server have to exchange four messages with a latency of around 50 milliseconds each. In total, we have at least 200 milliseconds of overhead while establishing a secure connection. In addition to that, both the client and the server need to perform public-key cryptographic operations in order to share a common secret. These operations are computationally expensive.</p><div class="mediaobject"><img src="graphics/B04282_06_02.jpg" alt="Enabling SSL session reuse"/><div class="caption"><p>Normal SSL handshake</p></div></div><p>The client can request an abbreviated handshake in effect (see the following figure), saving <a id="id344" class="indexterm"/>a full round-trip of 100 milliseconds and avoiding the most expensive part of the full SSL handshake:</p><div class="mediaobject"><img src="graphics/B04282_06_03.jpg" alt="Enabling SSL session reuse"/><div class="caption"><p>Abbreviated handshake</p></div></div><p>The abbreviated handshake can be accomplished either through the <span class="emphasis"><em>session identifiers</em></span> mechanism defined by RFC 5246, or through the <span class="emphasis"><em>session tickets</em></span> mechanism detailed in RFC 5077.</p><p>To make abbreviated handshakes with session identifiers possible, the server needs to store session parameters in a cache keyed by a session identifier. In Nginx, this cache can be configured to be shared with all worker processes. When a client requests an abbreviated handshake, it provides the server with a session identifier so that it can retrieve session parameters from the cache. After that, the handshake procedure can be shortened and public-key cryptography can be skipped.</p><p>To enable SSL session cache, use the <code class="literal">ssl_session_cache</code> directive:</p><div class="informalexample"><pre class="programlisting">http {
    ssl_session_cache builtin:40000;     [...]
}</pre></div><p>This configuration enables SSL session caching with built-in OpenSSL session cache. The number in the first argument (<code class="literal">40000</code>) specifies the size of the cache in sessions. The built-in cache cannot be shared between worker processes. Consequently, this reduces efficiency of SSL session reuse.</p><p>The <a id="id345" class="indexterm"/>following configuration enables SSL session caching with a cache shared between worker processes:</p><div class="informalexample"><pre class="programlisting">http {
    ssl_session_cache shared:ssl:1024k;
    [...]
}</pre></div><p>This creates a shared SSL session cache named <code class="literal">ssl</code> and enables SSL session reuse with this cache. The size of the cache is now specified in bytes. Each session occupies around 300 bytes in such cache.</p><p>It is possible to perform an abbreviated SSL handshake without the server state using an SSL session tickets mechanism. This is done by packaging session parameters into a binary object and encrypting it with a key known only to the server. This encrypted object is called a session ticket.</p><p>A session ticket then can be safely transferred to the client. When the client wishes to resume a session, it presents the session ticket to the server. The server decrypts it and extracts the session parameters.</p><p>Session tickets<a id="id346" class="indexterm"/> are an extension of the TLS protocol and can be used with TLS 1.0 and further (SSL is a predecessor of TLS).</p><p>To enable<a id="id347" class="indexterm"/> session tickets, use the <code class="literal">ssl_session_tickets</code> directive:</p><div class="informalexample"><pre class="programlisting">http {
    ssl_session_tickets on;
    [...]
}</pre></div><p>Naturally, both mechanisms can be enabled at once:</p><div class="informalexample"><pre class="programlisting">http {
    ssl_session_cache shared:ssl:1024k;
    ssl_session_tickets on;
    [...]
}</pre></div><p>For<a id="id348" class="indexterm"/> security reasons, cached session lifetime is limited so that session parameters cannot be attacked while session is active. Nginx sets the default maximum SSL session lifetime to 5 minutes. If security is not a big concern and visitors spend considerable time on your website, you can extend the maximum session lifetime, increasing the efficiency of SSL in effect.</p><p>The maximum SSL session lifetime is controlled by the <code class="literal">ssl_session_timeout</code> directive:</p><div class="informalexample"><pre class="programlisting">http {
    ssl_session_cache shared:ssl:1024k;
    ssl_session_tickets on;
    ssl_session_timeout 1h;
    [...]
}</pre></div><p>The preceding configuration enables both session reuse mechanisms and sets the maximum SSL session lifetime to 1 hour.</p></div>
<div class="section" title="Worker processes allocation on multi-core systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/>Worker processes allocation on multi-core systems</h1></div></div></div><p>If your <a id="id349" class="indexterm"/>Nginx workload is CPU-bound, such<a id="id350" class="indexterm"/> as when using response compression on proxied content, on systems with multiple processors or multiple processor cores, it might be possible to obtain additional performance by associating each worker process with its own processor/core.</p><p>In a multi-core processor, each core has its own instance of <span class="strong"><strong>Translation Lookaside Buffer</strong></span> (<span class="strong"><strong>TLB</strong></span>) that is<a id="id351" class="indexterm"/> used by the memory-management unit to accelerate virtual address translation. In a preemptive multitasking operating system, each process has its own virtual memory context. When an operating system assigns an active process to a processor core and the virtual memory context does not match the context that filled the TLB of that processor core, the operating system has to flush the TLB as its content is no longer valid.</p><p>The new active process then receives a performance penalty, because it has to fill the TLB with new entries as it reads or writes memory locations.</p><p>Nginx has an option to "stick" a process to a processor core. On a system with a single Nginx instance, worker processes will be scheduled most of the time. In such circumstances, there is a very high probability that the virtual memory context does not need to be switched and TLB does not need to be flushed. The "stickiness" of a process then becomes useful. The "stickiness" is called CPU affinity.</p><p>Consider <a id="id352" class="indexterm"/>a system with four processor cores. The CPU affinity can be configured as follows:</p><div class="informalexample"><pre class="programlisting">worker_processes 4;
worker_cpu_affinity 0001 0010 0100 1000;</pre></div><p>This<a id="id353" class="indexterm"/> configuration assigns each worker to its own processor core. The configuration directive <code class="literal">worker_cpu_affinity</code> receives many arguments as many worker process are to be started. Each argument specifies a mask, where a bit with a value of 1 specifies affinity with the corresponding processor, and a bit with a value of 0 specifies no affinity with the corresponding processor.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>CPU affinity does not guarantee an increase in performance, but make sure to give it a try if your Nginx server is performing CPU-bound tasks.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Summary</h1></div></div></div><p>In this chapter, you learned a number of recipes that will help you tackle performance and scalability challenges of your system. It is important to remember that these recipes are not solutions for all possible performance problems and represent mere trade-offs between different ways of using the resources of your system.</p><p>Nevertheless, they are must-haves in the toolbox of a web master or a site reliability engineer who wants to master Nginx and its performance and scalability features.</p></div></body></html>