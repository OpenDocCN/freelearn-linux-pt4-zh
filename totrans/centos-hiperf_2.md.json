["```\nsystemctl status pacemaker\nsystemctl status corosync\n```", "```\ncp /etc/corosync/corosync.conf.example /etc/corosync/corosync.conf\n```", "```\nsystemctl start pacemaker\nsystemctl enable corosync\nsystemctl enable pacemaker\n```", "```\n[root@node01 ~]# systemctl enable pacemaker\nln -s '/usr/lib/systemd/system/pacemaker.service' '/etc/systemd/system/multi-user.target.wants/pacemaker.service'\n[root@node01 ~]# systemctl enable corosync\nln -s '/usr/lib/systemd/system/corosync.service' '/etc/systemd/system/multi-user.target.wants/corosync.service'\n[root@node01 ~]#\n```", "```\n    systemctl start pcsd\n    systemctl enable pcsd\n    ```", "```\n    passwd hacluster\n    ```", "```\nsystemctl start corosync\n```", "```\njournalctl -xn\n```", "```\nyum –y install net-tools && netstat -npltu | grep -i corosync\n```", "```\n    iptables -I INPUT -m state --state NEW -p udp -m multiport --dports 5404,5405 -j ACCEPT\n    iptables -I INPUT -p tcp -m state --state NEW --dport 2224 -j ACCEPT\n    ```", "```\n    iptables -I INPUT -p igmp -j ACCEPT\n    iptables -I INPUT -m addrtype --dst-type MULTICAST -j ACCEPT\n    ```", "```\n    iptables -P INPUT DROP\n    ```", "```\n    [root@node01 ~]# iptables -L -v --line-numbers\n    Chain INPUT (policy DROP 0 packets, 0 bytes)\n    num   pkts bytes target     prot opt in     out     source               destination\n    1      423 48645 ACCEPT     all  --  any    any     anywhere             anywhere             ADDRTYPE match dst-type MULTICAST\n    2        0     0 ACCEPT     igmp --  any    any     anywhere             anywhere\n    3        0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere             state NEW tcp dpt:efi-mg\n    4     1200  124K ACCEPT     udp  --  any    any     anywhere             anywhere             state NEW multiport dports hpoms-dps-lstn,netsupport\n    5       86  7152 ACCEPT     all  --  any    any     anywhere             anywhere             state RELATED,ESTABLISHED\n    6        0     0 ACCEPT     icmp --  any    any     anywhere             anywhere\n    7      387 41151 ACCEPT     all  --  lo     any     anywhere             anywhere\n    8        0     0 ACCEPT     tcp  --  any    any     anywhere             anywhere             state NEW tcp dpt:ssh\n    9       65 10405 REJECT     all  --  any    any     anywhere             anywhere             reject-with icmp-host-prohibited\n\n    Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n    num   pkts bytes target     prot opt in     out     source               destination\n    1        0     0 REJECT     all  --  any    any     anywhere             anywhere             reject-with icmp-host-prohibited\n\n    Chain OUTPUT (policy ACCEPT 1149 packets, 127K bytes)\n    num   pkts bytes target     prot opt in     out     source               destination\n    ```", "```\n    iptables -D INPUT [rule number]\n    ```", "```\n    service iptables save\n    ```", "```\n[root@node02 ~]# cat /etc/sysconfig/iptables\n# Generated by iptables-save v1.4.21 on Sat Dec  5 10:09:24 2015\n*filter\n:INPUT DROP [0:0]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [263:28048]\n-A INPUT -m addrtype --dst-type MULTICAST -j ACCEPT\n-A INPUT -p igmp -j ACCEPT\n-A INPUT -p tcp -m state --state NEW -m tcp --dport 2224 -j ACCEPT\n-A INPUT -p udp -m state --state NEW -m multiport --dports 5404,5405 -j ACCEPT\n-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT\n-A INPUT -p icmp -j ACCEPT\n-A INPUT -i lo -j ACCEPT\n-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\nCOMMIT\n# Completed on Sat Dec  5 10:09:24 2015\n```", "```\ncp /etc/sysconfig/iptables-config /etc/sysconfig/iptables-config.orig\n```", "```\nIPTABLES_SAVE_ON_STOP=\"yes\"\nIPTABLES_SAVE_ON_RESTART=\"yes\"\n```", "```\nsetenforce 0\n```", "```\npcs\n```", "```\npcs cluster help\nUsage: pcs cluster [commands]...\nConfigure cluster for use with pacemaker\n\nCommands:\n auth [node] [...] [-u username] [-p password] [--force] [--local]\n Authenticate pcs to pcsd on nodes specified, or on all nodes\n configured in corosync.conf if no nodes are specified (authorization\n tokens are stored in ~/.pcs/tokens or /var/lib/pcsd/tokens for root).\n By default all nodes are also authenticated to each other, using\n --local only authenticates the local node (and does not authenticate\n the remote nodes with each other).  Using --force forces\n re-authentication to occur.\n\n setup [--start] [--local] [--enable] --name <cluster name> <node1[,node1-altaddr]>\n [node2[,node2-altaddr]] [..] [--transport <udpu|udp>] [--rrpmode active|passive]\n [--addr0 <addr/net> [[[--mcast0 <address>] [--mcastport0 <port>]\n [--ttl0 <ttl>]] | [--broadcast0]]\n [--addr1 <addr/net> [[[--mcast1 <address>] [--mcastport1 <port>]\n [--ttl1 <ttl>]] | [--broadcast1]]]]\n [--wait_for_all=<0|1>] [--auto_tie_breaker=<0|1>]\n [--last_man_standing=<0|1> [--last_man_standing_window=<time in ms>]]\n [--ipv6] [--token <timeout>] [--join <timeout>]\n [--consensus <timeout>] [--miss_count_const <count>]\n [--fail_recv_const <failures>]\n Configure corosync and sync configuration out to listed nodes\n --local will only perform changes on the local node\n --start will also start the cluster on the specified nodes\n --enable will enable corosync and pacemaker on node startup\n --transport allows specification of corosync transport (default: udpu)\n The --wait_for_all, --auto_tie_breaker, --last_man_standing,\n --last_man_standing_window options are all documented in corosync's'\n votequorum(5) man page.\n --ipv6 will configure corosync to use ipv6 (instead of ipv4)\n --token <timeout> sets time in milliseconds until a token loss is\n declared after not receiving a token (default 1000 ms)\n --join <timeout> sets time in milliseconds to wait for join mesages\n (default 50 ms)\n --consensus <timeout> sets time in milliseconds to wait for consensus\n to be achieved before starting a new round of membership configuration\n (default 1200 ms)\n --miss_count_const <count> sets the maximum number of times on\n receipt of a token a message is checked for retransmission before\n a retransmission occurs (default 5 messages)\n --fail_recv_const <failures> specifies how many rotations of the token\n without receiving any messages when messages should be received\n may occur before a new configuration is formed (default 2500 failures)\n```", "```\npcs cluster auth member1 member 2 … memberN\n```", "```\npcs cluster auth node01 node02\n```", "```\npcs cluster setup --name MyCluster node01 node02\n```", "```\n[root@node01 ~]# pcs cluster setup --name MyCluster node01 node02\nError: /etc/corosync/corosync.conf already exists, use --force to overwrite\n[root@node01 ~]# pcs cluster setup --name MyCluster node01 node02 --force\nShutting down pacemaker/corosync services...\nRedirecting to /bin/systemctl stop  pacemaker.service\nRedirecting to /bin/systemctl stop  corosync.service\nKilling any remaining services...\nRemoving all cluster configuration files...\nnode01: Succeeded\nnode02: Succeeded\n[root@node01 ~]#\n```", "```\nError: Unable to communicate with nodeXX\n```", "```\ndiff /etc/corosync/corosync.conf <(ssh node02 'cat /etc/corosync/corosync.conf')\n```", "```\npcs cluster start --all\n```", "```\n start [--all] [node] [...]\n Start corosync & pacemaker on specified node(s), if a node is not\n specified then corosync & pacemaker are started on the local node.\n If --all is specified then corosync & pacemaker are started on all\n nodes.\n```", "```\npcs cluster start --all\n[root@node01 ~]# pcs cluster start --all\nnode01: Starting Cluster...\nnode02: Starting Cluster...\n[root@node01 ~]#\n```", "```\n[root@node01 log]# pcs status cluster\nCluster Status:\n Last updated: Sat Dec  5 11:59:14 2015         Last change: Sat Dec  5 11:53:01 2015 by root via cibadmin on node01\n Stack: corosync\n Current DC: node02 (version 1.1.13-a14efad) - partition with quorum\n 2 nodes and 0 resources configured\n Online: [ node01 node02 ]\n[root@node01 log]#or just pcs status:\n[root@node01 log]# pcs status\nCluster name: MyCluster\nWARNING: no stonith devices and stonith-enabled is not false\nLast updated: Sat Dec  5 11:55:43 2015          Last change: Sat Dec  5 11:53:01 2015 by root via cibadmin on node01\nStack: corosync\nCurrent DC: node02 (version 1.1.13-a14efad) - partition with quorum\n2 nodes and 0 resources configured\n\nOnline: [ node01 node02 ]\n\nFull list of resources:\n\nPCSD Status:\n node01: Online\n node02: Online\n\nDaemon Status:\n corosync: active/disabled\n pacemaker: active/disabled\n pcsd: active/enabled\n```", "```\nOnline: [ node01 ]\nOFFLINE: [ node02 ]\n```", "```\npcs cluster stop\npcs cluster start\n```", "```\npcs status | grep -i dc\n```", "```\n[root@node01 ~]# pcs status | grep -i dc\nCurrent DC: node02 (version 1.1.13-a14efad) - partition with quorum\n[root@node01 ~]#\n```", "```\n[root@node01 ~]# pcs status nodes\nPacemaker Nodes:\nOnline: node01 node02\nStandby:\nOffline:\n[root@node01 ~]#\n```", "```\n[root@node01 ~]# corosync-cmapctl | grep -Ei 'cluster'_name|members'\nruntime.totem.pg.mrp.srp.members.1.config_version (u64) = 0\nruntime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(192.168.0.2)\nruntime.totem.pg.mrp.srp.members.1.join_count (u32) = 1\nruntime.totem.pg.mrp.srp.members.1.status (str) = joined\nruntime.totem.pg.mrp.srp.members.2.config_version (u64) = 0\nruntime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(192.168.0.3)\nruntime.totem.pg.mrp.srp.members.2.join_count (u32) = 1\nruntime.totem.pg.mrp.srp.members.2.status (str) = joined\ntotem.cluster_name (str) = MyCluster\n[root@node01 ~]#\n```", "```\npcs resource create virtual_ip ocf:heartbeat:IPaddr2 ip=192.168.0.4 cidr_netmask=24 nic=enp0s3 op monitor interval=30s\n```", "```\npcs status resources\n```", "```\n[root@node01 ~]# crm_verify -L -V\n error: unpack_resources:     Resource start-up disabled since no STONITH resources have been defined\n error: unpack_resources:     Either configure some or disable STONITH with the stonith-enabled option\n error: unpack_resources:     NOTE: Clusters with shared data need STONITH to ensure data integrity\nErrors found during check: config not valid\n[root@node01 ~]#\n```", "```\npcs property set stonith-enabled=false\n```", "```\nping -c 4 192.168.0.4\n```"]