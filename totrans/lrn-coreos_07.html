<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Creating a Virtual Tenant Network and Service Chaining Using OVS" id="190861-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Creating a Virtual Tenant Network and Service Chaining Using OVS </h1></div></div></div><p class="calibre9">In the previous chapter, we saw how different services running inside the CoreOS cluster can be linked with each other. The chapter described in detail how the services deployed by different customers/tenants across the CoreOS cluster can be linked/connected using OVS.</p><p class="calibre9">This chapter covers the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Introduction to OpenVSwitch/OVS</li><li class="listitem">Introduction to overlay and underlay networks</li><li class="listitem">Introduction to virtual tenant networks</li><li class="listitem">Docker networking using OVS</li></ul></div><p class="calibre9">As OVS is a production-quality, widely deployed software switch with a wide range of feature sets, we are going to see how OVS can be used to provide service chaining, which can differentiate between different customer services.</p></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Creating a Virtual Tenant Network and Service Chaining Using OVS" id="190861-31555e2039a14139a7f00b384a5a2dd8">
<div class="book" title="Introduction to OVS"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec37" class="calibre1"/>Introduction to OVS</h1></div></div></div><p class="calibre9">
<span class="strong"><strong class="calibre2">OpenVSwitch</strong></span> (<span class="strong"><strong class="calibre2">OVS</strong></span>) is <a id="id290" class="calibre1"/>a production-quality open source virtual switch application that can be run on any Unix-based systems. Typically, OVS is used in a virtualization environment to provide communication between the virtual machines/containers that are running inside the servers. OVS acts as a software switch that provides layer2 connectivity between the VMs running inside a server. Linux Bridge can also be used for providing communication between the VMs inside the server. However, OVS provides all the bells and whistles that are required in a typical server virtualization environment. The following diagram depicts how OVS provides connectivity across the VMs running inside the server:</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Introduction to OVS" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">In the<a id="id291" class="calibre1"/> diagram, there are three VMs that are running in a server. One end of the VM's virtual NIC is connected to <span class="strong"><strong class="calibre2">Open vSwitch</strong></span>. Here, <span class="strong"><strong class="calibre2">Open vSwitch</strong></span> provides connectivity across all the VMs in the server. <span class="strong"><strong class="calibre2">Open vSwitch</strong></span> is also connected to the physical NIC to provide communication to and from the VMs to the external world.</p><p class="calibre9">The OVS offers <span class="strong"><strong class="calibre2">Security</strong></span> by providing traffic <span class="strong"><strong class="calibre2">isolation</strong></span> using <span class="strong"><strong class="calibre2">VLAN</strong></span> and <span class="strong"><strong class="calibre2">traffic filtering</strong></span> based on various packet headers. OVS provides a way for <span class="strong"><strong class="calibre2">Monitoring</strong></span> the packets that are exchanged across the VMs in the server using protocols like <span class="strong"><strong class="calibre2">sFlow</strong></span>, <span class="strong"><strong class="calibre2">SPAN</strong></span>, <span class="strong"><strong class="calibre2">RSPAN</strong></span>, and so on. OVS also supports <span class="strong"><strong class="calibre2">QoS</strong></span> (quality of service) with <span class="strong"><strong class="calibre2">traffic queuing and shaping</strong></span> along with <span class="strong"><strong class="calibre2">OpenFlow</strong></span> support.</p></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Creating a Virtual Tenant Network and Service Chaining Using OVS" id="190861-31555e2039a14139a7f00b384a5a2dd8">
<div class="book" title="Introduction to OVS">
<div class="book" title="OVS architectural overview"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec33" class="calibre1"/>OVS architectural overview</h2></div></div></div><p class="calibre9">This<a id="id292" class="calibre1"/> section describes the high-level architectural overview of OVS and its components.</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="OVS architectural overview" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">The<a id="id293" class="calibre1"/> main components of OVS are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">ovs-vsctl</code>: This is the utility provided by OVS for configuring and querying the <code class="email">ovs-vswitchd</code> daemon via <code class="email">ovsdb-server</code></li><li class="listitem"><code class="email">ovs-appctl</code>: This is a utility for managing the logging level of OVS  </li><li class="listitem"><code class="email">ovs-ofctl</code>: This is the utility provided by OVS for managing OpenFlow entries in the switch</li><li class="listitem"><code class="email">ovs-dpctl</code>: This is the data-path management utility that is used to configure the data path of OVS</li><li class="listitem"><code class="email">ovsdb-server</code>: This is the DB that stores persistently all the configurations of OVS</li><li class="listitem"><code class="email">ovs-vswitchd</code>: This is the OVS switchd module that provides the core functionality, such as bridging, VLAN segregation, and so on of OVS</li><li class="listitem"><code class="email">Openvswitch.ko</code>: This is the data-path module for handling fast switching and tunneling of traffic</li></ul></div></div></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Creating a Virtual Tenant Network and Service Chaining Using OVS" id="190861-31555e2039a14139a7f00b384a5a2dd8">
<div class="book" title="Introduction to OVS">
<div class="book" title="Advantages of using OVS in CoreOS"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec34" class="calibre1"/>Advantages of using OVS in CoreOS</h2></div></div></div><p class="calibre9">In <a id="id294" class="calibre1"/>a CoreOS environment, OVS can replace docker0 bridge and can provide connectivity across the different containers in the CoreOS instance. docker0 bridge can only provide connectivity across the containers running in the same CoreOS instance. However, along with providing connectivity across the containers running in the same CoreOS instance, OVS can be used to provide connectivity across the containers running in different CoreOS instances. The following are the key advantages provided by OVS compared to other techniques mentioned in the previous chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem">As the name implies, OpenVSwitch/OVS does layer2 bridging/switching of data from one container to other containers. It does typical layer2 processing, such as flooding, learning, forwarding, traffic segregation based on VLAN tag, providing loop-free topology using spanning tree protocol, and so on.</li><li class="listitem">OVS supports tunneling protocols, such as GRE, VxLAN, and so on. These are the tunneling protocols that are used to carry layer2 traffic over a layer3 network. These tunnels are used to provide connectivity for containers running in different CoreOS instances. The VxLAN protocol is defined in detail in RFC 7348 and the GRE protocol is defined in detail in RFC 2784. These tunnels provide the virtual infrastructure for laying out the overlay network over the physical underlay network.</li><li class="listitem">OVS also supports the OpenFlow protocol that can be programmed by an external SDN controller like OpenDayLight Controller, RYU Controller, ONOS controller, and so on. This means the CoreOS cluster can be managed easily by a centralized controller in a typical SDN deployment.</li></ul></div><p class="calibre9">Before looking in detail at how OVS can be used to provide connectivity across containers and hence can provide service chaining, we may need to look into some of the core concepts and features, such as overlay network, underlay network, and Virtual Tenant Network.</p></div></div></div>
<div class="book" title="Introduction to overlay and underlay networks" id="19UOO1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec38" class="calibre1"/>Introduction to overlay and underlay networks</h1></div></div></div><p class="calibre9">The <a id="id295" class="calibre1"/>following<a id="id296" class="calibre1"/> diagram represents the typical service provided by OVS in a virtual machine environment:</p><div class="mediaobject"><img src="../images/00032.jpeg" alt="Introduction to overlay and underlay networks" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">Server1 <a id="id297" class="calibre1"/>and Server2 are the two physical servers wherein the customer <a id="id298" class="calibre1"/>applications are deployed inside the VM. There are two VMs in each server as VM1 and VM2. The green VM belongs to one customer and the orange VM belongs to another customer. A single instance of OVS is running in each of the servers.</p><p class="calibre9">In a typical virtualization environment, there are two kinds of network devices: the soft switch, which provides connectivity to the virtualization layer, and the physical switch, which provides connectivity to the physical infrastructure (such as servers, switches, and routers).</p><p class="calibre9">The OVS switch provides connectivity to the VMs/containers running inside the server instance. These server instances are also connected to each other physically in order to provide connectivity for all the servers.</p><p class="calibre9">The physical network that provides connectivity for the servers is termed the underlay network. This underlay network will have the physical infrastructure that comprises physical switches and routers, which provides connectivity for the servers.</p><p class="calibre9">Now, the complexity comes in providing connectivity across the containers that are running in the server to other containers that are running in different server instances. There are multiple solutions to solve this problem. One of the major and widely deployed solutions is using OVS to provide the overlay network.</p><p class="calibre9">As the term implies, an overlay network is a network that is overlaid on top of another network. Unlike physical underlay networks, overlay networks are virtual networks that comprise virtual links that share an underlying physical network (underlay network), allowing deployment of containers/virtual machines to provide connectivity with each other without the need to modify the underlying network. The virtual link here refers to the tunnels that provide connectivity across OVS. OVS supports multiple tunneling protocols; widely <a id="id299" class="calibre1"/>used tunnels are GRE and VxLAN.</p><p class="calibre9">The key <a id="id300" class="calibre1"/>benefits of the overlay network are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">As it is a logical network, it is possible to create and destroy the overlay network very easily without any change in the underlay networks. To create an overlay network between two nodes, just create a tunnel between the nodes, and to destroy the overlay network, unconfigure the tunnel interface.</li><li class="listitem">It is possible to create multiple overlay networks across the nodes. For instance, there is a possibility to create multiple overlay networks based on the number of customers deployed in a server instance. This provides a way of virtualizing the network similar to server virtualization. Let us look into the details of network virtualization.</li></ul></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Introduction to network virtualization" id="1AT9A1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec39" class="calibre1"/>Introduction to network virtualization</h1></div></div></div><p class="calibre9">Network<a id="id301" class="calibre1"/> virtualization is one of the most widely discussed topics in the recent past in the networking industry. To understand network virtualization better, think of server virtualization wherein the physical infrastructures are logically segregated into multiple virtual devices, each assigning to different containers for performing its workload. Similar to server virtualization, there is a requirement to virtualize the networking layer that provides connectivity for different virtual machines/containers.</p><p class="calibre9">As in server virtualization, wherein the customer will have full access to the virtualized server infrastructure, customers may also want to virtualize the networking infrastructure to secure data traffic between their VMs or containers. They don't want others to expose the data exchange that is happening between their applications to other customers' VMs or containers.</p><p class="calibre9">Network virtualization as a concept is not new to the networking world. Network virtualization is realized in existing networks using technologies or concepts such as VLAN, VRF, L2VPN, L3VPN, and so on. These network virtualization techniques provide a mechanism for isolating traffic from one customer to another customer. VLAN provides a way of logically segregating the layer2 broadcast domain based on the VLAN tag.</p><p class="calibre9">These technologies also define the necessary protocol support to have overlapping address spaces across different customers. Say, for instance, using VRF, it is possible for two or more customers to use and share their IP address across different sites.</p><p class="calibre9">However, these<a id="id302" class="calibre1"/> technologies are not providing true network virtualization throughout the network. These technologies also have their own limitations. The 1026 number of VLAN limits the number of tenants in the network. Similarly for VPN support, protocols like MPLS may be required that are typically deployed in a service provider network.</p><p class="calibre9">As <a id="id303" class="calibre1"/>more and more operators and cloud <a id="id304" class="calibre1"/>providers are deploying <span class="strong"><strong class="calibre2">Software Defined Networking</strong></span> (<span class="strong"><strong class="calibre2">SDN</strong></span>) and <span class="strong"><strong class="calibre2">Network Function Virtualization</strong></span> (<span class="strong"><strong class="calibre2">NFV</strong></span>), it is necessary to provide a mechanism to provide network virtualization and traffic isolation in a better way.</p><p class="calibre9">The overlay network described in the previous chapter can provide an effective mechanism to isolate data traffic across different tenants or customers. As multiple overlay networks (one for each different customer) can be laid out over the underlay physical infrastructure, we should be able to provide the required traffic isolation between different customer traffic. Hence, the overlay network infrastructure provides an easy way of providing network virtualization.</p><p class="calibre9">To create an overlay network for a customer or tenant, we need to create a tunnel across all the nodes wherein the customer's/tenant's application is deployed. OVS helps in creating a tunnel across the different OVS instances and hence supports the creation of VTNs and underlay networks.</p><p class="calibre9">Going back to the previous diagram, there are two customers, shown as green and orange. Both customers' VMs are running in both server1 and server2. In order to provide network virtualization and isolate the traffic across these two customers, the following steps can be used:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Create two bridge instances in OVS, one for each customer, as Greenbr and Orangebr.</li><li class="listitem">Attach the VM's virtual NIC interface (veth) to the corresponding bridge instance. For example, the green VM's virtual NIC should be attached to Greenbr and the orange VM's virtual NIC interface should be attached to Orangebr.</li><li class="listitem">Create two tunnels, say <code class="email">Green_tun</code> and <code class="email">Orange_tun</code>, between server1 and server2. The two server instances can be part of the same network or different networks. If they are part of different networks, one or more routers should be deployed to provide physical connectivity between these servers.<div class="note" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre9">To create a tunnel between two nodes, there should be IP reachability between these two nodes. IP reachability will be provided by the underlay network.</p></div></li><li class="listitem">Attach these two tunnels to the respective bridge instances.</li></ul></div><p class="calibre9">With<a id="id305" class="calibre1"/> these simple steps, it is possible to create a virtual network for different customers. This is illustrated in the following diagram:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="Introduction to network virtualization" class="calibre11"/></div><p class="calibre12"> </p></div>

<div class="book" title="Introduction to network virtualization" id="1AT9A1-31555e2039a14139a7f00b384a5a2dd8">
<div class="book" title="OpenFlow support in OVS"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec35" class="calibre1"/>OpenFlow support in OVS</h2></div></div></div><p class="calibre9">One <a id="id306" class="calibre1"/>of the key advantages of using OVS is that it supports the OpenFlow protocol and supports flow-based switching. <span class="strong"><strong class="calibre2">OpenFlow</strong></span> is <a id="id307" class="calibre1"/>a protocol defined by ONF to manage the network infrastructure centrally with standard interfaces between the controller (traditionally called the control plane) and the actual packet-forwarding entity (traditionally called the data plane). Enabling the network to be programmed centrally makes the whole system more agile and flexible.</p><p class="calibre9">OpenFlow promises to ease the way of provisioning large data centers and server clusters that can be managed centrally using OpenFlow controllers. With large data centers and server clusters, there is a clear necessity of changing the traditional control plane and data plane paradigm to move toward flow-based switching, which is more generic and can be adoptable for different avenues. Software Defined Networking (SDN) is a new paradigm shift in networking.</p><p class="calibre9">The <a id="id308" class="calibre1"/>OpenFlow specification defines three different components in an OpenFlow-based network as follows.</p><div class="book" title="OpenFlow switch"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec32" class="calibre1"/>OpenFlow switch</h3></div></div></div><p class="calibre9">An <span class="strong"><strong class="calibre2">OpenFlow switch</strong></span> consists of one or more flow tables, meter table, group table, and OpenFlow <a id="id309" class="calibre1"/>channels to the external<a id="id310" class="calibre1"/> controller. The flow tables and group table are used during the lookup or forwarding phase of packet pipeline processing in order to forward the packet to the appropriate port, whereas the meter table is used to perform simple QoS operations, such as rate-limiting to complex QOS operations, such as DiffServ and so on. The switch communicates with the controller and the controller manages the switch via the OpenFlow protocol using OpenFlow messages.</p></div><div class="book" title="OpenFlow controller"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec33" class="calibre1"/>OpenFlow controller</h3></div></div></div><p class="calibre9">An <span class="strong"><strong class="calibre2">OpenFlow controller</strong></span> typically manages one or more OpenFlow switches remotely via OpenFlow channels. Similarly, a single switch can be managed by multiple controllers <a id="id311" class="calibre1"/>for better reliability and better<a id="id312" class="calibre1"/> load balancing. The OpenFlow controller acts in a similar way to the control plane of typical traditional switches or routers. The controller is responsible for programming various tables, such as flow table, group table, and meter table using OpenFlow protocol messages to provide network connectivity or network functions across various nodes in the system</p></div><div class="book" title="OpenFlow channel"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec34" class="calibre1"/>OpenFlow channel</h3></div></div></div><p class="calibre9">An <span class="strong"><strong class="calibre2">OpenFlow channel</strong></span> is <a id="id313" class="calibre1"/>used to exchange<a id="id314" class="calibre1"/> OpenFlow messages between an OpenFlow switch and an OpenFlow controller. The switch must be able to create an OpenFlow channel by initiating a connection to the controller:</p><div class="mediaobject"><img src="../images/00034.jpeg" alt="OpenFlow channel" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">With<a id="id315" class="calibre1"/> OVS, the entire CoreOS cluster's overlay<a id="id316" class="calibre1"/> network can be centrally managed by a controller with very simple configurations. The ofctl utility provided by OVS is helpful in programming the flow tables using a command-line argument without being controlled by an external controller.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Running OVS in CoreOS"><div class="book" id="1BRPS2-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec40" class="calibre1"/>Running OVS in CoreOS</h1></div></div></div><p class="calibre9">There <a id="id317" class="calibre1"/>are two ways to run or install OVS in a CoreOS environment: </p><div class="book"><ul class="itemizedlist"><li class="listitem">Build a CoreOS image with OVS</li><li class="listitem">Run OVS inside a Docker container with the <code class="email">–net=host</code> option</li></ul></div><p class="calibre9">As we have already seen in <a class="calibre1" title="Chapter 1. CoreOS, Yet Another Linux Distro?" href="part0014_split_000.html#DB7S1-31555e2039a14139a7f00b384a5a2dd8">Chapter 1</a>, <span class="strong"><em class="calibre10">CoreOS,  Yet Another Linux Distro</em></span> in CoreOS there is no way to install an application. Any service/application should be deployed in a container. So the simple way to run OVS is to run OVS inside a Docker container. Let us see how we can install an OVS docker in CoreOS.</p><p class="calibre9">There is already a docker image available with OVS (coreos-ovs). Download this docker image <a id="id318" class="calibre1"/>from <a class="calibre1" href="https://github.com/theojulienne/coreos-ovs">https://github.com/theojulienne/coreos-ovs</a> github link. Use the following <code class="email">cloud-config</code> to<a id="id319" class="calibre1"/> start this container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">#cloud-config</strong></span>

<span class="strong"><strong class="calibre2">coreos:</strong></span>
<span class="strong"><strong class="calibre2">  units:</strong></span>
<span class="strong"><strong class="calibre2">    - name: docker.service</strong></span>
<span class="strong"><strong class="calibre2">      command: start</strong></span>
<span class="strong"><strong class="calibre2">      drop-ins:</strong></span>
<span class="strong"><strong class="calibre2">        - name: 50-custom-bridge.conf</strong></span>
<span class="strong"><strong class="calibre2">          content: |</strong></span>
<span class="strong"><strong class="calibre2">            [Service]</strong></span>
<span class="strong"><strong class="calibre2">            Environment='DOCKER_OPTS=--bip="10.0.11.0/8" --fixed-cidr="10.0.11.0/24"'</strong></span>
<span class="strong"><strong class="calibre2">    - name: OVS.service</strong></span>
<span class="strong"><strong class="calibre2">      command: start</strong></span>
<span class="strong"><strong class="calibre2">      content: |</strong></span>
<span class="strong"><strong class="calibre2">        [Unit]</strong></span>
<span class="strong"><strong class="calibre2">        Description=Open vSwitch Bridge</strong></span>
<span class="strong"><strong class="calibre2">        After=docker.service</strong></span>
<span class="strong"><strong class="calibre2">        Requires=docker.service</strong></span>

<span class="strong"><strong class="calibre2">        [Service]</strong></span>
<span class="strong"><strong class="calibre2">        Restart=always</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPre=/sbin/modprobe openvswitch</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPre=/sbin/modprobe af_key</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPre=-/usr/bin/docker run --name=openvswitch-cfg -v /opt/ovs/etc busybox true</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPre=-/usr/bin/docker rm -f openvswitch</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPre=/usr/bin/docker run -d --net=host --privileged --name=openvswitch --volumes-from=openvswitch-cfg theojulienne/coreos-ovs:latest</strong></span>
<span class="strong"><strong class="calibre2">        ExecStart=/usr/bin/docker attach openvswitch</strong></span>
<span class="strong"><strong class="calibre2">        ExecStartPost=/usr/bin/docker exec openvswitch /scripts/docker-attach</strong></span>
</pre></div><p class="calibre9">This starts a docker container that has OVS installed. Along with that, it removes the IP address of docker0 bridge and assigns it to OVS bridge (bridge0). docker0 bridge will be attached to bridge0 as a link.</p><p class="calibre9">As we are using the <code class="email">–net=host</code> option, any OVS command we are executing inside this container will result in changing the network configuration of the host OS, which is the CoreOS network stack.</p><p class="calibre9">This section describes in detail how to provide a virtual tenant network between docker containers that are running in two different CoreOS instances. There are multiple ways to provide the solution. We are going to see the two most common and simple ways of providing<a id="id320" class="calibre1"/> the solution:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Attaching docker0 bridge to OVS</li><li class="listitem">Attaching the container's veth interface to OVS</li></ul></div></div>

<div class="book" title="Running OVS in CoreOS">
<div class="book" title="Attaching docker0 bridge to OVS"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec36" class="calibre1"/>Attaching docker0 bridge to OVS</h2></div></div></div><p class="calibre9">This is <a id="id321" class="calibre1"/>a simple way of providing connectivity across different containers using OVS. In this case, OVS should be connected to docker0 bridge (which is already connected to all the containers) using a veth interface. Refer to the previous chapter for more detail about docker0 bridge and how it provides connectivity for the containers in a system.</p><p class="calibre9">The docker bridge is intern connected to the OVS bridge. The OVS bridge provides connectivity to the other CoreOS instances using GRE/VxLAN tunnels.</p><div class="mediaobject"><img src="../images/00035.jpeg" alt="Attaching docker0 bridge to OVS" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">The step-by-step procedure with configuration is described in detail as follows. This consists of the following major steps on both the CoreOS instances:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Configurations during the instantiation of a CoreOS node in a cluster</li><li class="listitem">Configurations during the creation of a container</li></ul></div><div class="book" title="Configuration in CoreOS Instance 1"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec35" class="calibre1"/>Configuration in CoreOS Instance 1</h3></div></div></div><p class="calibre9">This <a id="id322" class="calibre1"/>section describes in detail the operations to be performed on the coreos-ovs docker of CoreOS node1 to provide this solution.</p><div class="book" title="Configurations during the instantiation of a CoreOS node 1 in a cluster"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec20" class="calibre1"/>Configurations during the instantiation of a CoreOS node 1 in a cluster</h4></div></div></div><p class="calibre9">At the time of CoreOS server <a id="id323" class="calibre1"/>boot-up, OVS needs to be started and the procedure to start OVS is as follows. Note that the way in which the OVS command will be executed depends on whether OVS is deployed inside a docker container or the CoreOS host instance. However, in both cases, there is no change in the list of OVS commands to be used:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Run the OVS data-path module using the command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo modprobe openvswitch</strong></span>
</pre></div></li><li class="listitem" value="2">Create a configuration, <code class="email">db</code>, using the default schema file with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-tool create /var/lib/openvswitch/conf.db /usr/share/openvswitch/vswitch.ovsschema</strong></span>
</pre></div></li><li class="listitem" value="3">Run the OVS DB server using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-server /var/lib/openvswitch/conf.db --remote=punix:/var/run/openvswitch/db.sock --pidfile --detach --log-file</strong></span>
</pre></div></li><li class="listitem" value="4">Run OVS-VSCTL using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl --no-wait init</strong></span>
</pre></div></li><li class="listitem" value="5">Run the OVS switchd daemon using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vswitchd --pidfile --detach</strong></span>
</pre></div></li><li class="listitem" value="6">Create a bridge instance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-br br0</strong></span>
</pre></div></li><li class="listitem" value="7">Create a GRE tunnel with the remote node as <code class="email">172.17.8.103</code>. Here, the assumption is the etho IP of CoreOS instance 2 is <code class="email">172.17.8.103</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.103 options:key=100</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre9">The key needs to be different for each tunnel.</p></div></li><li class="listitem" value="8">Create a veth interface to provide a connection between docker0 bridge and OVS:<div class="book"><ul class="itemizedlist1"><li class="listitem">Create the veth pair:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	ip link add tap1 type veth peer name tap2</strong></span>
</pre></div></li><li class="listitem">Attach one end of the<a id="id324" class="calibre1"/> veth pair to docker0 bridge:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	sudo brctl addif docker0 tap1</strong></span>
</pre></div></li><li class="listitem">Attach the other end of the veth pair to OVS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	sudo ovs-vsctl add-port br0 tap2</strong></span>
</pre></div></li></ul></div></li></ol><div class="calibre16"/></div></div><div class="book" title="Configurations during the creation of a container for CoreOS Instance 1"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec21" class="calibre1"/>Configurations during the creation of a container for CoreOS Instance 1</h4></div></div></div><p class="calibre9">This section describes the<a id="id325" class="calibre1"/> configuration to be done when a new container is created in the CoreOS instance.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre9">As by default, the eth0 (one end of the veth pair) interface of the container is attached to docker0 bridge, we need not explicitly attach the container veth interface to docker0 bridge.</p></div><p class="calibre9">Set the IP address of the eth0 interface of the docker container. It is not possible to set the IP address of the docker container inside the docker instance. We need to use the <code class="email">nsenter</code> utility for this. To do this, follow these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">docker inspect --format {{.State.Pid}} &lt;container_name_or_ID&gt;</strong></span>
</pre></div></li><li class="listitem" value="2">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo nsenter --target $PID --mount --uts --ipc --net --pid ifconfig eth0 50.0.0.1</strong></span>
</pre></div></li></ol><div class="calibre16"/></div></div></div><div class="book" title="Configuration in CoreOS Instance 2"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec36" class="calibre1"/>Configuration in CoreOS Instance 2</h3></div></div></div><p class="calibre9">This <a id="id326" class="calibre1"/>section describes in detail the operations to be performed on the coreos-ovs docker of CoreOS node2 to provide this solution.</p><div class="book" title="Configurations during the instantiation of CoreOS node 2 in a cluster"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec22" class="calibre1"/>Configurations during the instantiation of CoreOS node 2 in a cluster</h4></div></div></div><p class="calibre9">This section describes the<a id="id327" class="calibre1"/> list of operations to be performed during the initialization of the CoreOS instance. During initialization, OVS needs to be started and the procedure to start OVS is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Run the OVS data-path module using the command: <div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo modprobe openvswitch</strong></span>
</pre></div></li><li class="listitem">Create a configuration, <code class="email">db</code>, using the default schema file with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-tool create /var/lib/openvswitch/conf.db /usr/share/openvswitch/vswitch.ovsschema</strong></span>
</pre></div></li><li class="listitem">Run the OVS DB <a id="id328" class="calibre1"/>server using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-server /var/lib/openvswitch/conf.db --remote=punix:/var/run/openvswitch/db.sock --pidfile --detach --log-file</strong></span>
</pre></div></li><li class="listitem">Run OVS-VSCTL using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl --no-wait init</strong></span>
</pre></div></li><li class="listitem">Run the OVS switchd daemon using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vswitchd --pidfile --detach</strong></span>
</pre></div></li><li class="listitem">Create a bridge instance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-br br0</strong></span>
</pre></div></li><li class="listitem">Create a GRE tunnel with the remote node as <code class="email">172.17.8.101</code>. Here, the assumption is the etho IP of CoreOS instance 1 is <code class="email">172.17.8.101</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.101 options:key=100</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre9">The key needs to be different for each tunnel.</p></div></li><li class="listitem">Now we need to create a veth interface to provide a connection between docker0 bridge and OVS:<div class="book"><ul class="itemizedlist1"><li class="listitem">Create the veth pair:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	ip link add tap1 type veth peer name tap2</strong></span>
</pre></div></li><li class="listitem">Attach one end of the veth pair to docker0 bridge:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	sudo brctl addif docker0 tap1</strong></span>
</pre></div></li><li class="listitem">Attach the other end of the veth pair to OVS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">	sudo ovs-vsctl add-port br0 tap2</strong></span>
</pre></div></li></ul></div></li></ul></div></div><div class="book" title="Configurations during the creation of a container for CoreOS Instance 2"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec23" class="calibre1"/>Configurations during the creation of a container for CoreOS Instance 2</h4></div></div></div><p class="calibre9">This section describes the <a id="id329" class="calibre1"/>configuration to be done when a new container is created in the CoreOS instance.</p><p class="calibre9">Set the IP address of the eth0 interface of the docker container. It is not possible to set the IP address of the docker container inside the docker instance. We need to use the <code class="email">nsenter</code> utility for this. To do this, follow these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Execute the following command <a id="id330" class="calibre1"/>and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">docker inspect --format {{.State.Pid}} &lt;container_name_or_ID&gt;</strong></span>
</pre></div></li><li class="listitem" value="2">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo nsenter --target $PID --mount --uts --ipc --net --pid ifconfig eth0 50.0.0.2</strong></span>
</pre></div></li></ol><div class="calibre16"/></div><p class="calibre9">Now you should be able to ping from the docker container running in CoreOS instance 1 to a docker container running in CoreOS instance 2. The main disadvantage of this solution is tha it is not possible to provide a virtual tenant network using this solution. This is because all the docker containers are attached to docker0 bridge, which is connected to OVS. OVS acts as a way to provide communication between different server instances.</p></div></div></div></div>

<div class="book" title="Running OVS in CoreOS">
<div class="book" title="Attaching container's veth interface to OVS"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec37" class="calibre1"/>Attaching container's veth interface to OVS</h2></div></div></div><p class="calibre9">In this case, all the docker containers in<a id="id331" class="calibre1"/> the CoreOS instance are attached directly to the OVS bridge. There will be multiple instance of bridge running inside OVS, each mapping to different customers/tenants. A new bridge needs to be created and provisioned for each tenant in the system. On the subsequent creation of containers (for the same tenant), the container's interface should be connected to the corresponding bridge instance. The OVS bridge provides connectivity to the other CoreOS instances using GRE/VxLAN tunnels.</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Attaching container's veth interface to OVS" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">The step-by-step procedure to configure this kind of solution is described in detail as follows. This consists of the following major steps to be performed on both the CoreOS instances:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Configurations during the <a id="id332" class="calibre1"/>instantiation of a CoreOS node in a cluster</li><li class="listitem">Configurations during the creation of the first container for a tenant</li><li class="listitem">Configurations during the creation of subsequent containers for a tenant</li></ul></div><div class="book" title="Configuration in CoreOS Instance 1"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec37" class="calibre1"/>Configuration in CoreOS Instance 1</h3></div></div></div><p class="calibre9">This <a id="id333" class="calibre1"/>section describes in detail the operations to be performed on the coreos-ovs docker of CoreOS node1 to provide this solution.</p><div class="book" title="Configurations during the instantiation of a CoreOS node in a cluster"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec24" class="calibre1"/>Configurations during the instantiation of a CoreOS node in a cluster</h4></div></div></div><p class="calibre9">During initialization, OVS needs<a id="id334" class="calibre1"/> to be started and the procedures to start OVS are as follows. Note that the way in which the OVS command will be executed depends on whether OVS is deployed inside a docker container or the CoreOS host instance. However, in both cases, there is no change in the list of OVS commands to be used.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Run the OVS data-path module using the command: <div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo modprobe openvswitch</strong></span>
</pre></div></li><li class="listitem" value="2">Create a configuration, <code class="email">db</code>, using the default schema file with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-tool create /var/lib/openvswitch/conf.db /usr/share/openvswitch/vswitch.ovsschema</strong></span>
</pre></div></li><li class="listitem" value="3">Run the OVS DB server <a id="id335" class="calibre1"/>using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-server /var/lib/openvswitch/conf.db --remote=punix:/var/run/openvswitch/db.sock --pidfile --detach --log-file</strong></span>
</pre></div></li><li class="listitem" value="4">Run OVS-VSCTL using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl --no-wait init</strong></span>
</pre></div></li><li class="listitem" value="5">Run the OVS switchd daemon using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vswitchd --pidfile --detach</strong></span>
</pre></div></li></ol><div class="calibre16"/></div></div><div class="book" title="Configurations during the creation of the first container for a tenant"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec25" class="calibre1"/>Configurations during the creation of the first container for a tenant</h4></div></div></div><p class="calibre9">When a container is created for a<a id="id336" class="calibre1"/> tenant for the first time, a new bridge needs to be created and this container should be connected to OVS. The procedure to do this is described in detail as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Bring down the docker0 bridge instance (the default bridge created by docker):  <div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ip link set dev docker0 down</strong></span>
</pre></div></li><li class="listitem" value="2">Detach the virtual interface that is created for the container from docker0 bridge. The virtual interface starts with the name as veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo brctl delif docker0 vethda0657c</strong></span>
</pre></div></li><li class="listitem" value="3">Create a bridge instance for a tenant:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-br br0</strong></span>
</pre></div></li><li class="listitem" value="4">Add the port that is created in docker. This interface starts with veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 vethda0657c</strong></span>
</pre></div></li><li class="listitem" value="5">Set the IP address of the eth0 interface of the docker container. It is not possible to set the IP address of the docker container inside the docker instance. We need to use the <code class="email">nsenter</code> utility for this. To do this, follow these steps:<div class="book"><ul class="itemizedlist1"><li class="listitem">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">docker inspect --format {{.State.Pid}} &lt;container_name_or_ID&gt;</strong></span>
</pre></div></li><li class="listitem">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo nsenter --target $PID --mount --uts --ipc --net --pid ifconfig eth0 50.0.0.1</strong></span>
</pre></div></li></ul></div></li><li class="listitem" value="6">Create a GRE tunnel with the remote <a id="id337" class="calibre1"/>node as <code class="email">172.17.8.103</code>. Here, the assumption is the eth0 IP of CoreOS instance 2 is <code class="email">172.17.8.103</code><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.103 options:key=100</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip12" class="calibre1"/>Tip</h3><p class="calibre9">The key needs to be different for each tunnel.</p></div></li></ol><div class="calibre16"/></div></div><div class="book" title="Configurations during the creation of subsequent containers for a tenant"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec26" class="calibre1"/>Configurations during the creation of subsequent containers for a tenant </h4></div></div></div><p class="calibre9">This section describes the <a id="id338" class="calibre1"/>configuration to be done when subsequent containers are being created in the CoreOS instance.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Add the port that is created in docker. This interface starts with veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 veth640b626</strong></span>
</pre></div></li><li class="listitem" value="2">Create a GRE tunnel with the remote node as <code class="email">172.17.8.103</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.103 options:key=100</strong></span>
</pre></div></li></ol><div class="calibre16"/></div></div></div><div class="book" title="Configuration in CoreOS Instance 2"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec38" class="calibre1"/>Configuration in CoreOS Instance 2</h3></div></div></div><p class="calibre9">This <a id="id339" class="calibre1"/>section describes in detail the operations to be performed on the coreos-ovs docker of CoreOS node2 to provide this solution.</p><div class="book" title="Configurations during the instantiation of a CoreOS node in a cluster"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec27" class="calibre1"/>Configurations during the instantiation of a CoreOS node in a cluster</h4></div></div></div><p class="calibre9">This section describes the list <a id="id340" class="calibre1"/>of operations to be performed during the initialization of the CoreOS instance. During initialization, OVS needs to be started and the procedure to start OVS is as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Run the OVS data-path module using the command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo modprobe openvswitch</strong></span>
</pre></div></li><li class="listitem" value="2">Create a configuration, <code class="email">db</code>, using the default schema file with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-tool create /var/lib/openvswitch/conf.db /usr/share/openvswitch/vswitch.ovsschema</strong></span>
</pre></div></li><li class="listitem" value="3">Run the OVS DB server using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovsdb-server /var/lib/openvswitch/conf.db --remote=punix:/var/run/openvswitch/db.sock --pidfile --detach --log-file</strong></span>
</pre></div></li><li class="listitem" value="4">Run OVS-VSCTL <a id="id341" class="calibre1"/>using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl --no-wait init</strong></span>
</pre></div></li><li class="listitem" value="5">Run the OVS switchd daemon using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vswitchd --pidfile --detach</strong></span>
</pre></div></li><li class="listitem" value="6">Create a bridge instance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-br br0</strong></span>
</pre></div></li><li class="listitem" value="7">Create a GRE tunnel with the remote node as <code class="email">172.17.8.101</code>. Here, the assumption is the etho IP of CoreOS instance 1 is <code class="email">172.17.8.101</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.101 options:key=100</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip13" class="calibre1"/>Tip</h3><p class="calibre9">The key needs to be different for each tunnel.</p></div></li></ol><div class="calibre16"/></div></div><div class="book" title="Configurations during the creation of the first container for a tenant"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec28" class="calibre1"/>Configurations during the creation of the first container for a tenant</h4></div></div></div><p class="calibre9">When <a id="id342" class="calibre1"/>a container is created for a tenant for the first time, a new bridge needs to be created and this container should be connected to OVS. The procedure to do this is described in detail as follows:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Bring down the docker0 bridge instance (the default bridge created by docker):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ip link set dev docker0 down</strong></span>
</pre></div></li><li class="listitem" value="2">Detach the virtual interface that is created for the container from docker0 bridge. The virtual interface starts with the name as veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo brctl delif docker0 vethda0657c</strong></span>
</pre></div></li><li class="listitem" value="3">Create a bridge instance for a tenant:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-br br0</strong></span>
</pre></div></li><li class="listitem" value="4">Add the port that is created in docker. This interface starts with veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 vethda0657c</strong></span>
</pre></div></li><li class="listitem" value="5">Set the IP address of the eth0 interface of the docker container. It is not possible to set the IP address of the docker container inside the docker instance. We need to use the <code class="email">nsenter</code> utility for this. To do this, follow these steps:<div class="book"><ul class="itemizedlist1"><li class="listitem">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">docker inspect --format {{.State.Pid}} &lt;container_name_or_ID&gt;</strong></span>
</pre></div></li><li class="listitem">Execute the following command and get the <code class="email">pid</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo nsenter --target $PID --mount --uts --ipc --net --pid ifconfig eth0 50.0.0.1</strong></span>
</pre></div></li></ul></div></li><li class="listitem" value="6">Create<a id="id343" class="calibre1"/> a GRE tunnel with the remote node as <code class="email">172.17.8.103</code>. Here, the assumption is the etho IP of CoreOS instance 2 is <code class="email">172.17.8.103</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.103 options:key=100</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip14" class="calibre1"/>Tip</h3><p class="calibre9">The key needs to be different for each tunnel.</p></div></li></ol><div class="calibre16"/></div></div><div class="book" title="Configurations during the creation of subsequent containers for a tenant"><div class="book"><div class="book"><div class="book"><h4 class="title3"><a id="ch07lvl4sec29" class="calibre1"/>Configurations during the creation of subsequent containers for a tenant </h4></div></div></div><p class="calibre9">This<a id="id344" class="calibre1"/> section describes the configuration to be done when subsequent containers are being created in the CoreOS instance.</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Add the port that is created in docker. This interface starts with veth:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 veth640b626</strong></span>
</pre></div></li><li class="listitem" value="2">Create a GRE tunnel with the remote node as <code class="email">172.17.8.103</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">sudo ovs-vsctl add-port br0 gre1 -- set Interface gre1 type=gre options:remote_ip=172.17.8.103 options:key=100</strong></span>
</pre></div></li></ol><div class="calibre16"/></div><p class="calibre9">Now you should be able to ping from the docker container running in CoreOS instance 1 to a docker container running in CoreOS instance 2. The main advantage of this solution is that it is possible to provide a virtual tenant network using this solution.</p></div></div><div class="book" title="Looping issue"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch07lvl3sec39" class="calibre1"/>Looping issue</h3></div></div></div><p class="calibre9">Everything <a id="id345" class="calibre1"/>works fine so far. However, when the number of CoreOS instances running in the cluster increases, we may need to create a mesh of tunnels between CoreOS instances for each customer/tenant. This ends up creating a loop in the network that will result in a traffic black hole. Let us look into this issue in detail and discuss the solution.</p><p class="calibre9">Consider a topology wherein you have three CoreOS instances running in the CoreOS cluster. In each of these instances, the green and orange customers' applications are deployed as a container. To provide VTN for each customer, we need to create tunnels across these CoreOS instances. In this case, we need to create two tunnels for each customer from every CoreOS instance. From CoreOS instance 1, we need to create two tunnels for each customer: one toward CoreOS instance 2 and the other toward CoreOS instance 3. Similarly, from CoreOS instance 2, we need to create two tunnels and so on. This will result in forming a layer2 loop in the customer's bridge instance.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip15" class="calibre1"/>Tip</h3><p class="calibre9">The total number of tunnels required to create a complete mesh in the topology is 2n-1, where n is the number of CoreOS instances wherein the tenant's service is deployed as a container.</p></div><p class="calibre9">As the <a id="id346" class="calibre1"/>bridge instance is a layer2 device, this results in forwarding the same packet multiple times in the loop:</p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Looping issue" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">A simple way to avoid this looping problem is by running <span class="strong"><strong class="calibre2">Spanning Tree Protocol</strong></span> (<span class="strong"><strong class="calibre2">STP</strong></span>) in <a id="id347" class="calibre1"/>OVS. STP is defined and standardized as IEEE 802.1D. STP identifies a loop-free topology, considering all the links in the topology based on different metrics. Once it identifies the loop-free topology, it will block one or more ports (in this case, tunnels) that are not part of the loop-free topology. The ports that are in a blocking state won't forward the traffic and hence avoid the traffic black hole.</p><p class="calibre9">In the preceding topology, when we run the spanning tree based on the priority or configured bridge-id, STP<a id="id348" class="calibre1"/> blocks one port, in this case blocks the port from CoreOS 3 to CoreOS 2:</p><div class="mediaobject"><img src="../images/00038.jpeg" alt="Looping issue" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">The list of commands to enable and configure the spanning tree in OVS are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Enable<a id="id349" class="calibre1"/> the spanning tree on a bridge instance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ovs-vsctl set Bridge br0 stp_enable=true</strong></span>
</pre></div></li><li class="listitem">Set the bridge priority:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ovs-vsctl set Bridge br0 other_config:stp-priority=0x7800</strong></span>
</pre></div></li><li class="listitem">Set the path cost of the port:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ovs-vsctl set Port eth0 other_config:stp-path-cost=10</strong></span>
</pre></div></li></ul></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip16" class="calibre1"/>Tip</h3><p class="calibre9">The bridge priority and path cost configurations are not mandatory configurations.</p></div><p class="calibre9">The spanning tree needs to be enabled on all the bridge instances of OVS to avoid any loop in the<a id="id350" class="calibre1"/> network.</p></div></div></div>
<div class="book" title="Summary" id="1CQAE1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec41" class="calibre1"/>Summary</h1></div></div></div><p class="calibre9">In this chapter, we have seen the importance of OVS in container communications and the various advantages provided by OVS. As there are multiple communication mechanisms available for container communications, while deploying the CoreOS cluster, based on the advantages, ease of use, and network management tools, you should cautiously choose one or more communication mechanisms in your deployment. In the next chapter, we are going to see some of the latest developments in CoreOS and advanced topics such as security, orchestration, container data volume management, and so on.</p></div></body></html>