<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Service Chaining and Networking Across Services" id="1394Q1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Service Chaining and Networking Across Services </h1></div></div></div><p class="calibre9">This chapter explains the need and mechanism for chaining different services running in a cluster.</p><p class="calibre9">This chapter covers the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Introduction to and necessity of service chaining</li><li class="listitem">Introduction to Docker networking</li><li class="listitem">Service chaining using Flannel/Rudder</li><li class="listitem">Service chaining using Weave </li></ul></div><p class="calibre9">In the previous chapter, we discussed in detail how the services running in different CoreOS instances can be discovered from other services. Once the services are discovered, one or more services may need to talk to each other. This chapter explains the need and mechanism for chaining different services running in the CoreOS cluster.</p></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Service Chaining and Networking Across Services" id="1394Q1-31555e2039a14139a7f00b384a5a2dd8">
<div class="book" title="Introduction to and necessity of service chaining"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec31" class="calibre1"/>Introduction to and necessity of service chaining</h1></div></div></div><p class="calibre9">As<a id="id235" class="calibre1"/> different <a id="id236" class="calibre1"/>services in the CoreOS clusters are deployed as a docker/Rackt container, it is inevitable that we will provide a mechanism to communicate between these services. These services may run in the same CoreOS instances of a cluster or they may run across different CoreOS instances in the cluster.</p><p class="calibre9">An example is, when a web server is deployed in node1 of a CoreOS cluster and database services are deployed in node2 of the cluster. Here, the database service provides a service to the web server and we can call this a service provider. Using the service discovery mechanisms described in the previous chapter, the web server service may discover the database service and its parameters such as its connection string with IP, port no., and so on. Once this information is discovered, the web server may need to interact with the database service for storing some information persistently or to fetch some information from the persistent storage. In order to do this, it may need to do the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Establish a network connection between each other</li><li class="listitem">Use the service provided by the service provider</li></ul></div><p class="calibre9">Everything<a id="id237" class="calibre1"/> looks fine. But when providing a network connection <a id="id238" class="calibre1"/>between the containers, there are some complexities. Let's look into those. Throughout this chapter, we assume that the services in the CoreOS instances are deployed as a docker container. </p><p class="calibre9">Each service/docker container in the CoreOS node is assigned an IP address. This IP address can be used by the applications running in the container to talk/communicate with each other. This works well when the services are running in the same CoreOS node. This is because all of the docker instances or services running in the same CoreOS node will be part of the same network, which will be connected by the docker0 bridge. When these services are running in different CoreOS nodes, then these nodes should use the port-mapping functionality provided by the host CoreOS to reach the desired container. But when using this mechanism, the containers should advertise the host machine's IP address in the discovery service. One option to push the host IP to the discovery service is by using the <code class="email">ExecStartPost</code> option in the fleet unit file. This way, the container will be able to access the host IP. The host machine IP address and network is not available to the containers. This allows some other external entity to provide this service.</p><p class="calibre9">Before looking into how this issue is being solved by mechanisms like Flannel and Weave, let us have a look at the details of Docker container networking.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Introduction to Docker networking"><div class="book" id="147LC2-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec32" class="calibre1"/>Introduction to Docker networking</h1></div></div></div><p class="calibre9">There<a id="id239" class="calibre1"/> are multiple communication requirements for containers/service as follows. CoreOS and Docker together should provide a mechanism to meet all the following requirements:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Container–Container communication in the same CoreOS node</li><li class="listitem">Container to CoreOS host communication</li><li class="listitem">Container to external world communication</li><li class="listitem">Container–Container communication in a different CoreOS node</li></ul></div><p class="calibre9">Let us look into how CoreOS provides these functionalities for the docker container in the following sections.</p></div>

<div class="book" title="Introduction to Docker networking">
<div class="book" title="Container–Container communication"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec28" class="calibre1"/>Container–Container communication</h2></div></div></div><p class="calibre9">This <a id="id240" class="calibre1"/>section describes in <a id="id241" class="calibre1"/>detail the different mechanisms provided by the CoreOS and Docker/Container technology to provide communication across different instances of Docker. There are multiple ways to provide this communication as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Docker0 bridge and veth pair</li><li class="listitem">Using Link</li><li class="listitem">Using common network stack</li></ul></div><div class="book" title="Docker0 bridge and veth pair"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec22" class="calibre1"/>Docker0 bridge and veth pair</h3></div></div></div><p class="calibre9">
<span class="strong"><strong class="calibre2">Docker0</strong></span> bridge is <a id="id242" class="calibre1"/>a Linux bridge <a id="id243" class="calibre1"/>created by docker in order to provide <a id="id244" class="calibre1"/>communication across different docker containers. By default, docker creates a Linux bridge called docker0 bridge, which provides connectivity for all the docker containers in the CoreOS host.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre9">Docker0 bridge is created only at the instantiation of the first container instance. No new bridge will be created on subsequent container instantiation.</p></div><p class="calibre9">Veth is a <span class="strong"><strong class="calibre2">Virtual Ethernet Device</strong></span> that can be used as a virtual link inside the Linux kernel. Typically, the <a id="id245" class="calibre1"/>veth device will be created in a pair (called veth pair) to provide connectivity across different instances of a container. When a new container/service is instantiated in the CoreOS node, a new veth pair will be created. One end of the veth pair is attached to the container service and the other end is connected to docker0 bridge. These docker0 bridge and veth pairs provide connectivity across different containers running in the same CoreOS node.</p><p class="calibre9">In the following diagram, the docker1 and docker2 containers are connected to docker0 bridge via the veth pair, which provides connectivity across the docker containers. One end of the veth pair, which is attached to the docker instance, will be visible inside the docker instance as the eth0 interface. It is possible to configure the IP address for this eth0 interface. The user can configure the eth0 interface of the docker1 and docker2 instances with the same network in order to provide connectivity across them.</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Docker0 bridge and veth pair" class="calibre11"/><div class="caption"><p class="calibre14">Container–Container communication using docker0 bridge</p></div></div><p class="calibre12"> </p><p class="calibre9">The<a id="id246" class="calibre1"/> docker instances are attached to <a id="id247" class="calibre1"/>docker0 bridge using a virtual subnet with an IP address ranging from <code class="email">172.17.51.1</code> – <code class="email">172.17.51.25</code>. As the docker side of the veth pair gets the IP in the same range, there is a possibility that, in two different servers/VM instances, two containers have the same IP address. This may result in problems while routing the IP packet.</p></div><div class="book" title="Using Link"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec23" class="calibre1"/>Using Link</h3></div></div></div><p class="calibre9">This is<a id="id248" class="calibre1"/> one of the simple ways of providing communication between Docker containers. <span class="strong"><strong class="calibre2">Docker Link</strong></span> is a unidirectional conduit/pipe between<a id="id249" class="calibre1"/> the source and the destination containers. The <code class="email">docker</code> command provides a way to link the containers while instantiating the container itself. The <code class="email">–link</code> option is used for this purpose. Docker Link can be used only to provide communication between containers running on the same host.</p><p class="calibre9">As an example, if the <code class="email">docker2</code> container wants to use the networking stack of another container, <code class="email">docker1</code>, then the command to start <code class="email">docker2</code> is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">/usr/bin/docker run --name docker2 –link docker1:docker1 ubuntu /bin/sh -c "while true; do echo Hello World; sleep 1; done"</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00024.jpeg" alt="Using Link" class="calibre11"/><div class="caption"><p class="calibre14">Container–Container communication using Docker Link</p></div></div><p class="calibre12"> </p></div><div class="book" title="Using common network stack"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec24" class="calibre1"/>Using common network stack</h3></div></div></div><p class="calibre9">In this mechanism, one docker container <a id="id250" class="calibre1"/>will use the networking stack provided by some other docker container's networking stack, instead of having its own networking stack. The docker container will not use the network namespace construct explained in the introduction section of this book, but shares the network namespace with another docker. As the container shares the namespace with another container, any application in one container can communicate with the other container as if both the docker container services are running as an application in one networking stack.</p><p class="calibre9">The <code class="email">docker</code> command provides a way to use another docker's networking stack while instantiating the container itself. The <code class="email">–net=container1</code> option is used for this purpose.</p><p class="calibre9">As an example, if the container <code class="email">cont_net1</code> wants to use the networking stack of another container, <code class="email">b1</code>, then the command to start <code class="email">cont_net1</code> is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">/usr/bin/docker run -d --name cont_net1 --net= cont_net1:b1 ubuntu /bin/sh -c "while true; do echo Hello World; sleep 1; done"</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00025.jpeg" alt="Using common network stack" class="calibre11"/><div class="caption"><p class="calibre14">Container–Container communication using common network stack</p></div></div><p class="calibre12"> </p></div></div></div>

<div class="book" title="Introduction to Docker networking">
<div class="book" title="Container to CoreOS host communication"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec29" class="calibre1"/>Container to CoreOS host communication</h2></div></div></div><p class="calibre9">Apart from the container to<a id="id251" class="calibre1"/> container communication mechanism, there are some instances where the <a id="id252" class="calibre1"/>service running inside the container may want to talk or exchange some information with applications running in the CoreOS host. CoreOS and docker provide some mechanisms to achieve this using the following mechanisms:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Host networking </li><li class="listitem">docker0 bridge</li></ul></div><div class="book" title="Host networking"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec25" class="calibre1"/>Host networking</h3></div></div></div><p class="calibre9">In this<a id="id253" class="calibre1"/> mechanism, the docker container will use the networking stack provided by the CoreOS host machine instead of having its own networking stack. The docker container will not use the network namespace construct explained in the introduction section of this book, but shares the network namespace with the host CoreOS operating system. As the container shares the namespace with the host CoreOS operating system, any application in the CoreOS host can communicate with the docker container as if the docker container service is running as an application in the CoreOS host. This is one of the simpler mechanisms to allow docker to host communication.</p><p class="calibre9">The <code class="email">docker</code> command provides a way to use the host machine's networking stack while instantiating the container itself. The <code class="email">–net=host</code> option is used for this purpose.</p><p class="calibre9">As an example, if the docker1 container wants to use the networking stack of the host CoreOS, then the command to<a id="id254" class="calibre1"/> start <code class="email">docker1</code> is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">/usr/bin/docker run -d --name docker1 --net=host ubuntu_ftp vsftpd</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00026.jpeg" alt="Host networking" class="calibre11"/><div class="caption"><p class="calibre14">Container–Container communication using the host network stack</p></div></div><p class="calibre12"> </p></div><div class="book" title="docker0 bridge"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec26" class="calibre1"/>docker0 bridge </h3></div></div></div><p class="calibre9">docker0 bridge<a id="id255" class="calibre1"/> can also be used to provide communication between docker and the host operating system. To do this, one of the interfaces in the host operating system should be attached to docker0 bridge, which provides communication. This is illustrated in detail in the following diagram where the <code class="email">eth1</code> interface of the CoreOS host machine is also connected to docker0 bridge, which provides connectivity to and from docker and the WAN. In this case, the eth0 interface of docker1, docker2, and the eth1 interface should be in the same network to provide network connectivity between docker and the host CoreOS.</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="docker0 bridge" class="calibre11"/><div class="caption"><p class="calibre14">Container – External world communication using docker0 bridge</p></div></div><p class="calibre12"> </p></div></div></div>

<div class="book" title="Introduction to Docker networking">
<div class="book" title="Container to CoreOS outside world communication"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec30" class="calibre1"/>Container to CoreOS outside world communication</h2></div></div></div><p class="calibre9">This is one of the basic<a id="id256" class="calibre1"/> requirements while <a id="id257" class="calibre1"/>deploying a service as a micro-service in the CoreOS cluster. The services running in the CoreOS cluster (as docker) should be accessible from the external world and vice versa. CoreOS and docker provide the following mechanisms to achieve this:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Host networking </li><li class="listitem">Port mapping</li><li class="listitem">Using docker0 bridge</li></ul></div><div class="book" title="Host networking"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec27" class="calibre1"/>Host networking</h3></div></div></div><p class="calibre9">Host networking<a id="id258" class="calibre1"/> is described in detail in the previous section. As the container shares the namespace with the host CoreOS operating system, when the host CoreOS operating system is connected to WAN, the service running inside the container should also be able to be part of the WAN network.</p><p class="calibre9">The <code class="email">docker</code> command provides a way to use the host machine's networking stack while instantiating the container itself. The <code class="email">–net=host</code> option is used for this purpose.</p><p class="calibre9">As an example, if the docker1 container wants to use the networking stack of the host CoreOS, then the command to <a id="id259" class="calibre1"/>start docker1 is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">/usr/bin/docker run -d --name docker1 --net=host ubuntu_ftp vsftpd</strong></span>
</pre></div></div><div class="book" title="Port mapping"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch06lvl3sec28" class="calibre1"/>Port mapping</h3></div></div></div><p class="calibre9">This is <a id="id260" class="calibre1"/>one of the most widely used mechanisms for communicating a docker container to the external world. In this mechanism, a port number in the host machine will be mapped to a port number in the docker container. Here, the port refers to transport layer ports like UDP port/TCP port. For example, if the user deploys a web server in a docker container, they can map the <code class="email">HTTP</code> port (port no. <code class="email">80</code>) in the host CoreOS operating system to the <code class="email">HTTP</code> port (port no. <code class="email">80</code>) of the docker container. So when a HTTP request is received by the CoreOS host, it forwards the request to the container, which processes this HTTP request. But one major challenge with respect to this mechanism is that the same service won't be able to deploy in multiple docker containers, as it results in port collision in the host operating system.</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Port mapping" class="calibre11"/><div class="caption"><p class="calibre14">Container – External world communication using port mapping</p></div></div><p class="calibre12"> </p></div></div></div>

<div class="book" title="Introduction to Docker networking">
<div class="book" title="Container – Container communication in different CoreOS nodes"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec31" class="calibre1"/>Container – Container communication in different CoreOS nodes</h2></div></div></div><p class="calibre9">We have seen how<a id="id261" class="calibre1"/> CoreOS or docker<a id="id262" class="calibre1"/> provides networking from a single node perspective. As the services are deployed inside the CoreOS cluster, it is necessary to provide communication between containers running in different CoreOS nodes in a cluster. The rest of the chapter discusses this communication mechanism in detail. There are multiple tools that provide this as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Weave</li><li class="listitem">Flannel/Rudder</li><li class="listitem">Using <a id="id263" class="calibre1"/><span class="strong"><strong class="calibre2">OVS</strong></span> (<span class="strong"><strong class="calibre2">OpenVSwitch</strong></span>)</li></ul></div><p class="calibre9">In this chapter, we are going to <a id="id264" class="calibre1"/>see how Flannel and Weave provide the communication mechanism. In the next chapter, we will discuss OVS in detail and how it can be used to provide communication between the various containers.</p></div></div>
<div class="book" title="Introduction to Weave" id="1565U1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec33" class="calibre1"/>Introduction to Weave</h1></div></div></div><p class="calibre9">We learned<a id="id265" class="calibre1"/> before that applications running inside Docker have no knowledge of the IP address of the host machine. Hence, they are not in position to register their IP for the service, since another container running outside the host has to use the host IP address for accessing the service.</p><p class="calibre9">If an IP address of the host machine is passed as an environment variable, service information can be stored in <code class="email">etcd</code> and read by the service user as illustrated in <a class="calibre1" title="Chapter 5. Discovering Services Running in a Cluster" href="part0033_split_000.html#VF2I1-31555e2039a14139a7f00b384a5a2dd8">Chapter 5</a>, <span class="strong"><em class="calibre10">Discovering Services Running in Cluster</em></span>. This approach requires the application code to be aware of how services can be discovered.</p><p class="calibre9">
<span class="strong"><strong class="calibre2">Weave</strong></span> simplifies<a id="id266" class="calibre1"/> service discovery and does a lot more. Weave provides a mechanism to connect applications running inside a Docker container irrespective of where they are deployed. Since application services are running as a Docker container, the ease of communication of micro-services running in Docker containers is very important.</p><p class="calibre9">Weave registers<a id="id267" class="calibre1"/> the named containers automatically in <span class="strong"><strong class="calibre2">weaveDNS</strong></span>, hence services or dockers can be accessed by resolving their names through regular name resolution. This requires application-specific code as routine system calls like <code class="email">gethostbyname</code>, or <code class="email">getaddrinfo</code> with a pre-defined Docker name used for service, will resolve the name to the IP address using <code class="email">weaveDNS</code>.</p><p class="calibre9">Weave sets up a Virtual Ethernet Switch connecting all docker containers and in turn services or applications running inside Docker. Weave builds up the network assigning unique IP addresses to each of the docker containers as they come up and free the IP address when they go down. With this, it is no longer required to export a port explicitly when starting Docker and enables service to be accessed from anywhere, thus not making it mandatory that frontend applications run on the host machine, which exposes the public network. It is also possible to assign the IP addresses to the containers manually, which can eventually be used to create isolated subnets. This enables the isolation of a group of applications from another group.</p><p class="calibre9">Weave is<a id="id268" class="calibre1"/> simple to integrate with Docker, which we will see when we go hands-on later in this chapter. Weave also offers security by encrypting traffic when docker containers need to be connected through public or untrusted networks.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Introduction to Flannel/Rudder"><div class="book" id="164MG2-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec34" class="calibre1"/>Introduction to Flannel/Rudder</h1></div></div></div><p class="calibre9">Similar to Weave, <span class="strong"><strong class="calibre2">Flannel</strong></span> also assigns an IP address to a container that can be used for container <a id="id269" class="calibre1"/>to container communication by creating an overlay mesh network. Flannel internally uses <code class="email">etcd</code> to store the mapping between the assigned container IP address and host IP address. It doesn't have elaborate features like Weave and can be used if other feature sets provided by Weave are not required. For example, Flannel doesn't provide automatic service discovery through DNS and still requires application coding or instrumentation to discover service endpoints.</p><p class="calibre9">By default, each container is assigned an IP address in the <code class="email">/24</code> subnet. Subnet size can be configured. Flannel uses UDP to encapsulate traffic to transmit to a destination.</p><p class="calibre9">In later sections, we will learn about using Flannel. Flannel was previously referred to as <span class="strong"><strong class="calibre2">Rudder</strong></span>.</p><p class="calibre9">Integrating Weave with <code class="email">CoreOSWeave</code> is rather simple to install. The standalone installation is as simple as pulling the Weave script from the repository and calling another command to set up and start the Weave router.</p><p class="calibre9">Let's run through the sequence of command manually, and then we will run the installation and setup through <code class="email">cloud config</code>. </p></div>

<div class="book" title="Introduction to Flannel/Rudder">
<div class="book" title="Installation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl3sec29" class="calibre1"/>Installation</h2></div></div></div><p class="calibre9">Weave <a id="id270" class="calibre1"/>can be installed onto the system by fetching the script using <code class="email">wget</code> or <code class="email">curl</code>. After downloading, change the permission to make it executable.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">/usr/bin/wget -N -P /opt/bin git.io/weave</strong></span>
<span class="strong"><strong class="calibre2">/usr/bin/chmod +x /opt/bin/weave</strong></span>
</pre></div></div></div>

<div class="book" title="Introduction to Flannel/Rudder">
<div class="book" title="Setting up Weave"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl3sec30" class="calibre1"/>Setting up Weave</h2></div></div></div><p class="calibre9">Run <a id="id271" class="calibre1"/>the command <code class="email">weave launch</code> to set up and start the Weave router, Weave DNS, and proxy for Docker API commands like <code class="email">docker run</code> and so on. This command also sets up the Weave network. When this command is run for the first time in the machine, the <code class="email">Weave Docker</code> image required for setup is downloaded.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">weave launch</strong></span>
<span class="strong"><strong class="calibre2">Unable to find image 'weaveworks/weave:latest' locally</strong></span>
<span class="strong"><strong class="calibre2">latest: Pulling from weaveworks/weave</strong></span>
<span class="strong"><strong class="calibre2">4c25b19b8af6: Pulling fs layer</strong></span>
<span class="strong"><strong class="calibre2">6498a5f7a259: Pulling fs layer</strong></span>
<span class="strong"><strong class="calibre2">638a117dec98: Pulling fs layer</strong></span>
<span class="strong"><strong class="calibre2">afebf09d0da1: Pulling fs layer</strong></span>
<span class="strong"><strong class="calibre2">e5ac6ff68d75: Pulling fs layer</strong></span>
<span class="strong"><strong class="calibre2">6498a5f7a259: Verifying Checksum</strong></span>
<span class="strong"><strong class="calibre2">6498a5f7a259: Download complete</strong></span>
<span class="strong"><strong class="calibre2">4c25b19b8af6: Verifying Checksum</strong></span>
<span class="strong"><strong class="calibre2">4c25b19b8af6: Download complete</strong></span>
<span class="strong"><strong class="calibre2">638a117dec98: Verifying Checksum</strong></span>
<span class="strong"><strong class="calibre2">638a117dec98: Download complete</strong></span>
<span class="strong"><strong class="calibre2">4c25b19b8af6: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">e5ac6ff68d75: Verifying Checksum</strong></span>
<span class="strong"><strong class="calibre2">e5ac6ff68d75: Download complete</strong></span>
<span class="strong"><strong class="calibre2">6498a5f7a259: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">638a117dec98: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">afebf09d0da1: Verifying Checksum</strong></span>
<span class="strong"><strong class="calibre2">afebf09d0da1: Download complete</strong></span>
<span class="strong"><strong class="calibre2">afebf09d0da1: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">e5ac6ff68d75: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">Digest: sha256:1a8565d24ef2b617619a482cbfe895f8fc27e7a4518ac18b9005ed7b4caa223f</strong></span>
<span class="strong"><strong class="calibre2">Status: Downloaded newer image for weaveworks/weave:latest</strong></span>
</pre></div><p class="calibre9">To check<a id="id272" class="calibre1"/> the status, the <code class="email">status</code> command is used to check the router status. If this command is run for the first time, the Weave Docker image required for setup is downloaded.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">weave status</strong></span>
<span class="strong"><strong class="calibre2">Unable to find image 'weaveworks/weaveexec:latest' locally</strong></span>
<span class="strong"><strong class="calibre2">latest: Pulling from weaveworks/weaveexec</strong></span>
<span class="strong"><strong class="calibre2">b6069e3f1ecc: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">326c397fb7ed: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">4d2b936d2fa5: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">16a356f92997: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">ae09ffb2bf28: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">14931fda689e: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">85d81711422f: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">16bfdc48cfb1: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">52bab2cc143b: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">82d8a8c031ec: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">6993b16a50ae: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">ee37b21b766d: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">3c16e5ee0357: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">77b8fe327374: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">23272d8d46c3: Pull complete</strong></span>
<span class="strong"><strong class="calibre2">Digest: sha256:1d34246eb53f070f0e35ad13974367e2a4fee78039da74b8760a4eff49a9334f</strong></span>
<span class="strong"><strong class="calibre2">Status: Downloaded newer image for weaveworks/weaveexec:latest</strong></span>
<span class="strong"><strong class="calibre2">        Version: git-efd4fc4704ce</strong></span>

<span class="strong"><strong class="calibre2">        Service: router</strong></span>
<span class="strong"><strong class="calibre2">       Protocol: weave 1..2</strong></span>
<span class="strong"><strong class="calibre2">           Name: 5a:62:3a:91:af:c5(core-01.testdomain.com)</strong></span>
<span class="strong"><strong class="calibre2">     Encryption: disabled</strong></span>
<span class="strong"><strong class="calibre2">  PeerDiscovery: enabled</strong></span>
<span class="strong"><strong class="calibre2">        Targets: 0</strong></span>
<span class="strong"><strong class="calibre2">    Connections: 0</strong></span>
<span class="strong"><strong class="calibre2">          Peers: 1</strong></span>
<span class="strong"><strong class="calibre2"> TrustedSubnets: none</strong></span>

<span class="strong"><strong class="calibre2">        Service: ipam</strong></span>
<span class="strong"><strong class="calibre2">         Status: idle</strong></span>
<span class="strong"><strong class="calibre2">          Range: 10.32.0.0-10.47.255.255</strong></span>
<span class="strong"><strong class="calibre2">  DefaultSubnet: 10.32.0.0/12</strong></span>

<span class="strong"><strong class="calibre2">        Service: dns</strong></span>
<span class="strong"><strong class="calibre2">         Domain: weave.local.</strong></span>
<span class="strong"><strong class="calibre2">       Upstream: 10.0.2.3</strong></span>
<span class="strong"><strong class="calibre2">            TTL: 1</strong></span>
<span class="strong"><strong class="calibre2">        Entries: 0</strong></span>

<span class="strong"><strong class="calibre2">        Service: proxy</strong></span>
<span class="strong"><strong class="calibre2">        Address: unix:///var/run/weave/weave.sock</strong></span>
</pre></div><p class="calibre9">If specific<a id="id273" class="calibre1"/> IP addresses were not provided during container startup, Weave assigns a free IP from the address pool to the container and releases that address (that is, marks it free) when the container exits. The IP address pool is maintained across all the Weave instances that are part of the cluster. Hence, at Weave launch either all the members of the cluster or one or more members of the cluster should be provided. Additionally, a parameter should be included to inform Weave about the number of members. To illustrate, if there are three members with the IP addresses <code class="email">172.17.8.101</code>, <code class="email">172.17.8.102</code>, and <code class="email">172.17.8.103</code> then the following commands are the right way to launch Weave for allocating the IP addresses.</p><p class="calibre9">Option one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">core@core-01 ~$ weave launch 172.17.8.102 172.17.8.103</strong></span>
<span class="strong"><strong class="calibre2">core@core-02 ~$ weave launch 172.17.8.101 172.17.8.103</strong></span>
<span class="strong"><strong class="calibre2">core@core-03 ~$ weave launch 172.17.8.101 172.17.8.102</strong></span>
</pre></div><p class="calibre9"> Option two:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">core@core-01 ~$ weave launch --init-peer-count 3</strong></span>
<span class="strong"><strong class="calibre2">core@core-02 ~$ weave launch --init-peer-count 3 172.17.8.101</strong></span>
<span class="strong"><strong class="calibre2">core@core-03 ~$ weave launch --init-peer-count 3 172.17.8.102</strong></span>
</pre></div><p class="calibre9">Weave allocates IP addresses in the <code class="email">10.32.0.0/12</code> range by default, unless it's overridden with the <code class="email">--ipalloc-range</code> option at the time of launch. For instance, if the subnet to be used is <code class="email">10.1.0.0</code> with a size of <code class="email">16</code>, the following command can be provided. The same value should be provided across all the members.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">weave launch --ipalloc-range 10.1.0.0/16</strong></span>
</pre></div><p class="calibre9">Weave <a id="id274" class="calibre1"/>gives an option to enable or disable the use of the Weave DNS service. By default, the Weave DNS service is enabled. To disable this service, the <code class="email">--without-dns</code> option can be provided while running Weave. Weave maintains an in-memory database of all the hosts. It builds up the database as the peer join. This is maintained on all the hosts and is replicated across hosts. If a hostname is in the <code class="email">.weave.local</code> domain, then Weave DNS records the association of that name with the container's Weave IP address (es). When DNS query arrives for the <code class="email">.weave.local</code> domain, the Weave DNS database is used to return with the IPs of all containers for that hostname across the entire cluster. When DNS query arrives for the name in a domain other than <code class="email">.weave.local</code>, it queries the host's configured nameserver, hence complying with default behavior.</p></div></div>

<div class="book" title="Introduction to Flannel/Rudder">
<div class="book" title="Container startup"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl3sec31" class="calibre1"/>Container startup</h2></div></div></div><p class="calibre9">Docker<a id="id275" class="calibre1"/> containers can be started by using the Weave proxy or without using the Weave proxy. When containers are created using the Weave proxy, the container initialization waits for the Weave network interface to become available and then proceeds with further startup of the container. The IP addresses of the containers are assigned and the container is connected to the Weave network.</p><p class="calibre9">The following command sets up the environment so that Docker containers can connect to the Weave network automatically. The usual Docker commands can be used to start the Docker container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">eval "$(weave env)"</strong></span>
<span class="strong"><strong class="calibre2">docker run ...</strong></span>
</pre></div><p class="calibre9">By default, Weave allows the communication of a container with all other containers in the cluster. This can be restricted by providing a subnet range from which an IP address can be allocated. Multiple subnets can also be provided. Also, an IP address can be provided that will be assigned to the container in addition to the automatic IP address allocation. It is also possible to avoid automatic IP address allocation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">docker run -e WEAVE_CIDR=net:10.32.7.0/24 ...</strong></span>
<span class="strong"><strong class="calibre2">docker run -e WEAVE_CIDR="net:10.32.1.0/24 net:10.32.8.0/24 ip:10.32.9.1/24" ...</strong></span>
</pre></div><p class="calibre9">Containers<a id="id276" class="calibre1"/> can be launched without the Weave proxy by starting them using the command <code class="email">weave run</code>.</p><p class="calibre9">The following is the setup that will be used to illustrate a network between two CoreOS hosts. We will instantiate two CoreOS members in a cluster and spawn two docker containers inside members. The docker container runs a busybox shell so that we can run the networking command and check the IP address assignments and peer container reachability. This setup illustrates the scenario where communication is happening between docker containers across two CoreOS hosts. </p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Container startup" class="calibre11"/><div class="caption"><p class="calibre14">Weave setup</p></div></div><p class="calibre12"> </p><p class="calibre9">The following is the <code class="email">cloud-config</code> file used to create the setup. The other configuration files reused are from the section <span class="strong"><em class="calibre10">Static discovery</em></span> in <a class="calibre1" title="Chapter 3. Creating Your CoreOS Cluster and Managing the Cluster" href="part0026_split_000.html#OPEK1-31555e2039a14139a7f00b384a5a2dd8">Chapter 3</a>, <span class="strong"><em class="calibre10">Creating your CoreOS cluster and managing the Cluster</em></span>. Set <code class="email">$num_instances</code> to <code class="email">2</code> in the <code class="email">config.rb</code> file as we need to start only two instances of members.</p><div class="informalexample"><pre class="programlisting">#cloud-config

---
write_files:
  - path: /etc/weave.core-01.testdomain.com.env
    permissions: 0644
    owner: root
    content: |
      WEAVE_LAUNCH_ARGS="172.17.8.102"
  - path: /etc/weave.core-02.testdomain.com.env
    permissions: 0644
    owner: root
    content: |
      WEAVE_LAUNCH_ARGS="172.17.8.101"

coreos:
  units:
    - name: 10-weave.network
      runtime: false
      content: |
        [Match]
        Type=bridge
        Name=weave*

        [Network]

    - name: install-weave.service
      command: start
      enable: true
      content: |
        [Unit]
        After=network-online.target
        After=docker.service
        Description=Install Weave
        Requires=network-online.target
        Requires=docker.service

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=/usr/bin/wget -N -P /opt/bin git.io/weave 
        ExecStartPre=/usr/bin/chmod +x /opt/bin/weave
        ExecStart=/bin/echo Wave Installed

    - name: weave.service
      command: start
      enable: true
      content: |
        [Unit]
        After=install-weave.service
        Description=Weave Network
        Requires=install-weave.service

        [Service]
        Type=oneshot
        EnvironmentFile=/etc/weave.%H.env
        ExecStart=/opt/bin/weave launch $WEAVE_LAUNCH_ARGS</pre></div><p class="calibre9">Let's run<a id="id277" class="calibre1"/> through the finer details of the <code class="email">cloud-config</code> file before we start containers in the members and check connectivity.</p><p class="calibre9">Firstly, we create two files using the <code class="email">write_files</code> section. They will be used before starting Weave on respective machines. Each file has the hostname in their name, so that using <code class="email">%H</code> in <code class="email">EnvironmentFile</code> results in referring the file meant for the member.</p><p class="calibre9">Unit file <code class="email">10-weave.network </code>is added to allow the Weave network to be used for DHCP queries. By default, <code class="email">docker0</code> bridge is used. This is optional and is required if the Weave network is being used for DHCP.</p><p class="calibre9">Unit <code class="email">install-weave.service</code> installs Weave onto the member and sets the required permissions. This is a one-shot service as it has served its purpose once Weave is installed. <code class="email">After=network-online.target </code>is added to ensure that this network is up before Weave is installed. This is required so that packages can be downloaded from the Internet.</p><p class="calibre9">Unit <code class="email">weave.service</code> sources the corresponding environment file and launches Weave.</p><p class="calibre9">Boot the cluster using <code class="email">Vagrant up</code>. After booting up, the nodes in the cluster comes up with weave networking up:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh core-01</strong></span>
<span class="strong"><strong class="calibre2">weave status</strong></span>

<span class="strong"><strong class="calibre2">        Version: 1.4.2</strong></span>

<span class="strong"><strong class="calibre2">        Service: router</strong></span>
<span class="strong"><strong class="calibre2">       Protocol: weave 1..2</strong></span>
<span class="strong"><strong class="calibre2">           Name: 96:63:f1:5a:ac:3a(core-01.testdomain.com)</strong></span>
<span class="strong"><strong class="calibre2">     Encryption: disabled</strong></span>
<span class="strong"><strong class="calibre2">  PeerDiscovery: enabled</strong></span>
<span class="strong"><strong class="calibre2">        Targets: 0</strong></span>
<span class="strong"><strong class="calibre2">    Connections: 1 (1 established)</strong></span>
<span class="strong"><strong class="calibre2">          Peers: 2 (with 2 established connections)</strong></span>
<span class="strong"><strong class="calibre2"> TrustedSubnets: none</strong></span>

<span class="strong"><strong class="calibre2">        Service: ipam</strong></span>
<span class="strong"><strong class="calibre2">         Status: idle</strong></span>
<span class="strong"><strong class="calibre2">          Range: 10.32.0.0-10.47.255.255</strong></span>
<span class="strong"><strong class="calibre2">  DefaultSubnet: 10.32.0.0/12</strong></span>

<span class="strong"><strong class="calibre2">        Service: dns</strong></span>
<span class="strong"><strong class="calibre2">         Domain: weave.local.</strong></span>
<span class="strong"><strong class="calibre2">       Upstream: 10.0.2.3</strong></span>
<span class="strong"><strong class="calibre2">            TTL: 1</strong></span>
<span class="strong"><strong class="calibre2">        Entries: 0</strong></span>

<span class="strong"><strong class="calibre2">        Service: proxy</strong></span>
<span class="strong"><strong class="calibre2">        Address: unix:///var/run/weave/weave.sock</strong></span>
</pre></div><p class="calibre9">We can see that both the peers are connected. We can also see that the DNS and router services are enabled with default settings.</p><p class="calibre9">Now we<a id="id278" class="calibre1"/> will start a Docker container on each of the members. We will run a simple shell on busybox. The following command is executed on both the members. Note that we are providing the name of the docker container explicitly. This will result in two DNS entries with the domain <code class="email">.weave.local</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh core-01</strong></span>
<span class="strong"><strong class="calibre2">eval "$(weave env)"</strong></span>
<span class="strong"><strong class="calibre2">/usr/bin/docker run --name=container2 -it busybox /bin/sh</strong></span>
<span class="strong"><strong class="calibre2">/ # ifconfig –a</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
<span class="strong"><strong class="calibre2">ethwe     Link encap:Ethernet  HWaddr E6:AA:25:26:EA:04</strong></span>
<span class="strong"><strong class="calibre2">          inet addr:10.32.0.1  Bcast:0.0.0.0  Mask:255.240.0.0</strong></span>
<span class="strong"><strong class="calibre2">          inet6 addr: fe80::e4aa:25ff:fe26:ea04/64 Scope:Link</strong></span>
<span class="strong"><strong class="calibre2">          UP BROADCAST RUNNING MULTICAST  MTU:1410  Metric:1</strong></span>
<span class="strong"><strong class="calibre2">          RX packets:13 errors:0 dropped:0 overruns:0 frame:0</strong></span>
<span class="strong"><strong class="calibre2">          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0</strong></span>
<span class="strong"><strong class="calibre2">          collisions:0 txqueuelen:1000</strong></span>
<span class="strong"><strong class="calibre2">          RX bytes:1385 (1.3 KiB)  TX bytes:620 (620.0 B)</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>

<span class="strong"><strong class="calibre2">vagrant ssh core-02</strong></span>
<span class="strong"><strong class="calibre2">eval "$(weave env)"</strong></span>
<span class="strong"><strong class="calibre2">/usr/bin/docker run --name=container2 -it busybox /bin/sh</strong></span>
<span class="strong"><strong class="calibre2">/ # ifconfig -a</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
<span class="strong"><strong class="calibre2">ethwe     Link encap:Ethernet  HWaddr DA:E1:B8:3A:39:FB</strong></span>
<span class="strong"><strong class="calibre2">          inet addr:10.40.0.0  Bcast:0.0.0.0  Mask:255.240.0.0</strong></span>
<span class="strong"><strong class="calibre2">          inet6 addr: fe80::d8e1:b8ff:fe3a:39fb/64 Scope:Link</strong></span>
<span class="strong"><strong class="calibre2">          UP BROADCAST RUNNING MULTICAST  MTU:1410  Metric:1</strong></span>
<span class="strong"><strong class="calibre2">          RX packets:11 errors:0 dropped:0 overruns:0 frame:0</strong></span>
<span class="strong"><strong class="calibre2">          TX packets:7 errors:0 dropped:0 overruns:0 carrier:0</strong></span>
<span class="strong"><strong class="calibre2">          collisions:0 txqueuelen:1000</strong></span>
<span class="strong"><strong class="calibre2">          RX bytes:969 (969.0 B)  TX bytes:550 (550.0 B)</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
</pre></div><p class="calibre9">Now<a id="id279" class="calibre1"/> ping the IP address and hostname of the other container to ensure that the network across containers and the DNS service is working. You can also run the status command to check that two DNS entries has been updated, once the containers are started.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">weave status</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
<span class="strong"><strong class="calibre2">        Service: dns</strong></span>
<span class="strong"><strong class="calibre2">         Domain: weave.local.</strong></span>
<span class="strong"><strong class="calibre2">       Upstream: 10.0.2.3</strong></span>
<span class="strong"><strong class="calibre2">            TTL: 1</strong></span>
<span class="strong"><strong class="calibre2">        Entries: 2</strong></span>

<span class="strong"><strong class="calibre2">        Service: proxy</strong></span>
<span class="strong"><strong class="calibre2">        Address: unix:///var/run/weave/weave.sock</strong></span>
</pre></div></div></div>
<div class="book" title="Integrating Flannel with CoreOS"><div class="book" id="173722-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec35" class="calibre1"/>Integrating Flannel with CoreOS</h1></div></div></div><p class="calibre9">Flannel <a id="id280" class="calibre1"/>runs a daemon <code class="email">flanneld </code>on each host, responsible for allocating a free IP within the configured subnet. <code class="email">flanneld</code> sets a <a id="id281" class="calibre1"/>watch on <code class="email">etcd</code> information and routes the packets using the mechanism configured.</p><p class="calibre9">Although the <code class="email">flanneld</code> service is not part of the standard CoreOS distribution, when the <code class="email">flanneld</code> service is started through <code class="email">cloud-config</code>, CoreOS internally starts a service before other initializations to pull <code class="email">flanneld</code> from the <code class="email">docker</code> registry. <code class="email">flanneld</code> is stored as a <code class="email">docker</code> container in the CoreOS enterprise registry.</p><p class="calibre9">The same setup used for Weave networking is being used here. Note that for Flannel, hostnames are irrelevant.</p><p class="calibre9">The following is the <code class="email">cloud-config</code> file used to create setup. The other configuration files are reused from the <span class="strong"><em class="calibre10">Static discovery</em></span> section in <a class="calibre1" title="Chapter 3. Creating Your CoreOS Cluster and Managing the Cluster" href="part0026_split_000.html#OPEK1-31555e2039a14139a7f00b384a5a2dd8">Chapter 3</a>, <span class="strong"><em class="calibre10">Creating your CoreOS cluster and Managing the Cluster</em></span>. Set <code class="email">$num_instances</code> to <code class="email">2</code> in the <code class="email">config.rb</code> file as we need to start only two instance of members.</p><div class="informalexample"><pre class="programlisting">#cloud-config

---
coreos:
  etcd2:
    name: core-03
    advertise-client-urls: http://$public_ipv4:2379
    initial-advertise-peer-urls: http://$private_ipv4:2380
    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
    initial-cluster-token: coreOS-static
    initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
  flannel:
    interface: $public_ipv4

  units:
  - name: etcd2.service
    command: start
    enable: true
  - name: flanneld.service
    drop-ins:
      - name: 50-network-config.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" }'
    command: start</pre></div><p class="calibre9">Vagrant <a id="id282" class="calibre1"/>setup will configure the interface<a id="id283" class="calibre1"/> that Flannel should use. This is done by providing the following configuration in <code class="email">cloud-config</code>:</p><div class="informalexample"><pre class="programlisting">flannel:
    interface: $public_ipv4</pre></div><p class="calibre9">Directive <code class="email">ExecStartPre</code> is added to the <code class="email">flanneld</code> service configuration as a drop-in file with the name <code class="email">50-network-config.conf</code>. Using <code class="email">ExecStartPre</code>,<code class="email"> Flannel </code>configuration is updated in <code class="email">etcd</code>. This is mandatory for Flannel to work as it looks up the configuration at <code class="email">/coreos.com/network/config</code>. The following are the Flannel configurations that can be provided as comma-separated values while setting the configuration to etcd:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">Network</code>: This specifies the subnets to be used across all Flannel networks. This field is mandatory. In the preceding example, the subnet configuration was provided as <code class="email">10.1.0.0/16</code>. Further subnets for each of the hosts will be created within this subnet.</li><li class="listitem"><code class="email">SubnetLen</code>: This specifies the size of the subnet as bits allocated to each host. This field should have a value less or equal to the subnet size provided for the network. If this field is not provided, a default value of <code class="email">24</code> is used if the <code class="email">Network</code> field has a subnet size more than or equal to <code class="email">24</code>. If the <code class="email">Network</code> field has a subnet size less than <code class="email">24</code> and this field is not configured, one less than the value configured for the <code class="email">Network</code> is used.</li><li class="listitem"><code class="email">SubnetMin</code>: This specifies the starting IP range from which the subnet allocation <a id="id284" class="calibre1"/>starts. This defaults<a id="id285" class="calibre1"/> to the first subnet of Network if this field is not provided.</li><li class="listitem"><code class="email">SubnetMax</code>: This specifies the end IP range from which the subnet allocation starts. This defaults to the first subnet of Network if this field is not provided. </li><li class="listitem"><code class="email">Backend</code>: This specifies the mechanism to be used for sending traffic across hosts. Supported values are <code class="email">udp, vxlan, host-gw</code>, and so on. If this field is not provided, <code class="email">udp</code> is used. If <code class="email">udp </code>is used, the port number to be used for UDP is configured. If the port is not provided, the default port of 8285 is used. This port should be allowed if the hosts are to be networked across firewalls.</li></ul></div><p class="calibre9">The following is another sample configuration for Flannel, which contains other optional parameters set to their respective defaults:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16", "SubnetLen": 24, "SubnetMin": "10.1.0.0", "SubnetMax": "10.1.255.0"}' </strong></span>
</pre></div><p class="calibre9">Boot the cluster using <code class="email">Vagrant up</code>. After booting up, the clusters come up with the interfaces setup on the host by Flannel.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh core-01</strong></span>
<span class="strong"><strong class="calibre2">ifconfig –a</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
<span class="strong"><strong class="calibre2">flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472</strong></span>
<span class="strong"><strong class="calibre2">        inet 10.1.35.0  netmask 255.255.0.0  destination 10.1.35.0</strong></span>
<span class="strong"><strong class="calibre2">        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 500  (UNSPEC)</strong></span>
<span class="strong"><strong class="calibre2">        RX packets 0  bytes 0 (0.0 B)</strong></span>
<span class="strong"><strong class="calibre2">        RX errors 0  dropped 0  overruns 0  frame 0</strong></span>
<span class="strong"><strong class="calibre2">        TX packets 0  bytes 0 (0.0 B)</strong></span>
<span class="strong"><strong class="calibre2">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</strong></span>

<span class="strong"><strong class="calibre2">...</strong></span>
</pre></div><p class="calibre9">Similarly, we can see that interfaces were created by Flannel on other instances also.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh core-02</strong></span>
<span class="strong"><strong class="calibre2">ifconfig –a</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
<span class="strong"><strong class="calibre2">flannel0: flags=4305&lt;UP,POINTOPOINT,RUNNING,NOARP,MULTICAST&gt;  mtu 1472</strong></span>
<span class="strong"><strong class="calibre2">        inet 10.1.27.0  netmask 255.255.0.0  destination 10.1.27.0</strong></span>
<span class="strong"><strong class="calibre2">        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 500  (UNSPEC)</strong></span>
<span class="strong"><strong class="calibre2">        RX packets 0  bytes 0 (0.0 B)</strong></span>
<span class="strong"><strong class="calibre2">        RX errors 0  dropped 0  overruns 0  frame 0</strong></span>
<span class="strong"><strong class="calibre2">        TX packets 0  bytes 0 (0.0 B)</strong></span>
<span class="strong"><strong class="calibre2">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
</pre></div><p class="calibre9">Flannel<a id="id286" class="calibre1"/> sets up subnets <code class="email">10.1.35.0</code> for host1 and <code class="email">10.1.27.0</code> for host2 to be used by containers. Flannel decides on the available subnets <a id="id287" class="calibre1"/>before allocating to a host. Now, we will start a <code class="email">Docker</code> container on each of the members. We will run a simple shell on <code class="email">busybox</code>. The following command is executed on both the members:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">vagrant ssh core-01</strong></span>
<span class="strong"><strong class="calibre2">/usr/bin/docker run -it busybox /bin/sh</strong></span>
<span class="strong"><strong class="calibre2">/ # ifconfig -a</strong></span>
<span class="strong"><strong class="calibre2">eth0      Link encap:Ethernet  HWaddr 02:42:0A:01:23:03</strong></span>
<span class="strong"><strong class="calibre2">          inet addr:10.1.35.3  Bcast:0.0.0.0  Mask:255.255.255.0</strong></span>
<span class="strong"><strong class="calibre2">          inet6 addr: fe80::42:aff:fe01:2303/64 Scope:Link</strong></span>
<span class="strong"><strong class="calibre2">          UP BROADCAST RUNNING MULTICAST  MTU:1472  Metric:1</strong></span>
<span class="strong"><strong class="calibre2">          RX packets:19 errors:0 dropped:0 overruns:0 frame:0</strong></span>
<span class="strong"><strong class="calibre2">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</strong></span>
<span class="strong"><strong class="calibre2">          collisions:0 txqueuelen:0</strong></span>
<span class="strong"><strong class="calibre2">          RX bytes:1611 (1.5 KiB)  TX bytes:508 (508.0 B)</strong></span>

<span class="strong"><strong class="calibre2">...</strong></span>

<span class="strong"><strong class="calibre2">vagrant ssh core-02</strong></span>
<span class="strong"><strong class="calibre2">/usr/bin/docker run -it busybox /bin/sh</strong></span>
<span class="strong"><strong class="calibre2">/ # ifconfig -a</strong></span>
<span class="strong"><strong class="calibre2">eth0      Link encap:Ethernet  HWaddr 02:42:0A:01:1B:03</strong></span>
<span class="strong"><strong class="calibre2">          inet addr:10.1.27.3  Bcast:0.0.0.0  Mask:255.255.255.0</strong></span>
<span class="strong"><strong class="calibre2">          inet6 addr: fe80::42:aff:fe01:1b03/64 Scope:Link</strong></span>
<span class="strong"><strong class="calibre2">          UP BROADCAST RUNNING MULTICAST  MTU:1472  Metric:1</strong></span>
<span class="strong"><strong class="calibre2">          RX packets:21 errors:0 dropped:0 overruns:0 frame:0</strong></span>
<span class="strong"><strong class="calibre2">          TX packets:9 errors:0 dropped:0 overruns:0 carrier:0</strong></span>
<span class="strong"><strong class="calibre2">          collisions:0 txqueuelen:0</strong></span>
<span class="strong"><strong class="calibre2">          RX bytes:1751 (1.7 KiB)  TX bytes:738 (738.0 B)</strong></span>
<span class="strong"><strong class="calibre2">...</strong></span>
</pre></div><p class="calibre9">As we see, an IP address from the corresponding subnet of the hosts has been allocated to the container and the IP addresses can be pinged from the other container. This also illustrates that with add-ons like Weave and Flannel, communication across containers is much<a id="id288" class="calibre1"/> simpler and closer to the communication <a id="id289" class="calibre1"/>of applications across bare metal.</p></div>
<div class="book" title="Summary" id="181NK1-31555e2039a14139a7f00b384a5a2dd8"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec36" class="calibre1"/>Summary</h1></div></div></div><p class="calibre9">In this chapter, we have seen the importance of container communications and the various possibilities provided by CoreOS and docker to provide the communication. In the next chapter, we are going to see how <span class="strong"><strong class="calibre2">OVS</strong></span> (<span class="strong"><strong class="calibre2">OpenVSwitch</strong></span>) can be used to provide the communication mechanism over an underlay network. Apart from Flannel, Weave, and OVS, there are other mechanisms like pipework available to provision the network inside the CoreOS and docker environment.</p></div></body></html>