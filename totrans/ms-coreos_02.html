<html><head></head><body>
<p id="filepos153225" class="calibre_"><span class="calibre1"><span class="bold">Chapter 2. Setting up the CoreOS Lab</span></span></p><p class="calibre_8">CoreOS can be deployed in Bare Metal, VMs, or a cloud provider such as Amazon AWS or Google GCE. In this chapter, we will cover how to set up the CoreOS development environment in Vagrant, Amazon AWS, Google GCE, and Bare Metal. This development environment will be used in all the chapters going forward.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Cloud-config for CoreOS</li><li value="2" class="calibre_13">CoreOS with Vagrant</li><li value="3" class="calibre_13">CoreOS with Amazon AWS</li><li value="4" class="calibre_13">CoreOS with Google GCE</li><li value="5" class="calibre_13">The CoreOS installation on Bare Metal.</li><li value="6" class="calibre_13">The basic debugging of the CoreOS cluster</li></ul><p class="calibre_8">Different CoreOS deployment options are covered here because of the following reasons:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Vagrant with Virtualbox is useful for users who don't have a cloud account.</li><li value="2" class="calibre_13">For some users, using a local machine might not be possible as VMs occupy a lot of resources, and using a cloud-based VM is the best choice in this case. As AWS and GCE are the most popular cloud providers, I chose these two.</li><li value="3" class="calibre_13">Bare metal installation would be preferable for traditional in-house data centers.</li><li value="4" class="calibre_13">In this book's examples, I have used one of the three approaches (Vagrant, AWS, and GCE) based on the simplicity of one of the approaches, better integration with one of the three approaches, or because of issues with a particular approach.</li></ul><div class="mbp_pagebreak" id="calibre_pb_48"/>


<p id="filepos155420" class="calibre_14"><span class="calibre1"><span class="bold">Cloud-config</span></span></p><p class="calibre_8">Cloud-config<a/> is a declarative configuration file format that is used by many Linux distributions to describe the initial server configuration. The cloud-init program takes care of parsing <tt class="calibre2">cloud-config</tt> during server initialization and configures the server appropriately. The <tt class="calibre2">cloud-config</tt> file provides you with a default configuration for the CoreOS node.</p><div class="mbp_pagebreak" id="calibre_pb_49"/>


<p id="filepos155962" class="calibre_9"><span class="calibre3"><span class="bold">The CoreOS cloud-config file format</span></span></p><p class="calibre_8">The <a/>
<tt class="calibre2">coreos-cloudinit</tt> program takes care of the default configuration of the CoreOS node during bootup using the <tt class="calibre2">cloud-config</tt> file. The <tt class="calibre2">cloud-config</tt> file describes the <a/>configuration in the YAML format (<a href="http://www.yaml.org/">http://www.yaml.org/</a>). CoreOS cloud-config follows the <tt class="calibre2">cloud-config</tt> specification with some CoreOS-specific options. The link, <a href="https://coreos.com/os/docs/latest/cloud-config.html">https://coreos.com/os/docs/latest/cloud-config.html</a> covers the <a/>details of CoreOS <tt class="calibre2">cloud-config</tt>.</p><p id="filepos156706" class="calibre_9"><span class="calibre3"><span class="bold">The main sections of cloud-config</span></span></p><p class="calibre_8">The <a/>following are the main sections in the CoreOS <tt class="calibre2">cloud-config</tt> YAML file:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">CoreOS:<ul class="calibre_58"><li value="1" class="calibre_13">Etcd2: config parameters for etcd2</li><li value="2" class="calibre_13">Fleet: config parameters for Fleet</li><li value="3" class="calibre_13">Flannel: config parameters for Flannel</li><li value="4" class="calibre_13">Locksmith: config parameters for Locksmith</li><li value="5" class="calibre_13">Update: config parameters for automatic updates</li><li value="6" class="calibre_13">Units: Systemd units that need to be started</li></ul></li><li value="2" class="calibre_13"><tt class="calibre2">ssh_authorized_keys</tt>: Public keys for the <span class="italic">core</span> user</li><li value="3" class="calibre_13"><tt class="calibre2">hostname</tt>: Hostname for the CoreOS system</li><li value="4" class="calibre_13"><tt class="calibre2">users</tt>: Additional user account and group details</li><li value="5" class="calibre_13"><tt class="calibre2">write_files</tt>: Creates files with specified user data</li><li value="6" class="calibre_13"><tt class="calibre2">manage_etc_hosts</tt>: Specifies the contents of <tt class="calibre2">/etc/hosts</tt></li></ul><p id="filepos158286" class="calibre_14"><span class="calibre3"><span class="bold">A sample CoreOS cloud-config</span></span></p><p class="calibre_8">The following is a<a/> sample <tt class="calibre2">cloud-config</tt> file for a single node CoreOS cluster:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    # Static cluster<br class="calibre4"/>    name: etcdserver<br class="calibre4"/>    initial-cluster-token: etcd-cluster-1<br class="calibre4"/>    initial-cluster: etcdserver=http://$private_ipv4:2380<br class="calibre4"/>    initial-cluster-state: new<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    # legacy ports can be omitted if your application doesn't depend on them<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>    metadata: "role=services"<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  update:<br class="calibre4"/>      reboot-strategy: "etcd-lock"<br class="calibre4"/>  units:<br class="calibre4"/>    # To use etcd2, comment out the above service and uncomment these<br class="calibre4"/>    # Note: this requires a release that contains etcd2<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: flanneld.service<br class="calibre4"/>      drop-ins:<br class="calibre4"/>        - name: 50-network-config.conf<br class="calibre4"/>          content: |<br class="calibre4"/>            [Service]<br class="calibre4"/>            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" }'<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: docker-tcp.socket<br class="calibre4"/>      command: start<br class="calibre4"/>      enable: true<br class="calibre4"/>      content: |<br class="calibre4"/>        [Unit]<br class="calibre4"/>        Description=Docker Socket for the API<br class="calibre4"/>        [Socket]<br class="calibre4"/>        ListenStream=2375<br class="calibre4"/>        Service=docker.service<br class="calibre4"/>        BindIPv6Only=both<br class="calibre4"/>        [Install]<br class="calibre4"/>        WantedBy=sockets.target<br class="calibre4"/><br class="calibre4"/>write_files:<br class="calibre4"/>  - path: "/etc/motd"<br class="calibre4"/>    permissions: "0644"<br class="calibre4"/>    owner: "root"<br class="calibre4"/>    content: |<br class="calibre4"/>      --- My CoreOS Cluster ---</tt></p><p class="calibre_8">The following are<a/> some notes on the preceding <tt class="calibre2">cloud-config</tt>:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The <tt class="calibre2">etcd2</tt> section specifies the configuration parameters for the <tt class="calibre2">etcd2</tt> service. In this case, we specify parameters needed to start <tt class="calibre2">etcd</tt> on the CoreOS node. The <tt class="calibre2">public_ipv4</tt> and <tt class="calibre2">private_ipv4</tt> environment variables are substituted with the CoreOS node's IP address. As there is only one node, we use the static cluster definition approach rather than using a discovery token. Based on the specified parameters, the <tt class="calibre2">20-cloudinit.conf</tt> Drop-In Unit gets created in <tt class="calibre2">/run/systemd/system/etcd2.service.d</tt> with the following environment variables:<p class="calibre_8"><tt class="calibre2">[Service]<br class="calibre4"/>Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.17.8.101:2379"<br class="calibre4"/>Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.17.8.101:2380"<br class="calibre4"/>Environment="ETCD_INITIAL_CLUSTER=etcdserver=http://172.17.8.101:2380"<br class="calibre4"/>Environment="ETCD_INITIAL_CLUSTER_STATE=new"<br class="calibre4"/>Environment="ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster-1"<br class="calibre4"/>Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001"<br class="calibre4"/>Environment="ETCD_LISTEN_PEER_URLS=http://172.17.8.101:2380,http://172.17.8.101:7001"<br class="calibre4"/>Environment="ETCD_NAME=etcdserver"</tt></p></li><li value="2" class="calibre_31">The <tt class="calibre2">fleet</tt> section specifies the configuration parameters for the <tt class="calibre2">fleet</tt> service, including any metadata for the node. The <tt class="calibre2">20-cloudinit.conf</tt> Drop-In Unit gets created in <tt class="calibre2">/run/systemd/system/fleet.service.d</tt> with the following environment variables:<p class="calibre_8"><tt class="calibre2">[Service]<br class="calibre4"/>Environment="FLEET_METADATA=role=services"<br class="calibre4"/>Environment="FLEET_PUBLIC_IP=172.17.8.101"</tt></p></li><li value="3" class="calibre_31">The update section specifies the update strategy for the CoreOS node. This gets updated in the node as <tt class="calibre2">/etc/coreos/update.conf</tt>:<p class="calibre_8"><tt class="calibre2">GROUP=alpha<br class="calibre4"/>REBOOT_STRATEGY=etcd-lock</tt></p></li><li value="4" class="calibre_31">The <tt class="calibre2">units</tt> section starts <tt class="calibre2">etcd2</tt>, <tt class="calibre2">fleet</tt>, and <tt class="calibre2">flannel</tt>. For <tt class="calibre2">flannel</tt>, we have a drop-in unit to update the subnet to be used for containers created with the Flannel network service. The <tt class="calibre2">50-network-config.conf</tt> Drop-in unit gets created in <tt class="calibre2">/etc/systemd/system/flanneld.service.d</tt>:<p class="calibre_8"><tt class="calibre2">[Service]<br class="calibre4"/>ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ "Network": "10.1.0.0/16" }'</tt></p></li><li value="5" class="calibre_31">The <tt class="calibre2">docker-tcp.socket</tt> unit in the <span class="italic">units</span> section is a new <tt class="calibre2">systemd</tt> unit, and we specified the service content that allows for the docker daemon to be exposed through port <tt class="calibre2">2375</tt>. The unit will be created in <tt class="calibre2">/etc/systemd/system</tt>.</li><li value="6" class="calibre_13">The <tt class="calibre2">write_files</tt> section can be used to create any static files. An example could be a hello text when a user logs in, which we can do with <tt class="calibre2">/etc/motd</tt>. The hello message would look as follows:<p class="calibre_8"><tt class="calibre2">Last login: Tue Sep 15 14:15:04 2015 from 10.0.2.2<br class="calibre4"/>--- My CoreOS Cluster ---<br class="calibre4"/>core@core01 ~ $</tt></p></li></ul><div class="mbp_pagebreak" id="calibre_pb_50"/>


<p id="filepos164625" class="calibre_9"><span class="calibre3"><span class="bold">The cloud-config validator</span></span></p><p class="calibre_8">Cloud-config <a/>uses the YAML syntax. YAML is a human-readable data serialization format and uses indents and spaces for alignment. It is better to validate the <tt class="calibre2">cloud-config</tt> YAML configuration files before using them. There are two options to validate the CoreOS <tt class="calibre2">cloud-config</tt>.</p><p id="filepos165068" class="calibre_9"><span class="calibre3"><span class="bold">A hosted validator</span></span></p><p class="calibre_8">Use <a/>this <a/>CoreOS-provided <a/>link (<a href="https://coreos.com/validate/">https://coreos.com/validate/</a>) to validate <tt class="calibre2">cloud-config</tt>.</p><p class="calibre_8">Here is an example of a valid and invalid <tt class="calibre2">cloud-config</tt> and the results using the validator.</p><p id="filepos165509" class="calibre_9"><span class="bold">Valid cloud-config</span></p><p class="calibre_8">As we can see in the<a/> following screenshot, the validator says that the <a/>following <tt class="calibre2">cloud-config</tt> is valid:</p><p class="calibre_9"><img src="images/00100.jpg" class="calibre_59"/></p><p class="calibre_8">
</p><p id="filepos165912" class="calibre_9"><span class="bold">Invalid cloud-config</span></p><p class="calibre_8">Here, we can see that the <a/>validator has specified that <tt class="calibre2">-</tt> is missing<a/> in line 14. YAML uses spaces for the delimiting, so we need to make sure that the number of spaces is exact:</p><p class="calibre_9"><img src="images/00167.jpg" class="calibre_60"/></p><p class="calibre_8">
</p><p id="filepos166390" class="calibre_9"><span class="calibre3"><span class="bold">The cloudinit validator</span></span></p><p class="calibre_8">We can <a/>use the <tt class="calibre2">coreos-cloudinit --validate</tt> option available in CoreOS to validate the cloud-config. Let's look at the following sample <tt class="calibre2">cloud-config</tt>:</p><p class="calibre_9"><img src="images/00109.jpg" class="calibre_61"/></p><p class="calibre_8">
</p><p class="calibre_8">When we validate this, we get no errors, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00112.jpg" class="calibre_62"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's try <a/>the same <tt class="calibre2">cloud-config</tt> with errors. Here, we have <tt class="calibre2">|</tt> missing in the content line:</p><p class="calibre_9"><img src="images/00017.jpg" class="calibre_63"/></p><p class="calibre_8">
</p><p class="calibre_8">We see the following errors when we validate:</p><p class="calibre_9"><img src="images/00120.jpg" class="calibre_64"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_51"/>


<p id="filepos167756" class="calibre_9"><span class="calibre3"><span class="bold">Executing cloud-config</span></span></p><p class="calibre_8">There are<a/> two <tt class="calibre2">cloud-config</tt> files that are run as part of the CoreOS bootup:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">System cloud-config</li><li value="2" class="calibre_13">User cloud-config</li></ul><p class="calibre_8">System <tt class="calibre2">cloud-config</tt> is given by the provider (such as Vagrant or AWS) and is embedded as part of the CoreOS provider image. Different providers such as Vagrant, AWS, and GCE have their <tt class="calibre2">cloud-config</tt> present in <tt class="calibre2">/usr/share/oem/cloud-config.yaml</tt>. This <tt class="calibre2">cloud-config</tt> is responsible for setting up the provider-specific configurations, such as networking, SSH keys, mount options, and so on. The <tt class="calibre2">coreos-cloudinit</tt> program first executes system <tt class="calibre2">cloud-config</tt> and then user <tt class="calibre2">cloud-config</tt>.</p><p class="calibre_8">Depending on the provider, user <tt class="calibre2">cloud-config</tt> can be supplied using either config-drive or an internal user data service. Config-drive is a universal way to provide <tt class="calibre2">cloud-config</tt> by mounting a read-only partition that contains <tt class="calibre2">cloud-config</tt> to the host machine. Rackspace uses config-drive to get user <tt class="calibre2">cloud-config</tt>, and AWS uses its internal user data service to fetch the user data and doesn't rely on config-drive. In the Vagrant scenario, Vagrantfile<a/> takes care of copying the <tt class="calibre2">cloud-config</tt> to the CoreOS VM.</p><div class="mbp_pagebreak" id="calibre_pb_52"/>


<p id="filepos169400" class="calibre_"><span class="calibre1"><span class="bold">The CoreOS cluster with Vagrant</span></span></p><p class="calibre_8">Vagrant <a/>can be installed in Windows or Linux. The following is my development environment for the Vagrant CoreOS:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Windows 7: I use mysysgit (<a href="https://git-for-windows.github.io/">https://git-for-windows.github.io/</a>) to get a Linux-like shell for Windows</li><a/><li value="2" class="calibre_13">Vagrant 1.7.2: <a href="https://www.vagrantup.com/downloads.html">https://www.vagrantup.com/downloads.html</a></li><li value="3" class="calibre_13">Virtualbox 4.3.28: <a href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a></li><a/></ul><p class="calibre_8">For a few of the examples in the book, I have used Vagrant to run CoreOS inside a Linux VM running on top of Windows laptop with Virtualbox.</p><div class="mbp_pagebreak" id="calibre_pb_53"/>


<p id="filepos170504" class="calibre_9"><span class="calibre3"><span class="bold">Steps to start the Vagrant environment</span></span></p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Check out the coreos-vagrant code base:<p class="calibre_8"><tt class="calibre2"><span class="bold">git clone https://github.com/coreos/coreos-vagrant.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Copy the sample <tt class="calibre2">user-data</tt> and <tt class="calibre2">config.rb</tt> files in the coreos-vagrant directory:<p class="calibre_8"><tt class="calibre2"><span class="bold">cd coreos-vagrant</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">mv user-data.sample user-data</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">mv config.rb.sample config.rb</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="3" class="calibre_31">Edit <tt class="calibre2">Vagrantfile</tt>, <tt class="calibre2">user-data</tt>, and <tt class="calibre2">config.rb</tt> based on your need.</li><li value="4" class="calibre_13">Start the CoreOS cluster:<p class="calibre_8"><tt class="calibre2"><span class="bold">Vagrant up</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="5" class="calibre_31">SSH to the individual node:<p class="calibre_8"><tt class="calibre2"><span class="bold">Vagrant ssh core-&lt;id&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><div class="mbp_pagebreak" id="calibre_pb_54"/>


<p id="filepos171826" class="calibre_9"><span class="calibre3"><span class="bold">Important files to be modified</span></span></p><p class="calibre_8">The following are <a/>important files to be modified along <a/>with commonly needed modifications.</p><p id="filepos172079" class="calibre_9"><span class="calibre3"><span class="bold">Vagrantfile</span></span></p><p class="calibre_8">Vagrant sets up the <a/>VM environment based on the configuration defined in <tt class="calibre2">Vagrantfile</tt>. The following are certain relevant functionalities in the CoreOS context:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The version of CoreOS software to be used is specified using <tt class="calibre2">update_channel</tt>. The version can be specified as <tt class="calibre2">stable</tt>, <tt class="calibre2">beta</tt>, and <tt class="calibre2">alpha</tt>. More details on CoreOS software versions are covered in <a href="index_split_075.html#filepos216260">Chapter 3</a>, <span class="italic">CoreOS Autoupdate</span>.</li><li value="2" class="calibre_13">CPU and memory for the VM and ports to be exposed from the VM.</li><li value="3" class="calibre_13">SSH key management.</li></ul><p id="filepos172982" class="calibre_14"><span class="calibre3"><span class="bold">User-data</span></span></p><p class="calibre_8">The <tt class="calibre2">user-data</tt> file<a/> is essentially the <tt class="calibre2">cloud-config</tt> file that specifies the discovery token, environment variables, and list of units to be started by default. Vagrant copies the <tt class="calibre2">cloud-config</tt> file to <tt class="calibre2">/var/lib/coreos-vagrant/vagrantfile-user-data</tt> inside the VM. The <tt class="calibre2">coreos-cloudinit</tt> reads <tt class="calibre2">vagrantfile-user-data</tt> on every boot and uses it to create the machine's user data file.</p><p id="filepos173545" class="calibre_9"><span class="calibre3"><span class="bold">Config.rb</span></span></p><p class="calibre_8">The <tt class="calibre2">config.rb</tt> file <a/>specifies the count of CoreOS nodes. This file also provides you with an option to automatically generate a discovery token. Some options here overlap with the <tt class="calibre2">Vagrantfile</tt> like image version.</p><div class="mbp_pagebreak" id="calibre_pb_55"/>


<p id="filepos173939" class="calibre_9"><span class="calibre3"><span class="bold">Vagrant – a three-node cluster with dynamic discovery</span></span></p><p class="calibre_8">Here, we will create a<a/> three-node CoreOS cluster <a/>with <tt class="calibre2">etcd2</tt> and <tt class="calibre2">fleet</tt> running on each node and nodes discovering each other dynamically.</p><p id="filepos174281" class="calibre_9"><span class="calibre3"><span class="bold">Generating a discovery token</span></span></p><p class="calibre_8">When we start a <a/>multinode CoreOS cluster, there needs to be a bootstrapping mechanism to discover the cluster members. For this, we generate a token specifying the number of initial nodes in the cluster as an argument. Each node needs to be started with this discovery token. Etcd will use the discovery token to put all the nodes with the same discovery token as part of the initial cluster. CoreOS runs the service to provide the discovery token from its central servers.</p><p class="calibre_8">There are two approaches to generate a discovery token:</p><p class="calibre_8">From the browser: <tt class="calibre2">https://discovery.etcd.io/new?size=3</tt>
</p><p class="calibre_8">Using curl: <tt class="calibre2">curl https://discovery.etcd.io/new?size=3</tt>
</p><p class="calibre_8">The following is a curl example with a generated discovery token. This token needs to be copied to <tt class="calibre2">user-data</tt>:</p><p class="calibre_9"><img src="images/00358.jpg" class="calibre_65"/></p><p class="calibre_8">
</p><p id="filepos175548" class="calibre_9"><span class="calibre3"><span class="bold">Steps for cluster creation</span></span></p><p class="calibre_8">The following<a/> is a <tt class="calibre2">cloud-config</tt> user data with the updated discovery token that we generated in the preceding section along with the necessary environment variables and service units. All three nodes will use this <tt class="calibre2">cloud-config</tt>:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    #generate a new token for each unique cluster from https://discovery.etcd.io/new<br class="calibre4"/>    discovery: https://discovery.etcd.io/9a6b7af06c8a677b4e5f76ae9ce0da9c<br class="calibre4"/>    # multi-region and multi-cloud deployments need to use $public_ipv4<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    # legacy ports can be omitted if your application doesn't depend on them<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    # Note: this requires a release that contains etcd2<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">We need to update <tt class="calibre2">num_instances</tt> to <tt class="calibre2">3</tt> in <tt class="calibre2">config.rb</tt> and then perform <tt class="calibre2">vagrant up</tt>.</p><p class="calibre_8">To verify the basic<a/> cluster operation, we can check the following output, where we should see the cluster members.</p><p class="calibre_8">The following <tt class="calibre2">etcdctl</tt> member output shows the three cluster members:</p><p class="calibre_9"><img src="images/00362.jpg" class="calibre_66"/></p><p class="calibre_8">
</p><p class="calibre_8">The following fleet member output shows the three cluster members:</p><p class="calibre_9"><img src="images/00364.jpg" class="calibre_67"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_56"/>


<p id="filepos178097" class="calibre_9"><span class="calibre3"><span class="bold">Vagrant – a three-node cluster with static discovery</span></span></p><p class="calibre_8">Here, we will create a<a/> three-node CoreOS cluster and use a static approach to mention its cluster neighbors. In the dynamic discovery approach, we need to use a discovery token to discover the cluster members. Static discovery can be used for scenarios where access to the token server is not available to cluster members, and the cluster member IP addresses are known in advance.</p><p class="calibre_8">Perform the following steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">First, we need to create three separate instances of the CoreOS Vagrant environment by performing <tt class="calibre2">git clone</tt> separately for each node.</li><li value="2" class="calibre_13">The <tt class="calibre2">config.rb</tt> file must be updated for each node with <tt class="calibre2">num_instances</tt> set to one.</li><li value="3" class="calibre_13">Vagrantfile should be updated for each node so that IP addresses are statically assigned as <tt class="calibre2">172.17.8.101</tt> for <tt class="calibre2">core-01</tt>, <tt class="calibre2">172.17.8.102</tt> for <tt class="calibre2">core-02</tt>, and <tt class="calibre2">172.17.8.103</tt> for <tt class="calibre2">core-03</tt>. IP addresses should be updated based on your environment.</li></ol><p class="calibre_8">The <tt class="calibre2">cloud-config</tt> user data for the first node is as follows:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    name: core-01<br class="calibre4"/>    initial-advertise-peer-urls: http://172.17.8.101:2380<br class="calibre4"/>    listen-peer-urls: http://172.17.8.101:2380<br class="calibre4"/>    listen-client-urls: http://172.17.8.101:2379,http://127.0.0.1:2379<br class="calibre4"/>    advertise-client-urls: http://172.17.8.101:2379<br class="calibre4"/>    initial-cluster-token: etcd-cluster-1<br class="calibre4"/>    initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380<br class="calibre4"/>    initial-cluster-state: new<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The <tt class="calibre2">cloud-config</tt> user <a/>data for the second node is as follows:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    name: core-02<br class="calibre4"/>    initial-advertise-peer-urls: http://172.17.8.102:2380<br class="calibre4"/>    listen-peer-urls: http://172.17.8.102:2380<br class="calibre4"/>    listen-client-urls: http://172.17.8.102:2379,http://127.0.0.1:2379<br class="calibre4"/>    advertise-client-urls: http://172.17.8.102:2379<br class="calibre4"/>    initial-cluster-token: etcd-cluster-1<br class="calibre4"/>    initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380<br class="calibre4"/>    initial-cluster-state: new<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The <tt class="calibre2">cloud-config</tt> user data for the third node is as follows:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    name: core-03<br class="calibre4"/>    initial-advertise-peer-urls: http://172.17.8.103:2380<br class="calibre4"/>    listen-peer-urls: http://172.17.8.103:2380<br class="calibre4"/>    listen-client-urls: http://172.17.8.103:2379,http://127.0.0.1:2379<br class="calibre4"/>    advertise-client-urls: http://172.17.8.103:2379<br class="calibre4"/>    initial-cluster-token: etcd-cluster-1<br class="calibre4"/>    initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380<br class="calibre4"/>    initial-cluster-state: new<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">We need to <a/>perform <tt class="calibre2">vagrant up</tt> separately for each of the nodes. We should see the cluster member list updated in both the <tt class="calibre2">etcdctl member list</tt> and <tt class="calibre2">fleetctl list-machines</tt> outputs.</p><div class="mbp_pagebreak" id="calibre_pb_57"/>


<p id="filepos182934" class="calibre_9"><span class="calibre3"><span class="bold">Vagrant – a production cluster with three master nodes and three worker nodes</span></span></p><p class="calibre_8">In <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>, we covered<a/> the CoreOS cluster architecture. A production cluster has one set of nodes (called <span class="bold">master</span>) to run critical services, and another set of<a/> nodes (called <span class="bold">worker</span>) to run application services. In this example, we create three master nodes running <tt class="calibre2">etcd</tt> and other critical services and another three worker nodes. Etcd in the worker nodes will proxy to the master nodes. Worker nodes will be used for user-created services while master nodes will be used for system services. This avoids resource contention. The following are the steps needed for this creation:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Create a Vagrant three-node cluster for the master and a three-node cluster for the worker.</li><li value="2" class="calibre_13">Update Vagrantfile to use non-conflicting IP address ranges between the master and worker nodes.</li><li value="3" class="calibre_13">Use the dynamic discovery token approach to create a token for the three-node clusters and update the <tt class="calibre2">cloud-config</tt> user data for both the master and worker nodes to the same token. We have specified the token size as <tt class="calibre2">3</tt> as worker nodes don't run <tt class="calibre2">etcd</tt>.</li></ul><p class="calibre_8">The following is the<a/> user data for the master cluster:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>     metadata: "role=master"<br class="calibre4"/>     public-ip: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The following is the user data for the worker cluster. The discovery token needs to be the same for the master and worker clusters:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>     metadata: "role=worker"<br class="calibre4"/>     public-ip: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The only difference <a/>between the master and worker user data is in the metadata used for fleet. In this example, we used <tt class="calibre2">role</tt> as <tt class="calibre2">master</tt> for the master cluster and <tt class="calibre2">role</tt> as <tt class="calibre2">worker</tt> for the worker cluster.</p><p class="calibre_8">Let's look at the <tt class="calibre2">etcdctl</tt> member list and fleet machine list. The following output will be the same across all the nodes in the master and worker cluster.</p><p class="calibre_8">The <tt class="calibre2">etcdctl</tt> member output is as follows:</p><p class="calibre_9"><img src="images/00306.jpg" class="calibre_68"/></p><p class="calibre_8">
</p><p class="calibre_8">The fleet member output is as follows:</p><p class="calibre_9"><img src="images/00370.jpg" class="calibre_69"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the <tt class="calibre2">journalctl –u etcd2.service</tt> output on worker nodes that show worker nodes proxying to master nodes:</p><p class="calibre_9"><img src="images/00372.jpg" class="calibre_70"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_58"/>


<p id="filepos187819" class="calibre_"><span class="calibre1"><span class="bold">A CoreOS cluster with AWS</span></span></p><p class="calibre_8">Amazon AWS<a/> provides you with a public cloud service. CoreOS can be run on the VMs provided by AWS. The following are some prerequisites for this setup:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">You need an account in AWS. AWS provides you with a one-year trial account for free.</li><li value="2" class="calibre_13">Create and download a key pair. The key pair is needed to SSH to the nodes.</li><li value="3" class="calibre_13">The AWS interface can be accessed through the AWS console, which is a GUI interface, or by AWS CLI. AWS CLI (<a href="http://aws.amazon.com/cli/">http://aws.amazon.com/cli/</a>) can be installed in either Windows or Linux.</li><a/></ul><p class="calibre_8">The following are two approaches of creating a CoreOS cluster with AWS.</p><div class="mbp_pagebreak" id="calibre_pb_59"/>


<p id="filepos188885" class="calibre_9"><span class="calibre3"><span class="bold">AWS – a three-node cluster using Cloudformation</span></span></p><p class="calibre_8">Cloudformation<a/> is an AWS<a/> orchestration tool to manage a collection of AWS resources that include compute, storage, and networking. The link, <a href="https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template">https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template</a>, has the template file for the <a/>CoreOS cluster. The following are some of the key sections in the template:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The AMI image ID to be used based on the region</li><li value="2" class="calibre_13">The EC2 Instance type</li><li value="3" class="calibre_13">The security group configuration</li><li value="4" class="calibre_13">The CoreOS cluster size including the minimum and maximum size to autoscale</li><li value="5" class="calibre_13">The initial cloud-config to be used</li></ul><p class="calibre_8">For the following example, I modified the template to use <tt class="calibre2">t2.micro</tt> instead of <tt class="calibre2">m3.medium</tt> for the instance size. The following CLI can be used to create a three-node CoreOS cluster using <tt class="calibre2">Cloudformation</tt>. The discovery token in the below command needs to be updated with the generated token for your case:</p><p class="calibre_8"><tt class="calibre2">aws cloudformation create-stack \<br class="calibre4"/>    --stack-name coreos-test \<br class="calibre4"/>    --template-body file://mycoreos-stable-hvm.template \<br class="calibre4"/>    --capabilities CAPABILITY_IAM \<br class="calibre4"/>    --tags Key=Name,Value=CoreOS \<br class="calibre4"/>    --parameters \      ParameterKey=DiscoveryURL,ParameterValue="https://discovery.etcd.io/925755234ab82c1ef7bcfbbacdd8c088" \<br class="calibre4"/>        ParameterKey=KeyPair,ParameterValue="keyname"</tt></p><p class="calibre_8">The following is the output of the successful stack using <tt class="calibre2">aws cloudformation list-stacks</tt>:</p><p class="calibre_9"><img src="images/00347.jpg" class="calibre_71"/></p><p class="calibre_8">
</p><p class="calibre_8">After the <a/>preceding step, we can see that members are getting discovered successfully by both <tt class="calibre2">etcd</tt> and <tt class="calibre2">fleet</tt>.</p><div class="mbp_pagebreak" id="calibre_pb_60"/>


<p id="filepos191444" class="calibre_9"><span class="calibre3"><span class="bold">AWS – a three-node cluster using AWS CLI</span></span></p><p class="calibre_8">The following are<a/> some prerequisites to create a CoreOS cluster in AWS using AWS CLI:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a token for a three-node cluster from the discovery token service.</li><li value="2" class="calibre_13">Set up a security group exposing the ports <tt class="calibre2">ssh</tt>, <tt class="calibre2">icmp</tt>, <tt class="calibre2">2379</tt>, and <tt class="calibre2">2380</tt>. <tt class="calibre2">2379</tt> and <tt class="calibre2">2380</tt> are needed for the <tt class="calibre2">etcd2</tt> client-to-server and server-to-server communication.</li><li value="3" class="calibre_13">Determine the AMI image ID using this link (<a href="https://coreos.com/os/docs/latest/booting-on-ec2.html">https://coreos.com/os/docs/latest/booting-on-ec2.html</a>) based on your AWS Zone and update channel. The latest image IDs for different AWS Zones get automatically updated in this link.</li><a/></ol><p class="calibre_8">The following CLI will create the three-node cluster:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos-test" --user-data file://cloud-config.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Here, the <tt class="calibre2">ami-85ada4b5</tt> image ID is from the stable update channel. The <tt class="calibre2">coreos-test</tt> security group has the necessary ports that need to be exposed outside.</p><p class="calibre_8">The following is the <tt class="calibre2">cloud-config</tt> that I used:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    # specify the initial size of your cluster with ?size=X<br class="calibre4"/>    discovery: https://discovery.etcd.io/47460367c9b15edffeb49de30cab9354<br class="calibre4"/>    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The following output shows the <tt class="calibre2">etcd</tt> member list and fleet member list with three nodes in the cluster:</p><p class="calibre_9"><img src="images/00379.jpg" class="calibre_72"/></p><p class="calibre_8">
</p><p class="calibre_8">The same<a/> example can be tried from the AWS Console, where we can specify the options from the GUI.</p><div class="mbp_pagebreak" id="calibre_pb_61"/>


<p id="filepos194473" class="calibre_"><span class="calibre1"><span class="bold">A CoreOS cluster with GCE</span></span></p><p class="calibre_8">Google's GCE<a/> is another public cloud provider like Amazon AWS. CoreOS can be run on the VMs provided by GCE. The following are some prerequisites for this setup:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">You need a GCE account. GCE provides you with a free trial account for 60 days.</li><li value="2" class="calibre_13">GCE resources can be accessed using gcloud SDK or GCE GUI Console. SDK can be downloaded from <a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a>.</li><a/><li value="3" class="calibre_13">A base project in GCE needs to be created under which all the resources reside.</li><li value="4" class="calibre_13">A security token needs to be created, which is used for SSH access.</li></ul><div class="mbp_pagebreak" id="calibre_pb_62"/>


<p id="filepos195502" class="calibre_14"><span class="calibre3"><span class="bold">GCE – a three-node cluster using GCE CLI</span></span></p><p class="calibre_8">The following are some<a/> prerequisites to create a CoreOS cluster in GCE:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Create a token for a three-node cluster from a discovery token service.</li><li value="2" class="calibre_13">Set up a security group exposing the ports <tt class="calibre2">ssh</tt>, <tt class="calibre2">icmp</tt>, <tt class="calibre2">2379</tt>, and <tt class="calibre2">2380</tt>. <tt class="calibre2">2379</tt> and <tt class="calibre2">2380</tt> are needed for the <tt class="calibre2">etcd2</tt> client-to-server and server-to-server communication.</li><li value="3" class="calibre_13">The link, <a href="https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html">https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html</a>, gets automatically updated with the latest GCE CoreOS releases from the stable, beta, and alpha channels. We need to pick the appropriate image that is needed.</li><a/></ul><p class="calibre_8">The following CLI can be used to create a three-node CoreOS GCE cluster from the stable release:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-stable-717-3-0-v20150710 --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=cloud-config.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the <a/>
<tt class="calibre2">cloud-config.yaml</tt> file that's used:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    # specify the initial size of your cluster with ?size=X<br class="calibre4"/>    discovery: https://discovery.etcd.io/46ad006905f767331a36bb2a4dbde3f5<br class="calibre4"/>    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">We can SSH to any of the nodes using <tt class="calibre2">gcloud compute ssh &lt;nodeid&gt;</tt>.</p><p class="calibre_8">The following output shows you that the cluster is created successfully and members are seen from both <tt class="calibre2">etcd</tt> and <tt class="calibre2">fleet</tt>:</p><p class="calibre_9"><img src="images/00382.jpg" class="calibre_73"/></p><p class="calibre_8">
</p><p class="calibre_8">The CoreOS cluster can also be created using the GCE Console GUI interface.</p><div class="mbp_pagebreak" id="calibre_pb_63"/>


<p id="filepos198605" class="calibre_"><span class="calibre1"><span class="bold">CoreOS installation on Bare Metal</span></span></p><p class="calibre_8">There are two <a/>approaches<a/> to install <a/>CoreOS on Bare Metal:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">CoreOS ISO image</li><li value="2" class="calibre_13">PXE or IPXE boot</li></ul><p class="calibre_8">The steps below covers the approach to install CoreOS on Bare Metal using an ISO image.</p><p class="calibre_8">I installed using a CoreOS ISO image on the Virtualbox CD drive. The procedure should be the same if we burn the ISO image on CD and then install on Bare Metal.</p><p class="calibre_8">The following is summary of the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Download the required ISO image based on stable, beta, and alpha versions from <a href="https://coreos.com/os/docs/latest/booting-with-iso.html">https://coreos.com/os/docs/latest/booting-with-iso.html</a>.</li><li value="2" class="calibre_13">Start a new Linux machine in Virtualbox with the required CPU, memory, and network settings, and mount the ISO image on the IDE drive.</li><a/><li value="3" class="calibre_13">Create an SSH key to log in to the CoreOS node using <tt class="calibre2">ssh-keygen</tt>.</li><li value="4" class="calibre_13">Start the Linux machine and then use the CoreOS script to install CoreOS on the hard disk with the necessary <tt class="calibre2">cloud-config</tt>. The cloud-config used here is similar to cloud-config is used in previous sections, SSH key needs to be manually updated.</li><li value="5" class="calibre_13">Remove the CD drive from Virtualbox and reboot. This will load the CoreOS image from the hard disk.</li></ol><p class="calibre_8">I have used the stable ISO image version 766.4.0.</p><p class="calibre_8">The following screenshot shows you the initial storage mounting on Virtualbox with the ISO image on the IDE drive:</p><p class="calibre_9"><img src="images/00386.jpg" class="calibre_74"/></p><p class="calibre_8">
</p><p class="calibre_8">The easiest way to get <tt class="calibre2">cloud-config</tt> is by <tt class="calibre2">wget</tt>. When we boot from the CD, we cannot cut and paste as there is no Windows manager. The easiest way to get <tt class="calibre2">cloud-config</tt> to the node is by having cloud-config in a hosting location and fetch it using <tt class="calibre2">wget</tt>. The SSH key needs to be updated appropriately.</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">wget https://github.com/smakam/coreos/raw/master/single-node-cloudconfig.yml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The installation of CoreOS to the hard disk can be done using the CoreOS-provided script:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo coreos-install -d /dev/sda -C stable -c ~/cloud-config.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">After successful installation, we can shut down the node and remove the IDE drive so that the bootup can happen from the hard disk. The following screenshot shows you the storage selection in Virtualbox to boot using the hard disk:</p><p class="calibre_9"><img src="images/00420.jpg" class="calibre_75"/></p><p class="calibre_8">
</p><p class="calibre_8">After the node is <a/>booted up, we can SSH to the node as we have <a/>already set up the SSH key. The following output<a/> shows you the CoreOS version on Bare Metal:</p><p class="calibre_9"><img src="images/00393.jpg" class="calibre_76"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_64"/>


<p id="filepos202675" class="calibre_"><span class="calibre1"><span class="bold">Basic debugging</span></span></p><p class="calibre_8">The following are some basic debugging tools<a/> and approaches to debug issues in the CoreOS cluster.</p><div class="mbp_pagebreak" id="calibre_pb_65"/>


<p id="filepos202945" class="calibre_9"><span class="calibre3"><span class="bold">journalctl</span></span></p><p class="calibre_8">Systemd-Journal <a/>takes care of<a/> logging all the kernel and systemd services. Journal log files from all the services are stored in a centralized location in <tt class="calibre2">/var/log/journal</tt>. The logs are stored in the binary format, and this keeps it easy to manipulate to different formats.</p><p class="calibre_8">Here are some common examples that shows how to use Journalctl:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">Journalctl</tt>: This lists the combined journal log from all the sources.</li><li value="2" class="calibre_13"><tt class="calibre2">Journalctl –u etcd2.service</tt>: This lists the logs from <tt class="calibre2">etcd2.service</tt>.</li><li value="3" class="calibre_13"><tt class="calibre2">Journalctl –u etcd2.service –f</tt>: This lists the logs from <tt class="calibre2">etcd2</tt>. service like <tt class="calibre2">tail –f</tt> format.</li><li value="4" class="calibre_13"><tt class="calibre2">Journalctl –u etcd2.service –n 100</tt>: This lists the logs of the last 100 lines.</li><li value="5" class="calibre_13"><tt class="calibre2">Journalctl –u etcd2.service –no-pager</tt>: This lists the logs with no pagination, which is useful for search.</li><li value="6" class="calibre_13"><tt class="calibre2">Journalctl –p err –n 100</tt>: This lists all 100 errors by filtering the logs.</li><li value="7" class="calibre_13"><tt class="calibre2">journalctl -u etcd2.service --since today</tt>: This lists today's logs of <tt class="calibre2">etcd2.service</tt>.</li><li value="8" class="calibre_13"><tt class="calibre2">journalctl -u etcd2.service -o json-pretty</tt>: This lists the logs of <tt class="calibre2">etcd2.service</tt> in JSON-formatted output.</li></ul><div class="mbp_pagebreak" id="calibre_pb_66"/>


<p id="filepos204837" class="calibre_14"><span class="calibre3"><span class="bold">systemctl</span></span></p><p class="calibre_8">The <tt class="calibre2">systemctl</tt> utility <a/>can be used for basic monitoring and troubleshooting of the systemd <a/>units.</p><p class="calibre_8">The following example shows you the status of the <tt class="calibre2">systemdunit docker.service</tt>:</p><p class="calibre_9"><img src="images/00395.jpg" class="calibre_77"/></p><p class="calibre_8">
</p><p class="calibre_8">We can stop and restart services in case there are issues with a particular service.</p><p class="calibre_8">The following command will restart docker.service:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo systemctl restart docker.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">When a service file is<a/> changed or environment variables are changed, we need to <a/>execute the following command to reload configuration before restarting the service for the changes to take effect:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo systemctl daemon-reload</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following command is useful to see the units that have failed:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Systemctl --failed</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><div class="mbp_pagebreak" id="calibre_pb_67"/>


<p id="filepos206377" class="calibre_9"><span class="calibre3"><span class="bold">Cloud-config</span></span></p><p class="calibre_8">Earlier, we looked at how<a/> the <tt class="calibre2">cloud-config</tt> YAML file can be prevalidated. In case there are runtime errors, we can check it with <tt class="calibre2">journalctl -b _EXE=/usr/bin/coreos-cloudinit</tt>.</p><p class="calibre_8">If we make changes to the <tt class="calibre2">cloud-config</tt> user data after the initial node setup, we can perform the following steps to activate the new configuration:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Perform <tt class="calibre2">vagrant reload --provision</tt> to get the new configuration.</li><li value="2" class="calibre_13">The new <tt class="calibre2">cloud-config</tt> user data will be in <tt class="calibre2">/var/lib/coreos-vagrant</tt> as <tt class="calibre2">vagrantfile-user-data</tt>. Perform <tt class="calibre2">sudo coreos-cloudinit --from-file vagrantfile-user-data</tt> to update the new configuration.</li></ul><div class="mbp_pagebreak" id="calibre_pb_68"/>


<p id="filepos207409" class="calibre_14"><span class="calibre3"><span class="bold">Logging from one CoreOS node to another</span></span></p><p class="calibre_8">Sometimes, it is<a/> useful to SSH to other nodes from one of the CoreOS nodes in the cluster. The following set of commands can be used to forward the SSH agent that we can use to SSH from other nodes. More information on SSH agent forwarding<a/> can be found at <a href="http://rabexc.org/posts/using-ssh-agent">http://rabexc.org/posts/using-ssh-agent</a>.</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">eval `ssh-agent`</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh-add &lt;key&gt; (Key is the private key)</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i &lt;key&gt; core@&lt;ip&gt; -A (key is the private key)</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">After this, we can either SSH to the machine ID or a specific Fleet unit, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00485.jpg" class="calibre_76"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: SSH agent forwarding is not secure and should be used only to debug.</p><div class="mbp_pagebreak" id="calibre_pb_69"/>


<p id="filepos208778" class="calibre_9"><span class="calibre3"><span class="bold">Important files and directories</span></span></p><p class="calibre_8">Knowing these files and directories helps with <a/>debugging the issues:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Systemd unit file location - <tt class="calibre2"> /usr/lib64/systemd/system</tt>.</li><li value="2" class="calibre_13">Network unit files <tt class="calibre2">- /usr/lib64/systemd/network</tt>.</li><li value="3" class="calibre_13">User-written unit files and drop-ins to change the default parameters <tt class="calibre2">- /etc/systemd/system</tt>. Drop-ins for specific configuration changes can be done using the configuration file under the specific service directory. For example, to modify the fleet configuration, create the <tt class="calibre2">fleet.service.d</tt> directory and put the configuration file in this directory.</li><li value="4" class="calibre_13">User-written network unit files <tt class="calibre2">- /etc/systemd/network</tt>.</li><li value="5" class="calibre_13">Runtime environment variables and drop-in configuration of individual components such as <tt class="calibre2">etcd</tt> and <tt class="calibre2">fleet</tt>
<tt class="calibre2">- /run/systemd/system/</tt>.</li><li value="6" class="calibre_13">The vagrantfile user data containing the <tt class="calibre2">cloud-config</tt> user data used with Vagrant <tt class="calibre2">- /var/lib/coreos-vagrant</tt>.</li><li value="7" class="calibre_13">The <tt class="calibre2">systemd-journald</tt> logs <tt class="calibre2">- /var/log/journal</tt>.</li><li value="8" class="calibre_13"><tt class="calibre2">cloud-config.yaml</tt> associated with providers such as Vagrant, AWS, and <tt class="calibre2">GCE- /usr/share/oem</tt>. (CoreOS first executes this <tt class="calibre2">cloud-config</tt> and then executes the user-provided <tt class="calibre2">cloud-config</tt>.)</li><li value="9" class="calibre_13">Release channel and update strategy <tt class="calibre2">- /etc/coreos/update.conf</tt>.</li><li value="10" class="calibre_13">The public and private IP address (<tt class="calibre2">COREOS_PUBLIC_IPV4</tt> and <tt class="calibre2">COREOS_PRIVATE_IPV4</tt>) <tt class="calibre2">- /etc/environment</tt>.</li><li value="11" class="calibre_13">The machine ID for the particular CoreOS node <tt class="calibre2">- /etc/machine-id</tt>.</li><li value="12" class="calibre_13">The flannel network configuration <tt class="calibre2">- /run/flannel/</tt>.</li></ul><div class="mbp_pagebreak" id="calibre_pb_70"/>


<p id="filepos211222" class="calibre_14"><span class="calibre3"><span class="bold">Common mistakes and possible solutions</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">For CoreOS on the cloud provider, there is a need to open up ports 2379 and 2380 on the VM. 2379 is used for etcd client-to-server communication, and 2380 is used for etcd server-to-server communication.</li><a/><li value="2" class="calibre_13">A discovery token needs to be generated every time for each cluster and cannot be shared. When a stale discovery token is shared, members will not be able to join the etcd cluster.</li><li value="3" class="calibre_13">Running multiple CoreOS clusters with Vagrant simultaneously can cause issues because of overlapping IP ranges. Care should be taken so that common parameters such as the IP address are not shared across clusters.</li><li value="4" class="calibre_13">Cloud-config YAML files need to be properly indented. It is better to use the cloud-config validator to check for issues.</li><li value="5" class="calibre_13">When using discovery token, CoreOS node needs to have Internet access to access the token service.</li><li value="6" class="calibre_13">When creating a discovery token, you need to use the size based on the count of members and all members need to be part of the bootstrap. If all members are not present, the cluster will not be formed. Members can be added or removed later.</li></ul><div class="mbp_pagebreak" id="calibre_pb_71"/>


<p id="filepos212812" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered the basics of CoreOS cloud-config and how to set up the CoreOS development environment with Vagrant, Amazon AWS, Google GCE, and Bare Metal. We also covered some basic debugging steps for commonly encountered issues. As described in this chapter, it is easy to install CoreOS in the local data center or Cloud environments. It is better to try out deployment in a development cluster before moving to production environments. In the next chapter, we will cover how the CoreOS automatic update works.</p><div class="mbp_pagebreak" id="calibre_pb_72"/>


<p id="filepos213481" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Vagrant installation: <a href="https://coreos.com/os/docs/latest/booting-on-vagrant.html">https://coreos.com/os/docs/latest/booting-on-vagrant.html</a></li><a/><li value="2" class="calibre_13">AWS installation: <a href="https://coreos.com/os/docs/latest/booting-on-ec2.html">https://coreos.com/os/docs/latest/booting-on-ec2.html</a></li><a/><li value="3" class="calibre_13">GCE installation: <a href="https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html">https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html</a></li><a/><li value="4" class="calibre_13">Bare Metal installation: <a href="https://coreos.com/os/docs/latest/installing-to-disk.html">https://coreos.com/os/docs/latest/installing-to-disk.html</a></li><a/><li value="5" class="calibre_13">CoreOS CloudInit: <a href="https://github.com/coreos/coreos-cloudinit">https://github.com/coreos/coreos-cloudinit</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_73"/>


<p id="filepos214695" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Introduction to the cloud-config format: <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting">https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting</a></li><a/><li value="2" class="calibre_13">CoreOS with AWS Cloudformation: <a href="http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/">http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/</a></li><a/><li value="3" class="calibre_13">The CoreOS bare-metal installation: <a href="http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial">http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial</a> and <a href="http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux">http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux</a></li><a/><li value="4" class="calibre_13">Using journalctl to view systemd logs: <a href="https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs">https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_74"/>
</body></html>