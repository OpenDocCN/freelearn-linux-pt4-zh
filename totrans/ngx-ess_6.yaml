- en: Chapter 6. Performance Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tuning is the improvement of system performance. In our context,
    it is the performance of an entire web service or an individual web server. The
    need for such activity arises when there is a real or anticipated performance
    problem, such as excessive response latency, insufficient upload or download rate,
    lack of system scalability, or excessive use of computer system resources for
    seemingly low service usage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at a number of topics that deal with performance
    problems using features of Nginx. Each section explains when and how a solution
    is applicable; that is, what kind of performance problems it addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter you will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: How to optimize static file retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up response compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize data buffer allocation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to accelerate SSL by enabling session caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize worker process allocation on multi-core systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing static file retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Static file retrieval performance directly affects visitors' perceived website
    performance. This happens because web pages usually contain numerous references
    to dependent resources. These resources need to be quickly retrieved before the
    entire page can be rendered. The faster the web server can start returning a static
    file (lower latency) and the higher the parallelism of retrieval, the higher the
    perceived performance of the website.
  prefs: []
  type: TYPE_NORMAL
- en: When the latency is the driving factor, it is important that files are returned
    predominantly from the main memory, as it has much lower latency compared to hard
    drives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the operating system already takes very good care of that through
    filesystem cache. You only need to stimulate cache usage by specifying some advisory
    parameters and eliminating waste:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By default, Nginx reads the content of a file into the user space before sending
    to the client. This is suboptimal and can be avoided by using the `sendfile()`
    system call if it is available. The `sendfile()` function implements a zero-copy
    transfer strategy by copying data from one file descriptor to another bypassing
    user space.
  prefs: []
  type: TYPE_NORMAL
- en: We enable `sendfile()` by specifying the `sendfile on` parameter in code. We
    limit the maximum amount of data that `sendfile()` can send in one invocation
    to 1 MB using the `sendfile_max_chunk` directive. In this way, we prevent a single
    fast connection from occupying the whole worker process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Response body filters such as the `.gzip` compressor require response data in
    the user space. They cannot be combined with a zero-copy strategy and consequently
    with `sendfile()`. When enabled, they cancel the effect of `sendfile()`.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding configuration is optimized for latency. Compare it to the example
    from the *Setting up Nginx to serve static data* section in [Chapter 2](ch02.html
    "Chapter 2. Managing Nginx"), *Managing Nginx*. You will see that the `tcp_nopush`
    directive is gone. The `off` state of this option will make network utilization
    a bit less efficient, but will deliver data—including the HTTP header—to the client
    as soon as possible.
  prefs: []
  type: TYPE_NORMAL
- en: With `tcp_nopush` set to `on`, the first packet of the response will be sent
    as soon as the chunk of data is obtained by `sendfile()`.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of static file retrieval is large file download. In this case,
    the startup time is not as important as the download throughput or, in other words,
    the download speed that a server can attain while returning a large file. Caching
    stops being desirable for large files. Nginx reads them sequentially, so cache
    hits are much less likely for them. Cached segments of a large file would therefore
    simply pollute the cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, caching can be bypassed by using Direct I/O. With Direct I/O enabled,
    the operating system translates read offsets into the underlying block device
    addresses, and queues read requests directly into the underlying block device
    queue. The following configuration shows how to enable Direct I/O:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `directio` directive takes a single argument that specifies the minimum
    size a file must have in order to be read with Direct I/O. In addition to specifying
    `direction`, we extend the output buffer using the `output_buffers` directive
    in order to increase system call efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that Direct I/O blocks the worker processes during reads. This reduces
    parallelism and throughput of file retrieval. To avoid blocking and increase parallelism,
    you can enable **Asynchronous I/O** (**AIO**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux, AIO is available as of kernel version 2.6.22 and it is non-blocking
    only in combination with Direct I/O. AIO and Direct I/O can be combined with `sendfile()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this case, files smaller than the size specified in `directio` will be send
    using `sendfile()`, or else with AIO plus Direct I/O.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of Nginx version 1.7.11, you can delegate file read operations to a pool
    of threads. This makes perfect sense if you are not limited by memory or CPU resources.
    As threads do not require Direct I/O, enabling them on large files will lead to
    aggressive caching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Threads are not compiled by default (at the moment of writing this chapter),
    so you have to enable them using the with-threads configuration switch. In addition
    to that, threads can work only with `epoll`, `kqueue`, and `eventport` event processing
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: With threads, both higher parallelism and caching can be attained without blocking
    the worker process, although threads and communication between threads require
    some additional resources.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling response compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance of your website can be improved by enabling response compression
    using GZIP. Compression reduces the size of a response body, reduces the bandwidth
    required to transfer the response data, and ultimately makes sure the resources
    of your website are delivered to the client side sooner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compression can be enabled using the `gzip` directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This directive is valid in the `http`, `server`, `location`, and `if` sections.
    Specifying `off` as the first argument of this directive disables compression
    in the corresponding location if it was enabled in outer sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, only documents with MIME type *text/HTML* are compressed. To enable
    compression for other types of documents, use the `gzip_types` directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration enables compression for MIME types that hypertext
    documents, cascading style sheets, and JavaScript files appear to be in. These
    are the types of documents that benefit most from the compression, as text files
    and source code files—if they are large enough—usually contain a lot of entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Archives, images, and movies are not suitable for compression, as they are usually
    already compressed. Executable files are less suitable for compression, but can
    benefit from it in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'It makes sense to disable compression for small documents, as compression efficiency
    might not be worth the efforts—or even worse—might be negative. In Nginx, you
    can implement compression using the `gzip_min_length` directive. This directive
    specifies the minimum length a document must be in order to be eligible for compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding configuration, all documents smaller than 512 bytes will
    not be compressed. The length information that is used to apply this restriction
    is extracted from the Content-Length response header. If no such header is present,
    the response will be compressed regardless of its length.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Response compression comes at a cost: it is CPU-intensive. You need to consider
    that in your capacity planning and system design. If CPU utilization becomes a
    bottleneck, try reducing the compression level using the `gzip_comp_level` directive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists some other directives that affect the behavior of
    compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Directive | Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `gzip_disable <regex>` | If the User-Agent field of a request matches the
    specified regular expression, the compression for that request will be disabled.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `gzip_comp_level <level>` | This specifies the GZIP compression level to
    use. The lowest is 1 and the highest is 9\. These values correspond to options
    -1 … -9 of the `gzip` command. |'
  prefs: []
  type: TYPE_TB
- en: The preceding directives can help you fine-tune the response compression in
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: The efficiency of response body compression can be monitored via the `$gzip_ratio`
    variable. This variable indicates the attained compression ratio equal to the
    ratio of the size of the original response body to the size of the compressed
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of this variable can be written to the log file and later extracted
    and picked up by your monitoring system. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration creates a log file format named `gzip` and uses
    this format to log HTTP requests in one of the virtual hosts. The last field in
    the log file will indicate the attained compression ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing buffer allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nginx uses buffers to store request and response data at various stages. Optimal
    buffer allocation can help you spare memory consumption and reduce CPU usage.
    The following table lists directives that control buffer allocation and the stages
    they are applied to:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Directive | Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `client_body_buffer_size <size>` | This specifies the size of the buffer
    that is used to receive the request body from the client. |'
  prefs: []
  type: TYPE_TB
- en: '| `output_buffers <number> <size>` | This specifies the number and the size
    of buffers that are used to send the response body to the client in case no acceleration
    is used. |'
  prefs: []
  type: TYPE_TB
- en: '| `gzip_buffers <number> <size>` | This specifies the number and the size of
    the buffers that are used to compress the response body. |'
  prefs: []
  type: TYPE_TB
- en: '| `proxy_buffers <number> <size>` | This specifies the number and the size
    of the buffers that are used to receive the response body from a proxied server.
    This directive makes sense only if buffering is enabled. |'
  prefs: []
  type: TYPE_TB
- en: '| `fastcgi_buffers <number> <size>` | This specifies the number and the size
    of the buffers that are used to receive the response body from a FastCGI server.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `uwcgi_buffers <number> <size>` | This specifies the number and the size
    of the buffers that are used to receive the response body from a UWCGI server.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `scgi_buffers <number> <size>` | This specifies the number and the size of
    the buffers that are used to receive the response body from a SCGI server. |'
  prefs: []
  type: TYPE_TB
- en: 'As you can see, most of the directives take two arguments: a number and a size.
    The number argument specifies the maximum number of buffers that can be allocated
    per request. The size argument specifies the size of each buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimizing buffer allocation](img/B04282_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure illustrates how buffers are allocated for a data stream.
    Part **a** shows what happens when an input data stream is shorter than the buffer
    size specified in the directives above. The data stream occupies the entire buffer
    even though the space for the whole buffer is allocated from the heap. Part **b**
    shows a data stream that is longer than a single buffer, but shorter than the
    longest allowed chain of buffers. As you can see, if the buffers are used in the
    most efficient way, some of them will be fully used and the last one might be
    only partially used. Part **c** shows a data stream that is much longer than the
    longest chain of buffers allowed. Nginx tries to fill all available buffers with
    input data and flushes them once the data is sent. After that, empty buffers wait
    until more input data becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: New buffers are allocated as long as there are no free buffers at hand and input
    data is available. Once the maximum number of buffers is allocated, Nginx waits
    until used buffers are emptied and reuses them. This makes sure that no matter
    how long the data stream, it will not consume more memory per request (the number
    of buffers multiplied by the size) than specified by the corresponding directive.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller the buffers, the higher the allocation overhead. Nginx needs to
    spend more CPU cycles to allocate and free buffers. The larger the buffers, the
    larger memory consumption overhead. If a response occupies only a fraction of
    a buffer, the remainder of the buffer is not used—even though the entire buffer
    has to be allocated from the heap.
  prefs: []
  type: TYPE_NORMAL
- en: The minimum portion of the configuration that the buffer size directives can
    be applied to is a location. This means that if mixtures of large and small responses
    share the same location, the combined buffer usage pattern will vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Static files are read into buffers controlled by the `output_buffers` directive
    unless `sendfile` is set to `on`. For static files, multiple output buffers don''t
    make much sense, as they are filled in the blocking mode anyway (this means a
    buffer cannot be emptied while the other one is being filled). However, larger
    buffers lead to lower system call rate. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If the output buffer size is too large without threads or AIO, it can lead to
    long blocking reads that will affect worker process responsiveness.
  prefs: []
  type: TYPE_NORMAL
- en: When a response body is pipelined from a proxied server, FastCGI, UWCGI, or
    SCGI server, Nginx is able to read data into one part of the buffers and simultaneously
    send the other part to the client. This makes the most sense for long replies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume you tuned your TCP stack before reading this chapter. The total size
    of a buffer chain is then connected to the kernel socket''s read and write buffer
    sizes. On Linux, the maximum size of a kernel socket read buffer can be examined
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'While the maximum size of a kernel socket write buffer can be examined using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These settings can be changed using the `sysctl` command or via `/etc/sysctl.conf`
    at system startup.
  prefs: []
  type: TYPE_NORMAL
- en: In my case, both of them are set to `163840` (160 KB). This is low for a real
    system, but let's use it as an example. This number is the maximum amount of data
    Nginx can read from or write to a socket in one system call without the socket
    being suspended. With reads and writes going asynchronously, we need a buffer
    space no less than the sum of `rmem_max` and `wmem_max` for optimal system call
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that the preceding Nginx proxies long files with `rmem_max` and `wmem_max`
    settings. The following configuration must yield the lowest system call rate with
    the minimum amount of memory per request in the most extreme case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The same considerations apply to the `fastcgi_buffers`, `uwcgi_buffers`, and
    `scgi_buffers` directives.
  prefs: []
  type: TYPE_NORMAL
- en: For short response bodies, the buffer size has to be a bit larger than the predominant
    size of a response. In this case, all replies will fit into one buffer—only one
    allocation per request will be needed.
  prefs: []
  type: TYPE_NORMAL
- en: For the preceding setup, assume that most of the replies fit 128 KB, while some
    span up to dozens of megabytes. The optimal buffer configuration will be somewhere
    between `proxy_buffers 2 160k` and `proxy_buffers 4 80k`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of response body compression, the size of the GZIP buffer chain
    must be downscaled by the average compression ratio. For the preceding setup,
    assume that the average compression ratio is 3.4\. The following configuration
    must yield the lowest system call rate with a minimal amount of memory per request
    in presence of response body compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding configuration we make sure that in the most extreme case, if
    half of the proxy buffers are being used for reception, the other half is ready
    for compression. GZIP buffers are configured in a way that makes sure that the
    compressor output for half of the uncompressed data occupies half of the output
    buffers, while the other half of the buffers with compressed data are sent to
    the client.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling SSL session reuse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An SSL session is started by a handshake procedure that involves multiple round
    trips (see the following figure). The client and server have to exchange four
    messages with a latency of around 50 milliseconds each. In total, we have at least
    200 milliseconds of overhead while establishing a secure connection. In addition
    to that, both the client and the server need to perform public-key cryptographic
    operations in order to share a common secret. These operations are computationally
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: '![Enabling SSL session reuse](img/B04282_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Normal SSL handshake
  prefs: []
  type: TYPE_NORMAL
- en: 'The client can request an abbreviated handshake in effect (see the following
    figure), saving a full round-trip of 100 milliseconds and avoiding the most expensive
    part of the full SSL handshake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Enabling SSL session reuse](img/B04282_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Abbreviated handshake
  prefs: []
  type: TYPE_NORMAL
- en: The abbreviated handshake can be accomplished either through the *session identifiers*
    mechanism defined by RFC 5246, or through the *session tickets* mechanism detailed
    in RFC 5077.
  prefs: []
  type: TYPE_NORMAL
- en: To make abbreviated handshakes with session identifiers possible, the server
    needs to store session parameters in a cache keyed by a session identifier. In
    Nginx, this cache can be configured to be shared with all worker processes. When
    a client requests an abbreviated handshake, it provides the server with a session
    identifier so that it can retrieve session parameters from the cache. After that,
    the handshake procedure can be shortened and public-key cryptography can be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable SSL session cache, use the `ssl_session_cache` directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This configuration enables SSL session caching with built-in OpenSSL session
    cache. The number in the first argument (`40000`) specifies the size of the cache
    in sessions. The built-in cache cannot be shared between worker processes. Consequently,
    this reduces efficiency of SSL session reuse.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following configuration enables SSL session caching with a cache shared
    between worker processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This creates a shared SSL session cache named `ssl` and enables SSL session
    reuse with this cache. The size of the cache is now specified in bytes. Each session
    occupies around 300 bytes in such cache.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to perform an abbreviated SSL handshake without the server state
    using an SSL session tickets mechanism. This is done by packaging session parameters
    into a binary object and encrypting it with a key known only to the server. This
    encrypted object is called a session ticket.
  prefs: []
  type: TYPE_NORMAL
- en: A session ticket then can be safely transferred to the client. When the client
    wishes to resume a session, it presents the session ticket to the server. The
    server decrypts it and extracts the session parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Session tickets are an extension of the TLS protocol and can be used with TLS
    1.0 and further (SSL is a predecessor of TLS).
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable session tickets, use the `ssl_session_tickets` directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Naturally, both mechanisms can be enabled at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For security reasons, cached session lifetime is limited so that session parameters
    cannot be attacked while session is active. Nginx sets the default maximum SSL
    session lifetime to 5 minutes. If security is not a big concern and visitors spend
    considerable time on your website, you can extend the maximum session lifetime,
    increasing the efficiency of SSL in effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum SSL session lifetime is controlled by the `ssl_session_timeout`
    directive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding configuration enables both session reuse mechanisms and sets the
    maximum SSL session lifetime to 1 hour.
  prefs: []
  type: TYPE_NORMAL
- en: Worker processes allocation on multi-core systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your Nginx workload is CPU-bound, such as when using response compression
    on proxied content, on systems with multiple processors or multiple processor
    cores, it might be possible to obtain additional performance by associating each
    worker process with its own processor/core.
  prefs: []
  type: TYPE_NORMAL
- en: In a multi-core processor, each core has its own instance of **Translation Lookaside
    Buffer** (**TLB**) that is used by the memory-management unit to accelerate virtual
    address translation. In a preemptive multitasking operating system, each process
    has its own virtual memory context. When an operating system assigns an active
    process to a processor core and the virtual memory context does not match the
    context that filled the TLB of that processor core, the operating system has to
    flush the TLB as its content is no longer valid.
  prefs: []
  type: TYPE_NORMAL
- en: The new active process then receives a performance penalty, because it has to
    fill the TLB with new entries as it reads or writes memory locations.
  prefs: []
  type: TYPE_NORMAL
- en: Nginx has an option to "stick" a process to a processor core. On a system with
    a single Nginx instance, worker processes will be scheduled most of the time.
    In such circumstances, there is a very high probability that the virtual memory
    context does not need to be switched and TLB does not need to be flushed. The
    "stickiness" of a process then becomes useful. The "stickiness" is called CPU
    affinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a system with four processor cores. The CPU affinity can be configured
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This configuration assigns each worker to its own processor core. The configuration
    directive `worker_cpu_affinity` receives many arguments as many worker process
    are to be started. Each argument specifies a mask, where a bit with a value of
    1 specifies affinity with the corresponding processor, and a bit with a value
    of 0 specifies no affinity with the corresponding processor.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPU affinity does not guarantee an increase in performance, but make sure to
    give it a try if your Nginx server is performing CPU-bound tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned a number of recipes that will help you tackle performance
    and scalability challenges of your system. It is important to remember that these
    recipes are not solutions for all possible performance problems and represent
    mere trade-offs between different ways of using the resources of your system.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, they are must-haves in the toolbox of a web master or a site reliability
    engineer who wants to master Nginx and its performance and scalability features.
  prefs: []
  type: TYPE_NORMAL
