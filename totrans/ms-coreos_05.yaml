- en: Chapter 5. CoreOS Networking and Flannel Internals
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章 CoreOS 网络和 Flannel 内部结构
- en: 'Microservices increased the need to have lots of containers and also connectivity
    between containers across hosts. It is necessary to have a robust Container networking
    scheme to achieve this goal. This chapter will cover the basics of Container networking
    with a focus on how CoreOS does Container networking with Flannel. Docker networking
    and other related container networking technologies will also be covered. The
    following topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务增加了对大量容器以及容器跨主机互联的需求。为了实现这一目标，必须有一个健壮的容器网络方案。本章将介绍容器网络的基础知识，重点讨论 CoreOS 如何通过
    Flannel 实现容器网络。还将涵盖 Docker 网络和其他相关的容器网络技术。本章将包括以下主题：
- en: Container networking basics
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器网络基础
- en: Flannel internals
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel 内部结构
- en: A CoreOS Flannel cluster using Vagrant, AWS, and GCE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Vagrant、AWS 和 GCE 搭建的 CoreOS Flannel 集群
- en: Docker networking and experimental Docker networking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 网络和实验性 Docker 网络
- en: Docker networking using Weave and Calico
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Weave 和 Calico 进行 Docker 网络连接
- en: Kubernetes networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Container networking basics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 容器网络基础
- en: 'The following are the reasons why we need Container networking:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要容器网络的原因：
- en: Containers need to talk to the external world.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器需要与外部世界进行通信。
- en: Containers should be reachable from the external world so that the external
    world can use the services that Containers provide.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器应该可以从外部世界访问，以便外部世界可以使用容器提供的服务。
- en: Containers need to talk to the host machine. An example can be sharing volumes.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器需要与主机进行通信。例如，可以共享卷。
- en: There should be inter-container connectivity in the same host and across hosts.
    An example is a WordPress container in one host talking to a MySQL container in
    another host.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一主机和不同主机之间应具有容器互联。例如，一个主机中的 WordPress 容器与另一个主机中的 MySQL 容器进行通信。
- en: Multiple solutions are currently available to interconnect Containers. These
    solutions are pretty new and actively under development. Docker, until release
    1.8, did not have a native solution to interconnect Containers across hosts. Docker
    release 1.9 introduced a Libnetwork-based solution to interconnect containers
    across hosts as well as perform service discovery. CoreOS is using Flannel for
    container networking in CoreOS clusters. There are projects such as Weave and
    Calico that are developing Container networking solutions, and they plan to be
    a networking container plugin for any Container runtime such as Docker or Rkt.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有多种解决方案可以实现容器之间的互联。这些解决方案相对较新，且正在积极开发中。在 Docker 1.8 版本之前，Docker 并没有提供原生的跨主机容器互联方案。Docker
    1.9 版本引入了一种基于 Libnetwork 的方案，用于实现跨主机容器互联以及服务发现。CoreOS 在 CoreOS 集群中使用 Flannel 进行容器网络连接。还有一些项目，如
    Weave 和 Calico，正在开发容器网络解决方案，计划成为任何容器运行时（如 Docker 或 Rkt）的网络容器插件。
- en: Flannel
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel
- en: Flannel is an open source project that provides a Container networking solution
    for CoreOS clusters. Flannel can also be used for non-CoreOS clusters. Kubernetes
    uses Flannel to set up networking between the Kubernetes pods. Flannel allocates
    a separate subnet for every host where a Container runs, and the Containers in
    this host get allocated an individual IP address from the host subnet. An overlay
    network is set up between each host that allows Containers on different hosts
    to talk to each other. In [Chapter 1](index_split_023.html#filepos77735), CoreOS
    Overview we provided an overview of the Flannel control and data path. This section
    will delve into the Flannel internals.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 是一个开源项目，提供 CoreOS 集群的容器网络解决方案。Flannel 也可以用于非 CoreOS 集群。Kubernetes 使用
    Flannel 设置 Kubernetes pod 之间的网络连接。Flannel 为每个运行容器的主机分配一个独立的子网，并从该主机的子网中为容器分配一个独立的
    IP 地址。在每个主机之间建立一个覆盖网络，使得不同主机上的容器可以互相通信。在[第1章](index_split_023.html#filepos77735)
    CoreOS 概述中，我们提供了 Flannel 控制路径和数据路径的概述。本节将深入探讨 Flannel 的内部结构。
- en: Manual installation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 手动安装
- en: 'Flannel can be installed manually or using the `systemd` unit, `flanneld.service`.
    The following command will install Flannel in the CoreOS node using a container
    to build the flannel binary. The flanneld Flannel binary will be available in
    `/home/core/flannel/bin` after executing the following commands:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 可以通过手动安装或使用 `systemd` 单元 `flanneld.service` 来安装。以下命令将在 CoreOS 节点上使用容器构建
    Flannel 二进制文件。执行完以下命令后，Flannel 的 flanneld 二进制文件将在 `/home/core/flannel/bin` 中可用：
- en: '`git clone https://github.com/coreos/flannel.git``docker run -v /home/core/flannel:/opt/flannel -i -t google/golang /bin/bash -c "cd /opt/flannel && ./build"`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/flannel.git``docker run -v /home/core/flannel:/opt/flannel -i -t google/golang /bin/bash -c "cd /opt/flannel && ./build"`'
- en: 'The following is the Flannel version after we build flannel in our CoreOS node:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在CoreOS节点中构建flannel后获得的Flannel版本：
- en: '![](img/00132.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpg)'
- en: Installation using flanneld.service
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用flanneld.service进行安装
- en: Flannel is not installed by default in CoreOS. This is done to keep the CoreOS
    image size to a minimum. Docker requires flannel to configure the network and
    flannel requires Docker to download the flannel container. To avoid this chicken-and-egg
    problem, `early-docker.service` is started by default in CoreOS, whose primary
    purpose is to download the flannel container and start it. A regular `docker.service`
    starts the Docker daemon with the flannel network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel在CoreOS中默认没有安装。这样做是为了保持CoreOS镜像的最小化。Docker需要flannel来配置网络，而flannel需要Docker来下载flannel容器。为了避免这种“先有鸡还是先有蛋”的问题，CoreOS中默认启动了`early-docker.service`，其主要目的是下载并启动flannel容器。一个常规的`docker.service`则启动Docker守护进程并使用flannel网络。
- en: 'The following figure shows you the sequence in `flanneld.service`, where the
    early Docker daemon starts the flannel container, which, in turn starts `docker.service`
    with the subnet created by flannel:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了`flanneld.service`中的序列，其中早期的Docker守护进程启动了flannel容器，进而启动了`docker.service`，并使用flannel创建的子网：
- en: '![](img/00136.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpg)'
- en: 'The following is the relevant section of `flanneld.service` that downloads
    the flannel container from the Quay repository:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flanneld.service`的相关部分，它从Quay仓库下载flannel容器：
- en: '![](img/00285.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00285.jpg)'
- en: 'The following output shows the early docker''s running containers. Early-docker
    will manage Flannel only:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了早期docker运行的容器。Early-docker只管理Flannel：
- en: '![](img/00142.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.jpg)'
- en: 'The following is the relevant section of `flanneld.service` that updates the
    docker options to use the subnet created by flannel:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flanneld.service`的相关部分，它更新了Docker选项以使用flannel创建的子网：
- en: '![](img/00146.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.jpg)'
- en: 'The following is the content of `flannel_docker_opts.env`—in my case—after
    flannel was started. The address, `10.1.60.1/24`, is chosen by this CoreOS node
    for its containers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flannel_docker_opts.env`的内容——在我的案例中——flannel启动后的内容。地址`10.1.60.1/24`是这个CoreOS节点为其容器选择的：
- en: '![](img/00042.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpg)'
- en: 'Docker will be started as part of `docker.service`, as shown in the following
    screenshot, with the preceding environment file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Docker将作为`docker.service`的一部分启动，如下图所示，并使用上述环境文件：
- en: '![](img/00151.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00151.jpg)'
- en: Control path
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 控制路径
- en: There is no central controller in flannel, and it uses etcd for internode communication.
    Each node in the CoreOS cluster runs a flannel agent and they communicate with
    each other using etcd.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel中没有中央控制器，它使用etcd进行节点间的通信。CoreOS集群中的每个节点都运行一个flannel代理，它们通过etcd进行相互通信。
- en: As part of starting the Flannel service, we specify the Flannel subnet that
    can be used by the individual nodes in the network. This subnet is registered
    with etcd so that every CoreOS node in the cluster can see it. Each node in the
    network picks a particular subnet range and registers atomically with etcd.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为启动Flannel服务的一部分，我们指定了可以由网络中各个节点使用的Flannel子网。这个子网注册在etcd中，确保集群中的每个CoreOS节点都能看到它。网络中的每个节点选择一个特定的子网范围，并通过etcd原子地进行注册。
- en: 'The following is the relevant section of `cloud-config` that starts `flanneld.service`
    along with specifying the configuration for Flannel. Here, we have specified the
    subnet to be used for flannel as `10.1.0.0/16` along with the encapsulation type
    as `vxlan`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`cloud-config`的相关部分，它启动了`flanneld.service`并指定了Flannel的配置。在这里，我们指定了用于flannel的子网为`10.1.0.0/16`，并将封装类型指定为`vxlan`：
- en: '![](img/00155.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00155.jpg)'
- en: 'The preceding configuration will create the following etcd key as seen in the
    node. This shows that `10.1.0.0/16` is allocated for flannel to be used across
    the CoreOS cluster and that the encapsulation type is `vxlan`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置将在节点中创建以下etcd键。它显示了`10.1.0.0/16`已分配给flannel，供CoreOS集群使用，并且封装类型为`vxlan`：
- en: '![](img/00309.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00309.jpg)'
- en: 'Once each node gets a subnet, containers started in this node will get an IP
    address from the IP address pool allocated to the node. The following is the etcd
    subnet allocation per node. As we can see, all the subnets are in the `10.1.0.0/16`
    range that was configured earlier with etcd and with a 24-bit mask. The subnet
    length per host can also be controlled as a flannel configuration option:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个节点获取了子网，该节点中启动的容器将从分配给该节点的IP地址池中获取IP地址。以下是每个节点的etcd子网分配情况。如我们所见，所有子网都位于之前与etcd配置的`10.1.0.0/16`范围内，且使用了24位掩码。每个主机的子网长度也可以作为Flannel配置选项进行控制：
- en: '![](img/00161.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.jpg)'
- en: 'Let''s look at `ifconfig` of the Flannel interface created in this node. The
    IP address is in the address range of `10.1.0.0/16`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下在此节点中创建的Flannel接口的`ifconfig`。该IP地址属于`10.1.0.0/16`地址范围：
- en: '![](img/00165.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpg)'
- en: Data path
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据路径
- en: Flannel uses the Linux bridge to encapsulate the packets using an overlay protocol
    specified in the Flannel configuration. This allows for connectivity between containers
    in the same host as well as across hosts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel使用Linux桥接来封装使用Flannel配置中指定的覆盖协议的数据包。这使得同一主机内的容器之间以及跨主机的容器之间能够实现连通性。
- en: 'The following are the major backends currently supported by Flannel and specified
    in the JSON configuration file. The JSON configuration file can be specified in
    the Flannel section of `cloud-config`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前Flannel支持的主要后端，并在JSON配置文件中指定。JSON配置文件可以在`cloud-config`的Flannel部分中指定：
- en: 'UDP: In UDP encapsulation, packets from containers are encapsulated in UDP
    with the default port number `8285`. We can change the port number if needed.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDP：在UDP封装中，来自容器的数据包会被封装到UDP中，默认端口号为`8285`。如果需要，我们可以更改端口号。
- en: 'VXLAN: From an encapsulation overhead perspective, VXLAN is efficient when
    compared to UDP. By default, port `8472` is used for VXLAN encapsulation. If we
    want to use an IANA-allocated VXLAN port, we need to specify the port field as
    `4789`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN：从封装开销的角度来看，VXLAN相对于UDP更高效。默认情况下，端口`8472`用于VXLAN封装。如果我们希望使用IANA分配的VXLAN端口，则需要将端口字段指定为`4789`。
- en: 'AWS-VPC: This is applicable to using Flannel in the AWS VPC cloud. Instead
    of encapsulating the packets using an overlay, this approach uses a VPC route
    table to communicate across containers. AWS limits each VPC route table entry
    to 50, so this can become a problem with bigger clusters.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS-VPC：适用于在AWS VPC云中使用Flannel的情况。与使用覆盖网络封装数据包不同，这种方法通过使用VPC路由表来跨容器进行通信。AWS限制每个VPC路由表条目的数量为50，因此在较大的集群中可能会出现问题。
- en: 'The following is an example of specifying the AWS type in the flannel configuration:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定Flannel配置中AWS类型的示例：
- en: '![](img/00168.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00168.jpg)'
- en: 'GCE: This is applicable to using Flannel in the GCE cloud. Instead of encapsulating
    the packets using an overlay, this approach uses the GCE route table to communicate
    across containers. GCE limits each VPC route table entry to `100`, so this can
    become a problem with bigger clusters.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCE：适用于在GCE云中使用Flannel的情况。与使用覆盖网络封装数据包不同，这种方法通过使用GCE路由表来跨容器进行通信。GCE限制每个VPC路由表条目的数量为`100`，因此在较大的集群中可能会出现问题。
- en: 'The following is an example of specifying the GCE type in the Flannel configuration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定Flannel配置中GCE类型的示例：
- en: '![](img/00171.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00171.jpg)'
- en: Let's create containers in two different hosts with a VXLAN encapsulation and
    check whether the connectivity is fine. The following example uses a Vagrant CoreOS
    cluster with the Flannel service enabled.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两台不同主机中创建容器，并使用VXLAN封装，检查连通性是否正常。以下示例使用启用了Flannel服务的Vagrant CoreOS集群。
- en: 'Configuration in Host1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Host1中的配置：
- en: 'Let''s start a `busybox` container:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来启动一个`busybox`容器：
- en: '![](img/00175.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpg)'
- en: 'Let''s check the IP address allotted to the container. This IP address comes
    from the IP pool allocated to this CoreOS node by the flannel agent. `10.1.19.0/24`
    was allocated to `host1` and this container got the `10.1.19.2` address:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来检查分配给容器的IP地址。该IP地址来自Flannel代理为此CoreOS节点分配的IP池。`10.1.19.0/24`分配给了`host1`，这个容器获得了`10.1.19.2`地址：
- en: '![](img/00177.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpg)'
- en: 'Configuration in Host2:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Host2中的配置：
- en: 'Let''s start a `busybox` container:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来启动一个`busybox`容器：
- en: '![](img/00180.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.jpg)'
- en: 'Let''s check the IP address allotted to this container. This IP address comes
    from the IP pool allocated to this CoreOS node by the flannel agent. `10.1.1.0/24`
    was allocated to `host2` and this container got the `10.1.1.2` address:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下分配给该容器的IP地址。这个IP地址来自Flannel代理为此CoreOS节点分配的IP池。`10.1.1.0/24`被分配给了`host2`，而这个容器获得了`10.1.1.2`的地址：
- en: '![](img/00184.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpg)'
- en: 'The following output shows you the ping being successful between container
    1 and container 2\. This ping packet is travelling across the two CoreOS nodes
    and is encapsulated using VXLAN:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示容器1和容器2之间的ping操作成功。这个ping包通过两个CoreOS节点传输，并使用VXLAN进行封装：
- en: '![](img/00326.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00326.jpg)'
- en: Flannel as a CNI plugin
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel作为CNI插件
- en: As explained in [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview,
    APPC defines a Container specification that any Container runtime can use. For
    Container networking, APPC defines a Container Network Interface (CNI) specification.
    With CNI, the Container networking functionality can be implemented as a plugin.
    CNI expects plugins to support APIs with a set of parameters and the implementation
    is left to the plugin. The plugin implements APIs like adding a container to a
    network and removing container from the network with a defined parameter list.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第一章](index_split_023.html#filepos77735)《CoreOS概述》所述，APPC定义了一个容器规范，任何容器运行时都可以使用该规范。对于容器网络，APPC定义了容器网络接口（CNI）规范。通过CNI，容器网络功能可以作为插件实现。CNI期望插件支持带有一组参数的API，具体实现则交由插件完成。插件实现的API包括将容器添加到网络中和从网络中移除容器，且有一个定义好的参数列表。
- en: 'This allows the implementation of network plugins by different vendors and
    also the reuse of plugins across different Container runtimes. The following figure
    shows the relationship between the RKT container runtime, CNI layer, and Plugin
    like Flannel. The IPAM Plugin is used to allocate an IP address to the containers
    and this is nested inside the initial networking plugin:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许不同供应商实现网络插件，并且可以在不同的容器运行时之间重用插件。下图显示了RKT容器运行时、CNI层和像Flannel这样的插件之间的关系。IPAM插件用于为容器分配IP地址，这个功能被嵌套在初始网络插件中：
- en: '![](img/00328.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00328.jpg)'
- en: Setting up a three-node Vagrant CoreOS cluster with Flannel and Docker
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel和Docker设置一个三节点Vagrant CoreOS集群
- en: 'The following example sets up a three-node Vagrant CoreOS cluster with the
    `etcd`, `fleet`, and `flannel` services turned on by default. In this example,
    `vxlan` is used for encapsulation. The following is the `cloud-config` used for
    this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例设置了一个三节点的Vagrant CoreOS集群，默认启用了`etcd`、`fleet`和`flannel`服务。在这个示例中，使用`vxlan`进行封装。以下是用于此配置的`cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <update this>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}''
          command: start`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: <update this>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}''
          command: start`'
- en: 'The following are the steps for this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此操作的步骤：
- en: Clone the CoreOS Vagrant repository.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆CoreOS Vagrant仓库。
- en: Change the instance count to three in `config.rb`.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config.rb`中将实例数量更改为三个。
- en: Update the discovery token in the `cloud-config` user data.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cloud-config`用户数据中更新发现令牌。
- en: Perform `vagrant up` to start the cluster.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`vagrant up`来启动集群。
- en: For more details on the steps, refer to [Chapter 2](index_split_048.html#filepos153225),
    Setting up the CoreOS Lab. We can test the container connectivity by starting
    `busybox` containers in both the hosts and checking that the ping is working between
    the two Containers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有关步骤的更多详细信息，请参考[第2章](index_split_048.html#filepos153225)，设置CoreOS实验室。我们可以通过在两个主机中启动`busybox`容器并检查容器间ping是否正常来测试容器的连接性。
- en: Setting up a three-node CoreOS cluster with Flannel and RKT
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel和RKT设置三节点CoreOS集群
- en: Here, we will set up a three-node CoreOS cluster with RKT containers using the
    Flannel CNI networking plugin to set up the networking. This example will allow
    RKT containers across hosts to communicate with each other.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用Flannel CNI网络插件设置一个包含RKT容器的三节点CoreOS集群，以设置网络。这将允许跨主机的RKT容器彼此通信。
- en: 'The following is the `cloud-config` used:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用的`cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <update token>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "network": "10.1.0.0/16" }''
          command: start # Rkt configuration write_files:   - path: "/etc/rkt/net.d/10-containernet.conf"
        permissions: "0644"     owner: "root"     content: |       {         "name": "containernet",
            "type": "flannel"       }`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: <update token>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "network": "10.1.0.0/16" }''
          command: start # Rkt 配置 write_files:   - path: "/etc/rkt/net.d/10-containernet.conf"
        permissions: "0644"     owner: "root"     content: |       {         "name": "containernet",
            "type": "flannel"       }`'
- en: The `/etc/rkt/net.d/10-containernet.conf` file sets up the CNI plugin type as
    Flannel and RKT containers use this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`/etc/rkt/net.d/10-containernet.conf`文件将CNI插件类型设置为Flannel，RKT容器使用此插件。'
- en: 'The following are the steps for this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具体步骤：
- en: Clone the CoreOS Vagrant repository.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆CoreOS Vagrant仓库。
- en: Change the instance count to three in `config.rb`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config.rb`中将实例数更改为三。
- en: Update the discovery token in the `cloud-config` user data.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`cloud-config`用户数据中的发现令牌。
- en: Perform `vagrant up` to start the cluster.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`vagrant up`以启动集群。
- en: 'Let''s start a `busybox` container in `node1`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`node1`中启动一个`busybox`容器：
- en: '![](img/00329.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00329.jpg)'
- en: 'The `ifconfig` output in `busybox node1` is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`busybox node1`中的`ifconfig`输出如下：'
- en: '![](img/00331.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00331.jpg)'
- en: 'Start a `busybox` container in `node2`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在`node2`中启动一个`busybox`容器：
- en: '![](img/00332.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00332.jpg)'
- en: 'The `ifconfig` output in `busybox node2` is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`busybox node2`中的`ifconfig`输出如下：'
- en: '![](img/00334.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00334.jpg)'
- en: 'The following screenshot shows you the successful ping output across containers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了跨容器的成功ping输出：
- en: '![](img/00336.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00336.jpg)'
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: `Docker.service` should not be started with RKT containers as the Docker
    bridge uses the same address that is allocated to Flannel for Docker container
    communication. Active work is going on to support running both Docker and RKT
    containers using Flannel. Some discussion on this topic can be found at [https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc](https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不应在RKT容器中启动`Docker.service`，因为Docker桥接使用的地址与Flannel分配给Docker容器通信的地址相同。目前正在积极研究支持在Flannel上同时运行Docker和RKT容器。有关此主题的讨论，可以参考[https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc](https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc)。
- en: An AWS cluster using Flannel
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel的AWS集群
- en: Flannel can be used to provide Container networking between CoreOS nodes in
    the AWS cloud. In the following two examples, we will create a three-node CoreOS
    cluster in AWS using Flannel with VXLAN and Flannel with AWS VPC networking. These
    examples are based on the procedure described at [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel可以用来在AWS云中的CoreOS节点之间提供容器网络连接。在以下两个示例中，我们将使用Flannel创建一个三节点CoreOS集群，分别使用VXLAN和AWS
    VPC网络。此示例基于[https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)中描述的过程。
- en: An AWS cluster using VXLAN networking
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用VXLAN网络的AWS集群
- en: 'The following are the prerequisities for this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现此目标的前提条件：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务为三节点集群创建一个令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, `2380`, and
    `8472`. `8472` is used for VXLAN encapsulation.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露`ssh`、`icmp`、`2379`、`2380`和`8472`端口。`8472`用于VXLAN封装。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html))
    based on your AWS Zone, and update the channel based on your AWS zone and update
    channel. For the following example, we will use ami-150c1425, which is the latest
    815 alpha image.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的AWS区域，使用此链接（[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)）来确定AMI镜像ID，并根据您的AWS区域和更新频道更新该链接。在以下示例中，我们将使用ami-150c1425，它是最新的815
    alpha镜像。
- en: Create `cloud-config-flannel-vxlan.yaml` with the same content used for the
    Vagrant CoreOS cluster with Flannel and Docker, as specified in the previous section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`cloud-config-flannel-vxlan.yaml`，其内容与前一节中为Vagrant CoreOS集群与Flannel和Docker所使用的内容相同。
- en: 'Use the following AWS CLI to start the three-node cluster:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下AWS CLI启动三节点集群：
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos " --user-data`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro
    --key-name "yourkey" --security-groups "coreos" --user-data`'
- en: We can test connectivity across containers using two `busybox` containers in
    two CoreOS nodes as specified in the previous sections.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两个`busybox`容器在两个CoreOS节点上测试容器之间的连接性，如前面部分所述。
- en: An AWS cluster using AWS-VPC
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS-VPC的AWS集群
- en: AWS VPC provides you with an option to create custom networking for the instances
    created in AWS. With AWS VPC, we can create subnets and route tables and configure
    custom IP addresses for the instances.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: AWS VPC为您提供了为在AWS中创建的实例创建自定义网络的选项。通过AWS VPC，我们可以创建子网和路由表，并为实例配置自定义IP地址。
- en: Flannel supports the encapsulation type, `aws-vpc`. When using this option,
    Flannel updates the VPC route table to route between instances by creating a custom
    route table per VPC based on the container IP addresses allocated to the individual
    node. From a data path perspective, there is no encapsulation such as UDP or VXLAN
    that's used. Instead, AWS VPC takes care of routing the packets to the appropriate
    instance using the route table configured by Flannel.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel支持`aws-vpc`封装类型。在使用此选项时，Flannel会更新VPC路由表，通过为每个VPC创建一个基于分配给各个节点的容器IP地址的自定义路由表，在实例之间进行路由。从数据路径角度来看，不使用像UDP或VXLAN这样的封装方式。相反，AWS
    VPC会根据Flannel配置的路由表，负责将数据包路由到适当的实例。
- en: 'The following are the steps to create the cluster:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建集群的步骤：
- en: Create a discovery token for the three-node cluster.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为三节点集群创建一个发现令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露`ssh`、`icmp`、`2379`和`2380`端口。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)).
    For the following example, we will use `ami-150c1425`, which is the latest 815
    alpha image.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此链接来确定AMI镜像ID（[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)）。在以下示例中，我们将使用`ami-150c1425`，它是最新的815
    alpha镜像。
- en: 'Create a VPC using the VPC wizard with a single public subnet. The following
    diagram shows you the VPC created from the AWS console:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用VPC向导创建一个VPC，带有一个公共子网。以下图表显示了通过AWS控制台创建的VPC：
- en: '![](img/00338.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00338.jpg)'
- en: 'Create an IAM policy, `demo-policy`, from the AWS console. This policy allows
    the instance to modify routing tables:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从AWS控制台创建一个IAM策略`demo-policy`。此策略允许实例修改路由表：
- en: '`{     "Version": "2012-10-17",     "Statement": [     {             "Effect": "Allow",
                "Action": [                 "ec2:CreateRoute",                 "ec2:DeleteRoute",
                    "ec2:ReplaceRoute"             ],             "Resource": [                 "*"
                ]     },     {             "Effect": "Allow",             "Action": [
                    "ec2:DescribeRouteTables",                 "ec2:DescribeInstances"
                ],             "Resource": "*"     }     ] }`'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`{     "Version": "2012-10-17",     "Statement": [     {             "Effect": "Allow",
                "Action": [                 "ec2:CreateRoute",                 "ec2:DeleteRoute",
                    "ec2:ReplaceRoute"             ],             "Resource": [                 "*"
                ]     },     {             "Effect": "Allow",             "Action": [
                    "ec2:DescribeRouteTables",                 "ec2:DescribeInstances"
                ],             "Resource": "*"     }     ] }`'
- en: Create an IAM role, `demo-role`, and associate `demo-policy` created in the
    preceding code with this role.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 IAM 角色 `demo-role`，并将前面代码中创建的 `demo-policy` 与该角色关联。
- en: 'Create `cloud-config-flannel-aws.yaml` with the following content. We will
    use the type as `aws-vpc`, as shown in the following code:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-aws.yaml` 文件，内容如下。我们将使用类型 `aws-vpc`，如下面的代码所示：
- en: '`Cloud-config-flannel-aws.yaml: #cloud-config coreos:   etcd2:     discovery: <your token>
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "aws-vpc"}}''
          command: start`'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Cloud-config-flannel-aws.yaml: #cloud-config coreos:   etcd2:     discovery: <your token>
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "aws-vpc"}}''
          command: start`'
- en: 'Create a three-node CoreOS cluster with a security group, IAM role, `vpcid/subnetid`,
    `security group`, and `cloud-config` file as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个三节点的 CoreOS 集群，并且配置一个安全组、IAM 角色、`vpcid/subnetid`、`安全组` 和 `cloud-config`
    文件，如下所示：
- en: '`aws ec2 run-instances --image-id ami-150c1425 --subnet subnet-a58fc5c0 --associate-public-ip-address --iam-instance-profile Name=demo-role --count 3 --security-group-ids sg-f22cb296 --instance-type t2.micro --key-name "smakam-oregon"  --user-data file://cloud-config-flannel-aws.yaml`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-150c1425 --subnet subnet-a58fc5c0 --associate-public-ip-address
    --iam-instance-profile Name=demo-role --count 3 --security-group-ids sg-f22cb296
    --instance-type t2.micro --key-name "smakam-oregon" --user-data file://cloud-config-flannel-aws.yaml`'
- en: Note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: It is necessary to disable the source and destination checks to allow
    traffic from containers as the IP address for the containers is allocated by flannel
    and not by AWS. To do this, we need to go to each instance in the AWS console
    and select Networking | change source/dest check | disable.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了允许来自容器的流量（因为容器的 IP 地址由 Flannel 分配，而不是 AWS 分配），需要禁用源和目标检查。为此，您需要进入 AWS 控制台中的每个实例，选择网络
    | 更改源/目标检查 | 禁用。
- en: Looking at the `etcdctl` output in one of the CoreOS nodes, we can see the following
    subnets allocated to each node of the three-node cluster.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 查看其中一个 CoreOS 节点上的 `etcdctl` 输出，我们可以看到分配给三节点集群中每个节点的子网。
- en: '![](img/00339.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00339.jpg)'
- en: 'Flannel will go ahead and update the VPC route table to route the preceding
    subnets based on the instance ID on which the subnets are present. If we check
    the VPC route table, we can see the following routes, which match the networks
    created by Flannel:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 会自动更新 VPC 路由表，根据子网所在的实例 ID 路由前述子网。如果检查 VPC 路由表，可以看到以下路由，它们与 Flannel
    创建的网络相匹配：
- en: '![](img/00343.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00343.jpg)'
- en: At this point, we can test connectivity across containers using two `busybox`
    containers in two CoreOS nodes, as specified in the previous sections.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用前面章节中指定的两个 CoreOS 节点中的两个 `busybox` 容器来测试容器间的连接性。
- en: A GCE cluster using Flannel
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Flannel 的 GCE 集群
- en: Flannel can be used to provide Container networking between CoreOS nodes in
    the GCE cloud. In the following two examples, we will create a three-node CoreOS
    cluster using Flannel with VXLAN and Flannel with GCE networking. These examples
    are based on the procedure described at [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 可用于在 GCE 云中的 CoreOS 节点之间提供容器网络。在以下两个示例中，我们将使用 Flannel 和 VXLAN，以及 Flannel
    和 GCE 网络创建一个三节点 CoreOS 集群。这些示例基于 [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)
    中描述的过程。
- en: GCE cluster using VXLAN networking
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VXLAN 网络的 GCE 集群
- en: 'The following are the prerequisities for this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此操作的前提条件：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务创建三节点集群的令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, `2380`, and
    `8472`. `8472` is used for VXLAN encapsulation.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露 `ssh`、`icmp`、`2379`、`2380` 和 `8472` 端口。`8472` 用于 VXLAN 封装。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)).
    We will use alpha image 815 for the following example.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下链接来确定 AMI 镜像 ID ([https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html))。我们将在以下示例中使用
    alpha 镜像 815。
- en: Create `cloud-config-flannel-vxlan.yaml` with the same content that was used
    for the Vagrant CoreOS cluster with Flannel and Docker specified in the previous
    section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-vxlan.yaml` 文件，内容与前一部分中指定的 Vagrant CoreOS 集群的 Flannel
    和 Docker 配置相同。
- en: 'The following command can be used to set up a three-node CoreOS cluster in
    GCE with Flannel and VXLAN encapsulation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可用于在 GCE 中设置一个包含 Flannel 和 VXLAN 封装的三节点 CoreOS 集群：
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --tags coreos --metadata-from-file user-data=cloud-config-flannel-vxlan.yaml`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --tags coreos --metadata-from-file user-data=cloud-config-flannel-vxlan.yaml`'
- en: A ping test across containers in different hosts can be done to verify that
    the Flannel control and data path is working fine.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在不同主机的容器之间进行 ping 测试来验证 Flannel 的控制和数据路径是否正常工作。
- en: A GCE cluster using GCE networking
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GCE 网络的 GCE 集群
- en: Similar to AWS VPC, the Google cloud also has its cloud networking service that
    provides you with the capability to create custom subnets, routes, and IP addresses.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 AWS VPC，Google 云也有其云网络服务，提供创建自定义子网、路由和 IP 地址的能力。
- en: 'The following are the steps to create a three-node CoreOS cluster using flannel
    and GCE networking:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 Flannel 和 GCE 网络创建三节点 CoreOS 集群的步骤：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务创建三节点集群的令牌。
- en: 'Create a custom network, `customnet`, with firewall rules allowing TCP ports
    `2379` and `2380`. The following is the custom network that I created with subnet
    `10.10.0.0/16`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自定义网络 `customnet`，并设置防火墙规则，允许 TCP 端口 `2379` 和 `2380`。以下是我创建的自定义网络，子网为 `10.10.0.0/16`：
- en: '![](img/00344.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00344.jpg)'
- en: 'Create `cloud-config-flannel-gce.yaml` with the following content. Use the
    Flannel type as `gce`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-gce.yaml` 文件，内容如下。将 Flannel 类型设置为 `gce`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <yourtoken>     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "gce"}}''
          command: start`'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create three CoreOS instances with a `customnet` network, IP forwarding turned
    on, and scope for the instance to modify the route table:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --network customnet --can-ip-forward --scopes compute-rw --metadata-from-file user-data=cloud-config-flannel-gce.yaml`'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following are the Flannel networks for containers created by each node:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00346.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the routing table in GCE. As shown by the following output,
    Flannel has updated the GCE route table for the container networks:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00348.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: At this point, we should have Container connectivity across nodes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Experimental multitenant networking
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: By default, Flannel creates a single network, and all the nodes can communicate
    with each other over the single network. This poses a security risk when there
    are multiple tenants using the same network. One approach to achieve multitenant
    networking is using multiple instances of flannel managing each tenant. This can
    get cumbersome to set up. As of version 0.5.3, Flannel has introduced multinetworking
    in the experimental mode, where a single Flannel daemon can manage multinetworks
    with isolation. When there are multiple tenants using the cluster, a multinetwork
    mode would help in isolating each tenant's traffic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps for this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Create subnet configurations for multiple tenants. This can be done by reserving
    a subnet pool in `etcd`. The following example sets up three networks, `blue`,
    `green`, and `red`, each having a different subnet:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`etcdctl set /coreos.com/network/blue/config  ''{ "Network": "10.1.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 } }''``etcdctl set /coreos.com/network/green/config ''{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 2 } }''``etcdctl set /coreos.com/network/red/config   ''{ "Network": "10.3.0.0/16", "Backend": { "Type": "vxlan", "VNI": 3 } }''`'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Start the Flannel agent with the networks that this Flannel agent needs to
    be part of. This will take care of reserving the IP pool per node per network.
    In this example, we have started the flannel agent to be part of all three networks,
    `blue`, `green`, and `red`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo flanneld --networks=blue,green,red &`'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Flannel picked three subnet ranges for the three networks, as shown in the
    following screenshot. `10.1.87.0/24` is allocated for the `blue` network, `10.2.4.0/24`
    is allocated for the `green` network, and `10.3.93.0/24` is allocated for the
    `red` network:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00350.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Under `/run/flannel`, multiple networks can be seen, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00351.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Now, we can start the Docker or Rkt container with the appropriate tenant network
    created. At this point, there is no automatic integration of `flanneld.service`
    with multinetworks; this has to be done manually.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link is a related Google discussion on this topic:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4](https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Experimental client-server networking
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'In the default Flannel mode, there is a flannel agent in each node, and the
    backend data is maintained in `etcd`. This keeps Flannel stateless. In this mode,
    there is a requirement for each Flannel node to run `etcd`. Flannel client-server
    mode is useful in the following scenarios:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Only the master node runs `etcd` and Worker nodes need not run `etcd`. This
    is useful from both the performance and security perspectives.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using other backends such as AWS with flannel, it's necessary to store
    the AWS key, and when using the client-server model, the key can be present in
    the master node only; this is again important from a security perspective.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flannel client-server feature is currently in experimental mode as of Flannel
    version 0.5.3.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure describes the interconnection between different components
    for the Flannel client-server networking:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00353.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: If necessary, we can use secure (HTTPS) means of communication both from the
    flanneld server to the etcd as well as between the flanneld client and server.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Setting up client-server Flannel networking
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a three-node CoreOS cluster without Flannel running on any
    node. Start the `flanneld` server and client in `node1` and client in `node2`
    and `node3`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Start flannel server as shown in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00354.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Start flannel client as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'It is necessary to specify the interface with `eth1` as an argument as `eth0`
    is used as the NAT interface and is common across all nodes, eth1 is unique across
    nodes:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00357.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'After starting the client in `node2` and `node3`, let''s look at the `etcd`
    output in `node1` showing the three subnets acquired by three CoreOS nodes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00308.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'To start `docker.service` manually, we first need to create `flannel_docker_opts.env`
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '`/usr/bin/docker run --net=host --rm -v /run:/run \``  quay.io/coreos/flannel:0.5.3 \``  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env –i`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image is the created `flannel_docker_opts.env`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00469.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Now, we can start `docker.service`, which uses environment variables in `flannel_docker_opts.env`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Start `docker.service`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo systemctl start docker.service`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the docker bridge gets the IP address in the range allocated
    to this node:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00052.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: This feature is currently experimental. There are plans to add a server failover
    feature in future.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Docker networking
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the Docker networking model to interconnect containers in
    a single host:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00312.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Each Container resides in its own networking namespace and uses a Linux bridge
    on the host machine to talk to each other. More details on Docker networking options
    can be found at [https://docs.docker.com/engine/userguide/networking/dockernetworks/](https://docs.docker.com/engine/userguide/networking/dockernetworks/).
    The following are the networking options available as of Docker release 1.9:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`--net=bridge`: This is the default option that Docker provides, where containers
    connect to the Linux `docker` bridge using a veth pair.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=host`: In this option, there is no new network namespace created for
    the container, and the container shares the same network namespace as the host
    machine.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net= (the container name or ID)`: In this option, the new container shares
    the same network namespace as the specified container in the `net` option. (For
    example: `sudo docker run -ti –name=ubuntu2 –net=container:ubuntu1 ubuntu:14.04
    /bin/bash`. Here, the `ubuntu2` container shares the same network namespace as
    the `ubuntu1` container.)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=none`: In this option, the container does not get allocated a new network
    namespace. Only the loopback interface is created in this case. This option is
    useful in scenarios where we want to create our own networking options for the
    container or where there is no need for connectivity.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--net=overlay`: This option was added in Docker release 1.9 to support overlay
    networking that allows Containers across hosts to be able to talk to each other.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker experimental networking
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: As of Docker release 1.8, Docker did not have a native solution to connect Containers
    across hosts. With the Docker experimental release, we can connect Containers
    across hosts using the Docker native solution as well as external networking plugins
    to connect Containers across hosts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates this:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00447.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the Docker libnetwork solution:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Docker runtime was previously integrated with the networking module and there
    was no way to separate them. Libnetwork is the new networking library that provides
    the networking functionality and is separated from Core Docker. Docker 1.7 release
    has already included the libnetwork and is backward-compatible from the end user's
    perspective.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drivers implement the APIs provided by libnetwork. Docker is leaning towards
    a plugin approach for major functionalities such as Networking, Storage, and Orchestration
    where Docker provides a native solution that can be substituted with technologies
    from other vendors as long as they implement the APIs provided by the common library.
    In this case, Bridge and Overlay are the native Docker networking drivers and
    remote drivers can be implemented by a third party. There are already many remote
    drivers available, such as Weave and Calico.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker experimental networking has the following concepts:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The Docker container attaches to the network using the endpoint or service.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple endpoints share a network. In other words, only endpoints located in
    the same network can talk to each other.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When creating the network, the network driver can be mentioned. This can be
    a Docker-provided driver, such as Overlay, or an external driver, such as Weave
    and Calico.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Libnetwork provides service discovery, where Containers can discover other endpoints
    in the same network. There is a plan in the future to make service discovery a
    plugin. Services can talk to each other using the service name rather than the
    IP address. Currently, Consul is used for service discovery; this might change
    later.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared storage such as `etcd` or `consul` is used to determine the nodes that
    are part of the same cluster.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multinetwork use case
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'With the latest Docker networking enhancements, Containers can be part of multiple
    networks and only Containers in the same network can talk to each other. To illustrate
    these concepts, let''s take a look at the following example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Set up two nginx containers and one HAProxy Container in the backend network,
    `be`.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the HAProxy Container in the frontend network, `fe`, as well.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect to the HAProxy Container using the busybox Container in the frontend
    network, `fe`. As the busybox Container is in the `fe` network and nginx Containers
    are in the `be` network, they cannot talk to each other directly.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Haproxy Container will load balance the web connection between the two nginx
    backend Containers.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are the command details:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `fe` and `be` networks:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`docker network create be``docker network create fe`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Create two nginx containers in the `be` network:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run --name nginx1 --net be -v ~/haproxy/nginx1.html:/usr/share/nginx/html/index.html -d nginx``docker run --name nginx2 --net be -v ~/haproxy/nginx2.html:/usr/share/nginx/html/index.html -d nginx`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Create `haproxy` in the `be` network:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d --name haproxy --net be -v ~/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg haproxy`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Attach `haproxy` to the `fe` network:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '`docker network connect fe haproxy`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a busybox container in the `fe` network accessing the `haproxy` web
    page:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -it --rm --net fe busybox wget -qO- haproxy/index.html`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: If we try running the busybox container multiple times, it will switch between
    `nginx1` and `nginx2` web server outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The Docker overlay driver
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows you how to do multihost container connectivity using
    the Docker experimental overlay driver. I have used a Ubuntu VM for the following
    example and not CoreOS because the experimental docker overlay driver needs a
    new kernel release, which is not yet available in CoreOS.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates the use case that is being tried in this example:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00314.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: 'The following is a summary of the steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Create two hosts with experimental Docker installed.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Consul on both the hosts with one of the hosts acting as the consul
    server. Consul is needed to store common data that is used for inter-container
    communication.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start Docker with Consul as the key store mechanism on both hosts.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create containers with different endpoints on both hosts sharing the same network.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first step is to create 2 host machines with experimental Docker installed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands creates two Docker hosts using docker-machine.
    We have used docker-machine with a custom ISO image for experimental Docker:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev1``docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev2`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Consul on both nodes. The following command shows you how to download
    and install consul:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '`curl -OL https://dl.bintray.com/mitchellh/consul/0.5.2_linux_amd64.zip``unzip 0.5.2_linux_amd64.zip``sudo mv consul /usr/local/bin/`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the consul server and docker daemon with the consul keystore in `node1`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands starts the consul server and Docker daemon with
    the consul agent in `node1`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '`Docker-machine ssh dev1``consul agent -server -bootstrap -data-dir /tmp/consul -bind=192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the consul agent and Docker daemon with the consul keystore in `node2`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands starts the consul agent and Docker daemon with
    the consul agent in `node2`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '`Docker-machine ssh dev2``consul agent -data-dir /tmp/consul -bind 192.168.99.101 &``consul join 192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1 --label=com.docker.network.driver.overlay.neighbor_ip=192.168.99.100`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the container with the `svc1` service, `dev` network, and `overlay` driver
    in `node1`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -i -t --publish-service=svc1.dev.overlay busybox`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the container with the `svc2` service, `dev` network, and `overlay` driver
    in node2:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -i -t --publish-service=svc2.dev.overlay busybox`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, we are able to ping `svc1` and `svc2` from `node1` successfully:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00491.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The overlay driver needs the Linux kernel version 3.16 or higher.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The external networking calico plugin
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will illustrate how to do Container networking using Calico
    as a plugin to the Docker libnetwork. This support was available originally in
    experimental networking and later in the Docker 1.9 release. More details about
    the Calico networking approach are mentioned in the following Calico networking
    section. This example is based on [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md).
    To set up a CoreOS Vagrant cluster for Calico, we can use the procedure in [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting up a Vagrant CoreOS cluster, we can see the two nodes of the
    CoreOS cluster. We should make sure that `etcd` is running successfully, as shown
    in the following output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00318.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'The following are the steps to get Calico working with Docker as a networking
    plugin:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Start Calico in both nodes with the libnetwork option:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo calicoctl node --libnetwork`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'We should see the following Docker containers in both nodes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Create the `net1` network with Calico driver:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '`docker network create --driver=calico --subnet=192.168.0.0/24 net1`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'This gets replicated to all the nodes in the cluster. The following is the
    network list in `node2`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00320.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: 'Create container 1 in `node1` with the `net1` network:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker run --net net1 --name workload-A -tid busybox`'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create container 2 in `node2` with the `net1` network:'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker run --net net1 --name workload-B -tid busybox`'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can ping the two containers as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: The Docker 1.9 update
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Docker 1.9 got released at the end of October 2015 that transitioned the experimental
    networking into production. There could be minor modifications necessary to the
    Docker networking examples in this chapter, which were tried with the Docker 1.8
    experimental networking version.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: With Docker 1.9, multihost networking is integrated with Docker Swarm and Compose.
    This allows us to orchestrate a multicontainer application spread between multiple
    hosts with a single command and the multi-host Container networking will be handled
    automatically.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Other Container networking technologies
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Weave and Calico are open source projects, and they develop Container networking
    technologies for Docker. Kubernetes is a Container orchestration open source project
    and it has specific networking requirements and implementations for Containers.
    There are also other projects such as Cisco Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin))
    that is targeted at Container networking. Container technologies like Weave, Calico
    and Contiv have plans to integrate with Rkt Container runtime in the future.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Weave networking
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Weaveworks has developed a solution to provide Container networking. The following
    are some details of their solution:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Weave creates a Weave bridge as well as a Weave router in the host machine.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Weave router establishes both TCP and UDP connections across hosts to other
    Weave routers. A TCP connection is used for discovery- and protocol-related exchange.
    UDP is used for data encapsulation. Encryption can be done if necessary.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Weave bridge is configured to sniff the packets that need to be sent across
    hosts and redirected to the Weave router. For local switching, the Weave router
    is not used.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave's Weavenet product provides you with container connectivity. They also
    have Weavescope that provides container visibility and Weaverun that provides
    service discovery and load balancing.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave is also available as a Docker plugin integrated with the Docker release
    1.9.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the solution from Weave:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: To run Weave on CoreOS, I used cloud-config from [https://github.com/lukebond/coreos-vagrant-weave](https://github.com/lukebond/coreos-vagrant-weave).
    In the following example, we will create containers in two CoreOS nodes and use
    Weave to communicate with each other. In this example, we have not used the Docker
    Weave plugin but used environment variables to communicate between Docker and
    Weave.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to create a Weave cluster:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Clone the repository (`git clone https://github.com/lukebond/coreos-vagrant-weave.git`).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the number of instances in `config.rb` to `3`.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get a new discovery token for node count 3 and update it in the user data.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform `vagrant up` to start the cluster.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `cloud-config` file takes care of downloading Weave agents in each node
    and starting them.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The following section of the service file downloads the Weave container:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00421.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'The following section of the service file starts the Weave container:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00340.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: 'On each of the nodes, we can see the following Weave containers started:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00278.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Before starting application containers, we need to set the environment variables
    so that Weave can intercept Docker commands and create their own networking. As
    part of starting Weave in `Weave.service`, environment variables have already
    been set up. The following command in the node shows this:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00292.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Source the Weave environment as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00366.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start busybox containers in two CoreOS nodes:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00315.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the Weave interface created in the busybox container of CoreOS
    node1:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00325.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the Weave interface created in the busybox container of CoreOS
    node2:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00374.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: Now, we can successfully ping between the two containers. As part of Docker
    1.9, Weave is available as a Docker networking plugin and this makes configuration
    much easier.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Calico networking
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Calico provides you with a Container networking solution for Docker similar
    to Weave. The following are some details of Calico''s implementation:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Calico provides container networking directly at L3 without using overlay technologies
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico uses BGP for route distribution
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two components of Calico: BIRD, which is used for route distribution
    and FELIX, which is an agent in each node that does discovery and routing'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico is also available as a Docker networking plugin integrated with Docker
    release 1.9
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure illustrates the Calico data path:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00365.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: Setting up Calico with CoreOS
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: I followed the procedure at [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md)
    to set up a two-node CoreOS cluster.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is checking out the repository:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/projectcalico/calico-docker.git`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three approaches described by Calico for Docker networking:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Powerstrip: Calico uses an HTTP proxy to listen to Docker calls and set up
    networking.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Default networking: Docker Containers are set up with no networking. Using
    Calico, network endpoints are added and networking is set up.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Libnetwork: Calico is integrated with Docker libnetwork as of Docker release
    1.9\. This will be the long-term solution.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following example, we have used the default networking approach to set
    up Container connectivity using Calico.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps needed to set up the default networking option
    with Calico:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Start calicoctl in all the nodes.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the containers with the `--no-net` option.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach the calico network specifying the IP address to each container.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a policy profile. Profiles set up the policy that allows containers to
    talk to each other.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attach profiles to the container.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following commands set up a container in `node1` and `node2` and establish
    a policy that allows containers to talk to each other.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands on `node1`:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run --net=none --name workload-A -tid busybox``sudo calicoctl container add workload-A 192.168.0.1``calicoctl profile add PROF_A_B``calicoctl container workload-A profile append PROF_A_B`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: This starts the docker container, attaches the calico endpoint, and applies
    the profile to allow Container connectivity.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands on `node2`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run --net=none --name workload-B -tid busybox``sudo calicoctl container add workload-B 192.168.0.2``calicoctl container workload-B profile append PROF_A_B`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: This starts the docker container, attaches the calico endpoint, and applies
    the same profile as in the preceding commands to allow Container connectivity.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can test intercontainer connectivity:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00383.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Kubernetes networking
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is a Container orchestration service. Kubernetes is an open source
    project that's primarily driven by Google. We will discuss about Kubernetes in
    the next chapter on Container orchestration. In this chapter, we will cover some
    of the Kubernetes networking basics.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some details as to how Kubernetes does the networking of
    containers:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has a concept called Pods, which is a collection of closely-tied
    containers. For example, a service and its logging service can be a single pod.
    Another example of a pod could be a service and sidekick service that checks the
    health of the main service. A single pod with its associated containers is always
    scheduled on one machine.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pod gets an IP address. All containers within a pod share the same IP address.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers within a pod share the same network namespace. For containers within
    a pod to communicate, they can use a regular process-based communication.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods can communicate with each other using a cloud networking VPC-based approach
    or container networking solution such as Flannel, Weave, or Calico.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As pods are ephemeral, Kubernetes has a unit called service. Each service has
    an associated virtual IP address and proxy agent running on the nodes' load balancers
    and directs traffic to the right pod.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an illustration of how Pods and Containers are related and
    how they communicate:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00403.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered different Container networking technologies with
    a focus on Container networking in CoreOS. There are many companies trying to
    solve this Container networking problem. CNI and Flannel have become the default
    for CoreOS and Libnetwork has become the default for Docker. Having standards
    and pluggable networking architecture is good for the industry as this allows
    interoperability. Container networking is still in the early stages, and it will
    take some time for the technologies to mature in this area. In the next chapter,
    we will discuss about CoreOS storage management.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Flannel docs: [https://coreos.com/flannel/docs/latest/](https://coreos.com/flannel/docs/latest/)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flannel GitHub page: [https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNI spec: [https://github.com/appc/cni/blob/master/SPEC.md](https://github.com/appc/cni/blob/master/SPEC.md)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flannel with AWS and GCE: [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weaveworks: [https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Libnetwork: [https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker experimental: [https://github.com/docker/docker/tree/master/experimental](https://github.com/docker/docker/tree/master/experimental)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calico: [https://github.com/projectcalico/calico-docker](https://github.com/projectcalico/calico-docker)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes: [http://kubernetes.io/](http://kubernetes.io/)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'The Flannel CoreOS Fest presentation: [https://www.youtube.com/watch?v=_HYeSaGtEYw](https://www.youtube.com/watch?v=_HYeSaGtEYw)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Calico and Weave presentation: [https://giantswarm.io/events/2015-04-20-docker-coreos/](https://giantswarm.io/events/2015-04-20-docker-coreos/)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contiv netplugin: [https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes networking: [https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md](https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
