- en: Chapter 5. CoreOS Networking and Flannel Internals
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章 CoreOS 网络和 Flannel 内部结构
- en: 'Microservices increased the need to have lots of containers and also connectivity
    between containers across hosts. It is necessary to have a robust Container networking
    scheme to achieve this goal. This chapter will cover the basics of Container networking
    with a focus on how CoreOS does Container networking with Flannel. Docker networking
    and other related container networking technologies will also be covered. The
    following topics will be covered in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务增加了对大量容器以及容器跨主机互联的需求。为了实现这一目标，必须有一个健壮的容器网络方案。本章将介绍容器网络的基础知识，重点讨论 CoreOS 如何通过
    Flannel 实现容器网络。还将涵盖 Docker 网络和其他相关的容器网络技术。本章将包括以下主题：
- en: Container networking basics
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器网络基础
- en: Flannel internals
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel 内部结构
- en: A CoreOS Flannel cluster using Vagrant, AWS, and GCE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Vagrant、AWS 和 GCE 搭建的 CoreOS Flannel 集群
- en: Docker networking and experimental Docker networking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 网络和实验性 Docker 网络
- en: Docker networking using Weave and Calico
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Weave 和 Calico 进行 Docker 网络连接
- en: Kubernetes networking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Container networking basics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 容器网络基础
- en: 'The following are the reasons why we need Container networking:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要容器网络的原因：
- en: Containers need to talk to the external world.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器需要与外部世界进行通信。
- en: Containers should be reachable from the external world so that the external
    world can use the services that Containers provide.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器应该可以从外部世界访问，以便外部世界可以使用容器提供的服务。
- en: Containers need to talk to the host machine. An example can be sharing volumes.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器需要与主机进行通信。例如，可以共享卷。
- en: There should be inter-container connectivity in the same host and across hosts.
    An example is a WordPress container in one host talking to a MySQL container in
    another host.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一主机和不同主机之间应具有容器互联。例如，一个主机中的 WordPress 容器与另一个主机中的 MySQL 容器进行通信。
- en: Multiple solutions are currently available to interconnect Containers. These
    solutions are pretty new and actively under development. Docker, until release
    1.8, did not have a native solution to interconnect Containers across hosts. Docker
    release 1.9 introduced a Libnetwork-based solution to interconnect containers
    across hosts as well as perform service discovery. CoreOS is using Flannel for
    container networking in CoreOS clusters. There are projects such as Weave and
    Calico that are developing Container networking solutions, and they plan to be
    a networking container plugin for any Container runtime such as Docker or Rkt.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有多种解决方案可以实现容器之间的互联。这些解决方案相对较新，且正在积极开发中。在 Docker 1.8 版本之前，Docker 并没有提供原生的跨主机容器互联方案。Docker
    1.9 版本引入了一种基于 Libnetwork 的方案，用于实现跨主机容器互联以及服务发现。CoreOS 在 CoreOS 集群中使用 Flannel 进行容器网络连接。还有一些项目，如
    Weave 和 Calico，正在开发容器网络解决方案，计划成为任何容器运行时（如 Docker 或 Rkt）的网络容器插件。
- en: Flannel
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel
- en: Flannel is an open source project that provides a Container networking solution
    for CoreOS clusters. Flannel can also be used for non-CoreOS clusters. Kubernetes
    uses Flannel to set up networking between the Kubernetes pods. Flannel allocates
    a separate subnet for every host where a Container runs, and the Containers in
    this host get allocated an individual IP address from the host subnet. An overlay
    network is set up between each host that allows Containers on different hosts
    to talk to each other. In [Chapter 1](index_split_023.html#filepos77735), CoreOS
    Overview we provided an overview of the Flannel control and data path. This section
    will delve into the Flannel internals.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 是一个开源项目，提供 CoreOS 集群的容器网络解决方案。Flannel 也可以用于非 CoreOS 集群。Kubernetes 使用
    Flannel 设置 Kubernetes pod 之间的网络连接。Flannel 为每个运行容器的主机分配一个独立的子网，并从该主机的子网中为容器分配一个独立的
    IP 地址。在每个主机之间建立一个覆盖网络，使得不同主机上的容器可以互相通信。在[第1章](index_split_023.html#filepos77735)
    CoreOS 概述中，我们提供了 Flannel 控制路径和数据路径的概述。本节将深入探讨 Flannel 的内部结构。
- en: Manual installation
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 手动安装
- en: 'Flannel can be installed manually or using the `systemd` unit, `flanneld.service`.
    The following command will install Flannel in the CoreOS node using a container
    to build the flannel binary. The flanneld Flannel binary will be available in
    `/home/core/flannel/bin` after executing the following commands:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 可以通过手动安装或使用 `systemd` 单元 `flanneld.service` 来安装。以下命令将在 CoreOS 节点上使用容器构建
    Flannel 二进制文件。执行完以下命令后，Flannel 的 flanneld 二进制文件将在 `/home/core/flannel/bin` 中可用：
- en: '`git clone https://github.com/coreos/flannel.git``docker run -v /home/core/flannel:/opt/flannel -i -t google/golang /bin/bash -c "cd /opt/flannel && ./build"`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/flannel.git``docker run -v /home/core/flannel:/opt/flannel -i -t google/golang /bin/bash -c "cd /opt/flannel && ./build"`'
- en: 'The following is the Flannel version after we build flannel in our CoreOS node:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在CoreOS节点中构建flannel后获得的Flannel版本：
- en: '![](img/00132.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00132.jpg)'
- en: Installation using flanneld.service
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用flanneld.service进行安装
- en: Flannel is not installed by default in CoreOS. This is done to keep the CoreOS
    image size to a minimum. Docker requires flannel to configure the network and
    flannel requires Docker to download the flannel container. To avoid this chicken-and-egg
    problem, `early-docker.service` is started by default in CoreOS, whose primary
    purpose is to download the flannel container and start it. A regular `docker.service`
    starts the Docker daemon with the flannel network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel在CoreOS中默认没有安装。这样做是为了保持CoreOS镜像的最小化。Docker需要flannel来配置网络，而flannel需要Docker来下载flannel容器。为了避免这种“先有鸡还是先有蛋”的问题，CoreOS中默认启动了`early-docker.service`，其主要目的是下载并启动flannel容器。一个常规的`docker.service`则启动Docker守护进程并使用flannel网络。
- en: 'The following figure shows you the sequence in `flanneld.service`, where the
    early Docker daemon starts the flannel container, which, in turn starts `docker.service`
    with the subnet created by flannel:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了`flanneld.service`中的序列，其中早期的Docker守护进程启动了flannel容器，进而启动了`docker.service`，并使用flannel创建的子网：
- en: '![](img/00136.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00136.jpg)'
- en: 'The following is the relevant section of `flanneld.service` that downloads
    the flannel container from the Quay repository:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flanneld.service`的相关部分，它从Quay仓库下载flannel容器：
- en: '![](img/00285.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00285.jpg)'
- en: 'The following output shows the early docker''s running containers. Early-docker
    will manage Flannel only:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了早期docker运行的容器。Early-docker只管理Flannel：
- en: '![](img/00142.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.jpg)'
- en: 'The following is the relevant section of `flanneld.service` that updates the
    docker options to use the subnet created by flannel:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flanneld.service`的相关部分，它更新了Docker选项以使用flannel创建的子网：
- en: '![](img/00146.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.jpg)'
- en: 'The following is the content of `flannel_docker_opts.env`—in my case—after
    flannel was started. The address, `10.1.60.1/24`, is chosen by this CoreOS node
    for its containers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`flannel_docker_opts.env`的内容——在我的案例中——flannel启动后的内容。地址`10.1.60.1/24`是这个CoreOS节点为其容器选择的：
- en: '![](img/00042.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00042.jpg)'
- en: 'Docker will be started as part of `docker.service`, as shown in the following
    screenshot, with the preceding environment file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Docker将作为`docker.service`的一部分启动，如下图所示，并使用上述环境文件：
- en: '![](img/00151.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00151.jpg)'
- en: Control path
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 控制路径
- en: There is no central controller in flannel, and it uses etcd for internode communication.
    Each node in the CoreOS cluster runs a flannel agent and they communicate with
    each other using etcd.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel中没有中央控制器，它使用etcd进行节点间的通信。CoreOS集群中的每个节点都运行一个flannel代理，它们通过etcd进行相互通信。
- en: As part of starting the Flannel service, we specify the Flannel subnet that
    can be used by the individual nodes in the network. This subnet is registered
    with etcd so that every CoreOS node in the cluster can see it. Each node in the
    network picks a particular subnet range and registers atomically with etcd.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为启动Flannel服务的一部分，我们指定了可以由网络中各个节点使用的Flannel子网。这个子网注册在etcd中，确保集群中的每个CoreOS节点都能看到它。网络中的每个节点选择一个特定的子网范围，并通过etcd原子地进行注册。
- en: 'The following is the relevant section of `cloud-config` that starts `flanneld.service`
    along with specifying the configuration for Flannel. Here, we have specified the
    subnet to be used for flannel as `10.1.0.0/16` along with the encapsulation type
    as `vxlan`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`cloud-config`的相关部分，它启动了`flanneld.service`并指定了Flannel的配置。在这里，我们指定了用于flannel的子网为`10.1.0.0/16`，并将封装类型指定为`vxlan`：
- en: '![](img/00155.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00155.jpg)'
- en: 'The preceding configuration will create the following etcd key as seen in the
    node. This shows that `10.1.0.0/16` is allocated for flannel to be used across
    the CoreOS cluster and that the encapsulation type is `vxlan`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置将在节点中创建以下etcd键。它显示了`10.1.0.0/16`已分配给flannel，供CoreOS集群使用，并且封装类型为`vxlan`：
- en: '![](img/00309.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00309.jpg)'
- en: 'Once each node gets a subnet, containers started in this node will get an IP
    address from the IP address pool allocated to the node. The following is the etcd
    subnet allocation per node. As we can see, all the subnets are in the `10.1.0.0/16`
    range that was configured earlier with etcd and with a 24-bit mask. The subnet
    length per host can also be controlled as a flannel configuration option:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦每个节点获取了子网，该节点中启动的容器将从分配给该节点的IP地址池中获取IP地址。以下是每个节点的etcd子网分配情况。如我们所见，所有子网都位于之前与etcd配置的`10.1.0.0/16`范围内，且使用了24位掩码。每个主机的子网长度也可以作为Flannel配置选项进行控制：
- en: '![](img/00161.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00161.jpg)'
- en: 'Let''s look at `ifconfig` of the Flannel interface created in this node. The
    IP address is in the address range of `10.1.0.0/16`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下在此节点中创建的Flannel接口的`ifconfig`。该IP地址属于`10.1.0.0/16`地址范围：
- en: '![](img/00165.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00165.jpg)'
- en: Data path
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据路径
- en: Flannel uses the Linux bridge to encapsulate the packets using an overlay protocol
    specified in the Flannel configuration. This allows for connectivity between containers
    in the same host as well as across hosts.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel使用Linux桥接来封装使用Flannel配置中指定的覆盖协议的数据包。这使得同一主机内的容器之间以及跨主机的容器之间能够实现连通性。
- en: 'The following are the major backends currently supported by Flannel and specified
    in the JSON configuration file. The JSON configuration file can be specified in
    the Flannel section of `cloud-config`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前Flannel支持的主要后端，并在JSON配置文件中指定。JSON配置文件可以在`cloud-config`的Flannel部分中指定：
- en: 'UDP: In UDP encapsulation, packets from containers are encapsulated in UDP
    with the default port number `8285`. We can change the port number if needed.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDP：在UDP封装中，来自容器的数据包会被封装到UDP中，默认端口号为`8285`。如果需要，我们可以更改端口号。
- en: 'VXLAN: From an encapsulation overhead perspective, VXLAN is efficient when
    compared to UDP. By default, port `8472` is used for VXLAN encapsulation. If we
    want to use an IANA-allocated VXLAN port, we need to specify the port field as
    `4789`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN：从封装开销的角度来看，VXLAN相对于UDP更高效。默认情况下，端口`8472`用于VXLAN封装。如果我们希望使用IANA分配的VXLAN端口，则需要将端口字段指定为`4789`。
- en: 'AWS-VPC: This is applicable to using Flannel in the AWS VPC cloud. Instead
    of encapsulating the packets using an overlay, this approach uses a VPC route
    table to communicate across containers. AWS limits each VPC route table entry
    to 50, so this can become a problem with bigger clusters.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS-VPC：适用于在AWS VPC云中使用Flannel的情况。与使用覆盖网络封装数据包不同，这种方法通过使用VPC路由表来跨容器进行通信。AWS限制每个VPC路由表条目的数量为50，因此在较大的集群中可能会出现问题。
- en: 'The following is an example of specifying the AWS type in the flannel configuration:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定Flannel配置中AWS类型的示例：
- en: '![](img/00168.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00168.jpg)'
- en: 'GCE: This is applicable to using Flannel in the GCE cloud. Instead of encapsulating
    the packets using an overlay, this approach uses the GCE route table to communicate
    across containers. GCE limits each VPC route table entry to `100`, so this can
    become a problem with bigger clusters.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCE：适用于在GCE云中使用Flannel的情况。与使用覆盖网络封装数据包不同，这种方法通过使用GCE路由表来跨容器进行通信。GCE限制每个VPC路由表条目的数量为`100`，因此在较大的集群中可能会出现问题。
- en: 'The following is an example of specifying the GCE type in the Flannel configuration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是指定Flannel配置中GCE类型的示例：
- en: '![](img/00171.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00171.jpg)'
- en: Let's create containers in two different hosts with a VXLAN encapsulation and
    check whether the connectivity is fine. The following example uses a Vagrant CoreOS
    cluster with the Flannel service enabled.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两台不同主机中创建容器，并使用VXLAN封装，检查连通性是否正常。以下示例使用启用了Flannel服务的Vagrant CoreOS集群。
- en: 'Configuration in Host1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Host1中的配置：
- en: 'Let''s start a `busybox` container:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来启动一个`busybox`容器：
- en: '![](img/00175.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00175.jpg)'
- en: 'Let''s check the IP address allotted to the container. This IP address comes
    from the IP pool allocated to this CoreOS node by the flannel agent. `10.1.19.0/24`
    was allocated to `host1` and this container got the `10.1.19.2` address:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来检查分配给容器的IP地址。该IP地址来自Flannel代理为此CoreOS节点分配的IP池。`10.1.19.0/24`分配给了`host1`，这个容器获得了`10.1.19.2`地址：
- en: '![](img/00177.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00177.jpg)'
- en: 'Configuration in Host2:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Host2中的配置：
- en: 'Let''s start a `busybox` container:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来启动一个`busybox`容器：
- en: '![](img/00180.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00180.jpg)'
- en: 'Let''s check the IP address allotted to this container. This IP address comes
    from the IP pool allocated to this CoreOS node by the flannel agent. `10.1.1.0/24`
    was allocated to `host2` and this container got the `10.1.1.2` address:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下分配给该容器的IP地址。这个IP地址来自Flannel代理为此CoreOS节点分配的IP池。`10.1.1.0/24`被分配给了`host2`，而这个容器获得了`10.1.1.2`的地址：
- en: '![](img/00184.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00184.jpg)'
- en: 'The following output shows you the ping being successful between container
    1 and container 2\. This ping packet is travelling across the two CoreOS nodes
    and is encapsulated using VXLAN:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示容器1和容器2之间的ping操作成功。这个ping包通过两个CoreOS节点传输，并使用VXLAN进行封装：
- en: '![](img/00326.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00326.jpg)'
- en: Flannel as a CNI plugin
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel作为CNI插件
- en: As explained in [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview,
    APPC defines a Container specification that any Container runtime can use. For
    Container networking, APPC defines a Container Network Interface (CNI) specification.
    With CNI, the Container networking functionality can be implemented as a plugin.
    CNI expects plugins to support APIs with a set of parameters and the implementation
    is left to the plugin. The plugin implements APIs like adding a container to a
    network and removing container from the network with a defined parameter list.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第一章](index_split_023.html#filepos77735)《CoreOS概述》所述，APPC定义了一个容器规范，任何容器运行时都可以使用该规范。对于容器网络，APPC定义了容器网络接口（CNI）规范。通过CNI，容器网络功能可以作为插件实现。CNI期望插件支持带有一组参数的API，具体实现则交由插件完成。插件实现的API包括将容器添加到网络中和从网络中移除容器，且有一个定义好的参数列表。
- en: 'This allows the implementation of network plugins by different vendors and
    also the reuse of plugins across different Container runtimes. The following figure
    shows the relationship between the RKT container runtime, CNI layer, and Plugin
    like Flannel. The IPAM Plugin is used to allocate an IP address to the containers
    and this is nested inside the initial networking plugin:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许不同供应商实现网络插件，并且可以在不同的容器运行时之间重用插件。下图显示了RKT容器运行时、CNI层和像Flannel这样的插件之间的关系。IPAM插件用于为容器分配IP地址，这个功能被嵌套在初始网络插件中：
- en: '![](img/00328.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00328.jpg)'
- en: Setting up a three-node Vagrant CoreOS cluster with Flannel and Docker
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel和Docker设置一个三节点Vagrant CoreOS集群
- en: 'The following example sets up a three-node Vagrant CoreOS cluster with the
    `etcd`, `fleet`, and `flannel` services turned on by default. In this example,
    `vxlan` is used for encapsulation. The following is the `cloud-config` used for
    this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例设置了一个三节点的Vagrant CoreOS集群，默认启用了`etcd`、`fleet`和`flannel`服务。在这个示例中，使用`vxlan`进行封装。以下是用于此配置的`cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <update this>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}''
          command: start`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: <update this>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}''
          command: start`'
- en: 'The following are the steps for this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此操作的步骤：
- en: Clone the CoreOS Vagrant repository.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆CoreOS Vagrant仓库。
- en: Change the instance count to three in `config.rb`.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config.rb`中将实例数量更改为三个。
- en: Update the discovery token in the `cloud-config` user data.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cloud-config`用户数据中更新发现令牌。
- en: Perform `vagrant up` to start the cluster.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`vagrant up`来启动集群。
- en: For more details on the steps, refer to [Chapter 2](index_split_048.html#filepos153225),
    Setting up the CoreOS Lab. We can test the container connectivity by starting
    `busybox` containers in both the hosts and checking that the ping is working between
    the two Containers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有关步骤的更多详细信息，请参考[第2章](index_split_048.html#filepos153225)，设置CoreOS实验室。我们可以通过在两个主机中启动`busybox`容器并检查容器间ping是否正常来测试容器的连接性。
- en: Setting up a three-node CoreOS cluster with Flannel and RKT
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel和RKT设置三节点CoreOS集群
- en: Here, we will set up a three-node CoreOS cluster with RKT containers using the
    Flannel CNI networking plugin to set up the networking. This example will allow
    RKT containers across hosts to communicate with each other.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用Flannel CNI网络插件设置一个包含RKT容器的三节点CoreOS集群，以设置网络。这将允许跨主机的RKT容器彼此通信。
- en: 'The following is the `cloud-config` used:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用的`cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <update token>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "network": "10.1.0.0/16" }''
          command: start # Rkt configuration write_files:   - path: "/etc/rkt/net.d/10-containernet.conf"
        permissions: "0644"     owner: "root"     content: |       {         "name": "containernet",
            "type": "flannel"       }`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: <update token>     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "network": "10.1.0.0/16" }''
          command: start # Rkt 配置 write_files:   - path: "/etc/rkt/net.d/10-containernet.conf"
        permissions: "0644"     owner: "root"     content: |       {         "name": "containernet",
            "type": "flannel"       }`'
- en: The `/etc/rkt/net.d/10-containernet.conf` file sets up the CNI plugin type as
    Flannel and RKT containers use this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`/etc/rkt/net.d/10-containernet.conf`文件将CNI插件类型设置为Flannel，RKT容器使用此插件。'
- en: 'The following are the steps for this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具体步骤：
- en: Clone the CoreOS Vagrant repository.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆CoreOS Vagrant仓库。
- en: Change the instance count to three in `config.rb`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`config.rb`中将实例数更改为三。
- en: Update the discovery token in the `cloud-config` user data.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`cloud-config`用户数据中的发现令牌。
- en: Perform `vagrant up` to start the cluster.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`vagrant up`以启动集群。
- en: 'Let''s start a `busybox` container in `node1`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`node1`中启动一个`busybox`容器：
- en: '![](img/00329.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00329.jpg)'
- en: 'The `ifconfig` output in `busybox node1` is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`busybox node1`中的`ifconfig`输出如下：'
- en: '![](img/00331.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00331.jpg)'
- en: 'Start a `busybox` container in `node2`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在`node2`中启动一个`busybox`容器：
- en: '![](img/00332.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00332.jpg)'
- en: 'The `ifconfig` output in `busybox node2` is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`busybox node2`中的`ifconfig`输出如下：'
- en: '![](img/00334.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00334.jpg)'
- en: 'The following screenshot shows you the successful ping output across containers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了跨容器的成功ping输出：
- en: '![](img/00336.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00336.jpg)'
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: `Docker.service` should not be started with RKT containers as the Docker
    bridge uses the same address that is allocated to Flannel for Docker container
    communication. Active work is going on to support running both Docker and RKT
    containers using Flannel. Some discussion on this topic can be found at [https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc](https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不应在RKT容器中启动`Docker.service`，因为Docker桥接使用的地址与Flannel分配给Docker容器通信的地址相同。目前正在积极研究支持在Flannel上同时运行Docker和RKT容器。有关此主题的讨论，可以参考[https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc](https://groups.google.com/forum/#!topic/coreos-user/Kl7ejtcRxbc)。
- en: An AWS cluster using Flannel
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Flannel的AWS集群
- en: Flannel can be used to provide Container networking between CoreOS nodes in
    the AWS cloud. In the following two examples, we will create a three-node CoreOS
    cluster in AWS using Flannel with VXLAN and Flannel with AWS VPC networking. These
    examples are based on the procedure described at [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel可以用来在AWS云中的CoreOS节点之间提供容器网络连接。在以下两个示例中，我们将使用Flannel创建一个三节点CoreOS集群，分别使用VXLAN和AWS
    VPC网络。此示例基于[https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)中描述的过程。
- en: An AWS cluster using VXLAN networking
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用VXLAN网络的AWS集群
- en: 'The following are the prerequisities for this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现此目标的前提条件：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务为三节点集群创建一个令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, `2380`, and
    `8472`. `8472` is used for VXLAN encapsulation.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露`ssh`、`icmp`、`2379`、`2380`和`8472`端口。`8472`用于VXLAN封装。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html))
    based on your AWS Zone, and update the channel based on your AWS zone and update
    channel. For the following example, we will use ami-150c1425, which is the latest
    815 alpha image.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的AWS区域，使用此链接（[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)）来确定AMI镜像ID，并根据您的AWS区域和更新频道更新该链接。在以下示例中，我们将使用ami-150c1425，它是最新的815
    alpha镜像。
- en: Create `cloud-config-flannel-vxlan.yaml` with the same content used for the
    Vagrant CoreOS cluster with Flannel and Docker, as specified in the previous section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`cloud-config-flannel-vxlan.yaml`，其内容与前一节中为Vagrant CoreOS集群与Flannel和Docker所使用的内容相同。
- en: 'Use the following AWS CLI to start the three-node cluster:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下AWS CLI启动三节点集群：
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos " --user-data`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro
    --key-name "yourkey" --security-groups "coreos" --user-data`'
- en: We can test connectivity across containers using two `busybox` containers in
    two CoreOS nodes as specified in the previous sections.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两个`busybox`容器在两个CoreOS节点上测试容器之间的连接性，如前面部分所述。
- en: An AWS cluster using AWS-VPC
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS-VPC的AWS集群
- en: AWS VPC provides you with an option to create custom networking for the instances
    created in AWS. With AWS VPC, we can create subnets and route tables and configure
    custom IP addresses for the instances.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: AWS VPC为您提供了为在AWS中创建的实例创建自定义网络的选项。通过AWS VPC，我们可以创建子网和路由表，并为实例配置自定义IP地址。
- en: Flannel supports the encapsulation type, `aws-vpc`. When using this option,
    Flannel updates the VPC route table to route between instances by creating a custom
    route table per VPC based on the container IP addresses allocated to the individual
    node. From a data path perspective, there is no encapsulation such as UDP or VXLAN
    that's used. Instead, AWS VPC takes care of routing the packets to the appropriate
    instance using the route table configured by Flannel.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel支持`aws-vpc`封装类型。在使用此选项时，Flannel会更新VPC路由表，通过为每个VPC创建一个基于分配给各个节点的容器IP地址的自定义路由表，在实例之间进行路由。从数据路径角度来看，不使用像UDP或VXLAN这样的封装方式。相反，AWS
    VPC会根据Flannel配置的路由表，负责将数据包路由到适当的实例。
- en: 'The following are the steps to create the cluster:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建集群的步骤：
- en: Create a discovery token for the three-node cluster.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为三节点集群创建一个发现令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露`ssh`、`icmp`、`2379`和`2380`端口。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)).
    For the following example, we will use `ami-150c1425`, which is the latest 815
    alpha image.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此链接来确定AMI镜像ID（[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)）。在以下示例中，我们将使用`ami-150c1425`，它是最新的815
    alpha镜像。
- en: 'Create a VPC using the VPC wizard with a single public subnet. The following
    diagram shows you the VPC created from the AWS console:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用VPC向导创建一个VPC，带有一个公共子网。以下图表显示了通过AWS控制台创建的VPC：
- en: '![](img/00338.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00338.jpg)'
- en: 'Create an IAM policy, `demo-policy`, from the AWS console. This policy allows
    the instance to modify routing tables:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从AWS控制台创建一个IAM策略`demo-policy`。此策略允许实例修改路由表：
- en: '`{     "Version": "2012-10-17",     "Statement": [     {             "Effect": "Allow",
                "Action": [                 "ec2:CreateRoute",                 "ec2:DeleteRoute",
                    "ec2:ReplaceRoute"             ],             "Resource": [                 "*"
                ]     },     {             "Effect": "Allow",             "Action": [
                    "ec2:DescribeRouteTables",                 "ec2:DescribeInstances"
                ],             "Resource": "*"     }     ] }`'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`{     "Version": "2012-10-17",     "Statement": [     {             "Effect": "Allow",
                "Action": [                 "ec2:CreateRoute",                 "ec2:DeleteRoute",
                    "ec2:ReplaceRoute"             ],             "Resource": [                 "*"
                ]     },     {             "Effect": "Allow",             "Action": [
                    "ec2:DescribeRouteTables",                 "ec2:DescribeInstances"
                ],             "Resource": "*"     }     ] }`'
- en: Create an IAM role, `demo-role`, and associate `demo-policy` created in the
    preceding code with this role.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 IAM 角色 `demo-role`，并将前面代码中创建的 `demo-policy` 与该角色关联。
- en: 'Create `cloud-config-flannel-aws.yaml` with the following content. We will
    use the type as `aws-vpc`, as shown in the following code:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-aws.yaml` 文件，内容如下。我们将使用类型 `aws-vpc`，如下面的代码所示：
- en: '`Cloud-config-flannel-aws.yaml: #cloud-config coreos:   etcd2:     discovery: <your token>
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "aws-vpc"}}''
          command: start`'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Cloud-config-flannel-aws.yaml: #cloud-config coreos:   etcd2:     discovery: <your token>
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "aws-vpc"}}''
          command: start`'
- en: 'Create a three-node CoreOS cluster with a security group, IAM role, `vpcid/subnetid`,
    `security group`, and `cloud-config` file as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个三节点的 CoreOS 集群，并且配置一个安全组、IAM 角色、`vpcid/subnetid`、`安全组` 和 `cloud-config`
    文件，如下所示：
- en: '`aws ec2 run-instances --image-id ami-150c1425 --subnet subnet-a58fc5c0 --associate-public-ip-address --iam-instance-profile Name=demo-role --count 3 --security-group-ids sg-f22cb296 --instance-type t2.micro --key-name "smakam-oregon"  --user-data file://cloud-config-flannel-aws.yaml`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-150c1425 --subnet subnet-a58fc5c0 --associate-public-ip-address
    --iam-instance-profile Name=demo-role --count 3 --security-group-ids sg-f22cb296
    --instance-type t2.micro --key-name "smakam-oregon" --user-data file://cloud-config-flannel-aws.yaml`'
- en: Note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: It is necessary to disable the source and destination checks to allow
    traffic from containers as the IP address for the containers is allocated by flannel
    and not by AWS. To do this, we need to go to each instance in the AWS console
    and select Networking | change source/dest check | disable.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了允许来自容器的流量（因为容器的 IP 地址由 Flannel 分配，而不是 AWS 分配），需要禁用源和目标检查。为此，您需要进入 AWS 控制台中的每个实例，选择网络
    | 更改源/目标检查 | 禁用。
- en: Looking at the `etcdctl` output in one of the CoreOS nodes, we can see the following
    subnets allocated to each node of the three-node cluster.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 查看其中一个 CoreOS 节点上的 `etcdctl` 输出，我们可以看到分配给三节点集群中每个节点的子网。
- en: '![](img/00339.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00339.jpg)'
- en: 'Flannel will go ahead and update the VPC route table to route the preceding
    subnets based on the instance ID on which the subnets are present. If we check
    the VPC route table, we can see the following routes, which match the networks
    created by Flannel:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 会自动更新 VPC 路由表，根据子网所在的实例 ID 路由前述子网。如果检查 VPC 路由表，可以看到以下路由，它们与 Flannel
    创建的网络相匹配：
- en: '![](img/00343.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00343.jpg)'
- en: At this point, we can test connectivity across containers using two `busybox`
    containers in two CoreOS nodes, as specified in the previous sections.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用前面章节中指定的两个 CoreOS 节点中的两个 `busybox` 容器来测试容器间的连接性。
- en: A GCE cluster using Flannel
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Flannel 的 GCE 集群
- en: Flannel can be used to provide Container networking between CoreOS nodes in
    the GCE cloud. In the following two examples, we will create a three-node CoreOS
    cluster using Flannel with VXLAN and Flannel with GCE networking. These examples
    are based on the procedure described at [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 可用于在 GCE 云中的 CoreOS 节点之间提供容器网络。在以下两个示例中，我们将使用 Flannel 和 VXLAN，以及 Flannel
    和 GCE 网络创建一个三节点 CoreOS 集群。这些示例基于 [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)
    中描述的过程。
- en: GCE cluster using VXLAN networking
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VXLAN 网络的 GCE 集群
- en: 'The following are the prerequisities for this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此操作的前提条件：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务创建三节点集群的令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, `2380`, and
    `8472`. `8472` is used for VXLAN encapsulation.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露 `ssh`、`icmp`、`2379`、`2380` 和 `8472` 端口。`8472` 用于 VXLAN 封装。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)).
    We will use alpha image 815 for the following example.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下链接来确定 AMI 镜像 ID ([https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html))。我们将在以下示例中使用
    alpha 镜像 815。
- en: Create `cloud-config-flannel-vxlan.yaml` with the same content that was used
    for the Vagrant CoreOS cluster with Flannel and Docker specified in the previous
    section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-vxlan.yaml` 文件，内容与前一部分中指定的 Vagrant CoreOS 集群的 Flannel
    和 Docker 配置相同。
- en: 'The following command can be used to set up a three-node CoreOS cluster in
    GCE with Flannel and VXLAN encapsulation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令可用于在 GCE 中设置一个包含 Flannel 和 VXLAN 封装的三节点 CoreOS 集群：
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --tags coreos --metadata-from-file user-data=cloud-config-flannel-vxlan.yaml`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --tags coreos --metadata-from-file user-data=cloud-config-flannel-vxlan.yaml`'
- en: A ping test across containers in different hosts can be done to verify that
    the Flannel control and data path is working fine.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在不同主机的容器之间进行 ping 测试来验证 Flannel 的控制和数据路径是否正常工作。
- en: A GCE cluster using GCE networking
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GCE 网络的 GCE 集群
- en: Similar to AWS VPC, the Google cloud also has its cloud networking service that
    provides you with the capability to create custom subnets, routes, and IP addresses.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 AWS VPC，Google 云也有其云网络服务，提供创建自定义子网、路由和 IP 地址的能力。
- en: 'The following are the steps to create a three-node CoreOS cluster using flannel
    and GCE networking:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 Flannel 和 GCE 网络创建三节点 CoreOS 集群的步骤：
- en: Create a token for the three-node cluster from the discovery token service.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务创建三节点集群的令牌。
- en: 'Create a custom network, `customnet`, with firewall rules allowing TCP ports
    `2379` and `2380`. The following is the custom network that I created with subnet
    `10.10.0.0/16`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自定义网络 `customnet`，并设置防火墙规则，允许 TCP 端口 `2379` 和 `2380`。以下是我创建的自定义网络，子网为 `10.10.0.0/16`：
- en: '![](img/00344.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/00344.jpg)'
- en: 'Create `cloud-config-flannel-gce.yaml` with the following content. Use the
    Flannel type as `gce`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `cloud-config-flannel-gce.yaml` 文件，内容如下。将 Flannel 类型设置为 `gce`：
- en: '`#cloud-config coreos:   etcd2:     discovery: <yourtoken>     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start     - name: flanneld.service
          drop-ins:         - name: 50-network-config.conf           content: |             [Service]
                ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type": "gce"}}''
          command: start`'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: <yourtoken>     advertise-client-urls:
    http://$private_ipv4:2379,http://$private_ipv4:4001     initial-advertise-peer-urls:
    http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls:
    http://$private_ipv4:2380   units:     - name: etcd2.service       command: start     -
    name: fleet.service       command: start     - name: flanneld.service       drop-ins:         -
    name: 50-network-config.conf           content: |             [Service]             ExecStartPre=/usr/bin/etcdctl
    set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" , "Backend": {"Type":
    "gce"}}''       command: start`'
- en: 'Create three CoreOS instances with a `customnet` network, IP forwarding turned
    on, and scope for the instance to modify the route table:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个 CoreOS 实例，使用 `customnet` 网络，启用 IP 转发，并为实例分配修改路由表的权限：
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924 --zone us-central1-a --machine-type n1-standard-1 --network customnet --can-ip-forward --scopes compute-rw --metadata-from-file user-data=cloud-config-flannel-gce.yaml`'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-alpha-815-0-0-v20150924
    --zone us-central1-a --machine-type n1-standard-1 --network customnet --can-ip-forward
    --scopes compute-rw --metadata-from-file user-data=cloud-config-flannel-gce.yaml`'
- en: 'The following are the Flannel networks for containers created by each node:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个节点为容器创建的 Flannel 网络：
- en: '![](img/00346.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00346.jpg)'
- en: 'Let''s look at the routing table in GCE. As shown by the following output,
    Flannel has updated the GCE route table for the container networks:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 GCE 中的路由表。如以下输出所示，Flannel 已经更新了 GCE 的容器网络路由表：
- en: '![](img/00348.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00348.jpg)'
- en: At this point, we should have Container connectivity across nodes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们应该已经能够跨节点连接容器。
- en: Experimental multitenant networking
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 实验性多租户网络
- en: By default, Flannel creates a single network, and all the nodes can communicate
    with each other over the single network. This poses a security risk when there
    are multiple tenants using the same network. One approach to achieve multitenant
    networking is using multiple instances of flannel managing each tenant. This can
    get cumbersome to set up. As of version 0.5.3, Flannel has introduced multinetworking
    in the experimental mode, where a single Flannel daemon can manage multinetworks
    with isolation. When there are multiple tenants using the cluster, a multinetwork
    mode would help in isolating each tenant's traffic.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Flannel 创建一个单一的网络，所有节点都可以在该网络上相互通信。当有多个租户使用相同的网络时，这会带来安全风险。一种实现多租户网络的方式是使用多个
    Flannel 实例来管理每个租户。这可能会变得难以配置。从版本 0.5.3 开始，Flannel 在实验模式下引入了多网络功能，在这种模式下，单个 Flannel
    守护进程可以管理具有隔离性的多个网络。当集群中有多个租户时，使用多网络模式有助于隔离每个租户的流量。
- en: 'The following are the steps for this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行此操作的步骤：
- en: 'Create subnet configurations for multiple tenants. This can be done by reserving
    a subnet pool in `etcd`. The following example sets up three networks, `blue`,
    `green`, and `red`, each having a different subnet:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为多个租户创建子网配置。可以通过在 `etcd` 中预留一个子网池来实现。以下示例设置了三个网络：`blue`、`green` 和 `red`，每个网络都有不同的子网：
- en: '`etcdctl set /coreos.com/network/blue/config  ''{ "Network": "10.1.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 } }''``etcdctl set /coreos.com/network/green/config ''{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 2 } }''``etcdctl set /coreos.com/network/red/config   ''{ "Network": "10.3.0.0/16", "Backend": { "Type": "vxlan", "VNI": 3 } }''`'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`etcdctl set /coreos.com/network/blue/config  ''{ "Network": "10.1.0.0/16",
    "Backend": { "Type": "vxlan", "VNI": 1 } }''``etcdctl set /coreos.com/network/green/config
    ''{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 2 } }''``etcdctl
    set /coreos.com/network/red/config   ''{ "Network": "10.3.0.0/16", "Backend":
    { "Type": "vxlan", "VNI": 3 } }''`'
- en: 'Start the Flannel agent with the networks that this Flannel agent needs to
    be part of. This will take care of reserving the IP pool per node per network.
    In this example, we have started the flannel agent to be part of all three networks,
    `blue`, `green`, and `red`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Flannel代理并指定该Flannel代理需要加入的网络。这样可以为每个节点每个网络预留IP池。在这个例子中，我们已经启动了flannel代理，并使其成为所有三个网络（`blue`、`green`
    和 `red`）的一部分：
- en: '`sudo flanneld --networks=blue,green,red &`'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo flanneld --networks=blue,green,red &`'
- en: 'Flannel picked three subnet ranges for the three networks, as shown in the
    following screenshot. `10.1.87.0/24` is allocated for the `blue` network, `10.2.4.0/24`
    is allocated for the `green` network, and `10.3.93.0/24` is allocated for the
    `red` network:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel为这三个网络选择了三个子网范围，如下图所示。`10.1.87.0/24`分配给`blue`网络，`10.2.4.0/24`分配给`green`网络，`10.3.93.0/24`分配给`red`网络：
- en: '![](img/00350.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00350.jpg)'
- en: 'Under `/run/flannel`, multiple networks can be seen, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在`/run/flannel`下可以看到多个网络，如下所示：
- en: '![](img/00351.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00351.jpg)'
- en: Now, we can start the Docker or Rkt container with the appropriate tenant network
    created. At this point, there is no automatic integration of `flanneld.service`
    with multinetworks; this has to be done manually.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动带有适当租户网络的Docker或Rkt容器。此时，`flanneld.service`与多网络的自动集成尚未完成；这需要手动完成。
- en: 'The following link is a related Google discussion on this topic:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接是关于此话题的相关Google讨论：
- en: '[https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4](https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4](https://groups.google.com/forum/#!topic/coreos-user/EIF-yGNWkL4)'
- en: Experimental client-server networking
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 实验性客户端-服务器网络
- en: 'In the default Flannel mode, there is a flannel agent in each node, and the
    backend data is maintained in `etcd`. This keeps Flannel stateless. In this mode,
    there is a requirement for each Flannel node to run `etcd`. Flannel client-server
    mode is useful in the following scenarios:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的Flannel模式下，每个节点都有一个flannel代理，并且后端数据保存在`etcd`中。这样Flannel保持无状态。在这种模式下，每个Flannel节点都需要运行`etcd`。Flannel客户端-服务器模式在以下场景中非常有用：
- en: Only the master node runs `etcd` and Worker nodes need not run `etcd`. This
    is useful from both the performance and security perspectives.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有主节点运行`etcd`，工作节点无需运行`etcd`。从性能和安全的角度来看，这一点非常有用。
- en: When using other backends such as AWS with flannel, it's necessary to store
    the AWS key, and when using the client-server model, the key can be present in
    the master node only; this is again important from a security perspective.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用其他后端（如AWS）与flannel一起时，必须存储AWS密钥，并且在使用客户端-服务器模型时，密钥只需要在主节点中存在；从安全角度来看，这一点非常重要。
- en: Flannel client-server feature is currently in experimental mode as of Flannel
    version 0.5.3.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel客户端-服务器功能目前在Flannel版本0.5.3中处于实验模式。
- en: 'The following figure describes the interconnection between different components
    for the Flannel client-server networking:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了Flannel客户端-服务器网络中的不同组件之间的互联：
- en: '![](img/00353.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00353.jpg)'
- en: If necessary, we can use secure (HTTPS) means of communication both from the
    flanneld server to the etcd as well as between the flanneld client and server.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们可以使用安全的（HTTPS）通信方式，既可以从flanneld服务器到etcd，也可以在flanneld客户端和服务器之间进行通信。
- en: Setting up client-server Flannel networking
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 设置客户端-服务器Flannel网络
- en: Let's start with a three-node CoreOS cluster without Flannel running on any
    node. Start the `flanneld` server and client in `node1` and client in `node2`
    and `node3`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个没有在任何节点上运行Flannel的三节点CoreOS集群开始。在`node1`上启动`flanneld`服务器和客户端，在`node2`和`node3`上启动客户端。
- en: 'Start flannel server as shown in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下截图启动flannel服务器：
- en: '![](img/00354.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00354.jpg)'
- en: 'Start flannel client as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下截图启动flannel客户端：
- en: 'It is necessary to specify the interface with `eth1` as an argument as `eth0`
    is used as the NAT interface and is common across all nodes, eth1 is unique across
    nodes:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 必须指定`eth1`作为参数接口，因为`eth0`用作NAT接口，并且在所有节点中都相同，而`eth1`在各节点之间是唯一的：
- en: '![](img/00357.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00357.jpg)'
- en: 'After starting the client in `node2` and `node3`, let''s look at the `etcd`
    output in `node1` showing the three subnets acquired by three CoreOS nodes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在`node2`和`node3`中启动客户端后，让我们查看`node1`中`etcd`的输出，显示了三个CoreOS节点获取的三个子网：
- en: '![](img/00308.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00308.jpg)'
- en: 'To start `docker.service` manually, we first need to create `flannel_docker_opts.env`
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动启动`docker.service`，我们首先需要创建`flannel_docker_opts.env`，如下所示：
- en: '`/usr/bin/docker run --net=host --rm -v /run:/run \``  quay.io/coreos/flannel:0.5.3 \``  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env –i`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`/usr/bin/docker run --net=host --rm -v /run:/run \``  quay.io/coreos/flannel:0.5.3  \``  /opt/bin/mk-docker-opts.sh
    -d /run/flannel_docker_opts.env –i`'
- en: 'The following image is the created `flannel_docker_opts.env`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建的`flannel_docker_opts.env`文件：
- en: '![](img/00469.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00469.jpg)'
- en: Now, we can start `docker.service`, which uses environment variables in `flannel_docker_opts.env`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动`docker.service`，它使用`flannel_docker_opts.env`中的环境变量。
- en: 'Start `docker.service`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 启动`docker.service`：
- en: '`sudo systemctl start docker.service`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo systemctl start docker.service`'
- en: 'As we can see, the docker bridge gets the IP address in the range allocated
    to this node:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，docker 桥接获取了分配给此节点的 IP 地址范围：
- en: '![](img/00052.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00052.jpg)'
- en: This feature is currently experimental. There are plans to add a server failover
    feature in future.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此功能目前处于实验阶段。未来计划添加服务器故障转移功能。
- en: Docker networking
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 网络
- en: 'The following is the Docker networking model to interconnect containers in
    a single host:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于在单个主机上互联容器的 Docker 网络模型：
- en: '![](img/00312.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00312.jpg)'
- en: 'Each Container resides in its own networking namespace and uses a Linux bridge
    on the host machine to talk to each other. More details on Docker networking options
    can be found at [https://docs.docker.com/engine/userguide/networking/dockernetworks/](https://docs.docker.com/engine/userguide/networking/dockernetworks/).
    The following are the networking options available as of Docker release 1.9:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器都位于其自己的网络命名空间中，并通过主机上的 Linux 桥接与其他容器通信。有关 Docker 网络选项的更多详细信息，可以参见 [https://docs.docker.com/engine/userguide/networking/dockernetworks/](https://docs.docker.com/engine/userguide/networking/dockernetworks/)。以下是
    Docker 1.9 版本提供的网络选项：
- en: '`--net=bridge`: This is the default option that Docker provides, where containers
    connect to the Linux `docker` bridge using a veth pair.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--net=bridge`：这是 Docker 提供的默认选项，容器通过 veth 对接到 Linux `docker` 桥接。'
- en: '`--net=host`: In this option, there is no new network namespace created for
    the container, and the container shares the same network namespace as the host
    machine.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--net=host`：在此选项中，不会为容器创建新的网络命名空间，容器与主机共享相同的网络命名空间。'
- en: '`--net= (the container name or ID)`: In this option, the new container shares
    the same network namespace as the specified container in the `net` option. (For
    example: `sudo docker run -ti –name=ubuntu2 –net=container:ubuntu1 ubuntu:14.04
    /bin/bash`. Here, the `ubuntu2` container shares the same network namespace as
    the `ubuntu1` container.)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--net= (容器名称或ID)`：在此选项中，新容器与在`net`选项中指定的容器共享相同的网络命名空间。（例如：`sudo docker run
    -ti –name=ubuntu2 –net=container:ubuntu1 ubuntu:14.04 /bin/bash`。这里，`ubuntu2`容器与`ubuntu1`容器共享相同的网络命名空间。）'
- en: '`--net=none`: In this option, the container does not get allocated a new network
    namespace. Only the loopback interface is created in this case. This option is
    useful in scenarios where we want to create our own networking options for the
    container or where there is no need for connectivity.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--net=none`：在此选项中，容器不会分配新的网络命名空间。此时仅会创建回环接口。此选项适用于我们希望为容器创建自定义网络选项或不需要连接的场景。'
- en: '`--net=overlay`: This option was added in Docker release 1.9 to support overlay
    networking that allows Containers across hosts to be able to talk to each other.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--net=overlay`：此选项在 Docker 1.9 版本中添加，用于支持覆盖网络，使得跨主机的容器能够相互通信。'
- en: Docker experimental networking
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 实验性网络
- en: As of Docker release 1.8, Docker did not have a native solution to connect Containers
    across hosts. With the Docker experimental release, we can connect Containers
    across hosts using the Docker native solution as well as external networking plugins
    to connect Containers across hosts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Docker 1.8 版本开始，Docker 没有本地解决方案来连接跨主机的容器。通过 Docker 实验性版本，我们可以使用 Docker 本地解决方案以及外部网络插件来连接跨主机的容器。
- en: 'The following figure illustrates this:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明此情况：
- en: '![](img/00447.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00447.jpg)'
- en: 'The following are some notes on the Docker libnetwork solution:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于 Docker libnetwork 解决方案的一些说明：
- en: Docker runtime was previously integrated with the networking module and there
    was no way to separate them. Libnetwork is the new networking library that provides
    the networking functionality and is separated from Core Docker. Docker 1.7 release
    has already included the libnetwork and is backward-compatible from the end user's
    perspective.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以前，Docker运行时与网络模块集成，没有办法将它们分离。Libnetwork是新的网络库，提供网络功能，并与Core Docker分离。Docker
    1.7版本已经包含了libnetwork，并且从终端用户的角度来看是向后兼容的。
- en: Drivers implement the APIs provided by libnetwork. Docker is leaning towards
    a plugin approach for major functionalities such as Networking, Storage, and Orchestration
    where Docker provides a native solution that can be substituted with technologies
    from other vendors as long as they implement the APIs provided by the common library.
    In this case, Bridge and Overlay are the native Docker networking drivers and
    remote drivers can be implemented by a third party. There are already many remote
    drivers available, such as Weave and Calico.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序实现了libnetwork提供的API。Docker正在倾向于为网络、存储和编排等主要功能采用插件方法，Docker提供的原生解决方案可以被其他供应商的技术替代，只要它们实现了通用库提供的API。在这种情况下，Bridge和Overlay是Docker的原生网络驱动程序，而远程驱动程序可以由第三方实现。现在已经有很多远程驱动程序可用，如Weave和Calico。
- en: 'Docker experimental networking has the following concepts:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Docker实验性网络有以下概念：
- en: The Docker container attaches to the network using the endpoint or service.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker容器通过端点或服务连接到网络。
- en: Multiple endpoints share a network. In other words, only endpoints located in
    the same network can talk to each other.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个端点共享同一网络。换句话说，只有位于同一网络中的端点才能相互通信。
- en: When creating the network, the network driver can be mentioned. This can be
    a Docker-provided driver, such as Overlay, or an external driver, such as Weave
    and Calico.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建网络时，可以指定网络驱动程序。这可以是Docker提供的驱动程序，如Overlay，或者是外部驱动程序，如Weave和Calico。
- en: Libnetwork provides service discovery, where Containers can discover other endpoints
    in the same network. There is a plan in the future to make service discovery a
    plugin. Services can talk to each other using the service name rather than the
    IP address. Currently, Consul is used for service discovery; this might change
    later.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork提供服务发现，容器可以发现同一网络中的其他端点。未来有计划将服务发现做成插件。服务之间可以通过服务名称而非IP地址进行通信。目前，Consul用于服务发现；这可能以后会有所变化。
- en: Shared storage such as `etcd` or `consul` is used to determine the nodes that
    are part of the same cluster.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用共享存储，如`etcd`或`consul`，来确定属于同一集群的节点。
- en: A multinetwork use case
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多网络用例
- en: 'With the latest Docker networking enhancements, Containers can be part of multiple
    networks and only Containers in the same network can talk to each other. To illustrate
    these concepts, let''s take a look at the following example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最新的Docker网络增强，容器可以是多个网络的一部分，只有在同一网络中的容器才能相互通信。为了说明这些概念，我们来看以下示例：
- en: Set up two nginx containers and one HAProxy Container in the backend network,
    `be`.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在后端网络`be`中设置两个nginx容器和一个HAProxy容器。
- en: Add the HAProxy Container in the frontend network, `fe`, as well.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样将HAProxy容器添加到前端网络`fe`中。
- en: Connect to the HAProxy Container using the busybox Container in the frontend
    network, `fe`. As the busybox Container is in the `fe` network and nginx Containers
    are in the `be` network, they cannot talk to each other directly.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用位于前端网络`fe`中的busybox容器连接到HAProxy容器。由于busybox容器位于`fe`网络中，而nginx容器位于`be`网络中，因此它们无法直接相互通信。
- en: The Haproxy Container will load balance the web connection between the two nginx
    backend Containers.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HAProxy容器将在两个nginx后端容器之间进行Web连接的负载均衡。
- en: 'The following are the command details:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是命令详情：
- en: 'Create the `fe` and `be` networks:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`fe`和`be`网络：
- en: '`docker network create be``docker network create fe`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker network create be` `docker network create fe`'
- en: 'Create two nginx containers in the `be` network:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在`be`网络中创建两个nginx容器：
- en: '`docker run --name nginx1 --net be -v ~/haproxy/nginx1.html:/usr/share/nginx/html/index.html -d nginx``docker run --name nginx2 --net be -v ~/haproxy/nginx2.html:/usr/share/nginx/html/index.html -d nginx`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run --name nginx1 --net be -v ~/haproxy/nginx1.html:/usr/share/nginx/html/index.html
    -d nginx` `docker run --name nginx2 --net be -v ~/haproxy/nginx2.html:/usr/share/nginx/html/index.html
    -d nginx`'
- en: 'Create `haproxy` in the `be` network:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在`be`网络中创建`haproxy`：
- en: '`docker run -d --name haproxy --net be -v ~/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg haproxy`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d --name haproxy --net be -v ~/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
    haproxy`'
- en: 'Attach `haproxy` to the `fe` network:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `haproxy` 附加到 `fe` 网络：
- en: '`docker network connect fe haproxy`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker network connect fe haproxy`'
- en: 'Create a busybox container in the `fe` network accessing the `haproxy` web
    page:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `fe` 网络中创建一个 busybox 容器，访问 `haproxy` 网页：
- en: '`docker run -it --rm --net fe busybox wget -qO- haproxy/index.html`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -it --rm --net fe busybox wget -qO- haproxy/index.html`'
- en: If we try running the busybox container multiple times, it will switch between
    `nginx1` and `nginx2` web server outputs.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们多次尝试运行 busybox 容器，它将在 `nginx1` 和 `nginx2` 网页服务器输出之间切换。
- en: The Docker overlay driver
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 覆盖驱动程序
- en: The following example shows you how to do multihost container connectivity using
    the Docker experimental overlay driver. I have used a Ubuntu VM for the following
    example and not CoreOS because the experimental docker overlay driver needs a
    new kernel release, which is not yet available in CoreOS.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Docker 的实验性覆盖驱动程序实现多主机容器连接。我在以下示例中使用的是 Ubuntu 虚拟机，而不是 CoreOS，因为实验性的
    Docker 覆盖驱动程序需要一个新的内核版本，而该版本在 CoreOS 中尚未提供。
- en: 'The following figure illustrates the use case that is being tried in this example:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了此示例中正在尝试的用例：
- en: '![](img/00314.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00314.jpg)'
- en: 'The following is a summary of the steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤的总结：
- en: Create two hosts with experimental Docker installed.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建两台安装了实验性 Docker 的主机。
- en: Install Consul on both the hosts with one of the hosts acting as the consul
    server. Consul is needed to store common data that is used for inter-container
    communication.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两台主机上安装 Consul，其中一台主机充当 consul 服务器。Consul 用于存储用于容器间通信的共享数据。
- en: Start Docker with Consul as the key store mechanism on both hosts.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两个主机上启动 Docker，并使用 Consul 作为密钥存储机制。
- en: Create containers with different endpoints on both hosts sharing the same network.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两个主机上创建具有不同端点的容器，这些容器共享相同的网络。
- en: The first step is to create 2 host machines with experimental Docker installed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建两台安装了实验性 Docker 的主机。
- en: 'The following set of commands creates two Docker hosts using docker-machine.
    We have used docker-machine with a custom ISO image for experimental Docker:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一组命令使用 docker-machine 创建两台 Docker 主机。我们使用了带有自定义 ISO 镜像的 docker-machine 来安装实验性的
    Docker：
- en: '`docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev1``docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso dev2`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso
    dev1``docker-machine create -d virtualbox --virtualbox-boot2docker-url=http://sirile.github.io/files/boot2docker-1.9.iso
    dev2`'
- en: 'Install Consul on both nodes. The following command shows you how to download
    and install consul:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个节点上安装 Consul。以下命令展示了如何下载并安装 Consul：
- en: '`curl -OL https://dl.bintray.com/mitchellh/consul/0.5.2_linux_amd64.zip``unzip 0.5.2_linux_amd64.zip``sudo mv consul /usr/local/bin/`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl -OL https://dl.bintray.com/mitchellh/consul/0.5.2_linux_amd64.zip``unzip
    0.5.2_linux_amd64.zip``sudo mv consul /usr/local/bin/`'
- en: 'Start the consul server and docker daemon with the consul keystore in `node1`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `node1` 上启动 consul 服务器并启动使用 consul 密钥存储的 Docker 守护进程：
- en: 'The following set of commands starts the consul server and Docker daemon with
    the consul agent in `node1`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一组命令在 `node1` 上启动 consul 服务器并使用 consul 代理启动 Docker 守护进程：
- en: '`Docker-machine ssh dev1``consul agent -server -bootstrap -data-dir /tmp/consul -bind=192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`Docker-machine ssh dev1``consul agent -server -bootstrap -data-dir /tmp/consul
    -bind=192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1`'
- en: 'Start the consul agent and Docker daemon with the consul keystore in `node2`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `node2` 上启动 consul 代理和使用 consul 密钥存储的 Docker 守护进程：
- en: 'The following set of commands starts the consul agent and Docker daemon with
    the consul agent in `node2`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一组命令启动在 `node2` 上运行的 consul 代理和 Docker 守护进程：
- en: '`Docker-machine ssh dev2``consul agent -data-dir /tmp/consul -bind 192.168.99.101 &``consul join 192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500 --label=com.docker.network.driver.overlay.bind_interface=eth1 --label=com.docker.network.driver.overlay.neighbor_ip=192.168.99.100`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`Docker-machine ssh dev2``consul agent -data-dir /tmp/consul -bind 192.168.99.101
    &``consul join 192.168.99.100 &``sudo docker -d --kv-store=consul:localhost:8500
    --label=com.docker.network.driver.overlay.bind_interface=eth1 --label=com.docker.network.driver.overlay.neighbor_ip=192.168.99.100`'
- en: 'Start the container with the `svc1` service, `dev` network, and `overlay` driver
    in `node1`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `node1` 上启动带有 `svc1` 服务、`dev` 网络和 `overlay` 驱动程序的容器：
- en: '`docker run -i -t --publish-service=svc1.dev.overlay busybox`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -i -t --publish-service=svc1.dev.overlay busybox`'
- en: 'Start the container with the `svc2` service, `dev` network, and `overlay` driver
    in node2:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在`node2`上使用`svc2`服务、`dev`网络和`overlay`驱动启动容器：
- en: '`docker run -i -t --publish-service=svc2.dev.overlay busybox`'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -i -t --publish-service=svc2.dev.overlay busybox`'
- en: 'As we can see, we are able to ping `svc1` and `svc2` from `node1` successfully:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们能够成功从`node1`ping到`svc1`和`svc2`：
- en: '![](img/00491.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00491.jpg)'
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: The overlay driver needs the Linux kernel version 3.16 or higher.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：overlay驱动需要Linux内核版本为3.16或更高。
- en: The external networking calico plugin
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 外部网络Calico插件
- en: In this example, we will illustrate how to do Container networking using Calico
    as a plugin to the Docker libnetwork. This support was available originally in
    experimental networking and later in the Docker 1.9 release. More details about
    the Calico networking approach are mentioned in the following Calico networking
    section. This example is based on [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md).
    To set up a CoreOS Vagrant cluster for Calico, we can use the procedure in [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将演示如何使用Calico作为Docker libnetwork的插件进行容器网络设置。最初，支持该功能的是实验性网络，后来在Docker
    1.9版本中发布。有关Calico网络方法的更多细节，请参阅以下Calico网络部分。本示例基于[https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/docker-network-plugin/README.md)。要为Calico设置CoreOS
    Vagrant集群，我们可以参考[https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md)中的过程。
- en: 'After setting up a Vagrant CoreOS cluster, we can see the two nodes of the
    CoreOS cluster. We should make sure that `etcd` is running successfully, as shown
    in the following output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置Vagrant CoreOS集群后，我们可以看到CoreOS集群的两个节点。我们应该确保`etcd`成功运行，如下所示：
- en: '![](img/00318.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00318.jpg)'
- en: 'The following are the steps to get Calico working with Docker as a networking
    plugin:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使Calico与Docker作为网络插件工作的步骤：
- en: 'Start Calico in both nodes with the libnetwork option:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个节点上使用libnetwork选项启动Calico：
- en: '`sudo calicoctl node --libnetwork`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo calicoctl node --libnetwork`'
- en: 'We should see the following Docker containers in both nodes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在两个节点上看到以下Docker容器：
- en: '![](img/00028.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpg)'
- en: 'Create the `net1` network with Calico driver:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Calico驱动创建`net1`网络：
- en: '`docker network create --driver=calico --subnet=192.168.0.0/24 net1`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker network create --driver=calico --subnet=192.168.0.0/24 net1`'
- en: 'This gets replicated to all the nodes in the cluster. The following is the
    network list in `node2`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这会被复制到集群中的所有节点。以下是`node2`中的网络列表：
- en: '![](img/00320.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00320.jpg)'
- en: 'Create container 1 in `node1` with the `net1` network:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`node1`上使用`net1`网络创建容器1：
- en: '`docker run --net net1 --name workload-A -tid busybox`'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`docker run --net net1 --name workload-A -tid busybox`'
- en: 'Create container 2 in `node2` with the `net1` network:'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`node2`上使用`net1`网络创建容器2：
- en: '`docker run --net net1 --name workload-B -tid busybox`'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`docker run --net net1 --name workload-B -tid busybox`'
- en: 'Now, we can ping the two containers as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按以下方式ping这两个容器：
- en: '![](img/00050.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00050.jpg)'
- en: The Docker 1.9 update
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.9更新
- en: Docker 1.9 got released at the end of October 2015 that transitioned the experimental
    networking into production. There could be minor modifications necessary to the
    Docker networking examples in this chapter, which were tried with the Docker 1.8
    experimental networking version.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.9版本在2015年10月底发布，标志着实验性网络进入生产环境。在本章中，使用Docker 1.8实验性网络版本进行的Docker网络示例可能需要进行小幅修改。
- en: With Docker 1.9, multihost networking is integrated with Docker Swarm and Compose.
    This allows us to orchestrate a multicontainer application spread between multiple
    hosts with a single command and the multi-host Container networking will be handled
    automatically.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Docker 1.9的发布，多主机网络已集成到Docker Swarm和Compose中。这使我们能够通过单个命令编排跨多个主机的多容器应用程序，且多主机容器网络将自动处理。
- en: Other Container networking technologies
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 其他容器网络技术
- en: Weave and Calico are open source projects, and they develop Container networking
    technologies for Docker. Kubernetes is a Container orchestration open source project
    and it has specific networking requirements and implementations for Containers.
    There are also other projects such as Cisco Contiv ([https://github.com/contiv/netplugin](https://github.com/contiv/netplugin))
    that is targeted at Container networking. Container technologies like Weave, Calico
    and Contiv have plans to integrate with Rkt Container runtime in the future.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Weave和Calico是开源项目，它们为Docker开发容器网络技术。Kubernetes是一个容器编排开源项目，具有特定的容器网络需求和实现。还有其他项目，如面向容器网络的Cisco
    Contiv（[https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)）。像Weave、Calico和Contiv这样的容器技术，未来计划与Rkt容器运行时集成。
- en: Weave networking
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Weave网络
- en: 'Weaveworks has developed a solution to provide Container networking. The following
    are some details of their solution:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Weaveworks开发了一种提供容器网络的解决方案。以下是他们解决方案的一些细节：
- en: Weave creates a Weave bridge as well as a Weave router in the host machine.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave在主机上创建Weave桥接和Weave路由器。
- en: The Weave router establishes both TCP and UDP connections across hosts to other
    Weave routers. A TCP connection is used for discovery- and protocol-related exchange.
    UDP is used for data encapsulation. Encryption can be done if necessary.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave路由器通过主机与其他Weave路由器之间建立TCP和UDP连接。TCP连接用于发现和协议相关的交换，UDP用于数据封装。必要时可以进行加密。
- en: The Weave bridge is configured to sniff the packets that need to be sent across
    hosts and redirected to the Weave router. For local switching, the Weave router
    is not used.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave桥接配置为嗅探需要跨主机发送并重定向到Weave路由器的包。对于本地交换，不使用Weave路由器。
- en: Weave's Weavenet product provides you with container connectivity. They also
    have Weavescope that provides container visibility and Weaverun that provides
    service discovery and load balancing.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave的Weavenet产品为你提供容器连接。它们还提供Weavescope，提供容器可视化，和Weaverun，提供服务发现和负载均衡。
- en: Weave is also available as a Docker plugin integrated with the Docker release
    1.9.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave也可以作为Docker插件，与Docker 1.9版本集成。
- en: 'The following figure illustrates the solution from Weave:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了来自Weave的解决方案：
- en: '![](img/00068.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00068.jpg)'
- en: To run Weave on CoreOS, I used cloud-config from [https://github.com/lukebond/coreos-vagrant-weave](https://github.com/lukebond/coreos-vagrant-weave).
    In the following example, we will create containers in two CoreOS nodes and use
    Weave to communicate with each other. In this example, we have not used the Docker
    Weave plugin but used environment variables to communicate between Docker and
    Weave.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoreOS上运行Weave时，我使用了来自[https://github.com/lukebond/coreos-vagrant-weave](https://github.com/lukebond/coreos-vagrant-weave)的cloud-config。在以下示例中，我们将在两个CoreOS节点中创建容器，并使用Weave进行互相通信。在此示例中，我们没有使用Docker
    Weave插件，而是通过环境变量来实现Docker和Weave之间的通信。
- en: 'The following are the steps to create a Weave cluster:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建Weave集群的步骤：
- en: Clone the repository (`git clone https://github.com/lukebond/coreos-vagrant-weave.git`).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆仓库（`git clone https://github.com/lukebond/coreos-vagrant-weave.git`）。
- en: Change the number of instances in `config.rb` to `3`.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`config.rb`中的实例数量更改为`3`。
- en: Get a new discovery token for node count 3 and update it in the user data.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的发现令牌，节点数量为3，并将其更新到用户数据中。
- en: Perform `vagrant up` to start the cluster.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`vagrant up`来启动集群。
- en: The `cloud-config` file takes care of downloading Weave agents in each node
    and starting them.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`cloud-config`文件负责在每个节点上下载Weave代理并启动它们。'
- en: 'The following section of the service file downloads the Weave container:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 服务文件的以下部分下载Weave容器：
- en: '![](img/00421.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00421.jpg)'
- en: 'The following section of the service file starts the Weave container:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 服务文件的以下部分启动Weave容器：
- en: '![](img/00340.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00340.jpg)'
- en: 'On each of the nodes, we can see the following Weave containers started:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点上，我们可以看到启动的Weave容器：
- en: '![](img/00278.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00278.jpg)'
- en: 'Before starting application containers, we need to set the environment variables
    so that Weave can intercept Docker commands and create their own networking. As
    part of starting Weave in `Weave.service`, environment variables have already
    been set up. The following command in the node shows this:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动应用程序容器之前，我们需要设置环境变量，以便Weave可以拦截Docker命令并创建自己的网络。作为`Weave.service`启动的一部分，环境变量已经设置好。以下命令显示了节点中的设置：
- en: '![](img/00292.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00292.jpg)'
- en: 'Source the Weave environment as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，配置Weave环境：
- en: '![](img/00366.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00366.jpg)'
- en: 'Let''s start busybox containers in two CoreOS nodes:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在两个 CoreOS 节点中启动 busybox 容器：
- en: '![](img/00315.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00315.jpg)'
- en: 'Let''s look at the Weave interface created in the busybox container of CoreOS
    node1:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 CoreOS 节点 1 的 busybox 容器中创建的 Weave 接口：
- en: '![](img/00325.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00325.jpg)'
- en: 'Let''s look at the Weave interface created in the busybox container of CoreOS
    node2:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 CoreOS 节点 2 的 busybox 容器中创建的 Weave 接口：
- en: '![](img/00374.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00374.jpg)'
- en: Now, we can successfully ping between the two containers. As part of Docker
    1.9, Weave is available as a Docker networking plugin and this makes configuration
    much easier.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在两个容器之间成功地进行 ping 操作。作为 Docker 1.9 的一部分，Weave 作为 Docker 网络插件可用，这使得配置变得更加简便。
- en: Calico networking
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 网络
- en: 'Calico provides you with a Container networking solution for Docker similar
    to Weave. The following are some details of Calico''s implementation:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 为 Docker 提供类似于 Weave 的容器网络解决方案。以下是 Calico 实现的一些细节：
- en: Calico provides container networking directly at L3 without using overlay technologies
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico 直接在 L3 层提供容器网络，而不使用覆盖技术。
- en: Calico uses BGP for route distribution
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico 使用 BGP 进行路由分发。
- en: 'There are two components of Calico: BIRD, which is used for route distribution
    and FELIX, which is an agent in each node that does discovery and routing'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico 有两个组件：BIRD，用于路由分发；FELIX，它是每个节点中的一个代理，负责发现和路由。
- en: Calico is also available as a Docker networking plugin integrated with Docker
    release 1.9
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico 也作为 Docker 网络插件与 Docker 1.9 版本集成可用。
- en: 'The following figure illustrates the Calico data path:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Calico 的数据路径：
- en: '![](img/00365.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00365.jpg)'
- en: Setting up Calico with CoreOS
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CoreOS 上设置 Calico。
- en: I followed the procedure at [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md)
    to set up a two-node CoreOS cluster.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我按照 [https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md](https://github.com/projectcalico/calico-containers/blob/master/docs/calico-with-docker/VagrantCoreOS.md)
    上的步骤设置了一个两节点的 CoreOS 集群。
- en: 'The first step is checking out the repository:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是检出该仓库：
- en: '`git clone https://github.com/projectcalico/calico-docker.git`'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/projectcalico/calico-docker.git`'
- en: 'There are three approaches described by Calico for Docker networking:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 为 Docker 网络提供了三种方法：
- en: 'Powerstrip: Calico uses an HTTP proxy to listen to Docker calls and set up
    networking.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powerstrip：Calico 使用 HTTP 代理来监听 Docker 调用并设置网络。
- en: 'Default networking: Docker Containers are set up with no networking. Using
    Calico, network endpoints are added and networking is set up.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认网络配置：Docker 容器默认没有网络设置。使用 Calico 后，网络端点会被添加，并配置网络。
- en: 'Libnetwork: Calico is integrated with Docker libnetwork as of Docker release
    1.9\. This will be the long-term solution.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libnetwork：从 Docker 1.9 版本起，Calico 与 Docker libnetwork 集成。这将是长期解决方案。
- en: In the following example, we have used the default networking approach to set
    up Container connectivity using Calico.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用默认网络方法通过 Calico 设置容器之间的连接。
- en: 'The following are the steps needed to set up the default networking option
    with Calico:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 Calico 设置默认网络选项的步骤：
- en: Start calicoctl in all the nodes.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有节点上启动 calicoctl。
- en: Start the containers with the `--no-net` option.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `--no-net` 选项启动容器。
- en: Attach the calico network specifying the IP address to each container.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个容器附加 Calico 网络，并指定 IP 地址。
- en: Create a policy profile. Profiles set up the policy that allows containers to
    talk to each other.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个策略配置文件。配置文件设置了允许容器互相通信的策略。
- en: Attach profiles to the container.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为容器附加配置文件。
- en: The following commands set up a container in `node1` and `node2` and establish
    a policy that allows containers to talk to each other.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令在 `node1` 和 `node2` 上设置容器，并建立允许容器相互通信的策略。
- en: 'Execute the following commands on `node1`:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `node1` 上执行以下命令：
- en: '`docker run --net=none --name workload-A -tid busybox``sudo calicoctl container add workload-A 192.168.0.1``calicoctl profile add PROF_A_B``calicoctl container workload-A profile append PROF_A_B`'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run --net=none --name workload-A -tid busybox``sudo calicoctl container
    add workload-A 192.168.0.1``calicoctl profile add PROF_A_B``calicoctl container
    workload-A profile append PROF_A_B`'
- en: This starts the docker container, attaches the calico endpoint, and applies
    the profile to allow Container connectivity.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动 Docker 容器，附加 Calico 端点，并应用配置文件以允许容器之间的连接。
- en: 'Execute the following commands on `node2`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `node2` 上执行以下命令：
- en: '`docker run --net=none --name workload-B -tid busybox``sudo calicoctl container add workload-B 192.168.0.2``calicoctl container workload-B profile append PROF_A_B`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run --net=none --name workload-B -tid busybox``sudo calicoctl container
    add workload-B 192.168.0.2``calicoctl container workload-B profile append PROF_A_B`'
- en: This starts the docker container, attaches the calico endpoint, and applies
    the same profile as in the preceding commands to allow Container connectivity.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动 Docker 容器，附加 Calico 终端节点，并应用与前面命令中相同的配置文件，以允许容器之间的连接。
- en: 'Now, we can test intercontainer connectivity:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试容器间的连接性：
- en: '![](img/00383.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00383.jpg)'
- en: Kubernetes networking
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络
- en: Kubernetes is a Container orchestration service. Kubernetes is an open source
    project that's primarily driven by Google. We will discuss about Kubernetes in
    the next chapter on Container orchestration. In this chapter, we will cover some
    of the Kubernetes networking basics.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个容器编排服务。Kubernetes 是一个由 Google 主导的开源项目。我们将在下一章讨论容器编排中的 Kubernetes。在本章中，我们将介绍一些
    Kubernetes 网络的基础知识。
- en: 'The following are some details as to how Kubernetes does the networking of
    containers:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Kubernetes 如何进行容器网络连接的一些细节：
- en: Kubernetes has a concept called Pods, which is a collection of closely-tied
    containers. For example, a service and its logging service can be a single pod.
    Another example of a pod could be a service and sidekick service that checks the
    health of the main service. A single pod with its associated containers is always
    scheduled on one machine.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 有一个概念叫做 Pods，它是多个紧密相关的容器的集合。例如，一个服务及其日志服务可以是一个单独的 Pod。另一个 Pod 的例子可以是一个服务和一个检查主服务健康状况的辅助服务。一个
    Pod 及其相关的容器始终会被调度到同一台机器上。
- en: Each pod gets an IP address. All containers within a pod share the same IP address.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 Pod 都有一个 IP 地址。Pod 内的所有容器共享相同的 IP 地址。
- en: Containers within a pod share the same network namespace. For containers within
    a pod to communicate, they can use a regular process-based communication.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 内的容器共享相同的网络命名空间。为了让 Pod 内的容器能够通信，它们可以使用常规的基于进程的通信方式。
- en: Pods can communicate with each other using a cloud networking VPC-based approach
    or container networking solution such as Flannel, Weave, or Calico.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods 可以通过基于云网络的 VPC 方法或容器网络解决方案（如 Flannel、Weave 或 Calico）相互通信。
- en: As pods are ephemeral, Kubernetes has a unit called service. Each service has
    an associated virtual IP address and proxy agent running on the nodes' load balancers
    and directs traffic to the right pod.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Pods 是短暂存在的，Kubernetes 有一个叫做服务（service）的单元。每个服务都有一个关联的虚拟 IP 地址和代理代理，这些代理运行在节点的负载均衡器上，并将流量引导到正确的
    Pod。
- en: 'The following is an illustration of how Pods and Containers are related and
    how they communicate:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Pods 和容器之间如何关联以及它们如何进行通信的示意图：
- en: '![](img/00403.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00403.jpg)'
- en: Summary
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered different Container networking technologies with
    a focus on Container networking in CoreOS. There are many companies trying to
    solve this Container networking problem. CNI and Flannel have become the default
    for CoreOS and Libnetwork has become the default for Docker. Having standards
    and pluggable networking architecture is good for the industry as this allows
    interoperability. Container networking is still in the early stages, and it will
    take some time for the technologies to mature in this area. In the next chapter,
    we will discuss about CoreOS storage management.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了不同的容器网络技术，重点是 CoreOS 中的容器网络。许多公司都在尝试解决这个容器网络问题。CNI 和 Flannel 已成为 CoreOS
    的默认选项，而 Libnetwork 已成为 Docker 的默认选项。拥有标准化和可插拔的网络架构对行业有好处，因为它能实现互操作性。容器网络仍处于早期阶段，技术成熟还需要一些时间。在下一章中，我们将讨论
    CoreOS 存储管理。
- en: References
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Flannel docs: [https://coreos.com/flannel/docs/latest/](https://coreos.com/flannel/docs/latest/)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flannel 文档: [https://coreos.com/flannel/docs/latest/](https://coreos.com/flannel/docs/latest/)'
- en: 'Flannel GitHub page: [https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flannel GitHub 页面: [https://github.com/coreos/flannel](https://github.com/coreos/flannel)'
- en: 'CNI spec: [https://github.com/appc/cni/blob/master/SPEC.md](https://github.com/appc/cni/blob/master/SPEC.md)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CNI 规范: [https://github.com/appc/cni/blob/master/SPEC.md](https://github.com/appc/cni/blob/master/SPEC.md)'
- en: 'Flannel with AWS and GCE: [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flannel 与 AWS 和 GCE: [https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/](https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/)'
- en: 'Weaveworks: [https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weaveworks: [https://github.com/weaveworks/weave](https://github.com/weaveworks/weave)'
- en: 'Libnetwork: [https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Libnetwork: [https://github.com/docker/libnetwork](https://github.com/docker/libnetwork)'
- en: 'Docker experimental: [https://github.com/docker/docker/tree/master/experimental](https://github.com/docker/docker/tree/master/experimental)'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker实验版: [https://github.com/docker/docker/tree/master/experimental](https://github.com/docker/docker/tree/master/experimental)'
- en: 'Calico: [https://github.com/projectcalico/calico-docker](https://github.com/projectcalico/calico-docker)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Calico: [https://github.com/projectcalico/calico-docker](https://github.com/projectcalico/calico-docker)'
- en: 'Kubernetes: [http://kubernetes.io/](http://kubernetes.io/)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes: [http://kubernetes.io/](http://kubernetes.io/)'
- en: Further reading and tutorials
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读和教程
- en: 'The Flannel CoreOS Fest presentation: [https://www.youtube.com/watch?v=_HYeSaGtEYw](https://www.youtube.com/watch?v=_HYeSaGtEYw)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flannel CoreOS Fest 演示: [https://www.youtube.com/watch?v=_HYeSaGtEYw](https://www.youtube.com/watch?v=_HYeSaGtEYw)'
- en: 'The Calico and Weave presentation: [https://giantswarm.io/events/2015-04-20-docker-coreos/](https://giantswarm.io/events/2015-04-20-docker-coreos/)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Calico 和 Weave 演示: [https://giantswarm.io/events/2015-04-20-docker-coreos/](https://giantswarm.io/events/2015-04-20-docker-coreos/)'
- en: 'Contiv netplugin: [https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Contiv 网络插件: [https://github.com/contiv/netplugin](https://github.com/contiv/netplugin)'
- en: 'Kubernetes networking: [https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md](https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 网络: [https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md](https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/networking.md)'
