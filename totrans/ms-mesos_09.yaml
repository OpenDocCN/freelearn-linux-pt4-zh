- en: Chapter 9. Mesos Big Data Frameworks 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a guide to deploying important big data storage frameworks,
    such as Cassandra, the Elasticsearch-Logstash-Kibana (ELK) stack, and Kafka, on
    Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra on Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will introduce Cassandra and explain how to set up Cassandra on
    Mesos while also discussing the problems commonly encountered during the setup
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Cassandra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cassandra** is an open source, scalable NoSQL database that is fully distributed
    with no single point of failure and is highly performant for most standard use
    cases. It is both horizontally as well as vertically scalable. **Horizontal scalability**
    or **scale-out solution** involves adding more nodes with commodity hardware to
    the existing cluster while **vertical scalability** or **scale-up solution** means
    adding more CPU and memory resources to a node with specialized hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Cassandra was developed by Facebook engineers to address the inbox search use
    case and was inspired by Google Bigtable, which served as the foundation for its
    storage model, and Amazon DynamoDB, which was the foundation of its distribution
    model. It was open sourced in 2008 and became an Apache top-level project in early
    2010\. It provides a query language called **Cassandra Query Language** or **CQL**,
    which has a SQL-like syntax, to communicate with the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cassandra provides various capabilities, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: High performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous uptime (no single point of failure)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data replication and distribution across datacenters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of using a traditional master-slave or sharded design, Cassandra's architecture
    uses an elegant and simple **ring design** without any masters. This allows it
    to provide all the features and benefits listed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Cassandra Ring Design diagram is shown as follows (source: [www.planetcassandra.org](http://www.planetcassandra.org)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Cassandra](img/B05186_09_00.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A large number of companies use Cassandra in production, including Apple, Instagram,
    eBay, Spotify, Comcast, and Netflix among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cassandra is best used when you need:'
  prefs: []
  type: TYPE_NORMAL
- en: No single failure point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time writes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal scaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clearly defined table schema in a NoSQL environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the common use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing, managing, and performing analysis on data generated by messaging applications
    (Instagram and Comcast, among others, use Cassandra for this purpose)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing data patterns for the detection of fraudulent activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing user-selected and curated items (shopping cart, playlists, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation and personalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance benchmark**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following performance benchmark conducted by an independent database firm
    showed that for mixed operational and analytical workloads, Cassandra was far
    superior to other open source NoSQL technologies (source: [www.datastax.com](http://www.datastax.com)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Cassandra](img/B05186_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up Cassandra on Mesos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section covers the process of deploying Cassandra on top of Mesos. The
    recommended way of deploying Cassandra on Mesos is through Marathon. At the time
    of writing this book, Cassandra on Mesos is in an experimental stage, and the
    configuration described here might change in future releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Mesosphere team has already packaged the necessary JAR files and the Cassandra
    executor in a tarball that can be directly submitted to Mesos through Marathon
    with the following JSON code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit the JSON code by pointing `MESOS_ZK` and any other parameters that you
    need to change accordingly, save this JSON code in `cassandra-mesos.json`, and
    then submit it to Marathon with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once submitted, the framework will bootstrap itself. We also need to expand
    the port ranges managed by each Mesos node to include the standard Cassandra ports.
    We can pass the port ranges as resources when starting the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Cassandra on Mesos provides a REST endpoint to tune the setup. We can access
    this endpoint on port `18080` by default (unless changed).
  prefs: []
  type: TYPE_NORMAL
- en: An advanced configuration guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned previously, Cassandra on Mesos takes in runtime configuration
    through environment variables. We can use the following environment variables
    to bootstrap the configuration of the framework. After the initial run, the configurations
    are read from the framework state stored in ZooKeeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/mesosphere/cassandra-mesos](https://github.com/mesosphere/cassandra-mesos)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://mesosphere.github.io/cassandra-mesos/](http://mesosphere.github.io/cassandra-mesos/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Elasticsearch-Logstash-Kibana (ELK) stack on Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will introduce the **Elasticsearch-Logstash-Kibana** (**ELK**)
    stack and explain how to set it up on Mesos while also discussing the problems
    commonly encountered during the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Elasticsearch, Logstash, and Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ELK stack, a combination of **Elasticsearch**, **Logstash**, and **Kibana**,
    is an end-to-end solution for **log analytics**. Elasticsearch provides search
    capabilities, Logstash is a log management software, while Kibana serves as the
    visualization layer. The stack is commercially backed by a company called **Elastic**.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Elasticsearch is a Lucene-based open source distributed search engine designed
    for high scalability and fast search query response time. It simplifies the usage
    of Lucene, a highly performant search engine library, by providing a powerful
    REST API on top. Some of the important concepts in Elasticsearch are highlighted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document**: This is a JSON object stored in an index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index**: This is a document collection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: This is a logical partition of an index representing a category of
    documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field**: This is a key-value pair within a document'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mapping**: This is used to map every field with its datatype'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shard**: This is the physical location where an index''s data is stored (the
    data is stored on one primary shard and copied on a set of replica shards)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logstash
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a tool to collect and process the log events generated by a wide variety
    of systems. It includes a rich set of input and output connectors to ingest the
    logs and make them available for analysis. Some of its important features are:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to convert logs to a common format for the ease of use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to process multiple log formats, including custom ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rich set of input and output connectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kibana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an Elasticsearch-based data visualization tool with a wide variety
    of charting and dashboarding capabilities. It is powered by the data stored in
    the Elasticsearch indexes and is entirely developed using HTML and JavaScript.
    Some of its most important features are:'
  prefs: []
  type: TYPE_NORMAL
- en: A graphical user interface for dashboard construction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rich set of charts (map, pie charts, histograms, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to embed charts in user applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ELK stack data pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram (source: *Learning ELK Stack* by Packt
    Publishing):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack data pipeline](img/B05186_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a standard ELK stack pipeline, logs from various application servers are
    transported through Logstash to a central indexer module. This indexer then transmits
    the output to an Elasticsearch cluster, where it can be queried directly or visualized
    in a dashboard by leveraging Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Elasticsearch-Logstash-Kibana on Mesos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section explains how to set up Elasticsearch, Logstash, and Kibana on top
    of Mesos. We will first take a look at how to set up Elasticsearch on top of Mesos
    followed by Logstash and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch on Mesos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use Marathon to deploy Elasticsearch, and this can be done in two ways:
    through the Docker image, which is highly recommended, and through `elasticsearch-mesos
    jar`. Both are explained in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the following Marathon file to deploy Elasticsearch on top of Mesos.
    It uses the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that `zookeeper-node` is changed to the address of the ZooKeeper node
    that you have on the cluster. We can save this to an `elasticsearch.json` file
    and then deploy it on Marathon with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned before, we can also use the JAR file to deploy Elasticsearch on
    top of Mesos with the following Marathon file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In both cases, the `JAVA_OPTS` environment variable is required, and if it''s
    not set, it will cause problems with the Java heap space. We can save this as
    `elasticsearch.json` and submit it to Marathon with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Both Docker image and the JAR file take in the following command-line arguments,
    similar to the `--zookeeperMesosUrl` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Logstash on Mesos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explains how to run Logstash on top of Mesos. Once Logstash is
    deployed on the cluster, any program that runs on Mesos can log an event that
    is then passed by Logstash and sent to a central log location.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run Logstash as a Marathon application and deploy it on top of Mesos
    with the following Marathon file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the Docker image for deployment, the configurations of which
    can be changed according to your cluster specification. Save the preceding file
    as `logstash.json` and submit it to Marathon with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Logstash on Mesos configurations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Logstash and Elasticsearch are tested with the Mesos version 0.25.0 and later.
    We need to add Logstash to the list of roles on every Mesos master machine. This
    can be done with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If the purpose of Logstash is to monitor `syslog` (a message logging standard),
    then we need to add the TCP and UDP port `514` to the resources list in every
    Mesos node in the cluster. This can be done by adding the following entry in the
    `/etc/mesos-slave/resources` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To monitor `collectd`, we need to add the TCP and UDP port `25826` to the resources
    for the Logstash role by adding the following line to the `/etc/mesos-slave/resources`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Kibana on Mesos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we run Kibana on Mesos, then each instance of Kibana will run as a Docker
    image in the Mesos cluster. For each instance of Elasticsearch, one or more instances
    of Kibana can be deployed to serve the users.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can clone Kibana on the Mesos project from the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the project with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This will generate the Kibana JAR file (`kibana.jar`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once `kibana.jar` is generated, we can deploy it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, `-zk` represents the ZooKeeper URI and the `-es` points to the Elasticsearch
    endpoint, which we deployed in the previous section. Set them accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command-line options are also supported by the `kibana.jar` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Short keyword | Keyword | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `-zk` | -`zookeeper` | This is the Mesos ZooKeeper URL (Required) |'
  prefs: []
  type: TYPE_TB
- en: '| `-di` | `-dockerimage` | This is the name of the Docker image to be used
    (The default is `kibana`) |'
  prefs: []
  type: TYPE_TB
- en: '| `-v` | `-version` | This is the version of the Kibana Docker image to be
    used (The default is `latest`) |'
  prefs: []
  type: TYPE_TB
- en: '| `-mem` | `-requiredMem` | This is the amount of memory (in MB) to be allocated
    to a single Kibana instance (The default is `128`) |'
  prefs: []
  type: TYPE_TB
- en: '| `-cpu` | `-requiredCpu` | This is the amount of CPUs to allocate to a single
    Kibana instance (The default is `0.1`) |'
  prefs: []
  type: TYPE_TB
- en: '| `-disk` | `-requiredDisk` | This is the amount of disk space (in MB) to be
    allocated to a single Kibana instance (The default is `25`) |'
  prefs: []
  type: TYPE_TB
- en: '| `-es` | `-elasticsearch` | These are the URLs of Elasticsearch to start a
    Kibana for at startup |'
  prefs: []
  type: TYPE_TB
- en: 'Here are some references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mesos-elasticsearch.readthedocs.org/en/latest/#elasticsearch-mesos-framework](http://mesos-elasticsearch.readthedocs.org/en/latest/#elasticsearch-mesos-framework)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mesos/logstash](https://github.com/mesos/logstash)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/mesos/kibana](https://github.com/mesos/kibana)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka on Mesos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will introduce Kafka and explain how to set it up on Mesos while
    also discussing the problems commonly encountered during the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kafka is a distributed publish-subscribe messaging system designed for speed,
    scalability, reliability, and durability. Some of the key terms used in Kafka
    are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topics**: These are the categories where message feeds are maintained by
    Kafka'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producers**: These are the upstream processes that send messages to a particular
    Kafka topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: These are the downstream processes that listen to the incoming
    messages in a topic and process them as per requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broker**: Each node in a Kafka cluster is called a broker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following high-level diagram of Kafka (source: [http://kafka.apache.org/documentation.html#introduction](http://kafka.apache.org/documentation.html#introduction)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Kafka](img/B05186_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A partitioned log is maintained by the Kafka cluster for every topic, which
    looks similar to the following (source: [http://kafka.apache.org/documentation.html#intro_topics](http://kafka.apache.org/documentation.html#intro_topics)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction to Kafka](img/B05186_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Use cases of Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some important uses of Kafka are described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Website activity tracking**: Site activity events, such as page views and
    user searches, can be sent by the web application to Kafka topics. Downstream
    processing systems can then subscribe to these topics and consume the messages
    for batch analytics, monitoring, real-time dashboarding, and other such use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log aggregation**: Kafka is used as an alternative to traditional log aggregation
    systems. Physical log files can be collected from various services and pushed
    to different Kafka topics, where different consumers can read and process them.
    File details are abstracted by Kafka, which enables faster processing and support
    for a variety of data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream processing**: Frameworks, for instance Spark Streaming, can consume
    data from a Kafka topic, process it as per requirements, and then publish the
    processed output to a different Kafka topic, where this output can, in turn, be
    consumed by other applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Kafka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before installing Kafka on Mesos, make sure the following applications are
    available on the machine:'
  prefs: []
  type: TYPE_NORMAL
- en: Java version 7 or later ([http://openjdk.java.net/install/](http://openjdk.java.net/install/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradle ([http://gradle.org/installation](http://gradle.org/installation))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can clone and build the Kafka on Mesos project from the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also require the Kafka executor, which can be downloaded with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need to set the following environment variable to point to the
    `libmesos.so` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once these are set, we can use the `kafka-mesos.sh` script to launch and configure
    Kafka on top of Mesos. Before doing so, we need to create the `kafka-mesos.properties`
    file with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This file can be used to configure the scheduler (`kafka-mesos.sh`) if we don''t
    need to pass the arguments to the scheduler all the time. The scheduler supports
    the following command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Option | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `--api` | This is the API URL—for example, `http://master:7000`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--bind-address` | This is the scheduler bind address (such as master, `0.0.0.0`,
    `192.168.50.*`, and `if:eth1`). The default is `all`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--debug <Boolean>` | This is the debug mode. The default is `false`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--framework-name` | This is the framework name. The default is `kafka`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--framework-role` | This is the framework role. The default is `*`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--framework-timeout` | This is the framework timeout (30s, 1m, or 1h). The
    default is `30d`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--jre` | This is the JRE zip file (`jre-7-openjdk.zip`). The default is
    `none`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--log` | This is the log file to use. The default is `stdout`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--master` | These are the master connection settings. Some examples are:`-
    master:5050``- master:5050,master2:5050``- zk://master:2181/mesos``- zk://username:password@master:2181``-
    zk://master:2181,master2:2181/mesos` |'
  prefs: []
  type: TYPE_TB
- en: '| `--principal` | This is the principal (username) used to register the framework.
    The default is `none`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--secret` | This is the secret (password) used to register the framework.
    The default is `none`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--storage` | This is the storage for the cluster state. Some examples are:`-
    file:kafka-mesos.json``- zk:/kafka-mesos`The default is `file:kafka-mesos.json`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--user` | This is the Mesos user to run tasks. The default is `none`. |'
  prefs: []
  type: TYPE_TB
- en: '| `--zk` | This is Kafka `zookeeper.connect`. Some examples are:`- master:2181``-
    master:2181,master2:2181` |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can use the scheduler to run a Kafka scheduler via the following commands
    listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next thing we need to do is to start up one Kafka broker with the default
    settings. This can be done via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our Kafka cluster will have one broker that is not yet started.
    We can verify this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now start this broker with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If the preceding output is shown, then our broker is ready to produce and consume
    messages. We can now test this setup with `kafkacat`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kafkacat` can be installed on the system with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have pushed the test to the broker, we can read it back with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at how we can add more brokers to the cluster at once.
    Run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command adds three `kafka` brokers to the cluster with the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can start all the three brokers at once with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If we need to change the location of the Kafka logs where the data is stored,
    we need to first stop the particular broker and then update the location with
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once done, we can start the broker back up with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Kafka logs management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can get the last 100 lines of the logs (`stdout` -default or `stderr`) of
    any broker in the cluster with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If we need to read from the `stderr` file, then we will use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can read any file in the `kafka-*/log/` directory by passing on the filename
    to the `--name` option. For example, if we need to read `server.log`, then it
    can be read with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, if we need to read more numbers of lines from the log, it can be read
    using the `--lines` option, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: An advanced configuration guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the configuration options available while *adding* broker(s)
    to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now take a look at the options that are available when starting the
    broker(s):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the configuration options available while *updating* broker(s)
    in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the configuration options available while stopping broker(s)
    in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the configuration options available while adding a topic
    to the broker(s) in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The reference for this is [https://github.com/mesos/kafka](https://github.com/mesos/kafka).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the reader to some important big data storage frameworks
    such as Cassandra, the ELK stack, and Kafka and covered topics such as the setup,
    configuration, and management of these frameworks on a distributed infrastructure
    using Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that this book has armed you with all the resources that you require
    to effectively manage the complexities of today's modern datacenter requirements.
    By following the detailed step-by-step guides to deploy a Mesos cluster using
    the DevOps tool of your choice, you should now be in a position to handle the
    system administration requirements of your organization smoothly.
  prefs: []
  type: TYPE_NORMAL
