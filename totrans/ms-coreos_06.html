<html><head></head><body>
<p id="filepos425088" class="calibre_"><span class="calibre1"><span class="bold">Chapter 6. CoreOS Storage Management</span></span></p><p class="calibre_8">Storage is a critical component of distributed infrastructure. The initial focus of Container technology was on Stateless Containers with Storage managed by traditional technologies such as NAS and SAN. Stateless Containers are typically web applications such as NGINX and Node.js where there is no need to persist data. In recent times, there has been a focus on Stateful Containers and there are many new technologies being developed to achieve Stateful Containers. Stateful Containers are databases such as SQL and redis that need data to be persisted. CoreOS and Docker integrates well with different Storage technologies and there is active work going on to fill the gaps in this area.</p><p class="calibre_8">Following three aspects of CoreOS storage will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The CoreOS base filesystem and partition table</li><li value="2" class="calibre_13">The Container filesystem, which is composed of the Union filesystem and <span class="bold">Copy-on-write</span> (<span class="bold">CoW</span>) storage driver</li><a/><li value="3" class="calibre_13">The Container data volumes for shared data persistence, which can be local, distributed, or shared external storage</li></ul><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The CoreOS filesystem and mounting AWS EBS and NFS storage to the CoreOS filesystem</li><li value="2" class="calibre_13">The Docker Container filesystem for storing Container images which includes both storage drivers and the Union filesystem.</li><li value="3" class="calibre_13">Docker data volumes</li><li value="4" class="calibre_13">Container data persistence using Flocker, GlusterFS and Ceph</li></ul><div class="mbp_pagebreak" id="calibre_pb_147"/>


<p id="filepos427270" class="calibre_14"><span class="calibre1"><span class="bold">Storage concepts</span></span></p><p class="calibre_8">The following are <a/>some storage terms along with their basic definitions that we will use in this chapter and beyond:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Local storage</span>: This is Storage attached to the localhost. An example is a local hard disk with ZFS.</li><a/><li value="2" class="calibre_13"><span class="bold">Network storage</span>: This is a common storage accessed through a network. This can either be SAN or a cluster storage such as Ceph and GlusterFS.</li><a/><li value="3" class="calibre_13"><span class="bold">Cloud storage</span>: This is Storage provided by a cloud provider such as AWS EBS, OpenStack Cinder, and Google cloud storage.</li><a/><li value="4" class="calibre_13"><span class="bold">Block storage</span>: This requires low latency and is typically used for an OS-related filesystem. Some examples are AWS EBS and OpenStack Cinder.</li><a/><li value="5" class="calibre_13"><span class="bold">Object storage</span>: This is used for immutable storage items where latency is not a big concern. Some examples are AWS S3 and OpenStack Swift.</li><a/><li value="6" class="calibre_13"><span class="bold">NFS</span>: This is a distributed filesystem. This can be run on top of any cluster storage.</li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_148"/>


<p id="filepos428751" class="calibre_"><span class="calibre1"><span class="bold">The CoreOS filesystem</span></span></p><p class="calibre_8">We covered the<a/> details of the CoreOS partition table in <a href="index_split_075.html#filepos216260">Chapter 3</a>, <span class="italic">CoreOS Autoupdate</span>. The following screenshot shows the default partitioning in the AWS CoreOS cluster:</p><p class="calibre_9"><img src="images/00390.jpg" class="calibre_239"/></p><p class="calibre_8">
</p><p class="calibre_8">By default, CoreOS uses root partitioning for the Container filesystem. In the preceding table, <tt class="calibre2">/dev/xvda9</tt> will be used to store Container images.</p><p class="calibre_8">Following output shows Docker using Ext4 filesystem with Overlay storage driver in a CoreOS node running in AWS:</p><p class="calibre_9"><img src="images/00440.jpg" class="calibre_240"/></p><p class="calibre_8">
</p><p class="calibre_8">To get extra storage, external <a/>storage can be mounted in CoreOS.</p><div class="mbp_pagebreak" id="calibre_pb_149"/>


<p id="filepos429925" class="calibre_9"><span class="calibre3"><span class="bold">Mounting the AWS EBS volume</span></span></p><p class="calibre_8">Amazon <span class="bold">Elastic Block Store</span> (<span class="bold">EBS</span>) provides you with persistent block-level storage volumes to be <a/>used with Amazon EC2 instances in the AWS cloud. The following example shows you how to<a/> add an extra EBS volume to the CoreOS node running in AWS and use it for the Container filesystem.</p><p class="calibre_8">Rename the following <tt class="calibre2">cloud-config</tt> as <tt class="calibre2">cloud-config-mntdocker.yml</tt>:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    name: etcdserver<br class="calibre4"/>    initial-cluster: etcdserver=http://$private_ipv4:2380<br class="calibre4"/>    advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: format-ephemeral.service<br class="calibre4"/>      command: start<br class="calibre4"/>      content: |<br class="calibre4"/>        [Unit]<br class="calibre4"/>        Description=Formats the ephemeral drive<br class="calibre4"/>        After=dev-xvdf.device<br class="calibre4"/>        Requires=dev-xvdf.device<br class="calibre4"/>        [Service]<br class="calibre4"/>        Type=oneshot<br class="calibre4"/>        RemainAfterExit=yes<br class="calibre4"/>        ExecStart=/usr/sbin/wipefs -f /dev/xvdf<br class="calibre4"/>        ExecStart=/usr/sbin/mkfs.btrfs -f /dev/xvdf<br class="calibre4"/>    - name: var-lib-docker.mount<br class="calibre4"/>      command: start<br class="calibre4"/>      content: |<br class="calibre4"/>        [Unit]<br class="calibre4"/>        Description=Mount ephemeral to /var/lib/docker<br class="calibre4"/>        Requires=format-ephemeral.service<br class="calibre4"/>        After=format-ephemeral.service<br class="calibre4"/>        Before=docker.service<br class="calibre4"/>        [Mount]<br class="calibre4"/>        What=/dev/xvdf<br class="calibre4"/>        Where=/var/lib/docker<br class="calibre4"/>        Type=btrfs</tt></p><p class="calibre_8">Following are some details on the preceding <tt class="calibre2">cloud-config</tt> unit file:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Format-ephemeral service takes care of formatting the filesystem as <tt class="calibre2">btrfs</tt></li><li value="2" class="calibre_13">The Mount service takes care of mounting the new volume in <tt class="calibre2">/var/lib/docker</tt> before <tt class="calibre2">docker.service</tt> is started</li></ul><p class="calibre_8">We can start the <a/>CoreOS node with the preceding <tt class="calibre2">cloud-config</tt> with <a/>extra EBS volume using the following commands:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">aws ec2 run-instances --image-id ami-85ada4b5 --count 1 --instance-type t2.micro --key-name "smakam-oregon" --security-groups "coreos-test" --user-data file://cloud-config-mntdocker.yaml --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"DeleteOnTermination\":false,\"VolumeSize\":8,\"VolumeType\":\"gp2\"}}]"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The preceding command creates a single-node CoreOS cluster with one extra volume of 8 GB. The new volume is mounted as <tt class="calibre2">/var/lib/docker</tt> with the <tt class="calibre2">btrfs</tt> filesystem. The <tt class="calibre2">/dev/sdf</tt> directory gets mounted in the CoreOS system as <tt class="calibre2">/dev/xvdf</tt>, so the mount file uses <tt class="calibre2">/dev/xvdf</tt>.</p><p class="calibre_8">The following is the partition table in the node with the preceding <tt class="calibre2">cloud-config</tt>:</p><p class="calibre_9"><img src="images/00461.jpg" class="calibre_241"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see, there<a/> is a new 8 GB partition where <tt class="calibre2">/var/lib/docker</tt> is mounted.</p><p class="calibre_8">The following output <a/>shows you that the docker filesystem is using the <tt class="calibre2">btrfs</tt> storage driver as we requested:</p><p class="calibre_9"><img src="images/00399.jpg" class="calibre_242"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_150"/>


<p id="filepos434563" class="calibre_9"><span class="calibre3"><span class="bold">Mounting NFS storage</span></span></p><p class="calibre_8">We can mount a volume on a <a/>CoreOS node using NFS. NFS allows a shared storage mechanism where all CoreOS nodes in the cluster <a/>can see the same data. This approach can be used for Container data persistence when Containers are moved across nodes. In the following example, we run the NFS server in a Linux server and mount this volume in a CoreOS node running in the Vagrant environment.</p><p class="calibre_8">The following are the steps to set up NFS mounting on the CoreOS node:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Start the NFS server and export directories that are to be shared.</li><li value="2" class="calibre_13">Set up the CoreOS <tt class="calibre2">cloud-config</tt> to start <tt class="calibre2">rpc-statd.service</tt>. Mount services also need to be started in the <tt class="calibre2">cloud-config</tt> to mount the necessary NFS directories to local directories.</li></ol><p id="filepos435660" class="calibre_14"><span class="calibre3"><span class="bold">Setting up NFS server</span></span></p><p class="calibre_8">Start the NFS server. I had set up my Ubuntu 14.04 machine as an NFS server. The following are the steps that I performed to set up the NFS server:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Install the NFS server:<p class="calibre_8"><tt class="calibre2"><span class="bold">sudo apt-get install nfs-kernel-server</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="2" class="calibre_31">Create an NFS directory with the appropriate owner:<p class="calibre_8"><tt class="calibre2"><span class="bold">sudo mkdir /var/nfs</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo chown core /var/nfs (I have created a core user)</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="3" class="calibre_31">Export the <tt class="calibre2">NFS</tt> directory to the necessary nodes. In my case, <tt class="calibre2">172.17.8.[101-103]</tt> are the IP addresses of the CoreOS cluster. Create <tt class="calibre2">/etc/exports</tt> with the following commands:<p class="calibre_8"><tt class="calibre2"><span class="bold">/var/nfs    172.17.8.101(rw,sync,no_root_squash,no_subtree_check)</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">/var/nfs    172.17.8.102(rw,sync,no_root_squash,no_subtree_check)</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">/var/nfs    172.17.8.103(rw,sync,no_root_squash,no_subtree_check)</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Start the NFS server:<p class="calibre_8"><tt class="calibre2"><span class="bold">sudo exportfs -a</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo service nfs-kernel-server start</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: NFS is pretty sensitive to <span class="bold">UserID</span> (<span class="bold">UID</span>) and <span class="bold">Group ID</span> (<span class="bold">GID</span>) checks, and write access from the client machine won't work unless this is properly set up. It is necessary for the UID and GID of the client user to match with the UID and GID of the directory setup in the server. Another option is to set the <tt class="calibre2">no_root_squash</tt> option (as in the preceding example) so that the root user from the client can make modifications as the UserID in the server.</p></li><a/></ol><p class="calibre_8">As shown in the following command, we can see the directory exported after making the necessary configuration:</p><p class="calibre_9"><img src="images/00005.jpg" class="calibre_243"/></p><p class="calibre_8">
</p><p id="filepos438357" class="calibre_9"><span class="calibre3"><span class="bold">Setting up the CoreOS node as a client for the NFS</span></span></p><p class="calibre_8">The following <tt class="calibre2">cloud-config</tt> can<a/> be used to mount <a/>remote <tt class="calibre2">/var/nfs in /mnt/data</tt> in all the nodes of the CoreOS cluster:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/><br class="calibre4"/>write-files:<br class="calibre4"/>  - path: /etc/conf.d/nfs<br class="calibre4"/>    permissions: '0644'<br class="calibre4"/>    content: |<br class="calibre4"/>      OPTS_RPC_MOUNTD=""<br class="calibre4"/><br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    discovery: &lt;yourtoken&gt;<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  flannel:<br class="calibre4"/>    interface: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: rpc-statd.service<br class="calibre4"/>      command: start<br class="calibre4"/>      enable: true<br class="calibre4"/>    - name: mnt-data.mount<br class="calibre4"/>      command: start<br class="calibre4"/>      content: |<br class="calibre4"/>        [Mount]<br class="calibre4"/>        What=172.17.8.110:/var/nfs<br class="calibre4"/>        Where=/mnt/data<br class="calibre4"/>        Type=nfs<br class="calibre4"/>        Options=vers=3,sec=sys,noauto</tt></p><p class="calibre_8">In the preceding config, <tt class="calibre2">cloud-config</tt>, <tt class="calibre2">rpc-statd.service</tt> is necessary for the NFS client service and <tt class="calibre2">mnt-data.mount</tt> is necessary to mount the NFS volume in the <tt class="calibre2">/mnt/data</tt> local directory.</p><p class="calibre_8">The following output is in <a/>one of the CoreOS nodes<a/> that have done the NFS mount. As we can see, the NFS mount is successful:</p><p class="calibre_9"><img src="images/00023.jpg" class="calibre_244"/></p><p class="calibre_8">
</p><p class="calibre_8">After this step, any CoreOS nodes in the cluster can read and write from <tt class="calibre2">/mnt/data</tt>.</p><div class="mbp_pagebreak" id="calibre_pb_151"/>


<p id="filepos440794" class="calibre_"><span class="calibre1"><span class="bold">The container filesystem</span></span></p><p class="calibre_8">Containers use the CoW filesystem to store Container images. The following are some characteristics of the CoW filesystem:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Multiple users/processes can share the same data as if they have their own copy of the data.</li><a/><li value="2" class="calibre_13">If data is changed by any one process or user, a new copy of the data is made for this process/user only at that point.</li><a/><li value="3" class="calibre_13">Multiple running containers share the same set of files till changes are made to the files. This makes starting the containers really fast.</li></ul><p class="calibre_8">These characteristics allow the Container filesystem operations to be really fast. Docker supports multiple storage drivers that are capable of CoW. Each OS chooses a default storage driver. Docker provides you with an option to change the storage driver. To change the storage driver, we need to specify the storage driver in <tt class="calibre2">/etc/default/docker</tt> and restart the Docker daemon:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">DOCKER_OPTS="--storage-driver=&lt;driver&gt;"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The major supported storage drivers are <tt class="calibre2">aufs</tt>, <tt class="calibre2">devicemapper</tt>, <tt class="calibre2">btrfs</tt>, and <tt class="calibre2">overlay</tt>. We need to make sure that the storage driver is supported by the OS on which Docker is installed before changing the Storage driver.</p><div class="mbp_pagebreak" id="calibre_pb_152"/>


<p id="filepos442549" class="calibre_9"><span class="calibre3"><span class="bold">Storage drivers</span></span></p><p class="calibre_8">The storage driver is <a/>responsible for managing the filesystem. The following table captures the differences between<a/> major storage drivers supported by Docker:</p><table border="1" valign="top" class="calibre_16"><tr valign="top" class="calibre_17"><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Property</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">AUFS</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Device mapper</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">BTRTS</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">OverlayFS</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">ZFS</span></p></th></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">File/block</p></td><td valign="top" class="calibre_19"><p class="calibre_8">File-based</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Block-based</p></td><td valign="top" class="calibre_19"><p class="calibre_8">File-based</p></td><td valign="top" class="calibre_19"><p class="calibre_8">File-based</p></td><td valign="top" class="calibre_19"><p class="calibre_8">File-based</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Linux kernel support</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Not in the main kernel</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Present in the main kernel</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Present in the main kernel</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Present in the main kernel &gt; 3.18</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Not in the main kernel</p></td></tr><a/><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">OS</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Ubuntu default</p></td><td valign="top" class="calibre_19"><p class="calibre_8"> Red Hat</p></td><td valign="top" class="calibre_19"> </td><td valign="top" class="calibre_19"><p class="calibre_8">Red Hat</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Solaris</p></td></tr><a/><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Performance</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Not suitable to write big files; useful for PaaS scenarios</p></td><td valign="top" class="calibre_19"><p class="calibre_8">First write slow</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Updating a lot of small files can cause low performance</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Better than AUFS</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Takes up a lot of memory</p></td></tr></table><p class="calibre_8">A storage driver needs to be chosen based on the type of workload, the need for availability in the main Linux kernel, and the comfort level with a particular storage driver.</p><p class="calibre_8">The following output shows the default AUFS storage driver used by Docker running on the Ubuntu system:</p><p class="calibre_9"><img src="images/00408.jpg" class="calibre_245"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows Docker using the Overlay driver in the CoreOS node. CoreOS was using <tt class="calibre2">btrfs</tt> sometime back. Due to <tt class="calibre2">btrfs</tt> stability issues, they moved to the Overlay driver recently.</p><p class="calibre_9"><img src="images/00046.jpg" class="calibre_246"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">/var/lib/docker</tt> directory is where the container metadata and volume data is stored. The following important information<a/> is stored here:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Containers: The container metadata</li><li value="2" class="calibre_13">Volumes: The host volumes</li><li value="3" class="calibre_13">Storage drivers such as aufs and device mapper: These will contain diffs and layers</li></ul><p class="calibre_8">The following screenshot shows the directory output in the Ubuntu system running Docker:</p><p class="calibre_9"><img src="images/00062.jpg" class="calibre_247"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_153"/>


<p id="filepos447806" class="calibre_9"><span class="calibre3"><span class="bold">Docker and the Union filesystem</span></span></p><p class="calibre_8">Docker images make<a/> use of the Union filesystem to create an image<a/> composed of multiple layers. The Union filesystem makes use of the CoW techniques. Each layer is like a <a/>snapshot of the image with a particular<a/> change. The following example shows you the image layers of an Ubuntu docker image:</p><p class="calibre_9"><img src="images/00417.jpg" class="calibre_110"/></p><p class="calibre_8">
</p><p class="calibre_8">Each layer shows the operations done on the base layer to get this new layer.</p><p class="calibre_8">To illustrate the layering, let's take this base Ubuntu image and create a new container image using the following Dockerfile:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">FROM ubuntu:14.04</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">MAINTAINER Sreenivas Makam &lt;smxxxx@yahoo.com&gt;</span></tt><tt class="calibre2"><br class="calibre4"/><br class="calibre4"/></tt><tt class="calibre2"><span class="bold"># Install apache2</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">RUN apt-get install -y apache2</span></tt><tt class="calibre2"><br class="calibre4"/><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">EXPOSE 80</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ENTRYPOINT ["/usr/sbin/apache2ctl"]</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">CMD ["-D", "FOREGROUND"]</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Build a new Docker image:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker build -t="smakam/apachetest"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the layers of this new screenshot:</p><p class="calibre_9"><img src="images/00419.jpg" class="calibre_248"/></p><p class="calibre_8">
</p><p class="calibre_8">The first four layers <a/>are the ones created from the Dockerfile, and the last four layers are part of the Ubuntu 14.04 base image. In case you have the Ubuntu 14.04 image in your system and try to download <tt class="calibre2">smakam/apachetest</tt>, only the first four layers would be downloaded as the other layers will already be present in the host machine and <a/>can be reused. This layer<a/> reuse mechanism allows a faster download of Docker images from the Docker hub as well as efficient storage of Docker images in the <a/>Container filesystem.</p><div class="mbp_pagebreak" id="calibre_pb_154"/>


<p id="filepos450322" class="calibre_"><span class="calibre1"><span class="bold">Container data</span></span></p><p class="calibre_8">Container data is<a/> not part of the Container filesystem and is stored in the host filesystem where Container runs. Container data can be used to store data that needs to be manipulated frequently, such as a database. Container data typically needs to be shared between multiple Containers.</p><div class="mbp_pagebreak" id="calibre_pb_155"/>


<p id="filepos450781" class="calibre_9"><span class="calibre3"><span class="bold">Docker volumes</span></span></p><p class="calibre_8">Changes made in the container are stored as part of the Union filesystem. If we want to save some data outside the scope of the container, volumes can be used. Volumes are stored as part of the host filesystem and it gets mou<a/>nted in the Container. When container changes are committed, volumes are not committed as they reside outside the Container filesystem. Volumes can be used to share the source code with<a/> the host filesystem, maintain persistent data like a database, share data between containers, and function as a scratch pad for the container. Volumes give better performance over the Union filesystem for applications such as databases where we need to do frequent read and write operations. Using volumes does not guarantee Container data persistence. Using data-only Containers is an approach to maintain the persistence and share data across Containers. There are other approaches<a/> such as using shared and distributed storage to persist Container data across hosts.</p><p id="filepos451914" class="calibre_9"><span class="calibre3"><span class="bold">Container volume</span></span></p><p class="calibre_8">The following <a/>example starts the Redis container with the <tt class="calibre2">/data</tt> volume:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name redis -v /data redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If we run Docker to inspect Redis, we can get details about the volumes mounted by this container, as can be seen in the following screenshot:</p><p class="calibre_9"><img src="images/00122.jpg" class="calibre_249"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">Source</tt> directory is the directory in the host machine and <tt class="calibre2">Destination</tt> is the directory in the Container.</p><p id="filepos452801" class="calibre_9"><span class="calibre3"><span class="bold">Volumes with the host mount directory</span></span></p><p class="calibre_8">The following<a/> is an example of code sharing with the mounting host directory using Volume:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name nginxpersist -v /home/core/local:/usr/share/nginx/html -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If we perform <tt class="calibre2">docker inspect nginxpersist</tt>, we can see both the host directory and the container mount directory:</p><p class="calibre_9"><img src="images/00123.jpg" class="calibre_250"/></p><p class="calibre_8">
</p><p class="calibre_8">In the host machine, code development can be done in the <tt class="calibre2">/home/core/local</tt> location, and any code change<a/> in the host machine automatically reflects in the container.</p><p class="calibre_8">As the host directory can vary across hosts, this makes Containers unportable and Dockerfile does not support the host mount option.</p><p id="filepos454003" class="calibre_9"><span class="calibre3"><span class="bold">A data-only container</span></span></p><p class="calibre_8">Docker has support<a/> for a data-only container that is pretty powerful. Multiple containers can inherit the volume from a data-only container. The advantage with a data-only container over regular host-based volume mounting is that we don't have to worry about host file permissions. Another advantage is a data-only container can be moved across hosts, and some of the recent Docker volume plugins take care of moving the volume data when the container moves across hosts.</p><p class="calibre_8">The following example shows you how volumes can be persisted when containers die and restart.</p><p class="calibre_8">Let's create a volume container, <tt class="calibre2">redisvolume</tt>, for <tt class="calibre2">redis</tt> and use this volume in the <tt class="calibre2">redis1</tt> container. The <tt class="calibre2">hellocounter</tt> container counts the number of web hits and uses the <tt class="calibre2">redis</tt> container for counter-persistence:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name redisvolume -v /data redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name redis1 --volumes-from redisvolume redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's see the running containers:</p><p class="calibre_9"><img src="images/00126.jpg" class="calibre_89"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's access the <a/>hellocounter container multiple times using curl, as shown in the following image:</p><p class="calibre_9"><img src="images/00129.jpg" class="calibre_251"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's stop this container and restart another container using the following commands. The new redis container, <tt class="calibre2">redis2</tt>, still uses the same <tt class="calibre2">redisvolume</tt> container:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker stop redis1 hello1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker rm redis1 hello1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name redis2 --volumes-from redisvolume redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name hello2 --link redis2:redis -p 5001:5000 smakam/hellocounter python app.py</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If we try to access the hellocounter container using port <tt class="calibre2">5001</tt>, we will see that the counter starts from <tt class="calibre2">6</tt> as the previous value <tt class="calibre2">5</tt> is persisted in the database even though we have stopped that container and restarted a new redis container:</p><p class="calibre_9"><img src="images/00380.jpg" class="calibre_252"/></p><p class="calibre_8">
</p><p class="calibre_8">A data-only container can also be used to share data between containers. An example use case could be a web container writing a log file and a log processing container processing the log file and exporting it to a central server. Both the web and log containers can mount the same volume with one container writing to the volume and another reading from the volume.</p><p class="calibre_8">To back up the <tt class="calibre2">redisvolume</tt> container data that we created, we can use the following command:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run --volumes-from redisvolume -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /data</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This will take <tt class="calibre2">/data</tt> from <tt class="calibre2">redisvolume</tt> and back up the content to <tt class="calibre2">backup.tar</tt> in the current host <a/>directory using an Ubuntu container to do the backup.</p><p id="filepos458113" class="calibre_9"><span class="calibre3"><span class="bold">Removing volumes</span></span></p><p class="calibre_8">As part of removing a container, if we use the <tt class="calibre2">docker rm –v</tt> option, the volume will be automatically deleted. If we <a/>forget to use the <tt class="calibre2">-v</tt> option, volumes will be left dangling. This has the disadvantage that the space allocated in the host machine for the volume will be unused and not removed.</p><p class="calibre_8">Docker until release 1.7 does not yet have a native solution to handle dangling volumes. There are some experimental containers available to clean up dangling volumes. I use this test Container, <tt class="calibre2">martin/docker-cleanup-volumes</tt>, to clean up my dangling volumes. First, we can determine the dangling volumes using the <tt class="calibre2">dry-run</tt> option. The following is an example that shows four dangling volumes and one volume that is in use:</p><p class="calibre_9"><img src="images/00131.jpg" class="calibre_154"/></p><p class="calibre_8">
</p><p class="calibre_8">If we remove the <tt class="calibre2">dry-run</tt> option, dangling volumes will be deleted, as shown in the following image:</p><p class="calibre_9"><img src="images/00134.jpg" class="calibre_253"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_156"/>


<p id="filepos459560" class="calibre_9"><span class="calibre3"><span class="bold">The Docker Volume plugin</span></span></p><p class="calibre_8">Like the Network plugin for Docker, the Volume plugin extends storage functionality for Docker containers. Volume <a/>plugins provide advanced storage functionality such as volume persistence across nodes. The following figure shows you the <a/>volume plugin architecture where the Volume driver exposes a standard set of APIs, which plugins can implement. GlusterFS, Flocker, Ceph, and a few other companies provide Docker volume plugins. Unlike the Docker networking plugin, Docker does not have a native volume plugin and relies on plugins from external vendors:</p><p class="calibre_9"><img src="images/00137.jpg" class="calibre_254"/></p><p class="calibre_8">
</p><p id="filepos460435" class="calibre_9"><span class="calibre3"><span class="bold">Flocker</span></span></p><p class="calibre_8">Docker data volumes are<a/> tied to a single node where the Container is created. When Containers are moved across nodes, data volumes don't get moved. Flocker <a/>addresses this issue of moving the data volumes along with the Container. The following figure shows you all the important blocks in the Flocker architecture:</p><p class="calibre_9"><img src="images/00139.jpg" class="calibre_255"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are<a/> some internals of the Flocker implementation:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Flocker agent runs in each node and takes care of talking to the Docker daemon and the Flocker control service.</li><a/><li value="2" class="calibre_13">The Flocker control service takes care of managing the volumes as well as the Flocker cluster.</li><li value="3" class="calibre_13">Currently supported backend storage includes Amazon AWS EBS, Rackspace block storage, and EMC ScaleIO. Local storage using ZFS is available on an experimental basis.</li><li value="4" class="calibre_13">Both the REST API and Flocker CLI are used to manage volumes as well as Docker containers.</li><li value="5" class="calibre_13">Docker can manage volumes using Flocker as a data volume plugin.</li><li value="6" class="calibre_13">The Flocker plugin will take care of managing data volumes, which includes migrating the volume associated with the Container when the Container moves across hosts.</li><li value="7" class="calibre_13">Flocker will use the Container networking technology to talk across hosts—this can be native Docker networking or Docker networking plugins such as Weave.</li></ul><p class="calibre_8">In the next three examples, we will illustrate how Flocker achieves Container data persistence in different environments.</p><p id="filepos462643" class="calibre_9"><span class="bold">Flocker volume migration using AWS EBS as a backend</span></p><p class="calibre_8">This example will illustrate <a/>data persistence using AWS EBS as a storage backend. In this example, we will create three Linux nodes in the AWS cloud. One node will serve as the Flocker master running the control service and the other two nodes will run Flocker agents running the containers and mounting the EBS storage. Using these nodes, we will create a stateful Container and demonstrate Container data persistence on Container migration. The example will use a hellocounter container with the redis container backend and illustrates data persistence when the redis counter is moved across hosts. The following figure shows you how the master and agents are tied to the EBS backend:</p><p class="calibre_9"><img src="images/00141.jpg" class="calibre_256"/></p><p class="calibre_8">
</p><p class="calibre_8">I followed the procedure<a/> mentioned on the Flocker web page—<a href="https://docs.clusterhq.com/en/1.4.0/labs/installer.html">https://docs.clusterhq.com/en/1.4.0/labs/installer.html</a>—for this example.</p><p class="calibre_8">The following are the summary of steps to setup Flocker volume migration using AWS EBS:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">It's necessary to have an AWS account to create VMs running Docker containers and Flocker services.</li><li value="2" class="calibre_13">For the execution of frontend Flocker commands, we need a Linux host. In my case, it's a Ubuntu 14.04 VM.</li><li value="3" class="calibre_13">Install Flocker frontend tools on the Linux host using Flocker scripts.</li><li value="4" class="calibre_13">Install the Flocker control service on the control node and Flocker agents on the slave nodes using Flocker scripts.</li><li value="5" class="calibre_13">At this point, we can create containers on slave nodes with a data volume and migrate containers keeping the volume persistent.</li></ol><p class="calibre_8">The following are the relevant outputs after installing the Flocker frontend tools and Flocker control service and agents.</p><p class="calibre_8">This is the version of the Flocker frontend tools:</p><p class="calibre_9"><img src="images/00144.jpg" class="calibre_257"/></p><p class="calibre_8">
</p><p class="calibre_8">The Flocker node list shows the two AWS nodes that will run Flocker agents:</p><p class="calibre_9"><img src="images/00147.jpg" class="calibre_258"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output<a/> shows you the Flocker volume list. Initially, there are no volumes:</p><p class="calibre_9"><img src="images/00148.jpg" class="calibre_259"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the main processes in the master node. We can see the control service running in the following screenshot:</p><p class="calibre_9"><img src="images/00150.jpg" class="calibre_260"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the main processes in the slave node. Here, we can see the Flocker agents and the Flocker docker plugin running:</p><p class="calibre_9"><img src="images/00153.jpg" class="calibre_119"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's create a hellocounter container with the redis container backend on a particular slave node, update the counter in the database, and then move the container to demonstrate that the data volume gets persisted as the container is moved.</p><p class="calibre_8">Let's first set up some<a/> shortcuts:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">NODE1="52.10.201.177" (this public ip address corresponds to the private address shown in flocker list-nodes output)</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">NODE2="52.25.14.152"</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">KEY="keylocation "</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's start the <tt class="calibre2">hellocontainer</tt> and <tt class="calibre2">redis</tt> containers on <tt class="calibre2">node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the volumes created and attached at this point. 100 GB EBS volume is attached to slave <tt class="calibre2">node1</tt> at this point:</p><p class="calibre_9"><img src="images/00156.jpg" class="calibre_261"/></p><p class="calibre_8">
</p><p class="calibre_8">From the following output, we can see the two containers running in <tt class="calibre2">node1</tt>:</p><p class="calibre_9"><img src="images/00158.jpg" class="calibre_119"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's create some entries in the database now. The counter value is currently at <tt class="calibre2">6</tt>, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00160.jpg" class="calibre_262"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's remove the<a/> containers in <tt class="calibre2">NODE1</tt> and create the <tt class="calibre2">hellocounter</tt> and <tt class="calibre2">redis</tt> containers in <tt class="calibre2">NODE2</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker stop hellocounter</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker stop redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker rm -f hellocounter</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE1 docker rm -f redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE2 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -i $KEY root@$NODE2 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">As we can see, the volume has migrated to the second slave node:</p><p class="calibre_9"><img src="images/00163.jpg" class="calibre_263"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the containers in <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00166.jpg" class="calibre_264"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's check whether the data is persistent:</p><p class="calibre_9"><img src="images/00441.jpg" class="calibre_265"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see from the preceding output, the counter value starts from the previous count of <tt class="calibre2">6</tt> and is incremented to <tt class="calibre2">7</tt>, which<a/> shows that the redis database is persistent when the redis container is moved across the nodes.</p><p id="filepos470730" class="calibre_9"><span class="bold">Flocker volume migration using the ZFS backend</span></p><p class="calibre_8">This example will illustrate <a/>data persistence using ZFS as a storage backend and Vagrant Ubuntu cluster. ZFS is an open source filesystem that focuses on data integrity, replication, and performance. I followed the procedure at <a href="https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html">https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html</a> to set up a two-node<a/> Vagrant Ubuntu Flocker cluster and at <a href="https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html">https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html</a> to try out the sample <a/>application that allows container migration with the associated volume migration. The sample application uses the MongoDB container for data storage and illustrates data persistence.</p><p class="calibre_8">The following are the summary of steps to setup Flocker volume migration using ZFS backend:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Install Flocker client tools and the <tt class="calibre2">mongodb</tt> client in the client machine. In my case, this is a Ubuntu 14.04 VM.</li><li value="2" class="calibre_13">Create a two-node Vagrant Ubuntu cluster. As part of the cluster setup, Flocker services are started in each of the nodes and this includes control and agent services.</li><li value="3" class="calibre_13">Start the flocker-deploy script starting the <tt class="calibre2">mongodb</tt> container on <tt class="calibre2">node1</tt>.</li><li value="4" class="calibre_13">Start the <tt class="calibre2">mongodb</tt> client and write some entries in <tt class="calibre2">node1</tt>.</li><li value="5" class="calibre_13">Start the flocker-deploy script moving the <tt class="calibre2">mongodb</tt> container from <tt class="calibre2">node1</tt> to <tt class="calibre2">node2</tt>.</li><li value="6" class="calibre_13">Start the <tt class="calibre2">mongbdb</tt> client to <tt class="calibre2">node2</tt> and check whether the data is retained.</li></ol><p class="calibre_8">After starting the two-node Vagrant cluster, let's check the relevant Flocker services.</p><p class="calibre_8"><tt class="calibre2">Node1</tt> has both the Flocker control and agent services running, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00170.jpg" class="calibre_39"/></p><p class="calibre_8">
</p><p class="calibre_8"><tt class="calibre2">Node2</tt> has only the Flocker agent service running and is being managed by <tt class="calibre2">Node1</tt>, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00173.jpg" class="calibre_192"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the<a/> Flocker node list; this shows the two nodes:</p><p class="calibre_9"><img src="images/00176.jpg" class="calibre_266"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's deploy the <tt class="calibre2">mongodb</tt> container on <tt class="calibre2">node1</tt> as follows:</p><p class="calibre_9"><img src="images/00221.jpg" class="calibre_195"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the volume list. As we can see, the volume is attached to <tt class="calibre2">node1</tt>:</p><p class="calibre_9"><img src="images/00179.jpg" class="calibre_267"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows you the container in <tt class="calibre2">node1</tt>:</p><p class="calibre_9"><img src="images/00182.jpg" class="calibre_268"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's add some data to <tt class="calibre2">mongodb</tt>:</p><p class="calibre_9"><img src="images/00185.jpg" class="calibre_269"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's redeploy<a/> the container to <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00127.jpg" class="calibre_270"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the volume output. As we can see, the volume is moved to <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00135.jpg" class="calibre_271"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see from the following output, the <tt class="calibre2">mongodb</tt> content, <tt class="calibre2">the data</tt>, is preserved:</p><p class="calibre_9"><img src="images/00140.jpg" class="calibre_272"/></p><p class="calibre_8">
</p><p id="filepos475884" class="calibre_9"><span class="bold">Flocker on CoreOS with an AWS EBS backend</span></p><p class="calibre_8">Flocker has recently <a/>integrated with <a/>CoreOS on an experimental basis with the AWS EBS backend storage. I followed the procedures at https://github.com/ clusterhq/flocker-coreos and <a href="https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/">https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/</a> for this example. I had some issues with getting <a/>version 1.4.0 of the Flocker tools to work with CoreOS nodes. The 1.3.0 version of tools (<a href="https://docs.clusterhq.com/en/1.3.0/labs/installer.html">https://docs.clusterhq.com/en/1.3.0/labs/installer.html</a>) worked fine. </p><p class="calibre_8">In this example, we have<a/> illustrated Container data persistence on the CoreOS cluster with Docker using the Flocker plugin.</p><p class="calibre_8">The following are the summary of steps to setup Flocker volume migration on CoreOS cluster running on AWS:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a CoreOS cluster using AWS Cloudformation with the template specified by Flocker along with a newly created discovery token.</li><li value="2" class="calibre_13">Create <tt class="calibre2">cluster.yml</tt> with the node IP and access details.</li><li value="3" class="calibre_13">Start the Flocker script to configure the CoreOS nodes with the Flocker control service as well as Flocker agents. Flocker scripts also take care of replacing the default Docker binary in the CoreOS node with the Docker binary that supports the volume plugin.</li><li value="4" class="calibre_13">Check that Container migration is working fine with data persistence.</li></ol><p class="calibre_8">I used the following <a/>Cloudformation script to create a CoreOS cluster using the template file from Flocker:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">aws cloudformation create-stack     --stack-name coreos-test1     --template-body file://coreos-stable-flocker-hvm.template     --capabilities CAPABILITY_IAM     --tags Key=Name,Value=CoreOS     --parameters      ParameterKey=DiscoveryURL,ParameterValue="your token"         ParameterKey=KeyPair,ParameterValue="your keypair"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following are the details of the CoreOS cluster that has three nodes:</p><p class="calibre_9"><img src="images/00145.jpg" class="calibre_273"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the old and new Docker versions installed. Docker version 1.8.3 supports the Volume plugin:</p><p class="calibre_9"><img src="images/00149.jpg" class="calibre_274"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the CoreOS version in the node:</p><p class="calibre_9"><img src="images/00154.jpg" class="calibre_275"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the Flocker node list with three CoreOS nodes running Flocker:</p><p class="calibre_9"><img src="images/00159.jpg" class="calibre_276"/></p><p class="calibre_8">
</p><p class="calibre_8">I tried the same <tt class="calibre2">hellocounter</tt> example as mentioned in the previous section, and the volume moved<a/> automatically across the nodes. The following output shows the volume initially attached to <tt class="calibre2">node1</tt> and later moved to <tt class="calibre2">node2</tt> as part of the Container move.</p><p class="calibre_8">This is the volume attached to <tt class="calibre2">node1</tt>:</p><p class="calibre_9"><img src="images/00164.jpg" class="calibre_164"/></p><p class="calibre_8">
</p><p class="calibre_8">This is the volume attached to <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00169.jpg" class="calibre_277"/></p><p class="calibre_8">
</p><p class="calibre_8">According to the Flocker documentation, they have a plan to support the ZFS backend on CoreOS at some point, to allow us to use local storage instead of AWS EBS. It's still not certain if CoreOS will support ZFS natively.</p><p id="filepos480634" class="calibre_9"><span class="bold">Flocker recent additions</span></p><p class="calibre_8">Flocker added the<a/> following functionality recently, as of November 2015:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Flocker volume hub (<a href="https://clusterhq.com/volumehub/">https://clusterhq.com/volumehub/</a>) manages all Flocker volumes from a central location.</li><a/><li value="2" class="calibre_13">Flocker dvol (<a href="https://clusterhq.com/dvol/">https://clusterhq.com/dvol/</a>) provides you with a Git-like functionality for data volumes. This can help manage databases such as a codebase.</li><a/></ul><p id="filepos481373" class="calibre_14"><span class="calibre3"><span class="bold">GlusterFS</span></span></p><p class="calibre_8">GlusterFS is a distributed<a/> filesystem where the storage is distributed <a/>across multiple nodes and presented as a single unit. GlusterFS is an open source project and works on any kind of storage hardware. Red Hat has acquired Gluster, which started GlusterFS. The following are some properties of GlusterFS:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Multiple servers with their associated storage are joined to a GlusterFS cluster using the peering relationship.</li><a/><li value="2" class="calibre_13">GlusterFS can work on top of the commodity storage as well as SAN.</li><li value="3" class="calibre_13">By avoiding a central metadata server and using a distributed hashing algorithm, GlusterFS clusters are scalable and can expand into very large clusters.</li><li value="4" class="calibre_13">Bricks are the smallest component of storage from the GlusterFS perspective. A brick consists of mount points created from a storage disk with a base filesystem. Bricks are tied to a single server. A single server can have multiple bricks.</li><li value="5" class="calibre_13">Volumes are composed of multiple bricks. Volumes are mounted to the client device as a mount directory.</li><li value="6" class="calibre_13">Major volume types are distributed, replicated, and striped. A distributed volume type allows the distributing of files across multiple bricks. A replicated volume type allows multiple replicas of the file, which is useful from a redundancy perspective. A striped volume type allows the splitting of a large file into multiple smaller files and distributing them across the bricks.</li><li value="7" class="calibre_13">GlusterFS supports multiple access methods to access the GlusterFS volume, and this includes native FUSE-based access, SMB, NFS, and REST.</li></ul><p class="calibre_8">The following figure shows you the different layers of GlusterFS:</p><p class="calibre_9"><img src="images/00174.jpg" class="calibre_278"/></p><p class="calibre_8">
</p><p id="filepos483749" class="calibre_9"><span class="bold">Setting up a GlusterFS cluster</span></p><p class="calibre_8">In the following example, I have set up a two-node GlusterFS 3.5 cluster with each server running a Ubuntu 14.04 VM. I have <a/>used the GlusterFS server node as the GlusterFS client as well.</p><p class="calibre_8">The following is a summary of steps to setup a GlusterFS cluster:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Install the GlusterFS server on both the nodes and the client software on one of the nodes in the cluster.</li><li value="2" class="calibre_13">GlusterFS nodes must be able to talk to each other. We can either set up DNS or use a static <tt class="calibre2">/etc/hosts</tt> approach for the nodes to talk to each other.</li><li value="3" class="calibre_13">Turn off firewalls, if needed, for the servers to be able to talk to each other.</li><li value="4" class="calibre_13">Set up GlusterFS server peering.</li><li value="5" class="calibre_13">Create bricks.</li><li value="6" class="calibre_13">Create volumes on top of the created bricks.</li><li value="7" class="calibre_13">In the client machine, mount the volumes to mountpoint and start using GlusterFS.</li></ol><p class="calibre_8">The following commands need to be executed in each server. This will install the GlusterFS server component. This needs to be executed on both the nodes:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo apt-get install software-properties-common</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo add-apt-repository ppa:gluster/glusterfs-3.5</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo apt-get update</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo apt-get install glusterfs-server</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following command will install the GlusterFS client. This is necessary only in <tt class="calibre2">node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo apt-get install glusterfs-client</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Set up <tt class="calibre2">/etc/hosts</tt> to allow nodes to talk to each other:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">192.168.56.102  gluster1</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">192.168.56.101  gluster2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Disable the firewall:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo iptables -I INPUT -p all -s 192.168.56.102 -j ACCEPT</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo iptables -I INPUT -p all -s 192.168.56.101 -j ACCEPT</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Create the replicated volume and start it:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo gluster volume create volume1 replica 2 transport tcp gluster1:/gluster-storage gluster2:/gluster-storage force (/gluster-storage is the brick in each node)</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo gluster volume start volume1</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Set up a server probe in each node. The following command is for <tt class="calibre2">Node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo gluster peer probe gluster2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following command is for <tt class="calibre2">Node2</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo gluster peer probe gluster1</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Do a client mount of the <a/>GlusterFS volume. This is needed in <tt class="calibre2">Node1</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo mkdir /storage-pool</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo mount -t glusterfs gluster2:/volume1 /storage-pool</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Now, let's look at the status of the GlusterFS cluster and created volume in <tt class="calibre2">Node1</tt>:</p><p class="calibre_9"><img src="images/00178.jpg" class="calibre_279"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's look at <tt class="calibre2">Node2</tt>:</p><p class="calibre_9"><img src="images/00183.jpg" class="calibre_280"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the volume detail. As we can see, <tt class="calibre2">volume1</tt> is set up as the replicated volume type with two bricks on <tt class="calibre2">gluster1</tt> and <tt class="calibre2">gluster2</tt>:</p><p class="calibre_9"><img src="images/00074.jpg" class="calibre_281"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the client mount point in the <tt class="calibre2">df –k</tt> output:</p><p class="calibre_9"><img src="images/00078.jpg" class="calibre_282"/></p><p class="calibre_8">
</p><p class="calibre_8">At this point, we can write<a/> and read contents from the client mount point, <tt class="calibre2">/storage-pool</tt>.</p><p id="filepos489215" class="calibre_9"><span class="bold">Setting up GlusterFS for a CoreOS cluster</span></p><p class="calibre_8">By setting up CoreOS <a/>nodes to use the GlusterFS filesystem, Container volumes can use GlusterFS to store volume-related data. This allows Containers to<a/> move across nodes and keep the volume persistent. CoreOS does not support a local GlusterFS client at this point. One way to use GlusterFS in CoreOS is to export the GlusterFS volume through NFS and do NFS mounting from the CoreOS node.</p><p class="calibre_8">Continuing to use the GlusterFS cluster created in the previous section, we can enable NFS in the GlusterFS cluster as follows:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo gluster volume set volume1 nfs.disable off</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The <tt class="calibre2">cloud-config</tt> for CoreOS that was used in the <span class="italic">Mounting NFS Storage</span> section can be used here as well. The following is the mount-specific section where we have mounted the GlusterFS volume, <tt class="calibre2">172.17.8.111:/volume1</tt>, in <tt class="calibre2">/mnt/data</tt> of the CoreOS node:</p><p class="calibre_8"><tt class="calibre2">    - name: mnt-data.mount<br class="calibre4"/>      command: start<br class="calibre4"/>      content: |<br class="calibre4"/>        [Mount]<br class="calibre4"/>        What=172.17.8.111:/volume1<br class="calibre4"/>        Where=/mnt/data<br class="calibre4"/>        Type=nfs<br class="calibre4"/>        Options=vers=3,sec=sys,noauto</tt></p><p class="calibre_8">I created a bunch of files in the<a/> GlusterFS volume, <tt class="calibre2">/volume1</tt>, and I <a/>was able to read and write from the CoreOS node. The following output shows you the <tt class="calibre2">/mnt/data</tt> content in the CoreOS node:</p><p class="calibre_9"><img src="images/00082.jpg" class="calibre_125"/></p><p class="calibre_8">
</p><p id="filepos491194" class="calibre_9"><span class="bold">Accessing GlusterFS using the Docker Volume plugin</span></p><p class="calibre_8">Using the GlusterFS <a/>volume plugin (<a href="https://github.com/calavera/docker-volume-glusterfs">https://github.com/calavera/docker-volume-glusterfs</a>) for Docker, we can create and manage volumes using a regular Docker volume CLI.</p><p class="calibre_8">In the following example, we <a/>will install the GlusterFS Docker volume plugin and create a persistent <tt class="calibre2">hellocounter</tt> application. I used the same <a/>Ubuntu 14.04 VM that is running GlusterFS volumes to run Docker as well.</p><p class="calibre_8">The following are the steps needed to set up the Docker volume plugin:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Docker experimental release supports the GlusterFS volume plugin, so the experimental Docker release needs to be downloaded.</li><li value="2" class="calibre_13">The GlusterFS Docker volume plugin needs to be downloaded and started. GO (<a href="https://golang.org/doc/install">https://golang.org/doc/install</a>) needs to be installed to get the volume plugin.</li><a/><li value="3" class="calibre_13">Use Docker with the GlusterFS Docker volume plugin. For this, the Docker service needs to be stopped and restarted.</li></ul><p class="calibre_8">The following is the Docker experimental release version running in both the nodes:</p><p class="calibre_9"><img src="images/00087.jpg" class="calibre_283"/></p><p class="calibre_8">
</p><p class="calibre_8">Download and start the GlusterFS volume plugin:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">go get github.com/calavera/docker-volume-glusterfs</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo docker-volume-glusterfs -servers gluster1:gluster2 &amp;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the <tt class="calibre2">redis</tt> container with the GlusterFS volume driver as follows:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d -v volume1:/data --volume-driver=glusterfs --name=redis redis:latest</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Start the <tt class="calibre2">hellocounter</tt> container and link it to the <tt class="calibre2">redis</tt> container:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Update the counter by<a/> accessing it a few times, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00089.jpg" class="calibre_284"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, stop the containers in <tt class="calibre2">node1</tt> and start them in <tt class="calibre2">node2</tt>. Let's see the running containers in <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00092.jpg" class="calibre_285"/></p><p class="calibre_8">
</p><p class="calibre_8">If we access the <tt class="calibre2">hellocounter</tt> container now, we can see that the counter starts from <tt class="calibre2">3</tt> as the previous count is persisted:</p><p class="calibre_9"><img src="images/00094.jpg" class="calibre_286"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_157"/>


<p id="filepos494948" class="calibre_9"><span class="calibre3"><span class="bold">Ceph</span></span></p><p class="calibre_8">Ceph provides you <a/>with distributed storage like GlusterFS and is an open source project. Ceph<a/> was originally developed by Inktank and later acquired by Red Hat. The following are some properties of Ceph:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Ceph uses <span class="bold">Reliable Autonomic Distributed Object Store</span> (<span class="bold">RADOS</span>) as the storage mechanism. Other storage access mechanisms such as file and block are implemented on top of RADOS.</li><a/><li value="2" class="calibre_13">Both Ceph and GlusterFS seem to have similar properties. According to Red Hat, Ceph is positioned more for OpenStack integration and GlusterFS is for Big data analytics, and there will be some overlap.</li><li value="3" class="calibre_13">There are two key components in Ceph. They are Monitor and OSD. Monitor stores the cluster map and <span class="bold">Object Storage Daemons</span> (<span class="bold">OSD</span>) are the individual storage nodes that form the storage cluster. Both storage clients and OSDs use the CRUSH algorithm to distribute the data across the cluster.</li><a/></ul><p class="calibre_8">Compared to GlusterFS, setting up Ceph seemed a little complex and there is active work going on to run Ceph components as Docker containers as well as integrate Ceph with CoreOS. There is also work going on for the Ceph Docker volume plugin.</p><div class="mbp_pagebreak" id="calibre_pb_158"/>


<p id="filepos496541" class="calibre_9"><span class="calibre3"><span class="bold">NFS</span></span></p><p class="calibre_8">NFS is a distributed <a/>filesystem that allows client computers to access network storage as if the storage is attached locally. We can achieve Container data persistence using shared <a/>NFS storage.</p><p id="filepos496870" class="calibre_9"><span class="calibre3"><span class="bold">Container data persistence using NFS</span></span></p><p class="calibre_8">In this section, we will<a/> cover a web application example that uses NFS for data persistence. The following are some details of the application:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The <tt class="calibre2">hellocounter.service</tt> unit starts a container that keeps track of the number of web accesses to the application</li><li value="2" class="calibre_13"><tt class="calibre2">Hellocounter.service</tt> uses the <tt class="calibre2">redis.service</tt> container to keep track of the access count</li><li value="3" class="calibre_13">The Redis container uses NFS storage to store the data</li><li value="4" class="calibre_13">When the database container dies for some reason, Fleet restarts the container in another node in the cluster, and as the service uses NFS storage, the count is persisted</li></ul><p class="calibre_8">The following figure shows you the example used in this section:</p><p class="calibre_9"><img src="images/00097.jpg" class="calibre_287"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the<a/> prerequisites and the required steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Start the NFS server and a three-node CoreOS cluster mounting the NFS data as specified in the <span class="italic">Mounting NFS storage</span> section.</li><li value="2" class="calibre_13">Start <tt class="calibre2">hellocounter.service</tt> and <tt class="calibre2">redis.service</tt> using fleet with the X-fleet property to control the scheduling of the Containers. The <tt class="calibre2">hellocounter.service</tt> is started on all the nodes; <tt class="calibre2">redis.service</tt> is started on one of the nodes.</li></ol><p class="calibre_8">The code for <tt class="calibre2">Hellocounter@.service</tt> is as follows:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=hello counter with redis backend<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>Restart=always<br class="calibre4"/>RestartSec=15<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill %p%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm %p%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull smakam/hellocounter<br class="calibre4"/><br class="calibre4"/>ExecStart=/usr/bin/docker run --name %p%i -e SERVICE_NAME=redis -p 5000:5000 smakam/hellocounter python<br class="calibre4"/>app.py<br class="calibre4"/><br class="calibre4"/>ExecStop=/usr/bin/docker stop %p%i<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>X-Conflicts=%p@*.service</tt></p><p class="calibre_8">The code for <tt class="calibre2">Redis.service</tt> is as follows:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=app-redis<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>Restart=always<br class="calibre4"/>RestartSec=5<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill %p<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm %p<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull redis<br class="calibre4"/>ExecStart=/usr/bin/docker run --name redis -v /mnt/data/hellodata:/data redis<br class="calibre4"/><br class="calibre4"/>ExecStop=/usr/bin/docker stop %p<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>Conflicts=redis.service</tt></p><p class="calibre_8">Let's start three instances<a/> of <tt class="calibre2">hellocounter@.service</tt> and one instance of <tt class="calibre2">redis.service</tt>. The following screenshot shows three instances of <tt class="calibre2">hellocounter</tt> service and 1 instance of <tt class="calibre2">redis</tt> service running in the CoreOS cluster.</p><p class="calibre_9"><img src="images/00099.jpg" class="calibre_288"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see in the preceding screenshot, <tt class="calibre2">hellocounter@2.service</tt> and <tt class="calibre2">redis.service</tt> are in the same node node3.</p><p class="calibre_8">Let's try accessing the web <a/>service from <tt class="calibre2">node3</tt> a few times to check the count:</p><p class="calibre_9"><img src="images/00102.jpg" class="calibre_289"/></p><p class="calibre_8">
</p><p class="calibre_8">The counter value is currently at <tt class="calibre2">6</tt> and stored in NFS.</p><p class="calibre_8">Now, let's reboot <tt class="calibre2">node3</tt>. As shown in the following output, we can see only two machines:</p><p class="calibre_9"><img src="images/00104.jpg" class="calibre_290"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the services running. As can be seen from the following output, <tt class="calibre2">redis.service</tt> has moved from <tt class="calibre2">node3</tt> to <tt class="calibre2">node2</tt>:</p><p class="calibre_9"><img src="images/00108.jpg" class="calibre_291"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's check the web access count in <tt class="calibre2">node2</tt>. As we can see from the following output, the count started at <tt class="calibre2">7</tt> as the<a/> previous count was set to <tt class="calibre2">6</tt> on <tt class="calibre2">node3</tt>. This proves that container data is persisted:</p><p class="calibre_9"><img src="images/00110.jpg" class="calibre_292"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: This example is not practical as there are multiple instances of a web server operating independently. In a more practical example, a load balancer would be the frontend. This example's purpose is just to illustrate container data persistence using NFS.</p><div class="mbp_pagebreak" id="calibre_pb_159"/>


<p id="filepos502733" class="calibre_"><span class="calibre1"><span class="bold">The Docker 1.9 update</span></span></p><p class="calibre_8">Docker 1.9 added<a/> named volumes, and this makes volumes as a first-class citizen in Docker. Docker volumes can be managed using <tt class="calibre2">docker volume</tt>.</p><p class="calibre_8">The following screenshot shows you the options in <tt class="calibre2">docker volume</tt>:</p><p class="calibre_9"><img src="images/00114.jpg" class="calibre_293"/></p><p class="calibre_8">
</p><p class="calibre_8">A named volume deprecates a data-only container that was used earlier to share volumes across Containers.</p><p class="calibre_8">The following set of commands shows the same example used earlier with a named volume instead of a data-only container:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker volume create --name redisvolume</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name redis1 -v redisvolume:/data redis</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">In the preceding example, we create a named volume, <tt class="calibre2">redisvolume</tt>, which is used in the <tt class="calibre2">redis1</tt> container. The <tt class="calibre2">hellocounter</tt> application links to the <tt class="calibre2">redis1</tt> container.</p><p class="calibre_8">The following screenshot shows you information about the <tt class="calibre2">redis1</tt> volume:</p><p class="calibre_9"><img src="images/00116.jpg" class="calibre_294"/></p><p class="calibre_8">
</p><p class="calibre_8">Another advantage with<a/> named volumes is that we don't need to worry about the dangling volume problem that was present before.</p><div class="mbp_pagebreak" id="calibre_pb_160"/>


<p id="filepos504721" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered different storage options available for the storing of Container images and Container data in a CoreOS system. Technologies such as Flocker, GlusterFS, NFS, and Docker volumes and their integration with Containers and CoreOS were illustrated with practical examples. Container storage technologies are still evolving and will take some time to mature. There is a general industry trend to move away from expensive SAN technologies toward local and distributed storage. In the next chapter, we will discuss Container runtime Docker and Rkt and how they integrate with CoreOS.</p><div class="mbp_pagebreak" id="calibre_pb_161"/>


<p id="filepos505465" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">GlusterFS: <a href="http://gluster.readthedocs.org/">http://gluster.readthedocs.org/</a></li><li value="2" class="calibre_13">The GlusterFS Docker volume plugin: <a href="https://github.com/calavera/docker-volume-glusterfs">https://github.com/calavera/docker-volume-glusterfs</a></li><a/><li value="3" class="calibre_13">Flocker: <a href="https://docs.clusterhq.com">https://docs.clusterhq.com</a></li><li value="4" class="calibre_13">Docker volume plugins: <a href="https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md">https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md</a></li><a/><li value="5" class="calibre_13">The Docker Storage driver: <a href="https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/">https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/</a></li><a/><li value="6" class="calibre_13">Docker volumes: <a href="https://docs.docker.com/userguide/dockervolumes/">https://docs.docker.com/userguide/dockervolumes/</a></li><a/><li value="7" class="calibre_13">Mounting storage on CoreOS: <a href="https://coreos.com/os/docs/latest/mounting-storage.html">https://coreos.com/os/docs/latest/mounting-storage.html</a></li><a/><li value="8" class="calibre_13">The Container filesystem: <a href="http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1">http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1</a></li><a/><li value="9" class="calibre_13">Ceph: <a href="http://docs.ceph.com/">http://docs.ceph.com/</a></li><li value="10" class="calibre_13">Ceph Docker: <a href="https://github.com/ceph/ceph-docker">https://github.com/ceph/ceph-docker</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_162"/>


<p id="filepos507610" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Persistent data storage in the CoreOS cluster: <a href="https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1">https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1</a></li><a/><li value="2" class="calibre_13">Creating a GlusterFS cluster: <a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers">https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers</a></li><a/><li value="3" class="calibre_13">GlusterFS Overview: <a href="https://www.youtube.com/watch?v=kvr6p9gSOX0">https://www.youtube.com/watch?v=kvr6p9gSOX0</a></li><a/><li value="4" class="calibre_13">Stateful Containers using Flocker on CoreOS: <a href="http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047">http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047</a></li><a/><li value="5" class="calibre_13">Docker Storage webinar: <a href="https://blog.docker.com/2015/12/persistent-storage-docker/">https://blog.docker.com/2015/12/persistent-storage-docker/</a></li><a/><li value="6" class="calibre_13">The Contiv volume plugin: <a href="https://github.com/contiv/volplugin">https://github.com/contiv/volplugin</a></li><a/><li value="7" class="calibre_13">Ceph RADOS: <a href="http://ceph.com/papers/weil-rados-pdsw07.pdf">http://ceph.com/papers/weil-rados-pdsw07.pdf</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_163"/>
</body></html>