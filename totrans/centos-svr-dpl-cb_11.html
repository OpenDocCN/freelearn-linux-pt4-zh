<html><head></head><body><div class="chapter" title="Chapter&#xA0;11.&#xA0;Safeguarding Against Threats"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Safeguarding Against Threats</h1></div></div></div><p>This chapter contains the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sending messages to Syslog</li><li class="listitem" style="list-style-type: disc">Rotating log files with logrotate</li><li class="listitem" style="list-style-type: disc">Using Tripwire to detect modified files</li><li class="listitem" style="list-style-type: disc">Using ClamAV to fight viruses</li><li class="listitem" style="list-style-type: disc">Checking for rootkits with chkrootkit</li><li class="listitem" style="list-style-type: disc">Using Bacula for network backups</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec86"/>Introduction</h1></div></div></div><p>From logging your system's activities to sniffing out rootkits, this chapter presents recipes to help protect the investment you've made in your system and its data against various threats. First, you'll learn how to set up a central log server using Syslog, and then, how to rotate log files to make sure that they don't grow out of control. Then, we'll look at how Tripwire is used to detect system intrusion by checking if changes have been made to important system files. This chapter also contains recipes for setting up ClamAV and chkrootkit to keep your system free of viruses, Trojans, rootkits, and other malware. We'll finish with how to set up a centralized backup server using Bacula to safeguard your data from everyday threats such as accidental deletion and hardware failures.</p></div></div>
<div class="section" title="Sending messages to Syslog"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec87"/>Sending messages to Syslog</h1></div></div></div><p>Syslog is a process that listens for messages from other applications and writes them to its log files, providing a common service to handle all logging activity. Messages can also be sent to a running instance of Syslog on a remote system acting as a centralized log server for your entire network. Apart from convenience, centralized logging can be useful for security reasons and also because it's harder for an attacker to cover their tracks when it's logged to a second system. In this recipe, you'll learn how to configure local and remote instances of Syslog to run your own log server.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec281"/>Getting ready</h2></div></div></div><p>This recipe requires two CentOS systems with working network connections. The recipe will refer to the first system as the local system and assume that it is configured with the IP address <code class="literal">192.168.56.100</code> and the hostname <code class="literal">benito</code>. The second system, referred to as the remote system, is assumed to have the address <code class="literal">192.168.56.35</code> and the hostname <code class="literal">logs</code>. The systems should be able to access each other by the hostnames; so, you will need to add the appropriate DNS records or override entries in the systems' <code class="literal">/etc/hosts</code> files. Administrative privileges are also required either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec282"/>How to do it...</h2></div></div></div><p>To forward log messages from the local system to the remote system, perform the following steps on the local system:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open Syslog's configuration file using your text editor:<pre class="programlisting">
<span class="strong"><strong>vi /etc/rsyslog.conf</strong></span>
</pre></li><li class="listitem">At the end of the file, add the following rule:<pre class="programlisting">
<span class="strong"><strong>*.*  @logs.example.com</strong></span>
</pre></li><li class="listitem">Save the change and close the configuration file.</li><li class="listitem">Restart Syslog for the updated configuration to take effect:<pre class="programlisting">
<span class="strong"><strong>systemctl restart rsyslog</strong></span>
</pre></li></ol></div><p>Then, to accept incoming log messages, perform the following steps on the remote system:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open Syslog's configuration file using your text editor:<pre class="programlisting">
<span class="strong"><strong>vi /etc/rsyslog.conf</strong></span>
</pre></li><li class="listitem">Locate the <code class="literal">$ModLoad</code> directive responsible for loading the <code class="literal">imudp</code> module and uncomment it by removing the leading <code class="literal">#</code> character. Uncomment the <code class="literal">$UDPServerRun</code> directive that immediately follows it as well:<pre class="programlisting">
<span class="strong"><strong>$ModLoad imudp</strong></span>
<span class="strong"><strong>$UDPServerRun 514</strong></span>
</pre></li><li class="listitem">Save the changes and close the configuration file.</li><li class="listitem">Restart Syslog for the updated configuration to take effect:<pre class="programlisting">
<span class="strong"><strong>systemctl restart rsyslog</strong></span>
</pre></li><li class="listitem">Open the firewall to UDP traffic on port <code class="literal">514</code>:<pre class="programlisting">
<span class="strong"><strong>firewall-cmd --zone=public --permanent --add-port=514/udp</strong></span>
<span class="strong"><strong>firewall-cmd --reload</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec283"/>How it works...</h2></div></div></div><p>Syslog receives messages through several logging facilities, and each message has an assigned priority/severity. Messages can be filtered based on their facility and priority so that the desired messages are relayed while the rest are discarded. A list of facilities and priorities are both outlined in RFC-5424 (the Syslog protocol), and Rsyslog (the version of Syslog available in CentOS) implements most of them.</p><p>Facilities offer a broad categorization designed to organize messages by the type of service that generates them. You can think of them as channels, where a message that logs a user's failed login attempt can be sent over the <code class="literal">auth</code> channel separate from messages logging the restart of a service sent over the <code class="literal">daemon</code> channel. Rsyslog's facilities are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">auth</code>: Security and authorization-related messages</li><li class="listitem" style="list-style-type: disc"><code class="literal">cron</code>: Messages from cron</li><li class="listitem" style="list-style-type: disc"><code class="literal">daemon</code>: Messages from system daemons</li><li class="listitem" style="list-style-type: disc"><code class="literal">kern</code>: Messages from the Linux kernel</li><li class="listitem" style="list-style-type: disc"><code class="literal">lpr</code>: Messages from the system's printer services</li><li class="listitem" style="list-style-type: disc"><code class="literal">mail</code>: Messages from the system's mail services</li><li class="listitem" style="list-style-type: disc"><code class="literal">news</code>: Messages from NTTP services</li><li class="listitem" style="list-style-type: disc"><code class="literal">syslog</code>: Messages generated by Syslog itself</li><li class="listitem" style="list-style-type: disc"><code class="literal">user</code>: User-level messages</li><li class="listitem" style="list-style-type: disc"><code class="literal">uucp</code>: Messages from UUCP services</li><li class="listitem" style="list-style-type: disc"><code class="literal">local0</code>-<code class="literal">local7</code>: User-level facilities for messages that aren't handled by the other facilities</li></ul></div><p>Priorities indicate the severity of the message, for example, a situation that generates an error message is more severe than one generating an informational or debugging message. Rsyslog's priorities are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">emerg</code>, <code class="literal">panic</code>: The system is unusable</li><li class="listitem" style="list-style-type: disc"><code class="literal">alert</code>: Immediate action is required</li><li class="listitem" style="list-style-type: disc"><code class="literal">crit</code>: A critical event happened</li><li class="listitem" style="list-style-type: disc"><code class="literal">err</code>, <code class="literal">error</code>: An error happened</li><li class="listitem" style="list-style-type: disc"><code class="literal">warn</code>, <code class="literal">warning</code>: A significant condition is encountered</li><li class="listitem" style="list-style-type: disc"><code class="literal">notice</code>: Notice messages</li><li class="listitem" style="list-style-type: disc"><code class="literal">info</code>: Informational messages</li><li class="listitem" style="list-style-type: disc"><code class="literal">debug</code>: Debugging messages</li></ul></div><p>The rules in Syslog's configuration file specify where a log is written to and they are made up of two parts—the first part is a pattern that identifies a facility and priority. It consists of both the facility and priority names separated by a dot, for example, <code class="literal">auth.warn</code> or <code class="literal">local2.debug</code>. More than one facility can be separated by commas, as in <code class="literal">auth,daemon,cron.warn</code>. Additionally, <code class="literal">*</code> can be used as a wildcard to match all facilities or priorities. <code class="literal">auth.*</code> represents messages coming through the <code class="literal">auth</code> facility of any priority, <code class="literal">*.warn</code> represents messages with a priority of <code class="literal">warn</code> or above from any facility, and <code class="literal">*.*</code> represents all messages regardless of facility or priority.</p><p>Messages that match the pattern are processed by the rule's second part, the action. Usually, the action is the location of a file that the message is written to, but it can also discard the message (use <code class="literal">~</code> as the location), send the message to a named pipe to be handled by an external process (prefix the location with <code class="literal">|</code>), or forward the message to another system (give a hostname as the location prefixed with <code class="literal">@</code>).</p><p>Since Rsyslog is installed, the service's configuration file is <code class="literal">/etc/rsyslogd.conf</code>. On the local system we added the following rule:</p><pre class="programlisting">
<span class="strong"><strong>*.*  @logs.example.com</strong></span>
</pre><p>This rule matches all messages and sends them to the server <code class="literal">logs.example.com</code>. One <code class="literal">@</code> means messages will be sent using UDP while two means they will be sent using TCP:</p><pre class="programlisting">
<span class="strong"><strong>*.*  @@archive.example.com</strong></span>
</pre><p>Then, we uncommented the following configuration on the remote system:</p><pre class="programlisting">
<span class="strong"><strong>$ModLoad imudp</strong></span>
<span class="strong"><strong>$UDPServerRun 514</strong></span>
</pre><p><code class="literal">$ModLoad</code> loads a Syslog module, in this case <code class="literal">imudp</code>, which handles incoming messages over UDP. The <code class="literal">$UDPServerRun</code> directive specifies the port which the module listens to for the messages. Traditionally, Syslog messages are sent to port <code class="literal">514</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note73"/>Note</h3><p>Syslog can be configured to transmit messages using TCP, but unless you have specific need to do so, I recommend that you use UDP. UDP is less reliable, but TCP entails more overhead and can result in more severe network congestion during heavy logging events.</p></div></div><div class="mediaobject"><img alt="How it works..." src="graphics/image_11_001.jpg"/><div class="caption"><p>The configuration file contains rules to direct messages to different files based on their facility and priorities</p></div></div><p>Many applications are capable of sending messages to Syslog, even if they write to their own log files by default. Some programs do so when given an appropriate argument on the command line, for example, MySQL accepts the <code class="literal">--syslog</code> argument. Others, such as BIND and Apache, require changes in their configuration files. Even the shell scripts you write can send messages to Syslog using the <code class="literal">logger</code> command as follows:</p><pre class="programlisting">
<span class="strong"><strong>logger -n logs.example.com -p user.notice "Test notice"</strong></span>
</pre><p><code class="literal">logger</code> accepts several arguments and then the log message. <code class="literal">-n</code> specifies the server where the message is sent (messages are sent to the local system's Syslog instance when not provided) and <code class="literal">-p</code> specifies the facility and priority for the message.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec284"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with Syslog:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Rsyslog website (<a class="ulink" href="http://www.rsyslog.com/">http://www.rsyslog.com/</a>)</li><li class="listitem" style="list-style-type: disc">Basic configuration of Rsyslog (<a class="ulink" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-basic_configuration_of_rsyslog.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-basic_configuration_of_rsyslog.html</a>)</li><li class="listitem" style="list-style-type: disc">RFC5424: The Syslog protocol (<a class="ulink" href="http://www.rfc-base.org/txt/rfc-5424.tx">http://www.rfc-base.org/txt/rfc-5424.tx</a>t)</li></ul></div></div></div>
<div class="section" title="Rotating log files with logrotate"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec88"/>Rotating log files with logrotate</h1></div></div></div><p>Log files are important because they provide better insight into what is happening on a system. The debugging and error messages in a log can be used to track down the source of a problem and resolve it quickly. Authentication messages maintain a record of who accessed the system and when, and repeated authentication failures can be a sign that an attacker is trying to gain unauthorized access. However, the usefulness of logs typically diminishes with age, and chatty applications that generate a lot of log entries could, if left unchecked, easily consume all of the system's storage resources. This recipe will show you how to rotate the log files to prevent the files from growing out of control and stale logs from wasting space.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec285"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with a working network connection. Administrative privileges are also required either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec286"/>How to do it...</h2></div></div></div><p>Follow these steps to configure log file rotation using <code class="literal">logrotate</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the <code class="literal">/etc/logrotate.d/example</code> file:<pre class="programlisting">
<span class="strong"><strong>vi /etc/logrotate.d/example</strong></span>
</pre></li><li class="listitem">Write the following contents to the file:<pre class="programlisting">
<span class="strong"><strong>/var/log/example.log {</strong></span>
<span class="strong"><strong>       monthly</strong></span>
<span class="strong"><strong>       rotate 4</strong></span>
<span class="strong"><strong>       missingok</strong></span>
<span class="strong"><strong>       notifempty</strong></span>
<span class="strong"><strong>       create 0600 root root</strong></span>
<span class="strong"><strong>       postrotate</strong></span>
<span class="strong"><strong>           kill -HUP $(cat /var/run/example.pid)</strong></span>
<span class="strong"><strong>       endscript</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></li><li class="listitem">Save your update and close the file.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec287"/>How it works...</h2></div></div></div><p><code class="literal">logrotate</code> rotates the log files by renaming them as sequential backups and creating a new file for the application to write to. While rotating <code class="literal">example.log</code>, it renames <code class="literal">example.log</code> to <code class="literal">example.log.1</code>. If <code class="literal">example.log.1</code> exists, it renames that file to <code class="literal">example.log.2</code> first (and so on for the other enumerated files).</p><p>For the sake of this example, this recipe created a new configuration to rotate the <code class="literal">/var/log/example.log</code> file. The main configuration file of <code class="literal">logrotate</code> is <code class="literal">/etc/logrotate.conf</code>, while additional files can be placed in the <code class="literal">/etc/logrotate.d</code> directory. You'll want to check <code class="literal">logrotate.d</code> to see if rotation for the application's logs you want to manage is already configured (many packages will drop a configuration file there as a courtesy). You can then update the configuration if the package maintainer's configuration doesn't suit your needs. Directives in the main file set the global behavior, which is overridden on a per-configuration basis by the additional files in <code class="literal">logrotate.d</code>.</p><p>The configuration supplies the name of the targeted log file followed by a braced set of directives that specifies how <code class="literal">logrotate</code> should manage the file. <code class="literal">*</code> can be used as a wildcard to match multiple files which is useful when an application writes to more than one log file. For example, the Apache HTTP server logs messages to <code class="literal">access_log</code> and <code class="literal">error_log</code> in /<code class="literal">var/log/http</code>. So it's configuration targets the log files as follows:</p><pre class="programlisting">
<span class="strong"><strong>/var/log/http/*log {</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>}</strong></span>
</pre><p>The <code class="literal">monthly</code> directive instructs <code class="literal">logrotate</code> to rotate the files on a monthly basis. Other options are <code class="literal">daily</code>, <code class="literal">weekly</code>, and <code class="literal">yearly</code>. Alternatively, you can instruct <code class="literal">logrotate</code> to manage files based on their size—the <code class="literal">size</code> directive specifies a size and <code class="literal">logrotate</code> will rotate those files that are larger than that.</p><pre class="programlisting">
<span class="strong"><strong>size 30k</strong></span>
</pre><p>If a value is given without a unit, the given value is understood as bytes. <code class="literal">logrotate</code> also supports <code class="literal">k</code> for kilobytes, <code class="literal">M</code> for megabytes, and <code class="literal">G</code> for gigabytes.</p><p>The <code class="literal">rotate</code> directive specifies how many log files to keep in the rotation. In our scenario, four files are allowed; so, <code class="literal">example.log.3</code> overwrites <code class="literal">example.log.4</code> and there is no <code class="literal">example.log.5</code>. The <code class="literal">missingok</code> directive lets <code class="literal">logrotate</code> know that it's okay to go on if a log file doesn't exist (its default behavior is to raise an error). Also, the <code class="literal">notifempty</code> directive instructs <code class="literal">logrotate</code> to skip rotating if the file is empty. The <code class="literal">create</code> directive instructs <code class="literal">logrotate</code> to create a new log file after renaming the original and supplies the mode, user, and group for the new file:</p><pre class="programlisting">
<span class="strong"><strong>rotate 4</strong></span>
<span class="strong"><strong>missingok</strong></span>
<span class="strong"><strong>notifempty</strong></span>
<span class="strong"><strong>create 0600 root root</strong></span>
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_11_002.jpg"/><div class="caption"><p>Rotated log files are numbered in sequence</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note74"/>Note</h3><p>The content of the original <code class="literal">example.log.4</code> file doesn't have to be lost. One option is to use the <code class="literal">mail</code> directive to instruct <code class="literal">logrotate</code> to e-mail its contents to you before overwriting it.</p><p><code class="literal">
<span class="strong"><strong>mail tboronczyk@example.com</strong></span>
</code></p><p>Personally though, I recommend using <code class="literal">mail</code> only if the file is relatively small since sending a large file can cause undue strain on the mail server. Also, a log file that contains sensitive information shouldn't be transmitted by e-mail. For sensitive logs and larger files, I recommend using <code class="literal">prerotate</code> to invoke <code class="literal">scp</code> or another utility to copy the file elsewhere before the rotation.</p><pre class="programlisting"><span class="strong"><strong>prerotate</strong></span>
<span class="strong"><strong> scp /var/log/example.log.4  storage@archive.example.com:example.log-$ (date +%F)</strong></span>
<span class="strong"><strong>endscript</strong></span></pre></div></div><p>We can specify external actions to be performed before and after the log files are rotated. The <code class="literal">prerotate</code> directive supplies a set of shell commands that will be executed before the rotation process begins, and the <code class="literal">postrotate</code> directive supplies commands that will be run after rotation. Both directives use <code class="literal">endscript</code> to mark the end of the command set as shown in the preceding tip and in the recipe's configuration. The configuration invokes <code class="literal">kill</code> to send the hang-up signal (<code class="literal">HUP</code>) to the example process which would reload that daemon. Some programs might be confused if the log file they're writing to is moved and recreated, and reloading it causes the program to reopen its connection to the log file so that it can continue logging:</p><pre class="programlisting"><span class="strong"><strong>postrotate</strong></span>
<span class="strong"><strong>    kill -HUP $(cat /var/run/example.pid)</strong></span>
<span class="strong"><strong>endscript</strong></span>
</pre><p><code class="literal">logrotate</code> is run daily via <code class="literal">cron</code>, so once you've created/adjusted your rotation's configuration you should be finished. The next time <code class="literal">logrotate</code> runs, it will pick up the update as it re-reads all of the configuration files.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec288"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with <code class="literal">logrotate</code>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">logrotate</code> manual page (<code class="literal">man 8 logrotate</code>)</li><li class="listitem" style="list-style-type: disc">Manage Linux log files with Logrotate (<a class="ulink" href="http://www.techrepublic.com/article/manage-linux-log-files-with-logrotate">http://www.techrepublic.com/article/manage-linux-log-files-with-logrotate</a>)</li><li class="listitem" style="list-style-type: disc">How to manage system logs (<a class="ulink" href="http://www.tecmint.com/manage-linux-system-logs-using-rsyslogd-and-logrotate/">http://www.tecmint.com/manage-linux-system-logs-using-rsyslogd-and-logrotate/</a>)</li></ul></div></div></div>
<div class="section" title="Using Tripwire to detect modified files"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec89"/>Using Tripwire to detect modified files</h1></div></div></div><p>This recipe shows you how to set up Tripwire, an auditing tool for detecting changes made to files on your system. Most often, Tripwire is positioned as an intrusion detection system because the unexpected modification of important configuration files is usually a sign of intrusion or malicious activity. Being able to monitor for such changes gives you the ability to detect and put a stop to malicious activity in a timely manner should it occur.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec289"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with a working network connection. The <code class="literal">tripwire</code> package is found in the EPEL repository, so the repository must be registered as discussed in <a class="link" href="ch04.html" title="Chapter 4. Software Installation Management">Chapter 4</a>, <span class="emphasis"><em>Software Installation Management</em></span>. Administrative privileges are also required, either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec290"/>How to do it...</h2></div></div></div><p>Follow these steps to monitor for system intrusions using Tripwire:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the <code class="literal">tripwire</code> package from the EPEL repository:<pre class="programlisting">
<span class="strong"><strong>yum install tripwire</strong></span>
</pre></li><li class="listitem">Run <code class="literal">tripwire-setup-keyfiles</code> to generate Tripwire's keyfiles and configuration and policy files:<pre class="programlisting">
<span class="strong"><strong>tripwire-setup-keyfiles</strong></span>
</pre><p>You will be prompted to provide a passphrase for the site keyfile and local keyfiles and then to give the site passphrase again to sign the configuration and policy files that are generated.</p></li><li class="listitem">Initialize Tripwire's database. You will be prompted to provide your local passphrase:<pre class="programlisting">
<span class="strong"><strong>tripwire --init 2&gt;output.txt</strong></span>
</pre></li><li class="listitem">Review warnings in the output to identify files that are defined in the policy but do not exist on your system:<pre class="programlisting">
<span class="strong"><strong>cat output.txt</strong></span>
</pre></li><li class="listitem">Comment out the entries in <code class="literal">/etc/tripwire/twpol.txt</code> that reference the nonexisting files in <code class="literal">output.txt</code>. If all of the warnings in <code class="literal">output.txt</code> were caused by nonexisting files, then you can automate this step as follows:<pre class="programlisting">
<span class="strong"><strong>for f in $(grep "Filename:" output.txt | cut -f2 -d:); do</strong></span>
<span class="strong"><strong>       sed -i "s|\($f\) |#\\1|g" /etc/tripwire/twpol.txt</strong></span>
<span class="strong"><strong>done</strong></span>
</pre></li><li class="listitem">Regenerate the signed policy file. Provide the password for the site keyfile when prompted:<pre class="programlisting">
<span class="strong"><strong>twadmin --create-polfile -S /etc/tripwire/site.key  &#13;
       /etc/tripwire/twpol.txt</strong></span>
</pre></li><li class="listitem">Delete the original database and initialize a new one. This time, the process should finish without generating any warnings:<pre class="programlisting">
<span class="strong"><strong>rm /var/lib/tripwire/benito.twd</strong></span>
<span class="strong"><strong>tripwire --init</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec291"/>How it works...</h2></div></div></div><p>Tripwire audits your system to detect which files have changed. The idea behind this is, if an attacker gains access to your system, they'll inevitably create or modify keyfiles to secure their presence. However, it would be trivial for an attacker to modify Tripwire's policy files to create the illusion that nothing has changed; so, the configuration and policy files are signed with a keyfile. The configuration file, policy file, and the keyfile are all generated when we run:</p><pre class="programlisting">
<span class="strong"><strong>tripwire-setup-keyfiles&#13;
</strong></span>
</pre><p>Because the default policy tries to be as comprehensive as possible for most users, there will be entries that aren't applicable to our CentOS system. If we were to run with the unmodified defaults then Tripwire would report the missing files, and sifting through the list of false positives would make it more difficult to identify if someone deleted a file of legitimate concern. Rather than reviewing the policy file manually, especially if you're not an expert and familiar with some of the files, the best approach is to run an initial scan on a system that is known to be clean and then let Tripwire report the nonexistent files. This will help save time as we try to tailor the policy to our system.</p><p>Initializing Tripwire's database is done using <code class="literal">tripwire --init</code>. The program will scan the system, comparing the filesystem with what it knows about in the policy file and collect statistics on the files that do exist. These statistics are stored in the database as a baseline metric for comparison the next time Tripwire runs to see if there have been changes. The recipe redirected the error output containing the list of missing files to a separate text file for two reasons: the list will be lengthy and it's sometimes easier to page through a file than scroll the terminal session, and we can script the process of customizing the policy based on that output:</p><pre class="programlisting">
<span class="strong"><strong>tripwire --init 2&gt;output.txt</strong></span>
</pre><p><code class="literal">sed</code> is the traditional search-and-replace workhorse and <code class="literal">grep</code> is great for finding and extracting lines of interest, so we can use these two tools to update the policy <code class="literal">/etc/tripwire/twpol.txt</code>. First, we need to know what the messages in <code class="literal">output.txt</code> look like:</p><pre class="programlisting">
<span class="strong"><strong>cat output.txt</strong></span>
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_11_003.jpg"/><div class="caption"><p>Nonexistent files generate a warning when initializing the Tripwire database</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note75"/>Note</h3><p>If all of the warnings in the output file are related to nonexistent files then it's safe to automate updating the policy. This is why we then carefully reviewed the contents before continuing.</p></div></div><p>We use <code class="literal">grep</code> to target the lines containing <code class="literal">Filename:</code> and then use <code class="literal">cut</code> to split the line on the colon and capture the second part—the name of the nonexistent file. The <code class="literal">for</code> loop captures each filename and assigns it to the variable <code class="literal">f</code>, which we can then reference in our pattern to <code class="literal">sed</code>. The pattern performs a global search and replace, using capturing parentheses and numeric back references to overwrite the filename with a leading <code class="literal">#</code>:</p><pre class="programlisting">
<span class="strong"><strong>for f in $(grep "Filename:" output.txt | cut -f2 -d:); do</strong></span>
<span class="strong"><strong>    sed -i "s|\($f\) |#\\1|g" /etc/tripwire/twpol.txt;</strong></span>
<span class="strong"><strong>done</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note76"/>Note</h3><p>It's important there is a space in the search space after the filename to make sure we only match the entire file. For example, we want to avoid a scenario where <code class="literal">/etc/rc.d</code> will also match <code class="literal">/etc/rc.d/init</code> because of the common prefix.</p></div></div><p>An unsigned, plain-text copy of the policy is stored at <code class="literal">/etc/tripwire/twpol.txt</code>. After we make our changes, we want to create a signed policy file which is used by Tripwire for the security reasons mentioned earlier. This is done with <code class="literal">twadmin</code> and the <code class="literal">--create-policy</code> argument. The <code class="literal">-S</code> argument provides the command with the path to our signing key and then we supply the plain-texted copy of the policy as the input:</p><pre class="programlisting">
<span class="strong"><strong>twadmin --create-polfile -S /etc/tripwire/site.key&#13;
/etc/tripwire/twpol.txt</strong></span>
</pre><p><code class="literal">twadmin</code> will sign the policy and write the result to <code class="literal">/etc/tripwire/tw.pol</code>. After the policy file has been modified we can then reinitialize the database. In fact, any time the policy file is updated you should regenerate the database, which is stored in <code class="literal">/var/lib/tripwire</code> and is named using the system's hostname:</p><pre class="programlisting">
<span class="strong"><strong>rm /var/lib/tripwire/benito.twd</strong></span>
<span class="strong"><strong>tripwire --init</strong></span>
</pre><p>To scan the system for violations, run Tripwire with the <code class="literal">--check</code> option:</p><pre class="programlisting">
<span class="strong"><strong>tripwire --check</strong></span>
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_11_004.jpg"/><div class="caption"><p>Tripwire reports its findings after a scan is performed</p></div></div><p>Of course, to be effective, a scan must be performed at least once a day. For this reason, a cron job is installed in <code class="literal">/etc/cron.daily</code> by the <code class="literal">tripwire</code> package which runs a Tripwire scan. Depending on how cron is configured, the output of the scan will probably be e-mailed by cron to the system's <code class="literal">root</code> user (and will most likely end up in <code class="literal">/var/spool/mail/root</code>). You can edit <code class="literal">/etc/cron.daily/tripwire-check</code> so that the output is e-mailed to you instead:</p><pre class="programlisting">
<span class="strong"><strong>test -f /etc/tripwire/tw.cfg &amp;&amp; /usr/sbin/tripwire --check |  &#13;
/bin/mailx -s "Tripwire Report" tboronczyk@example.com 2&gt;&amp;1</strong></span>
</pre><p>You can also configure Tripwire to send e-mails itself if you prefer. First, you'll want to ensure that Tripwire can send mail to your address. Issue the following to send a test message and then check to make sure it arrives in your inbox:</p><pre class="programlisting">
<span class="strong"><strong>tripwire --test --email tboronczyk@example.com</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note77"/>Note</h3><p>You can use supply the <code class="literal">--email-report</code> option when running a manual scan to have Tripwire send its results to your e-mail.</p><p>
</p><p><code class="literal">
<span class="strong"><strong>tripwire --check --email-report</strong></span>
</code></p><p>
</p><p>By default, Tripwire will attempt to send the e-mail via sendmail (or Postfix's sendmail interface). If you need to send the mail through an SMTP server instead, review the <span class="emphasis"><em>Email Notification Variables</em></span> section in <code class="literal">man 4 twconfig</code>.</p></div></div><p>Specifying the destination e-mail address is a bit more involved in Tripwire's configuration. The tests defined in the Tripwire policy file are grouped into rulesets, which allows files to be grouped together in a logical fashion. For example, there is a ruleset that tests the integrity of the Tripwire binaries themselves, which is separate from the ruleset that tests system administration programs. Each ruleset can have a defined e-mail address to send notifications to, which is great for flexibility where one administrator should be notified of modifications to one set of files and another admin should be notified about others:</p><pre class="programlisting">
<span class="strong"><strong>(</strong></span>
<span class="strong"><strong>  rulename = "Tripwire Binaries",</strong></span>
<span class="strong"><strong>  emailto = tboronczyk@example.com,</strong></span>
<span class="strong"><strong>  severity = $(SIG_HI)</strong></span>
<span class="strong"><strong>)</strong></span>
</pre><p>If you're the only administrator, repeatedly specifying the same address can be tedious. A better approach would define the e-mail address as a global variable and then let the creative use of <code class="literal">sed</code> come to the rescue.</p><p>First, edit <code class="literal">twpol.txt</code> to include the variable assignment for your e-mail address in the global variable definitions section:</p><pre class="programlisting">
<span class="strong"><strong>@@section GLOBAL</strong></span>
<span class="strong"><strong>TWROOT=/usr/sbin;</strong></span>
<span class="strong"><strong>TWBIN=/usr/sbin;</strong></span>
<span class="strong"><strong>TWPOL=/"/etc/tripwire";</strong></span>
<span class="strong"><strong>TWD="/var/lib/tripwire";</strong></span>
<span class="strong"><strong>TWSKEY="/etc/tripwire";</strong></span>
<span class="strong"><strong>TWLKEY="/etc/tripwire";</strong></span>
<span class="strong"><strong>TWREPORT="/var/lib/tripwire/report";</strong></span>
<span class="strong"><strong>HOSTNAME=benito;</strong></span>
<span class="strong"><strong>EMAILADDR="tboronczyk@example.com";</strong></span>
</pre><p>Save the change and close the file. Then, knowing each ruleset contains a <code class="literal">severity</code> directive, we can use a replacement pattern to insert the <code class="literal">mailto</code> directive:</p><pre class="programlisting">
<span class="strong"><strong>sed -i "s|\( \+\)\(severity = \)|\\1mailto =  \$(EMAILADDR),\n\\1\\2|g" &#13;
    /etc/tripwire/twpol.txt</strong></span>
</pre><p>The end result should include the <code class="literal">emailto</code> directive in each ruleset's definition:</p><pre class="programlisting">
<span class="strong"><strong>(</strong></span>
<span class="strong"><strong>  rulename = "Tripwire Binaries",</strong></span>
<span class="strong"><strong>  emailto = $(EMAILADDR),</strong></span>
<span class="strong"><strong>  severity = $(SIG_HI)</strong></span>
<span class="strong"><strong>)</strong></span>
</pre><p>After you inspect the results, resign the policy file and reinitialize the database.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec292"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with Tripwire:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to Tripwire (<code class="literal">man 8 twintro</code>)</li><li class="listitem" style="list-style-type: disc">Tripwire configuration manual page (<code class="literal">man 4 twconfig</code>)</li><li class="listitem" style="list-style-type: disc">Tripwire policy manual page (<code class="literal">man 4 twpolicy</code>)</li><li class="listitem" style="list-style-type: disc">Intrusion detection with Tripwire (<a class="ulink" href="http://www.akadia.com/services/tripwire.html">http://www.akadia.com/services/tripwire.html</a>)</li><li class="listitem" style="list-style-type: disc">How to set up and use Tripwire (<a class="ulink" href="http://www.linuxjournal.com/article/8758">http://www.linuxjournal.com/article/8758</a>)</li></ul></div></div></div>
<div class="section" title="Using ClamAV to fight viruses"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec90"/>Using ClamAV to fight viruses</h1></div></div></div><p>The threat from viruses, Trojans, and other forms of malware is real. They have grown exponentially in both quantity and in sophistication, and antivirus software have had to adopt sophisticated detection methods. While there's no guarantee that your system will not fall victim to these unwanted bits of code, remaining mindful when using the Internet and sharing files, implementing common-sense security policies, and using an up-to-date antivirus program can go a long way in protecting you. This recipe will show you how to install ClamAV, the professional-grade open-source antivirus program, keep its threat database up to date, and scan your system.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec293"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with a working network connection. The ClamAV packages can be found in the EPEL repository, so the repository must be registered as discussed in <a class="link" href="ch04.html" title="Chapter 4. Software Installation Management">Chapter 4</a>, <span class="emphasis"><em>Software Installation Management</em></span>. Administrative privileges are also required either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec294"/>How to do it...</h2></div></div></div><p>Follow these steps to install ClamAV and scan for viruses and Trojans:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the <code class="literal">clamav</code> and <code class="literal">clamav-update</code> packages from the EPEL repository:<pre class="programlisting">
<span class="strong"><strong>yum install clamav clamav-update</strong></span>
</pre></li><li class="listitem">Open the <code class="literal">freshclam</code> configuration file with your text editor:<pre class="programlisting">
<span class="strong"><strong>vi /etc/freshclam.conf</strong></span>
</pre></li><li class="listitem">Locate the <code class="literal">Example</code> line and add an <code class="literal">#</code> to the start of its line to comment it out:<pre class="programlisting">
<span class="strong"><strong># Comment or remove the line below</strong></span>
<span class="strong"><strong>#Example</strong></span>
</pre></li><li class="listitem">Save the update and close the file.</li><li class="listitem">Run <code class="literal">freshclam</code> to update the scanner's threat database:<pre class="programlisting">
<span class="strong"><strong>freshclam</strong></span>
</pre></li><li class="listitem">Create a <code class="literal">systemd</code> service file to manage the <code class="literal">freshclam</code> daemon for automate updates:<pre class="programlisting">
<span class="strong"><strong>vi /lib/systemd/system/freshclam.service</strong></span>
</pre></li><li class="listitem">Use the following for the file's content:<pre class="programlisting">
<span class="strong"><strong>[Unit]</strong></span>
<span class="strong"><strong>Description = freshclam daemon to update clamav</strong></span>
<span class="strong"><strong>After = network.target</strong></span>
<span class="strong"><strong>[Service]</strong></span>
<span class="strong"><strong>Type = forking</strong></span>
<span class="strong"><strong>ExecStart = /usr/bin/freshclam -d</strong></span>
<span class="strong"><strong>Restart = on-failure</strong></span>
<span class="strong"><strong>[Install]</strong></span>
<span class="strong"><strong>WantedBy=multi-user.target</strong></span>
</pre></li><li class="listitem">Force <code class="literal">systemd</code> to reload its services:<pre class="programlisting">
<span class="strong"><strong>systemctl daemon-reload</strong></span>
</pre></li><li class="listitem">Start the new <code class="literal">freshclam</code> service and enable it to start when the system reboots:<pre class="programlisting">
<span class="strong"><strong>systemctl start freshclam.service</strong></span>
<span class="strong"><strong>systemctl enable freshclam.service</strong></span>
</pre></li><li class="listitem">Scan the files in your <code class="literal">home</code> directory for threats using <code class="literal">clamscan</code>:<pre class="programlisting">
<span class="strong"><strong>clamscan -ir /home/tboronczyk</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec295"/>How it works...</h2></div></div></div><p>First, we installed the <code class="literal">clamav</code> and <code class="literal">clamav-update</code> packages. The <code class="literal">clamav</code> package contains the virus scanner while <code class="literal">clamav-update</code> contains the <code class="literal">freshclam</code> program, which updates ClamAV's virus definitions to keep it up to date:</p><pre class="programlisting">
<span class="strong"><strong>yum install clamav clamav-update</strong></span>
</pre><p><code class="literal">freshclam</code> reads its configuration from <code class="literal">/etc/freshclam.conf</code>. The file contains a line with the word <code class="literal">Example</code> to prevent users from using the defaults blindly and we must remove it or comment it out before we can use <code class="literal">freshclam</code>. The defaults settings are fine for our purposes and this is more of an annoyance than anything else, but it does force us to look at the file and see what behavior can be tweaked. Each directive is commented with an explanation and what the default behavior is.</p><p>Then, we ran <code class="literal">freshclam</code> to update the scanner's databases:</p><pre class="programlisting">
<span class="strong"><strong>freshclam</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note78"/>Note</h3><p>The process outputs its progress to the terminal and you may see several error messages. For example, it may report that it was unable to download a daily file. Don't panic; <code class="literal">freshclam</code> will try several mirrors. As long as it reports that <code class="literal">main.cvd</code>, <code class="literal">daily.cvd</code>, and <code class="literal">bytecode.cvd</code> are up to date when it's finished you know you have the latest definitions.</p></div></div><p>We can run <code class="literal">freshclam</code> any time we want to make sure the definition databases are up to date, but it would be inconvenient to have to always run it manually. When launched with the <code class="literal">-d</code> argument, <code class="literal">freshclam</code> will run in the daemon mode and periodically check for updates throughout the day (every two hours by default). To keep things clean, we created a service file to run <code class="literal">freshclam</code> and registered it with <code class="literal">systemd</code>:</p><pre class="programlisting">
<span class="strong"><strong>[Unit]</strong></span>
<span class="strong"><strong>Description = freshclam clamav update daemon</strong></span>
<span class="strong"><strong>After = network.target</strong></span>
<span class="strong"><strong>[Service]</strong></span>
<span class="strong"><strong>Type = forking</strong></span>
<span class="strong"><strong>ExecStart = /usr/bin/freshclam -d</strong></span>
<span class="strong"><strong>Restart = on-failure</strong></span>
<span class="strong"><strong>[Install]</strong></span>
<span class="strong"><strong>WantedBy=multi-user.target</strong></span>
</pre><p>The <code class="literal">[Unit]</code> section defines the basic attributes of the service, such as its description and that it relies on a network connection. The <code class="literal">[Service]</code> section defines the service itself, <code class="literal">ExecStart</code> will run <code class="literal">freshclam</code> with the <code class="literal">-d</code> argument, <code class="literal">Type</code> lets systemd know that the process will fork and run in the background as a daemon, and <code class="literal">Restart</code> will have systemd monitor the service and restart it automatically if it crashes. The <code class="literal">[Install]</code> section defines how it will be linked when we run <code class="literal">systemctl enable</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note79"/>Note</h3><p>The system file's content is pretty basic and can be used as a starting point for other custom services you write.</p></div></div><p>Scanning files for threats is done with <code class="literal">clamscan</code>:</p><pre class="programlisting">
<span class="strong"><strong>clamscan -ir /home/tboronczyk</strong></span>
</pre><p>The <code class="literal">-i</code> argument instructs the scanner to only output infected files as opposed to the name of every file it scans. <code class="literal">-r </code>triggers a recursive scan, descending into subdirectories. The path given can be an individual file to scan or a directory, in this case, our <code class="literal">home</code> directory:</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_11_005.jpg"/><div class="caption"><p>ClamAV provides a summary of its scan results</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note80"/>Note</h3><p>You can use EICAR's test files from <a class="ulink" href="http://www.eicar.org/85-0-Download.html">http://www.eicar.org/85-0-Download.html</a> to verify if ClamAV is working. Read their intended use page for more information at <a class="ulink" href="http://www.eicar.org/86-0-Intended-use.html">http://www.eicar.org/86-0-Intended-use.html</a>.</p></div></div><p>ClamAV is generally used in two ways—as a scanner to examine existing files to detect threats or as a filter to detect threats in a stream of data in real time. The easiest way to schedule a reoccurring scan is by setting up a cron job.</p><p>To create a personal cron job that runs <code class="literal">clamav</code> to scan your <code class="literal">home</code> directory, use <code class="literal">crontab</code>:</p><pre class="programlisting">
<span class="strong"><strong>crontab -e</strong></span>
</pre><p><code class="literal">crontab</code> will launch your default editor for you to enter the job schedule. Then <code class="literal">crontab</code> will automatically activate the job after you save the schedule and close the file.</p><p>An example schedule that runs <code class="literal">clamscan</code> every day at 3:00 a.m. might look as follows:</p><pre class="programlisting">
<span class="strong"><strong>0 3 * * * clamscan &gt;&gt; $HOME/clamscan.log</strong></span>
</pre><p>The first five columns specify the time when the job should run. The first column is the time's minutes, the second is hours, the third is the day of the month, the fourth is the month, and last is the day of the week when the job will run. <code class="literal">*</code> is used as a shorthand to indicate the entire range, thus the example will run every day of every month. More information can be found in the man page outlining the format of the <code class="literal">crontab</code> file (<code class="literal">man 5 crontab</code>).</p><p>On a server system, ClamAV is often run as a real-time scanner as a mail filter. Messages are received by the mail server, for example Postfix, and passed off to ClamAV for scanning. Assuming that you're running Postfix, as discussed in <a class="link" href="ch09.html" title="Chapter 9. Managing E-mails">Chapter 9</a>, <span class="emphasis"><em>Managing E-mails</em></span>, here's what you'll need to do to set up ClamAV and Postfix to work together.</p><p>First, we need to install some additional packages. The <code class="literal">clamav-scanner-systemd</code> package will install the functionality we need to run <code class="literal">clamscan</code> as a daemon so that it's always available and the <code class="literal">clamav-milter-systemd</code> package installs a mail filter that acts as a proxy between Postfix and the scanner:</p><pre class="programlisting">
<span class="strong"><strong>yum install clamav-scanner-systemd clamav-milter-systemd</strong></span>
</pre><p>Then, edit the configuration file <code class="literal">/etc/clamd.d/scan.conf</code>. Comment out the <code class="literal">Example</code> line and uncomment the <code class="literal">LocalSocket</code> option:</p><pre class="programlisting">
<span class="strong"><strong>LocalSocket /var/run/clamd.scan/clamd.sock</strong></span>
</pre><p>The value given with <code class="literal">LocalSocket</code> is the socket file used by the scanner daemon for communicating with outside processes.</p><p>Next, edit the <code class="literal">/etc/mail/clamav-milter.conf</code> file, which is the configuration file for the <code class="literal">clamav-milter</code> mail filter. Comment out the <code class="literal">Example</code> line, uncomment the first <code class="literal">MilterSocket</code> directive, and add the <code class="literal">ClamdSocket</code> directive. The value for <code class="literal">ClamdSocket</code> should be the same as the <code class="literal">LocalSocket</code> in <code class="literal">scan.conf</code> but prefixed with <code class="literal">unix:</code> to denote that it's a Unix socket:</p><pre class="programlisting">
<span class="strong"><strong>MilterSocket /var/run/clamav-milter/clamav.socket</strong></span>
<span class="strong"><strong>ClamdSocket unix:/var/run/clamd.scan/clamd.sock</strong></span>
</pre><p>Start and enable the scanner daemon and the filter services:</p><pre class="programlisting">
<span class="strong"><strong>system start clamd@scan.service clamav-milter.service</strong></span>
<span class="strong"><strong>system enable clamd@scan.service clamav-milter.service</strong></span>
</pre><p>Finally, open <code class="literal">/etc/postfix/main.cnf</code> and add an <code class="literal">smtpd_milters</code> entry which lets Postfix know about the filter:</p><pre class="programlisting">
<span class="strong"><strong>smtpd_milters=unix:/var/run/clamav-milter/clamav.socket</strong></span>
</pre><p>Don't forget to restart Postfix after updating its configuration:</p><pre class="programlisting">
<span class="strong"><strong>systemctl restart postfix.service</strong></span>
</pre></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec296"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with ClamAV:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">ClamAV documentation (<a class="ulink" href="http://www.clamav.net/documents/installing-clamav">http://www.clamav.net/documents/installing-clamav</a>)</li><li class="listitem" style="list-style-type: disc">European Institute for Computer Anti-Virus Research (<a class="ulink" href="http://www.eicar.org/">http://www.eicar.org/</a>)</li></ul></div></div></div>
<div class="section" title="Checking for rootkits with chkrootkit"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec91"/>Checking for rootkits with chkrootkit</h1></div></div></div><p>In the unfortunate event that an attacker gains access to your system, one of the first things they'll do is try to hide their intrusion while preserving access for as long as possible, perhaps by installing a rootkit. A rootkit is a program that runs stealthily and gives the attacker administrator access. They embed themselves in the Linux kernel to prevent detection, and there are even rootkits that can hide in a system firmware's dedicated memory allowing an attacker to control the system even when it's powered down. This recipe shows you how to check your system for rootkits using chkrootkit.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec297"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with a working network connection. Administrative privileges are also required, either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec298"/>How to do it...</h2></div></div></div><p>Follow these steps to use chkrootkit to check for rootkits:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the <code class="literal">gcc</code> and <code class="literal">glibc-static</code> packages that are needed to compile <code class="literal">chkrootkit</code> binaries:<pre class="programlisting">
<span class="strong"><strong>yum install gcc glibc-static</strong></span>
</pre></li><li class="listitem">Download <code class="literal">chkrootkit</code> source code:<pre class="programlisting">
<span class="strong"><strong>       curl -O ftp://ftp.pangeia.com.br/pub/seg/pac/chkrootkit.tar.gz</strong></span>
</pre></li><li class="listitem">Extract the downloaded source code archive and enter into the code's directory:<pre class="programlisting">
<span class="strong"><strong>tar xzvf chkrootkit.tar.gz</strong></span>
<span class="strong"><strong>cd chkrootkit-0.50</strong></span>
</pre></li><li class="listitem">Run make to compile chkrootkit's binary components:<pre class="programlisting">
<span class="strong"><strong>make</strong></span>
</pre></li><li class="listitem">chkrootkit requires <code class="literal">netstat</code> to conduct its network tests which is available in the <code class="literal">net-tools</code> package:<pre class="programlisting">
<span class="strong"><strong>yum install net-tools</strong></span>
</pre></li><li class="listitem">Run chkrootkit to scan for rootkits:<pre class="programlisting">
<span class="strong"><strong>./chkrootkit</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec299"/>How it works...</h2></div></div></div><p>chkrootkit consists of a shell script and a small collection of compiled utilities distributed as source code so we need to compile it. This means you'll need a compiler installed on your system. Minimally, <code class="literal">gcc</code> will suffice. Also, we need to install the <code class="literal">glibc-static</code> package because the project's <code class="literal">Makefile</code> builds a statically compiled binary—all of the binaries' dependencies are compiled in; it doesn't dynamically reference the copy of the system's shared libraries:</p><pre class="programlisting">
<span class="strong"><strong>yum install gcc glibc-static</strong></span>
</pre><p>The source code for chkrootkit is available on the project's website. The link used in the recipe is a direct link to the latest source archive and is downloaded using <code class="literal">curl</code>:</p><pre class="programlisting">
<span class="strong"><strong>curl -O ftp://ftp.pangeia.com.br/pub/seg/pac/chkrootkit.tar.gz</strong></span>
</pre><p>Once the download is complete, building chkrootkit's is a matter of extracting the archive, entering into the newly created directory, and running <code class="literal">make</code>:</p><pre class="programlisting">
<span class="strong"><strong>make</strong></span>
</pre><p>When you learned how to compile a program from source code in the <span class="emphasis"><em>Compiling a program from source</em></span> recipe of <a class="link" href="ch04.html" title="Chapter 4. Software Installation Management">Chapter 4</a>, <span class="emphasis"><em>Software Installation Management</em></span>, you used the common <code class="literal">configure</code>, <code class="literal">make</code>, and <code class="literal">make install</code> approach. However, chkrootkit doesn't ship with a configure script and its <code class="literal">Makefile</code> doesn't contain an <code class="literal">install</code> target. All we need to do here to kick off the compilation process is invoke <code class="literal">make</code> itself.</p><p>chkrootkit runs a series of tests to check for known rootkit signatures. Some of these tests use its compiled utilities while others use common system utilities. One of its network tests checks which ports are open using <code class="literal">netstat</code>, which is not installed by default on CentOS but is available in the <code class="literal">net-tools</code> package. So, before we can use chkrootkit, we need to install this dependency:</p><pre class="programlisting">
<span class="strong"><strong>   yum install net-tools</strong></span>
</pre><p>Once everything is installed, we can execute the chkrootkit script. When run without any arguments, chkrootkit executes all of its tests. Otherwise, we can specify one or more tests and only those will run. The <code class="literal">-l</code> (lowercase L) argument will display a list of possible tests:</p><pre class="programlisting">
<span class="strong"><strong>   ./chkrootkit -l</strong></span>
</pre></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec300"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with chkrootkit:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The chkrootkit website (<a class="ulink" href="http://www.chkrootkit.org">http://www.chkrootkit.org</a>)</li><li class="listitem" style="list-style-type: disc">Chkrootkit: check your system for hidden rootkits (<a class="ulink" href="https://www.youtube.com/watch?v=IdvdUv0Nsq4">https://www.youtube.com/watch?v=IdvdUv0Nsq4</a>)</li></ul></div></div></div>
<div class="section" title="Using Bacula for network backups"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec92"/>Using Bacula for network backups</h1></div></div></div><p>The fact of the matter is that we are living in a world that is becoming increasingly dependent on data. Also, from accidental deletion to a catastrophic hard drive failure, there are many threats to the safety of your data. The more important your data is and the more difficult it is to recreate if it were lost, the more important it is to have backups. So, this recipe shows you how you can set up a backup server using Bacula and how to configure other systems on your network to backup their data to it.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec301"/>Getting ready</h2></div></div></div><p>This recipe requires at least two CentOS systems with working network connections. The first system is the local system which we'll assume has the hostname <code class="literal">benito</code> and the IP address <code class="literal">192.168.56.41</code>. The second system is the backup server. You'll need administrative access on both systems, either by logging in with the <code class="literal">root</code> account or through the use of <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec302"/>How to do it...</h2></div></div></div><p>Perform the following steps on your local system to install and configure the Bacula file daemon:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the <code class="literal">bacula-client</code> package:<pre class="programlisting">
<span class="strong"><strong>       yum install bacula-client</strong></span>
</pre></li><li class="listitem">Open the file daemon's configuration file with your text editor:<pre class="programlisting">
<span class="strong"><strong>       vi /etc/bacula/bacula-fd.conf</strong></span>
</pre></li><li class="listitem">In the <code class="literal">FileDaemon</code> resource, update the value of the <code class="literal">Name</code> directive to reflect the system's hostname with the suffix <code class="literal">-fd</code>:<pre class="programlisting">
<span class="strong"><strong>       FileDaemon { &#13;
         Name = benito-fd &#13;
       ... &#13;
       }</strong></span>
</pre></li><li class="listitem">Save the changes and close the file.</li><li class="listitem">Start the file daemon and enable it to start when the system reboots:<pre class="programlisting">
<span class="strong"><strong>       systemctl start bacula-fd.service &#13;
       systemctl enable bacula-fd.service &#13;
</strong></span>
</pre></li><li class="listitem">Open the firewall to allow TCP traffic through to port <code class="literal">9102</code>:<pre class="programlisting">
<span class="strong"><strong>       firewall-cmd --zone=public --permanent --add-port=9102/tcp &#13;
       firewall-cmd --reload</strong></span>
</pre></li><li class="listitem">Repeat steps 1-6 on each system that will be backed up.</li></ol></div><p>Perform the following steps on the system designated as the backup server to install and configure the Bacula director, storage, and file daemons.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the <code class="literal">bacula-console</code>, <code class="literal">bacula-director</code>, <code class="literal">bacula-storage</code>, and <code class="literal">bacula-client</code> packages:<pre class="programlisting">
<span class="strong"><strong>yum install bacula-console bacula-director bacula-storage &#13;
       bacula-client</strong></span>
</pre></li><li class="listitem">Re-link the catalog library to use SQLite database storage:<pre class="programlisting">
<span class="strong"><strong>alternatives --config libbaccats.so</strong></span>
</pre></li><li class="listitem">Type 2 when asked to provide the selection number.</li><li class="listitem">Create the SQLite database file and import the table schema:<pre class="programlisting">
<span class="strong"><strong>       /usr/libexec/bacula/create_sqlite3_database &#13;
       /usr/libexec/bacula/make_sqlite3_tables &#13;
</strong></span>
</pre></li><li class="listitem">Open the director's configuration file with your text editor:<pre class="programlisting">
<span class="strong"><strong>vi /etc/bacula/bacula-dir.conf</strong></span>
</pre></li><li class="listitem">In the <code class="literal">Job</code> resource where <code class="literal">Name</code> has the value <code class="literal">BackupClient1</code>, change the value of the <code class="literal">Name</code> directive to reflect one of the local systems. Then add a <code class="literal">Client</code> directive with a value that matches that system's <code class="literal">FileDaemon</code><code class="literal">Name</code>:<pre class="programlisting">
<span class="strong"><strong>       Job { &#13;
         Name = "BackupBenito" &#13;
         Client = benito-fd &#13;
         JobDefs = "DefaultJob" &#13;
       }</strong></span>
</pre></li><li class="listitem">Duplicate the <code class="literal">Job</code> resource and update its directive values as necessary so that there is a <code class="literal">Job</code> resource defined for each system to be backed up.</li><li class="listitem">For each system that will be backed up, duplicate the <code class="literal">Client</code> resource where the <code class="literal">Name</code> directive is set to <code class="literal">bacula-fd</code>. In the copied resource, update the <code class="literal">Name</code> and <code class="literal">Address</code> directives to identify that system:<pre class="programlisting">
<span class="strong"><strong>       Client { &#13;
         Name = bacula-fd &#13;
         Address = localhost &#13;
         ... &#13;
       } &#13;
       Client { &#13;
         Name = benito-fd &#13;
         Address = 192.168.56.41 &#13;
         ... &#13;
       } &#13;
       Client { &#13;
         Name = javier-fd &#13;
         Address = 192.168.56.42 &#13;
         ... &#13;
       }</strong></span>
</pre></li><li class="listitem">Save your changes and close the file.</li><li class="listitem">Open the storage daemon's configuration file:<pre class="programlisting">       <span class="strong"><strong>vi /etc/bacula/bacula-sd.conf &#13;
</strong></span>
</pre></li><li class="listitem">In the <code class="literal">Device</code> resource where <code class="literal">Name</code> has the value <code class="literal">FileStorage</code>, change the value of the <code class="literal">Archive Device</code> directive to <code class="literal">/bacula</code>:<pre class="programlisting">       <span class="strong"><strong>Device { &#13;
         Name = FileStorage &#13;
         Media Type = File &#13;
         Archive Device = /bacula &#13;
       ...</strong></span>
</pre></li><li class="listitem">Save the update and close the file.</li><li class="listitem">Create the <code class="literal">/bacula</code> directory and assign it the proper ownership:<pre class="programlisting">      <span class="strong"><strong> mkdir /bacula &#13;
       chown bacula:bacula /bacula</strong></span>
</pre></li><li class="listitem">If you have SELinux enabled, reset the security context on the new directory:<pre class="programlisting">
<span class="strong"><strong>       restorecon -Rv /bacula</strong></span>
</pre></li><li class="listitem">Start the director and storage daemons and enable them to start when the system reboots:<pre class="programlisting">      <span class="strong"><strong> systemctl start bacula-dir.service bacula-sd.service &#13;
       bacula-fd.service &#13;
       systemctl enable bacula-dir.service bacula-sd.service &#13;
       bacula-fd.service</strong></span>
</pre></li><li class="listitem">Open the firewall to allow TCP traffic through to ports <code class="literal">9101-9103</code>:<pre class="programlisting">       <span class="strong"><strong>firewall-cmd --zone=public --permanent --add-port=9101-9103/tcp &#13;
       firewall-cmd -reload</strong></span>
</pre></li><li class="listitem">Launch Bacula's console interface:<pre class="programlisting">      <span class="strong"><strong> bconsole &#13;
</strong></span>
</pre></li><li class="listitem">Enter <code class="literal">label</code> to create a destination for the backup. When prompted for the volume name, use <code class="literal">Volume0001</code> or a similar value. When prompted for the pool, select the <code class="literal">File</code> pool:<pre class="programlisting">      <span class="strong"><strong> label &#13;
</strong></span>
</pre></li><li class="listitem">Enter <code class="literal">quit</code> to leave the console interface.</li></ol></div></div><div class="section" title="How it works"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec303"/>How it works</h2></div></div></div><p>Configuring Bacula can be a daunting task for the most part because of the suite's distributed architecture and the level of flexibility it offers in organizing and scheduling backup and restore jobs. However, once everything is up and running, I'm sure you'll have peace of mind knowing that your data is safe from accidents and disasters.</p><p>Bacula is made up of several components. In this recipe, our efforts were centered on three daemons—the director, the file daemon, and the storage daemon. The file daemon is installed on each of the client systems to be backed up and listens for connections from the director. The director connects to each file daemon as scheduled and tells it which files to backup and where to copy them to (the storage daemon). The storage daemon receives the backed up data and writes it to the backup medium, for example, the disk or tape drive.</p><p>First, we installed the file daemon with the <code class="literal">bacula-client</code> package on our client systems. Then we edited the file daemon's configuration file found at <code class="literal">/etc/bacula/bacula-fd.conf</code> to specify the name of the process. The convention is to add the suffix <code class="literal">-fd</code> to the system's hostname:</p><pre class="programlisting">
<span class="strong"><strong>    FileDaemon { &#13;
      Name = benito-fd &#13;
      FDPort = 9102 &#13;
      WorkingDirectory = /var/spool/bacula &#13;
      Pid Directory = /var/run &#13;
      Maximum Concurrent Jobs = 20 &#13;
    }</strong></span>
</pre><p>After the update is made to the configuration, we started the service and opened the appropriate port in the system firewall. The file daemon is now listening, waiting for the director to connect and tell it what it needs to do.</p><p>On the backup server, we installed the <code class="literal">bacula-director</code>, <code class="literal">bacula-storage</code>, and <code class="literal">bacula-client</code> packages. This gives us the director and storage daemon, and another file daemon. The file daemon's purpose here on the backup server is to backup Bacula's catalog:</p><div class="mediaobject"><img alt="How it works" src="graphics/image_11_006.jpg"/><div class="caption"><p>This image reproduced from Bacula's documentation shows how the different applications relate to one another</p></div></div><p>Bacula maintains a database of metadata about previous backup jobs called the catalog, which can be managed by MySQL, PostgreSQL, or SQLite. SQLite is an embedded database library, meaning the program using it links against the SQLite library and manages its own database files. To support multiple databases, Bacula's code is written so that all the database access routines are contained in separate shared libraries with a different library for each database. Then, when Bacula wants to interact with a database, it does so through <code class="literal">libbaccats.so</code>, a <span class="emphasis"><em>fake</em></span> library that is nothing more than a symbolic link pointing to one of the specific database libraries. This let's Bacula support different databases without requiring us to recompile its source code.</p><p>To create the symbolic link, we used <code class="literal">alternatives</code> and select the real library that we want to use:</p><pre class="programlisting">
<span class="strong"><strong>    alternatives --config libbaccats.so</strong></span>
</pre><p>Then, we initialized the database's schema using the scripts that come with Bacula:</p><pre class="programlisting">
<span class="strong"><strong>    /usr/libexec/bacula/create_sqlite3_database &#13;
    /usr/libexec/bacula/make_sqlite3_tables</strong></span>
</pre><div class="mediaobject"><img alt="How it works" src="graphics/image_11_007.jpg"/><div class="caption"><p>Bacula supports multiple databases without recompiling</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note81"/>Note</h3><p>This recipe took advantage of Bacula's SQLite support because it's convenient and doesn't require additional effort to set up. If you want to use MySQL, install MySQL as discussed in <a class="link" href="ch07.html" title="Chapter 7. Working with Databases">Chapter 7</a>, <span class="emphasis"><em>Working with Databases</em></span>, create a dedicated MySQL user for Bacula to use, and then initialize the schema with the following scripts:</p><pre class="programlisting"><span class="strong"><strong>/usr/libexec/bacula/grant_mysql_privileges</strong></span>
<span class="strong"><strong> /usr/libexec/bacula/create_mysql_database</strong></span>
<span class="strong"><strong> /usr/libexec/bacula/make_mysql_tables</strong></span></pre><p>You'll also need to review Bacula's configuration files to provide Bacula with the required MySQL credentials.</p></div></div><p>Different resources are defined in the director's configuration file at <code class="literal">/etc/bacula/bacula-dir.conf</code>, many of which consist not only of their own values but also reference to other resources. For example, the <code class="literal">FileSet</code> resource specifies which files are included or excluded in backups and restores, while a <code class="literal">Schedule</code> resource specifies when backups should be made. A <code class="literal">JobDef</code> resource can contain various configuration directives that are common to multiple backup jobs and also reference particular <code class="literal">FileSet</code> and <code class="literal">Schedule</code> resources. <code class="literal">Client</code> resources identify the names and addresses of systems running file daemons, and a <code class="literal">Job</code> resource will pull together a <code class="literal">JobDef</code> and <code class="literal">Client</code> resource to define the backup or restore task for a particular system. Some resources define things at a more granular level and are used as building blocks to define other resources, creating complex definitions in a flexible manner.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip82"/>Tip</h3><p>The default resource definitions define basic backup and restore jobs sufficient for this recipe. You'll want to study the configuration and see how the different resources fit together so you can tweak them to better suit your backup needs.</p></div></div><div class="mediaobject"><img alt="How it works" src="graphics/image_11_008.jpg"/><div class="caption"><p>This image, reproduced from Bacula's documentation shows, how the different resources relate to one another</p></div></div><p>To get started, we customized the existing backup <code class="literal">Job</code> by changing its name and client. Then we customized the existing <code class="literal">Client</code> resource by changing its name and address to point to a specific system running a file daemon. The pair of <code class="literal">Job</code> and <code class="literal">Client</code> resources were duplicated, a pair for each system we're backing up. Notice that we also left a default <code class="literal">Client</code> resource that defines <code class="literal">bacula-fd</code> for the localhost. This is the file daemon that's local to the backup server and will be the target for things such as restore jobs and catalog backups:</p><pre class="programlisting">    <span class="strong"><strong>Job { &#13;
      Name = "BackupBenito" &#13;
      Client = benito-fd &#13;
      JobDefs = "DefaultJob" &#13;
    } &#13;
 &#13;
    Job { &#13;
      Name = "BackupJavier" &#13;
      Client = javier-fd &#13;
      JobDefs = "DefaultJob" &#13;
    } &#13;
 &#13;
    Client { &#13;
      Name = bacula-fd &#13;
      Address = localhost &#13;
      ... &#13;
    } &#13;
 &#13;
    Client { &#13;
      Name = benito-fd</strong></span>
<span class="strong"><strong>    Address = 192.168.56.100 &#13;
      ... &#13;
    } &#13;
 &#13;
    Client { &#13;
      Name = javier-fd &#13;
      Address = 192.168.56.100 &#13;
      ... &#13;
    }</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip83"/>Tip</h3><p>If you have a lot of client systems or a lot of job definitions, you can stay better organized by defining these resources in their own files and read them into <code class="literal">bacula-dir.conf</code>. Create the directory <code class="literal">/etc/bacula/config.d</code>, and place the individual configuration files there. Then add the following line to <code class="literal">bacula-dir.conf</code> to read them:</p><p>
</p><p><code class="literal"><span class="strong"><strong>@|"find /etc/bacula/config.d -name '*.conf' f -exec echo @{} \;" </strong></span></code></p></div></div><p>To complete the setup, we need to label a backup volume. This task, as with most others, is performed through <code class="literal">bconsole</code>, a console interface to the Bacula director.</p><p>We used the <code class="literal">label</code> command to define a label for the backup volume, and when prompted for the pool, we assigned the labeled volume to the <code class="literal">File</code> pool. In a way very similar to how logical volumes work (refer to <a class="link" href="ch05.html" title="Chapter 5. Managing Filesystems and Storage">Chapter 5</a>, <span class="emphasis"><em>Managing Filesystems and Storage</em></span>), an individual device or storage unit is allocated as a volume and the volumes are grouped into storage pools. If a pool contains two volumes backed by tape drives for example, and one of the drives is full, the storage daemon will write the backup data to the tape that has space available. Even though in our configuration we're storing the backup to disk, we still need to create a volume as the destination for data to be written to.</p><p>At this point, you should consider which backup strategy works best for you. A full backup is a complete copy of your data, a differential backup captures only the files that have changed since the last full backup, and an incremental backup copies the files that have changed since the last backup (regardless of the type of backup). Commonly, administrators employ a combination of these, perhaps making a full backup at the start of the week and then differential or incremental backups each day thereafter. This saves storage space because the differential and incremental backups are smaller and also convenient when the need to restore a file arises, because a limited number of backups need to be searched for the file.</p><p>Another consideration is the expected size of each backup and how long it will take for the backup to run to completion. Full backups obviously take longer to run, and in an office with 9-5 working hours, Monday through Friday, it may not be possible to run a full backup during the evenings. Performing a full backup on Fridays gives the backup time over the weekend to run. Smaller, incremental backups can be performed on the other days when time is lesser.</p><p>Still another point that is important in your backup strategy is how long the backups will be kept and where they will be kept. This touches on a larger issue, disaster recovery. If your office burns down, a year's worth of backups will be of no use if they were sitting in the office's IT closet. At one employer, we kept the last full backup and last day's incremental on a disk on site. These were then duplicated to tape and shipped off site.</p><p>Regardless of the strategy you choose to implement, your backups are only as good as your ability to restore data from them. You should periodically test your backups to make sure you can restore your files.</p><p>To run a backup job on demand, enter <code class="literal">run</code> in <code class="literal">bconsole</code>. You'll be prompted with a menu to select one of the current configured jobs. You'll then be presented with the job's options, such as what level of backup will be performed (full, incremental, or differential), it's priority, and when it will run. You can type <code class="literal">yes</code> or <code class="literal">no</code> to accept or cancel it or <code class="literal">mod</code> to modify a parameter. Once accepted, the job will be queued and assigned a job ID.</p><p>To restore files from a backup, use the <code class="literal">restore</code> command. You'll be presented with a list of options allowing you to specify which backup the desired files will be retrieved from. Depending on your selection, the prompts will be different. Bacula's prompts are rather clear, so read them carefully and it will guide you through the process.</p><p>Apart from the <code class="literal">run</code> and <code class="literal">restore</code> commands, another useful command is <code class="literal">status</code>. It will allow you to see the current status of the Bacula components, if there are any jobs currently running, and which jobs have completed. A full list of commands can be retrieved by typing <code class="literal">help</code> in <code class="literal">bconsole</code>.</p><div class="mediaobject"><img alt="How it works" src="graphics/image_11_009.jpg"/><div class="caption"><p>bconsole is a console interface to the Bacula director</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec304"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with Bacula:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Bacula documentation (<a class="ulink" href="http://blog.bacula.org/documentation/">http://blog.bacula.org/documentation/</a>)</li><li class="listitem" style="list-style-type: disc">How to use Bacula on CentOS 7 (<a class="ulink" href="https://www.digitalocean.com/community/tutorial_series/how-to-use-bacula-on-centos-7">https://www.digitalocean.com/community/tutorial_series/how-to-use-bacula-on-centos-7</a>)</li><li class="listitem" style="list-style-type: disc">Bacula-Web (a web-based reporting and monitoring tool for Bacula) (<a class="ulink" href="http://www.bacula-web.org/">http://www.bacula-web.org/</a>)</li></ul></div></div></div></body></html>