- en: Chapter 3. NSX Manager Installation and Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter explains the list of prerequisites and installation steps for
    NSX Manager installation. The following are the key points that we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: NSX Manager requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controller requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Controller deployments design considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX data plane installation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VMware NSX Manager is a preconfigured virtual appliance which we can download
    from the VMware website just like any other VMware software. This preconfigured
    virtual machine comes with 16 GB of memory, 4 VCPUs, and 60 GB of storage space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a quick look at the prerequisites for NSX Manager 6.2 installation:'
  prefs: []
  type: TYPE_NORMAL
- en: VMware vCenter server 6.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware ESXi 6.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Host clusters prepared with NSX 6.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere distributed switch which is supported with the respective version of
    host and virtual center
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure we have shared data stores through which we can leverage vSphere HA/DRS
    features to prevent downtime for NSX Manager VM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confirm whether we are using a dual stack or IPV4/IPV6 only networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect the Gateway, DNS, Syslog, and NTP server configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All port requirements are updated in the VMware public knowledge base article
    at [https://kb.vmware.com/kb/2079386](https://kb.vmware.com/kb/2079386).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After downloading the NSX Manager OVA, we will follow the four-step process
    for installation and configuration of the manager software as listed in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the NSX Manager OVA file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to NSX Manager
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish the NSX Manager and vCenter Server connection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure backup options
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that said, let's get started with the installation process. From my experience,
    I can confidently comment that deploying any appliances is a very easy task. However,
    when things start getting weird, we are forced to go back and review the deployment
    process and we find that the majority of the issues are something that we totally
    missed during the installation process wherein we just clicked the **Next**, and
    **Finish** buttons.
  prefs: []
  type: TYPE_NORMAL
- en: NSX Manager installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For installing NSX Manager, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open vCenter via VMware web client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **VMs and Templates**, right-click your data center, and select **Deploy
    OVF Template**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the VMware download URL or click **Browse** to select the file on your
    computer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click in the checkbox **Accept extra configuration options**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This allows you to set IPv4 and IPv6 addresses, default gateway, DNS, NTP, and
    SSH properties during the installation. If we do not set these configurations
    during deployment, we can always set them after the deployment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Accept the VMware license agreements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit the NSX Manager name (if required). Select the location for the deployed
    NSX Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This name will appear in the vCenter Server inventory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The folder you select will be used to apply permissions to the NSX Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a host or cluster on which to deploy the NSX Manager appliance. I would
    prefer selecting a cluster and letting DRS decide the best host for placing the
    appliance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the virtual disk format to **Thick Provision**, and select the destination
    data store for the virtual machine configuration files and the virtual disks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the management port group for NSX Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, review the screen after all the options are configured and we can see
    that this is an IPV4 only deployment. So stay focused and review the screen once
    more and if any corrections are required, we should go back to previous steps
    and correct it.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully deployed NSX Manager, it is worth double-checking
    all the configurations are intact and the manager appliance can ping DNS, NTP,
    Gateway, ESXi hosts, and VC. This is a very important step as any communication
    issues between manager and these components will have a direct impact on the functionality/deployment
    of VMware NSX features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows NSX Manager OVF details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![NSX Manager installation](img/image_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's make a note of the key configuration details from the preceding image
    and I will explain the design decisions for choosing it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the key configuration details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I said earlier, there are a few key design factors which we need to take
    care of while deploying NSX Manager appliances. Let's talk more about them.
  prefs: []
  type: TYPE_NORMAL
- en: Target - Management and Edge cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The management and edge cluster is a dedicated cluster in vSphere and it is
    always recommended to have a unique cluster for deploying all management components,
    which will ease any upgrade activity on the cluster without impacting the compute
    cluster, which is explicitly used for deploying end user virtual machines. In
    this example, we have a preconfigured vSphere cluster. And we leverage this cluster
    for deploying the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: NSX Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vcenter Server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any other third-party or VMware management software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But remember that NSX can be integrated with a wide range of VMware products,
    such as Horizon View, SRM, vCloud Director, VRA, VIO, and VCO. So, based on the
    type of product integration and data center design, situations might demand having
    multiple management clusters for isolation purposes. For example, if we have two
    vCenter Servers in the primary site, one for vSphere with NSX integration and
    a second vCenter Server for SRM integration, it is okay to create two separate
    management clusters. Again, this is a design choice, whether we want all our eggs
    in one basket or we are okay to place them in unique baskets (cluster, rack).
  prefs: []
  type: TYPE_NORMAL
- en: Network mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have connected NSX Manager to a vSphere distributed switch port group called
    **Mgmt_VDS_MGT**. These are preconfigured port groups which will be ideally configured
    with a VLAN port group. Either we can have a separate distributed switch for the
    management and edge cluster or we can have one single distributed switch which
    spans across multiple clusters. The preferred method of deployment would be having
    a unique distributed switch, as that would remain a VMotion boundary for management
    virtual machines. Yes, we don''t want a **Distributed Resource Scheduler** (**DRS**)
    or manual migration movement for those virtual machines to any other compute cluster,
    as this would defeat the purpose of having a unique cluster for management and
    compute machines. Added to that, based on the physical network design, let''s
    assume, as shown in the following screenshot, that we have a top-of-rack switch
    for the management and edge cluster which is running on a single rack. Any network-level
    changes in TOR switches have no impact on the compute cluster. Are we planning
    for LACP, static EtherChannel, or virtual PortChannel like configurations? It
    is important to note that the LACP configuration should be consistent across all
    port groups which are from the same distributed switch. The following diagram
    depicts a typical NSX enterprise vSphere design with separate compute and management
    and edge clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network mapping](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Compute and edge cluster with top of rack design
  prefs: []
  type: TYPE_NORMAL
- en: 'After confirming all our initial configurations are intact, we can log in to
    NSX Manager via a supported web browser, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network mapping](img/image_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: NSX Manager initial screen
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding image, we can see the NSX Manager initial page and for me
    there is a significant difference between vCloud networking security and the NSX
    Manager initial page: all the options are well arranged with a straightforward
    explanation. By default, we can log in to NSX Manager using the following credentials. The
    default username and password for NSX Manager is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: `admin`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Password: `default`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For security-hardening purposes, we can always change the password and create
    multiple users for management access. We can also see from the preceding image
    that on the right-hand side top corner, we have the initial configuration for
    NSX Manager along with the version of NSX, which is 6.2, and the build number.
  prefs: []
  type: TYPE_NORMAL
- en: NSX Manager virtual appliance management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s discuss different options that are listed in the preceding GUI. All
    these are key NSX appliance configuration and integration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**View summary**: The view summary page gives us the complete configuration
    summary of NSX manager. This includes Virtual Appliance DNS, IP, version, and
    uptime/current time, along with common components and management services as shown
    in the following screenshot:![NSX Manager virtual appliance management](img/image_03_004.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Manager summary
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One look at the preceding screenshot and we decipher that vPostgres, RabbitMQ,
    and management services should be up and running. **RabbitMQ** Server is a process
    which is hosted on NSX Manager and they interact with the **Firewall daemon (vsfwd**),
    which is running on the ESXi host via the message bus. We will discuss vsfwd in
    more detail in Chapter 6, *Configuring and Managing NSX Network Services*; however,
    as of now we have to understand the importance of this service and how they communicate
    in the NSX world. Postgres is the NSX Manager database which comes along with
    the appliance. Hence, it is very important to note that any editing or table changes
    in the database through any methods will have a direct impact and it is highly
    recommended to perform such practices with the help of the VMware support team.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Manage appliance settings**: Manage settings will be extremely helpful to
    make any configuration changes against NSX Manager. For example, there is a new
    Syslog Server configured and we would like to leverage a new Syslog Server to
    be configured with the current NSX Manager. We can easily go ahead and update
    the changes by editing the **Syslog Server** tab. In earlier versions of NSX,
    SSL was disabled by default. However, starting from the NSX 6.1 release, SSL is
    enabled by default. So let''s take a look at the following screenshot, which shows
    the **General** settings of NSX Manager:![NSX Manager virtual appliance management](img/B03244_03_05-1024x586.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time Settings, Syslog Server and Locale settings of NSX Manager
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Manage vCenter registration**: NSX Manager and vCenter Server have a one-to-one
    relationship. With cross VCenter Server NSX installation, this is one of the most
    confused topics as we believe multiple VC is supported with a single NSX instance,
    which is totally wrong. NSX 6.2 allows you to manage multiple vCenter NSX environments
    from a single primary NSX Manager. Even in a Cross vCenter Server NSX installation,
    the NSX Manager to vCenter Server relationship is still one-to-one. More about
    cross VC installation will be discussed in [Chapter 7](ch07.html "Chapter 7. NSX
    Cross vCenter"), *NSX Cross vCenter*, NSX-vCross-vCenter feature and design decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register vCenter Server with NSX Manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will quickly go ahead and register NSX Manager with vCenter Server by following
    steps. The procedure for NSX Manager registration:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, log in to the NSX Manager virtual appliance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Appliance Management**, click **Manage Appliance Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to type the IP address of the vCenter Server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type the vCenter Server username and password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click `OK`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for few seconds for successful connection to the vCenter Server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Register vCenter Server with NSX Manager](img/B03244_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Register SSO with NSX Manager
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For **single sign-on** (**SSO**) service integration with NSX, we need to configure
    the lookup service, which will improve the security of user authentication for
    vCenter users and enables NSX to authenticate from identity services such as AD,
    NIS, and LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Procedure for LookUp service registration:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the NSX Manager virtual appliance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Appliance Management**, click **Manage Settings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **NSX Management Service**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Edit** next to **Lookup Service**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type the name or IP address of the host that has the lookup service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the port number if required. The default port is **7444**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The lookup service URL is displayed based on the specified host and port. Type
    the vCenter administrator user name and password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This enables NSX Manager to register itself with the Security Token Service
    server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Confirm that the lookup service status is connected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure shows successful registration of the lookup service and
    vCenter Server registration for NSX Manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Register SSO with NSX Manager](img/B03244_03_07-1024x628.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s time to explore other NSX Manager settings, such as Tech Support Logs,
    upgrading, and restoring the management appliance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download Tech Support Log**: For diagnostic purposes, we can go ahead and
    download the NSX Manager logs by clicking the **download** button under **NSX
    Manager virtual appliance management**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backup and Restore**: NSX Manager data, including system configuration, events,
    and audit log tables (stored in the Postgres DB), can be backed up at any time
    by performing an on-demand backup from the NSX Manager GUI. It is also possible
    to schedule periodic backups to be performed (hourly, daily or weekly).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade**: Based on the version of NSX Manager or vCloud network security
    solution, we will be in need of upgrading the management plane. Once we download
    the upgrade bundle, we need to upload the same to the **NSX Manager-Upgrade-Upload
    New Bundle** tab and click on **Upgrade**. Please note, upgrading NSX Managers
    won''t upgrade control plane or data plane components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep in mind that we are using an administrator role account to register the
    vCenter Server with NSX. Also note that on ASCII characters in the password will
    create synchronization issues with NSX Manager. A successful registration of NSX
    Manager with vCenter Server will let us manage NSX Manager via VMware web client
    and we will see **Networking & Security** solution in the web client inventory
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Register SSO with NSX Manager](img/B03244_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Lastly, let's go ahead and license NSX Manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to the vCenter Server with the vSphere web client and perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the middle pane, click the **Solutions** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **NSX** for vSphere solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Assign License Key** link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Assign License Key** panel, select **Assign a new license key** from
    the drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **License key** textbox, enter or paste your NSX for vSphere license
    key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **OK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The VMware NSX for multi-hypervisor license key may also be used to license
    VMware NSX for vSphere.
  prefs: []
  type: TYPE_NORMAL
- en: NSX Manager deployment consideration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the critical role NSX plays in a vSphere environment, it is extremely
    important to know and implement NSX management, control plane and data plane features.
    NSX management is NSX Manager, which provides a single point for configuring all
    NSX features and in addition we can leverage REST-API calls for deployment, configuration,
    and other tasks. So let's talk about communication channels from the management
    plane to other components.
  prefs: []
  type: TYPE_NORMAL
- en: The communication path
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following list shows the communication path between NSX Manager and various
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NSX Manager to VCenter Server**: Communication between manager and VC is
    via vSphere API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX Manager to Controllers**: Communication between manager and controllers
    is via HTTPs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX Manager to ESXi hosts**: Communication between manager and underlying
    ESXi hosts would be via message bus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network and port requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NSX Manager virtual machines being part of the management plane, typically the
    NSX Manager and vCenter are placed on a single management network (vSphere PortGroup).
    I know most of the architects would be wondering having isolated networks (different
    subnets) for NSX Manager and vCenter Server will remain supported? The answer
    is *yes*, they can reside in different networks and also in different VLANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'NSX for vSphere protocol and port requirements are updated as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network and port requirements](img/B03244_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ensure that all these ports and protocols are allowed in the network.
  prefs: []
  type: TYPE_NORMAL
- en: User roles and permissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Firstly, NSX roles and permissions are totally different from vCenter Server
    roles and permissions. Hence, it is important to secure user access. Using **Role
    Based Access Control** (**RBAC**), we can secure a user access. Adding to that,
    we can also leverage vCenter SSO identity source users and groups once after properly
    configuring a lookup service with NSX Manager. NSX provides scope to restrict
    the area that a user can access in the NSX system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global**: The user has access to all areas of NSX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited access**: The user has access to only the NSX areas defined in the
    user profile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Various user roles are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enterprise Administrator:** NSX operations and security'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX Administrator**: NSX operations only, for example, install virtual appliances,
    configure port groups, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security Administrator**: Read and write access to NSX security area, such
    as defining data security policies, creating port groups, and creating reports
    for NSX modules, and has read-only access to other areas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auditor**: Has read-only access to all areas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A user cannot be defined without a role. After a role is assigned to users,
    the role can be changed. All these roles are extremely important when giving the
    permissions to NSX users and ensure we are giving limited access to non-enterprise
    administrator accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Controller requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we are clear about manager deployment and design decisions, let''s
    discuss controller requirements. NSX Controllers are also deployed as virtual
    appliances with default compute resources per controllers. Since we have already
    registered NSX Manager with virtual center and ensured that we have ports and
    protocols opened, let me re-emphasize once again why controllers are required:'
  prefs: []
  type: TYPE_NORMAL
- en: VXLAN unicast and hybrid mode replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed logical routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since NSX Controllers are virtual machines with control plane intelligence,
    from a network requirement perspective, they need to have an IP address. However,
    we don't stick with the traditional method of manual or DHCP discovery processes
    for IP assignments. Prior to controller deployment, let's configure IP pools.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure for controller IP pool creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'IP pools are used for assigning IP addresses to controllers and **VXLAN Tunnel
    Endpoints** (**VTEP**). I certainly love this feature, which is a less manual
    process. All we need is to create an IP pool and the controller will pick an IP
    from the pool while it is getting created; also, it will release an IP during
    the deletion time:'
  prefs: []
  type: TYPE_NORMAL
- en: From **vCenter Server Web Client**, navigate to **Networking Security** and
    under **Manage** select **Grouping Objects** as shown in the following screenshot:![The
    procedure for controller IP pool creation](img/B03244_03_10-1024x280.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **+ **icon and we need to configure a static IP pool so that individual
    controllers can pick one IP from this pool as shown in the following screenshot:![The
    procedure for controller IP pool creation](img/B03244_03_11.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have an IP pool ready, we can switch back to the NSX home installation
    page and click on the **+** sign under **NSX Controller Nodes** as shown in the
    following screenshot:![The procedure for controller IP pool creation](img/B03244_03_12-1024x624.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select and update respectively **vCenter Datacenter**, **Cluster/Resource Pool**,
    shared **Datastore** location, vSphere PortGroup for connectivity, and lastly
    the name of **IP pool** which we created in step 2\. After accomplishing the task,
    you'll see the following screenshot:![The procedure for controller IP pool creation](img/image_03_013.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the first controller is deployed, we can deploy an additional two more
    controllers by following the same steps since the three node control cluster is
    mandatory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Successful deployment of controllers will have a **Normal** status and a green
    check mark as shown in the following screenshot:![The procedure for controller
    IP pool creation](img/image_03_014.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s make a note of all three controller IPs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Controller 1**: **192.168.110.201**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller 2**: **192.168.110.202**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller 3**: **192.168.110.203**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even though we have controllers deployed and the status is green, it is important
    to check the control cluster connections and their status from the command line,
    which would give granular-level details. SSH to all three controllers and issue
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The controller types and their status are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Join status`: Verify the controller node is reporting join complete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Majority status`: Verify the controller is connected to the cluster majority'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster ID`: All the controller nodes in a cluster should have the same cluster
    ID'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Remember the controller roles that we discussed in [Chapter 2](ch02.html "Chapter 2. 
    NSX Architecture"), *NSX Architecture*? Yes, those are the five roles which are
    populated here, as shown in the following screenshot - `api_provider`, `persistence_server`,
    `switch_manager`, `logical_manager`, and `directory_server`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The procedure for controller IP pool creation](img/image_03_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Okay, this is all really great, but I know we have a few questions left. Let's
    have a look at those questions one by one.
  prefs: []
  type: TYPE_NORMAL
- en: How do we check which controller is master for those roles?
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will display that output for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do a random check on one of those controllers, in this example, `Controller
    2`, which is `192.168.110.202`. As we can see from the following screenshot, except
    for the persistence server, all the roles are master in controller `192.168.110.202`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The procedure for controller IP pool creation](img/image_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SSH session NSX Controller
  prefs: []
  type: TYPE_NORMAL
- en: 'The controller cluster majority leader listens on port `2878` and will have
    a `Y` in the `listening` column. To check that, let''s issue the following command
    and check on the same controller `192.168.110.202` that we have checked during
    step 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We got the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The procedure for controller IP pool creation](img/image_03_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Does that look bit weird?
  prefs: []
  type: TYPE_NORMAL
- en: We know that the controller cluster majority leader listens on port `2878` and
    would have a `**-**` in the `listening` column for persistence server for controller
    `192.168.110.202` . No rocket science here; as per the `Show Control-Cluster roles`
    output which we have tested in step 7, except for the persistence server role,
    for the rest of the roles, controller cluster 2 was master, hence `Show Control-Cluster
    Connections` is reporting `-` for the persistence server. I hope we now have some
    basic understanding of controller roles. We will discuss a few troubleshooting
    scenarios in [Chapter 8](ch08.html "Chapter 8.  NSX Troubleshooting"), *NSX Troubleshooting*.
  prefs: []
  type: TYPE_NORMAL
- en: NSX Controller design consideration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NSX Controller virtual machines are the DNA of the control plane, hence it is
    important to take decisions on where to install and connect the controller. Lastly,
    we don't want the controller to get exposed to users who are leveraging NSX features;
    basically, no control plane attack.
  prefs: []
  type: TYPE_NORMAL
- en: Communication path
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s good to know the communication protocol used between NSX Manager, controllers
    and NSX Edges:'
  prefs: []
  type: TYPE_NORMAL
- en: Communication between controller and NSX Manager - HTTPS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication between Edge and controller - HTTPS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network and port requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure that the port requirements mentioned in the following screenshot are
    met for controller communication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Network and port requirements](img/image_03_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Controller deployment consideration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I know, I keep telling you this: the real power of NSX is all about controllers.
    How we deploy our controllers, what best practices are implemented, all makes
    a vital difference in NSX design. You know by now, because of overlay networks,
    there will be a whole bunch of design best practices that we might need to do
    in both the physical and virtual worlds. But here, we will discuss controller
    deployment considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: First and foremost, NSX Controllers should be deployed in the same vCenter where
    NSX Manager is registered. The only exception would be while leveraging cross-VC
    NSX design, which we will discuss in [Chapter 7](ch07.html "Chapter 7. NSX Cross
    vCenter"), *NSX Cross vCenter*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While deploying the controllers, don't make any other configuration changes
    or deploy any other NSX features. The rule is the same even if we are deleting
    a controller and deploying a new one during break fix scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a separate vSphere Edge cluster for controller deployments. For small-scale
    deployments, it is okay to deploy controllers in a management cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX Controllers should be reachable with all ESXi host vmkernel networks and
    NSX Manager management networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The controller being a VM, most of the enterprise environments will have the
    vSphere **Distributed Resource Schedular** (**DRS**) feature and it will consider
    it as a normal virtual machine and will migrate or place it on same ESXi host
    based on the placement algorithm. To ensure controllers are not getting deployed
    or migrated to the same host and if there is a host failure this will have a direct
    impact on controllers and the entire control plane will be down. To avoid such
    situations, we will have to leverage vSphere anti affinity rules to avoid deploying
    more than one controller on the same ESXi host. Adding to that, I would highly
    recommend starting with more than three host clusters and later scale accordingly.
    This way, we can easily place controllers on separate ESXi hosts and scale accordingly.
    Don't get my message wrong, we are deploying three controllers on three different
    hosts and not leaving the rest of the host as a spare one. In any environment,
    we will be doing maintenance activity, sometimes as a part of a software upgrade
    or maybe adding or removing hardware devices from the server. For such scenarios,
    when we take a downtime for one of the hosts, we are still left with more than
    three ESXi hosts and controllers would be placed on them based on the anti affinity
    rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the hosts in the cluster should have automatic VM startup/shutdown enabled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first host where the controllers are deployed will have automatic VM startup/shutdown
    enabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: That summarizes controller deployment and key design aspects. With that, let's
    move to data plane preparation. Time to memorize what we did so far? Yes, let's
    do it. We have deployed NSX Manager (management plane) and registered the solution
    with vCenter Server. Later, we deployed NSX Controllers (control plane) and, finally,
    we are in the last phase of installation, which is the data plane.
  prefs: []
  type: TYPE_NORMAL
- en: The NSX data plane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NSX data plane consists of **vSphere Distributed Switch** (**VDS**), kernel
    modules, User World Agents, NSX Edge, and Distributed routing/firewall and bridging
    modules. Firstly, let's discuss preparing ESXi clusters for NSX.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is host preparation? All it does is install the kernel modules and builds
    a management and control plane domain. The kernel modules that we refer to here
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed firewall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VXLAN bridging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a multi-cluster environment, we need to perform this installation per cluster
    level. Based on the vSphere design, we can always introduce new hosts to the cluster
    and preparation is automated for newly added hosts, which makes life easier for
    architects.
  prefs: []
  type: TYPE_NORMAL
- en: The host preparation procedure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s discuss how an ESXi host is prepared to push the kernel modules:'
  prefs: []
  type: TYPE_NORMAL
- en: In vCenter, navigate to **Home** | **Networking & Security** | **Installation**
    and select the **Host Preparation** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the respective cluster and click the gear icon and click **Install**
    as shown in the following screenshot:![The host preparation procedure](img/image_03_019.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor the installation. Repeat the same steps for other clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, one common question from all vSphere folks: do we need to reboot the host
    since VIBS got installed? The answer is a **BIG FAT NO**! Only after uninstallation
    scenarios do we need a host reboot. We humans tend to forget things easily, don''t
    we? No problem, there would be a message populated near our ESXi host icon notifying
    *Reboot Required*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For each vSphere cluster, we will go ahead and configure ****VXLAN**** networking
    prerequisites.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Firstly, we will configure a **static pool** for the **VTEP IP** assignment,
    which is similar to the controller **IP pool** configuration that we did earlier.
    The **DHCP** pool assignment is also possible; however, in this case, I'm showcasing
    the IP assignment with static pools.
  prefs: []
  type: TYPE_NORMAL
- en: 'From NSX Manager, navigate to manage **IP pools** and click on the **+** sign.
    Update the following:'
  prefs: []
  type: TYPE_NORMAL
- en: VTEP **Name**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway** address'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prefix Length**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static IP Pool** for VTEP IP assignments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Have a look at the following screenshot for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The host preparation procedure](img/image_03_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Select one of the vSphere clusters and click the configure link provided in
    the VXLAN column:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the vSphere distributed switch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the **VLAN** number. Enter `0` if you're not using a VLAN, which will
    pass along untagged traffic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure **MTU** is `**1600**` (VXLAN overhead)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For VMKnic IP addressing, we need to make use of the earlier VTEP IP pool that
    we configured.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**VMKNic Teaming Policy** method is used for bonding the vmnics (physical NICs)
    for use with the VTEP port group. I have selected for fail over. The other options
    are Static EtherChannel, LACP (Active), LACP (Passive), Load Balance by Source
    ID, Load Balance by Source MAC, and Enhanced LACP. The following screenshot is
    updated with a list of supported VXLAN NIC teaming policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The host preparation procedure](img/image_03_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: VTEP NIC teaming design is extremely critical in NSX environments. Most customers
    would go with single VTEP configuration primarily because of the simplicity in
    the design. However, if we have more than 10G VXLAN traffic, LACP or Static EtherChannel
    would be the preferred load balancing policy.
  prefs: []
  type: TYPE_NORMAL
- en: VTEP Value is the number of VTEPs per host. Is there any specific reason why
    we would go for multi VTEP configuration? Well, if we have more than one physical
    link that we would like to use for VXLAN traffic and the upstream switches do
    not support LACP the use of multiple VTEPs allows us to balance the traffic between
    physical links.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the preceding step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The host preparation procedure](img/image_03_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Based on the requirement, we can repeat the same step for other clusters as
    well, and each such configuration will create a VXLAN distributed port group in
    vSphere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Successful installation of VXLAN modules will show as **Configured** as highlighted
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The host preparation procedure](img/image_03_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'I know most us will have a few design-related queries on VXLAN and how it works.
    Stay focused: we are going in the right direction and will discuss that in upcoming
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with an introduction to NSX Manager requirements and
    we covered all design aspects of the management plane. Later, we discussed NSX
    Controller requirements and key design decisions. Finally, we moved to data plane
    installation. In the next chapter, we will discuss managing and deploying NSX
    logical networks.
  prefs: []
  type: TYPE_NORMAL
