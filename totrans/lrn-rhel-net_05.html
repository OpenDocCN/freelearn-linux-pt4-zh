<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Implementing btrfs" id="aid-1565U1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Implementing btrfs</h1></div></div></div><p>In this chapter, we will investigate what is on offer with <code class="literal">btrfs</code> (pronounced as <span class="emphasis"><em>Better FS</em></span>). Although not directly related to networking, we will soon look at how to share filesystems; for this reason and as <code class="literal">btrfs</code> is so incredibly good, we will take a look at it right here and right now. <code class="literal">Btrfs</code> is a local filesystem that provides the benefits of integrated volume management operations with easy growth and a fault-tolerance built-in the filesystem. It's not fully supported by Red Hat and ships as a technology preview; it has to be said that Red Hat is cautious on this matter because SUSE has had <code class="literal">btrfs</code> as their default filesystem since Enterprise Linux 11 SP2 and continues on SLES 12.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Overview of <code class="literal">btrfs</code></li><li class="listitem">Overview of the lab environment</li><li class="listitem">Creating the <code class="literal">btrfs</code> filesystem</li><li class="listitem">The copy-on-write technology</li><li class="listitem">Resizing the <code class="literal">btrfs</code> filesystem</li><li class="listitem">Adding devices to the <code class="literal">btrfs</code> filesystem</li><li class="listitem">Mounting multidisk <code class="literal">btrfs</code> volumes from <code class="literal">/etc/fstab</code></li><li class="listitem">Implementing RAID with <code class="literal">btrfs</code></li><li class="listitem">Optimizing solid state drives</li><li class="listitem">Point-in-time data backups using snapshots</li><li class="listitem">Snapshot management with snappers</li></ul></div><div class="section" title="Overview of btrfs"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Overview of btrfs</h1></div></div></div><p>If there is one thing that <a id="id178" class="indexterm"/>Linux is able to offer you at present, it's a choice of filesystems with over 55 kernel-based filesystems on the Linux kernel tree. So, why do we need more? We are already seeing that older filesystems such as <code class="literal">xfs</code> are making a second coming with Red Hat championing this original filesystem from SGI. The <code class="literal">btrfs</code> filesystem provides a unique solution that combines the management of volume and a filesystem <a id="id179" class="indexterm"/>to a unified solution. <code class="literal">Btrfs</code> is licensed under the <span class="strong"><strong>General Public License</strong></span> (<span class="strong"><strong>GPL</strong></span>) and ships as standard on Red Hat Enterprise 7 and 7.1. It does not just provide access to file management, but also provides access <a id="id180" class="indexterm"/>to volume and the <span class="strong"><strong>Redundant Array of Inexpensive Disks</strong></span> (<span class="strong"><strong>RAID</strong></span>) management. This simple administration means that you can create RAID devices or extend volumes using single commands, rather than relying on LVM for logical volumes or <code class="literal">mdadm</code> for RAID. Scalability is also a major factor in choosing <code class="literal">btrfs</code>. This scales to 16 EB (Exabytes) and brings the following reliability <a id="id181" class="indexterm"/>features not found previously:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Very fast filesystem creation</li><li class="listitem">Data and metadata checksums</li><li class="listitem">Snapshotting</li><li class="listitem">Online scrub to fix issues</li></ul></div><p>When you look at organizations that use <code class="literal">btrfs</code> in production, which includes Facebook and TripAdvisor among others, you will understand the importance of including it in this book.</p><p>In many ways, the <code class="literal">btrfs</code> filesystem was born from the failing of the ReiserFS file system after it lost its lead developer, Hans Reiser. Chris Mason, who had helped develop ReiserFS before moving on to SUSE, was hired by Oracle to develop high-end filesystems. This was the start of <code class="literal">btrfs</code>.</p></div></div>
<div class="section" title="Overview of the lab environment" id="aid-164MG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Overview of the lab environment</h1></div></div></div><p>The Red Hat <a id="id182" class="indexterm"/>Enterprise Linux 7.1 virtual machine we will use for this book will have additional drives added for this section. Currently, we will use three disks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">/dev/sda</code>: This disk is used by the root filesystem</li><li class="listitem"><code class="literal">/dev/sdb</code>: This disk is used to house the yum repository</li><li class="listitem"><code class="literal">/dev/sdc</code>: This disk is used as the iSCSI LUN store</li></ul></div><p>To demonstrate some key features of <code class="literal">btrfs</code>, we will add four additional virtual disks to the system so that we can use them while demonstrating the <code class="literal">btrfs</code> filesystem. Feel free to do the same if you are using a virtualized system. The different drives that we will add are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><code class="literal">/dev/sdd</code></li><li class="listitem"><code class="literal">/dev/sde</code></li><li class="listitem"><code class="literal">/dev/sdf</code></li><li class="listitem"><code class="literal">/dev/sdg</code></li></ul></div><p>Using the <code class="literal">lsblk</code> command on the demonstration system, you will be able to view the starting configuration that we will <a id="id183" class="indexterm"/>use from this point onward, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00239.jpeg" alt="Overview of the lab environment"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Installing btrfs" id="aid-173721"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Installing btrfs</h1></div></div></div><p>Using Red Hat Enterprise <a id="id184" class="indexterm"/>Linux 7 or later, you will find that <code class="literal">btrfs</code> is installed by default even on a minimal installation. However, if you are using earlier versions, you can install the <code class="literal">btrfs</code> filesystem with yum in the normal way, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># yum install -y btrfs-progs</strong></span>
</pre></div><p>With the filesystem installed, we can check the version that we have implemented using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ btrfs --version</strong></span>
</pre></div><p>On RHEL 7, the version is <code class="literal">3.12</code>, whereas on RHEL 7.1, the version is <code class="literal">3.16.2</code>.</p><p>Now that we understand a little of the power behind <code class="literal">btrfs</code>, let's begin with some simple implementation examples.</p></div>
<div class="section" title="Creating the btrfs filesystem" id="aid-181NK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Creating the btrfs filesystem</h1></div></div></div><p>To begin with, we will <a id="id185" class="indexterm"/>create a <code class="literal">btrfs</code> filesystem on the <code class="literal">/dev/sdd</code> complete disk. We do not need to partition the disk first, saving us time from the outset. This is shown in the following command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkfs.btrfs /dev/sdd</strong></span>
</pre></div><p>With the filesystem created, we can take the time to become familiar with the integrity check tool:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfsck /dev/sdd</strong></span>
</pre></div><p>The following screenshot shows the output from my system:</p><div class="mediaobject"><img src="../Images/image00240.jpeg" alt="Creating the btrfs filesystem"/></div><p style="clear:both; height: 1em;"> </p><p>To verify that the <code class="literal">btrfs</code> filesystem is in operation, we will create a directory and mount it therein. We will also copy some data and display the usage information for the disk:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkdir -p /data/simple</strong></span>
<span class="strong"><strong># mount /dev/sdd /data/simple</strong></span>
<span class="strong"><strong># find /usr/share/doc -name '*.pdf' -exec cp {} /data/simple \;</strong></span>
<span class="strong"><strong># btrfs filesystem show /dev/sdd</strong></span>
</pre></div><p>The output from the final command is shown in the following screenshot. We can see that we have 5.96 MiB of file space used:</p><div class="mediaobject"><img src="../Images/image00241.jpeg" alt="Creating the btrfs filesystem"/></div><p style="clear:both; height: 1em;"> </p><p>The additional space used (which shows as <code class="literal">138.38MiB</code>) includes typical metadata related to any filesystem, but additionally, by default, the <code class="literal">btrfs</code> filesystem stores free space information on the disk so <a id="id186" class="indexterm"/>that it's quick to retrieve it rather than searching the disk. This is controlled through the <code class="literal">space_cache</code> mount option, which is set by default. If you would like to disable this feature, use the <code class="literal">nospace_cache</code> mount option.</p></div>
<div class="section" title="The Copy-On-Write technology" id="aid-190861"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>The Copy-On-Write technology</h1></div></div></div><p>One of the underpinning <a id="id187" class="indexterm"/>technologies that helps with the success of the <code class="literal">btrfs</code> filesystem is <span class="strong"><strong>Copy-On-Write</strong></span> (<span class="strong"><strong>CoW</strong></span>). <code class="literal">CoW</code> is used in logical volume management filesystems, including <span class="strong"><strong>ZFS</strong></span> used in Solaris (an Oracle product), Microsoft's <span class="strong"><strong>Volume Shadow Copy</strong></span>, and <code class="literal">btrfs</code>.</p><p>These CoW filesystems allow you to take instant snapshots or backups. This is due to the fact that as a file is written and a copy of it is made; hence, Copy-on-Write. As traditional filesystems implement this, the virtual disk technology can also implement this <code class="literal">CoW</code> technology in <code class="literal">qcow2</code>. In this way, any allocated disk space in the <code class="literal">qcow2</code> disk file is not used on the host until it's written to.</p><p>For generic filesystems, you will find the <code class="literal">CoW</code> technology very useful. Being able to revert to previous file versions is like gold dust on traditional file servers. However, if you use <code class="literal">btrfs</code> to host very large data files, such as virtual disk files, the <code class="literal">CoW</code> technology can perform slow writes.</p><p>Using the <code class="literal">chattr</code> command in Linux, we can set or change the attributes of files and/or directories. Supported for <code class="literal">btrfs</code> filesystems, there is a file attribute to disable CoW. This attribute is useful only when it is set on an empty file. To ensure its effectiveness, we generally set this on a directory, so that all the files inherit this attribute at the time of file creation. The following commands show how to achieve this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkdir /data/simple/cow</strong></span>
<span class="strong"><strong># chattr +C /data/simple/cow</strong></span>
<span class="strong"><strong># lsattr -d /data/simple/cow</strong></span>
<span class="strong"><strong># touch /data/simple/cow/vdisk1</strong></span>
<span class="strong"><strong># lsattr /data/simple/cow/vdisk1</strong></span>
</pre></div><p>In the following screenshot, we <a id="id188" class="indexterm"/>can see that creating a new file will automatically assign the <code class="literal">NoDataCoW</code> option. It does not matter how this file was created:</p><div class="mediaobject"><img src="../Images/image00242.jpeg" alt="The Copy-On-Write technology"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Resizing btrfs filesystems" id="aid-19UOO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Resizing btrfs filesystems</h1></div></div></div><p>With <code class="literal">btrfs</code>, it's possible to <a id="id189" class="indexterm"/>resize the <code class="literal">btrfs</code> filesystem when it is online and is being accessed by users. The size of a filesystem will grow automatically if we add or remove devices; we will see this in the next subsection of this chapter; however; we can resize the filesystem should we need to even on a single device that we have created. Using the following command, we will shrink the assigned space to the filesystem by 500MiB:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs filesystem resize -500m /data/simple</strong></span>
</pre></div><p>If we check the size of the filesystem before and after, we can see the dynamic change that takes place:</p><div class="mediaobject"><img src="../Images/image00243.jpeg" alt="Resizing btrfs filesystems"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Adding devices to the btrfs filesystem" id="aid-1AT9A1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Adding devices to the btrfs filesystem</h1></div></div></div><p>We have already <a id="id190" class="indexterm"/>seen a little of volume management using LVM when we looked at iSCSI in <a class="link" title="Chapter 4. Implementing iSCSI SANs" href="part0032.xhtml#aid-UGI01">Chapter 4</a>, <span class="emphasis"><em>Implementing iSCSI SANs</em></span>, and it's not exactly simple.</p><div class="section" title="Volume management the old way"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec33"/>Volume management the old way</h2></div></div></div><p>The following <a id="id191" class="indexterm"/>commands are used in order to manage the disk volumes in the old, traditional way:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># pvcreate /dev/sde1</strong></span>
<span class="strong"><strong># vgextend vg1 /dev/sde1</strong></span>
<span class="strong"><strong># lvextend -L+1000M /dev/vg1/data_lv</strong></span>
<span class="strong"><strong># resize2fs /dev/vg1/data</strong></span>
</pre></div></div><div class="section" title="Volume management with btrfs"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Volume management with btrfs</h2></div></div></div><p>To start with, we will <a id="id192" class="indexterm"/>return the volume back to its original size before we add the second disk. Using the <code class="literal">max</code> option, we will ensure that the <code class="literal">btrfs</code> filesystem uses the maximum space available on the single disk we have in place so far:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs filesystem resize max /data/simple</strong></span>
</pre></div><p>In LVM and traditional filesystems, there were a total of four commands to be executed. In <code class="literal">btrfs</code>, we can perform this with a single command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs device add /dev/sde /data/simple</strong></span>
</pre></div><p>This is all we need to do. The device is added and the filesystem is automatically increased to the available maximum space. We can use the <code class="literal">btrfs filesystem show</code> command against either <code class="literal">/dev/sdd</code> or /<code class="literal">sdv/sde</code> because both devices will hold a copy of the metadata by <a id="id193" class="indexterm"/>default. In the following commands, we can see that this in place and the screenshot will reinforce this message:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs filesystem show /dev/sdd</strong></span>
<span class="strong"><strong># df -hT /data/simple</strong></span>
</pre></div><p>After reviewing the following screenshot, we can see the command and output that is generated:</p><div class="mediaobject"><img src="../Images/image00244.jpeg" alt="Volume management with btrfs"/></div><p style="clear:both; height: 1em;"> </p><p>Having the metadata stored on both devices allow for fault-tolerance and weakens the device to be queried:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs fi show /dev/sdd</strong></span>
<span class="strong"><strong># btrfs filesystem show /dev/sde</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title"><a id="tip17"/>Tip</h3><p>Note that some subcommands can be shortened; in this case, <code class="literal">fi</code> is equivalent to filesystem.</p></div></div></div>
<div class="section" title="Balancing the btrfs filesystem" id="aid-1BRPS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Balancing the btrfs filesystem</h1></div></div></div><p>If the need to add the <a id="id194" class="indexterm"/>additional disk to the volume was due to it running out of disk space, then we may choose to help performance by spreading the data across both devices. This is achieved using the <code class="literal">balance</code> subcommand:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs filesystem balance start -d -m /data/simple</strong></span>
</pre></div><p>The <code class="literal">-m</code> argument represents metadata and <code class="literal">-d</code> represents data. In this way, the disks are used at an equal ratio.</p><p>The output from the demonstration system is shown in the following command; note that you can omit <code class="literal">filesystem</code> <a id="id195" class="indexterm"/>from the <code class="literal">balance</code> subcommand because it's optional in this case:</p><div class="mediaobject"><img src="../Images/image00245.jpeg" alt="Balancing the btrfs filesystem"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Mounting multidisk btrfs volumes from /etc/fstab"><div class="titlepage" id="aid-1CQAE2"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Mounting multidisk btrfs volumes from /etc/fstab</h1></div></div></div><p>If we are <a id="id196" class="indexterm"/>mounting the <code class="literal">btrfs</code> volumes from the <code class="literal">/etc/fstab</code> file, we need to ensure that a <code class="literal">btrfs</code> scan is effected before we mount the <code class="literal">/data/simple</code> directory. This will locate all the devices that participate within the volume. The <code class="literal">initramfs</code> file system can complete this task for us on a later system including RHEL 7. If your existing filesystem was already using <code class="literal">btrfs</code>, the scan will be built-in your current <code class="literal">initramfs</code>. If <code class="literal">btrfs</code> is new to your system, you will need to generate a new initial RAM disk. Make sure that you use the correct <code class="literal">initramfs</code> and kernel version for your system when running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># dracut -v -a btrfs -f /boot/initramfs-$(uname -r) /boot/vmlinuz-$(uname -r)</strong></span>
</pre></div><p>We can then add an entry into the <code class="literal">/etc/fstab</code> file similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/dev/sdd  /data/simple  btrfs  defaults  0 0</strong></span>
</pre></div><div class="section" title="Creating a RAID1 mirror"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec35"/>Creating a RAID1 mirror</h2></div></div></div><p>The <span class="strong"><strong>RAID</strong></span> (<span class="strong"><strong>Redundant Array of Inexpensive Disks</strong></span>) software is also supported by <code class="literal">btrfs</code>. The following are <a id="id197" class="indexterm"/>the currently supported <a id="id198" class="indexterm"/>RAID levels:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>RAID 0</strong></span>: Striping without redundancy</li><li class="listitem"><span class="strong"><strong>RAID 1</strong></span>: Disk mirroring</li><li class="listitem"><span class="strong"><strong>RAID 10</strong></span>: Striped mirror</li></ul></div><p>Currently, we have a multidisk <code class="literal">btrfs</code> filesystem, but without fault-tolerance. The implementation we used is RAID 0 / striping without parity. We can convert this to a RAID 1 system and mirror the metadata and the filesystem data as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs balance start -dconvert=raid1 -mconvert=raid1 /data/simple</strong></span>
</pre></div><p>As you can see from the preceding command, the metadata and the filesystem data are converted to the software mirror of RAID 1.</p><p>We can create a mirrored device using <code class="literal">btrfs</code> from the outset easily and quickly. Mirroring does not give us extra disk space, but this does provide great fault-tolerance if the worst happens and we experience a disk failure. We can demonstrate this on our demonstration system using the extra disk that we have not used so far:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkfs.btrfs -m raid1 -d raid1 /dev/sdf /dev/sdg</strong></span>
<span class="strong"><strong># mkdir /data/mirror</strong></span>
<span class="strong"><strong># mount /dev/sdg /data/mirror</strong></span>
</pre></div><p>To create a mirror, we will use RAID1 for the metadata and the <code class="literal">-m</code> and <code class="literal">-d</code> data, as we did in the preceding convert example. The disk space available is 1 GB. Whatever we write to <code class="literal">/dev/sdf</code> is mirrored to <code class="literal">/dev/sdg</code>; with mirror, we lose 50 percent of the data storage, but have a high level of redundancy. We will similarly need to add an entry to the <code class="literal">/etc/fstab</code> file to ensure that the raid system mounts correctly at boot time. As <code class="literal">initramfs</code> now supports <code class="literal">btrfs</code> by running the device scan for us, there is no requirement to create <code class="literal">initramfs</code> at this stage:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/dev/sdf  /data/mirror  btrfs  defaults  0 0</strong></span>
</pre></div><p>Displaying the free <a id="id199" class="indexterm"/>disk space with standard tools—such as <code class="literal">df</code>—will not supply correct information; we need to use <code class="literal">btrfs</code> tools. The following command will list the free space available to the <code class="literal">/data/mirror</code> mount point:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs fi df /data/mirror</strong></span>
</pre></div><p>The output from the command is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00246.jpeg" alt="Creating a RAID1 mirror"/></div><p style="clear:both; height: 1em;"> </p><p>I know that we risk 7 years of bad luck even talking about it; however, mirrors can break. Part of the reason to create a mirror is to provide fault-tolerance. This is in itself an acceptance that hard disks can and do fail.</p><p>For this demonstration, we will destroy the <code class="literal">/data/simple/</code> volume and reuse the devices that we employed for the simple volume. To destroy the <code class="literal">btrfs</code> metadata, the preferred utility is <code class="literal">wipefs</code>, which is part of the <code class="literal">util-linux</code> package. Firstly, we need to run the <code class="literal">wipefs</code> command against the disk or partition we need to wipe and then use the offset value with the <code class="literal">-o</code> option. Take a look at how we can wipe <code class="literal">/dev/sdd</code> and <code class="literal">/dev/sde</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># umount /data/simple</strong></span>
<span class="strong"><strong># wipefs /dev/sdd</strong></span>
<span class="strong"><strong># wipefs -o 0x10040 /dev/sdd</strong></span>
<span class="strong"><strong># wipefs /dev/sde</strong></span>
<span class="strong"><strong># wipefs -o 0x10040 /dev/sde</strong></span>
</pre></div><p>The output from the first drive is listed for convenience in the following screenshot; the sequence is repeated from the second drive. Do not forget to remove the entry from the <code class="literal">/etc/fstab</code> file:</p><div class="mediaobject"><img src="../Images/image00247.jpeg" alt="Creating a RAID1 mirror"/></div><p style="clear:both; height: 1em;"> </p><p>With these disks wiped, we can reuse them in other arrays.</p><p>We will add data to the mirror volume in the same way that we did with the simple volume. In this way, we can be sure that data stays intact:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># find /usr/share/doc -name '*.pdf' -exec cp {} /data/mirror \;</strong></span>
</pre></div><p>We will unmount the mirror volume now and emulate the failure of one of the disks as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># umount /data/mirror</strong></span>
<span class="strong"><strong># wipefs -o 0x10040 /dev/sdg</strong></span>
</pre></div><p>We will now experience a problem when we try to remount the mirror volume using the mount command, and we <a id="id200" class="indexterm"/>will have to mount the mirror volume using the <code class="literal">-o</code> degraded option:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mount -o degraded /dev/sdf /data/mirror</strong></span>
</pre></div><p>At this stage, our data is available, so we can breathe a sigh of relief:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ls /data/mirror</strong></span>
</pre></div><p>We still have a RAID 1 array and the minimum number of members for this is two, so we need to add a new device as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs device add /dev/sdd /data/mirror</strong></span>
</pre></div><p>We can now remove the failed or missing device:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs device delete missing /data/mirror</strong></span>
</pre></div><p>The<code class="literal"> missing</code> keyword will search for the first missing member in the array. We can then delete this device. The <a id="id201" class="indexterm"/>RAID 1 array is now fully operational, provisioning software mirroring across two devices again.</p></div></div>
<div class="section" title="Using btrfs snapshots"><div class="titlepage" id="aid-1DOR02"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Using btrfs snapshots</h1></div></div></div><p>Hopefully, what you <a id="id202" class="indexterm"/>have seen so far in <code class="literal">btrfs</code> will be of interest, but, of course, there is always much more to see and learn. We will now look at snapshots. Btrfs snapshots can be used as read-only or read/write copies of your data. With <code class="literal">btrfs</code> as a <a id="id203" class="indexterm"/>Copy-on-Write-based filesystem, there is no need to copy large amounts of data across because we only need to copy the data when it changes. In the meantime, the original data is linked to the new location. In this way, a snapshot of a large filesystem can be taken instantly. Snapshots can be put to use in a couple of ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">As part of a backup solution where you may be concerned with open files affecting the backup; the snapshot will be created as read-only. Subsequently, you will implement a backup of the snapshot. In this way, the backup will be of the host filesystem at the point in time that the snapshot was created.</li><li class="listitem">Snapshots can be useful where you feel that rolling back to the original data may be useful, perhaps in a testing environment where you need to implement many changes and easily be able to restore back to the original data very quickly.</li></ul></div><p>Btrfs snapshots rely on subvolumes; source and destination subvolumes need to be within the same filesystem. If you'll recall the data is just linked until it's changed; this is handled in the same way as traditional hard links.</p><p>Subvolumes within the <code class="literal">btrfs</code> filesystem are discrete management identities, which allow more granular control of elements of a single filesystem. We will begin by creating a single subvolume so that we may gain a little understanding of this technology before creating snapshots. We will re-employ the <code class="literal">/dev/sde</code> disk to be mounted as our simple volume and start by reformatting the mirror volume:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkfs.btrfs /dev/sde</strong></span>
<span class="strong"><strong># mount /dev/sde /data/simple</strong></span>
</pre></div><p>At this stage, the complete filesystem for <code class="literal">/dev/sde</code> is available and mounted at the <code class="literal">/data/simple</code> directory. There is no data stored here yet, but we effectively have a single view of the filesystem with the simple directory. Subvolumes allow you to view the same filesystem in different ways by mounting elements of the filesystem (subvolumes) to the directories that we choose and with selected mount options appropriate for the data.</p><p>We will create a new subvolume after the existing <code class="literal">/data/simple</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume create /data/simple/vol1</strong></span>
</pre></div><p>The output is quite <a id="id204" class="indexterm"/>minimal, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00248.jpeg" alt="Using btrfs snapshots"/></div><p style="clear:both; height: 1em;"> </p><p>We can list the subvolumes, as shown in the following command and screenshot:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume list /data/simple</strong></span>
<span class="strong"><strong># ls /data/simple</strong></span>
</pre></div><p>The following screenshot shows the output of the preceding command:</p><div class="mediaobject"><img src="../Images/image00249.jpeg" alt="Using btrfs snapshots"/></div><p style="clear:both; height: 1em;"> </p><p>We can also see that <a id="id205" class="indexterm"/>creating the subvolume also created the directory within the filesystem itself. We will not be able to remove the directory from the filesystem because this is not only a directory, but also a subvolume. To delete a directory, you will need to delete the subvolume.</p><p>We won't delete the directory, but should we need to delete it at a later stage, the command to delete it will be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume delete /data/simple/vol1</strong></span>
</pre></div><p>This will delete the subvolume along with the directory in very much the same way as creating the subvolume also created the directory within the filesystem.</p><p>We will now add some data to the subvolume; if you did delete it, you can simply recreate it again. We can copy the PDF files that we have become familiar with to this volume:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># cp /data/mirror/* /data/simple/vol1</strong></span>
</pre></div><p>If we need to make this data available elsewhere, we can mount the subvolume wherever we need and with mount options that we feel appropriate. For example, we have documentation in this directory so that we can mount it as read-only in another directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mount -o ro,subvol=vol1 /dev/sde /mnt</strong></span>
</pre></div><p>At the root of the <code class="literal">/mnt</code> mount point, we will see the PDF files we added to the <code class="literal">vol1</code> directory. They are still available in the original location under <code class="literal">/data/simple/vol1</code>. In this way, we can control access to the data from how it's mounted.</p><p>Now that we have some knowledge of subvolumes, we will investigate snapshots. The snapshot must be created in the same filesystem as the target data; as we mentioned before, the instant generation <a id="id206" class="indexterm"/>of a snapshot is affected by a form of internal linking within the filesystem.</p><p>We will generate the <a id="id207" class="indexterm"/>snapshot of the existing <code class="literal">vol1</code> data and also specify the option <code class="literal">-r</code> to ensure that the backup is read-only. In this way, we can return to this <span class="emphasis"><em>point in time</em></span> backup by copying the data back from the <code class="literal">backup</code> directory. No additional disk space is used unless the original data is changed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume snapshot -r /data/simple/vol1/ /data/simple/backup</strong></span>
</pre></div><p>We can list the subvolumes easily using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume list /data/simple</strong></span>
</pre></div><p>We may base the backup scenario around the fact that the documentation may be written too frequently. Also, we want a solution to be able to recover from poorly executed edits quickly.</p><p>To create a read-only snapshot of the working subvolume, use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume snapshot -r /data/simple/vol1 /data/simple/backup/</strong></span>
</pre></div><p>Listing the contents of the working directory and the backup directory should reveal that the contents are the same:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ls /data/simple/vol1</strong></span>
<span class="strong"><strong># ls /data/simple/backup</strong></span>
</pre></div><p>The name <code class="literal">backup</code> is not important, but useful in the context of its use. As always, a good naming scheme can help understand the directory's purpose unlike the name we gave to <code class="literal">vol1</code>.</p><p>Should we accidently delete all the files from <code class="literal">/data/simple/vol1</code>, the CoW technology in <code class="literal">btrfs</code> will then write the changed data to the backup snapshot: <code class="literal">/data/simple/backup</code>. This will also be the case if the files were modified in any way rather than deleted; the snapshot holds files as they were at the time the snapshot was created. We can simply copy <a id="id208" class="indexterm"/>the files back to the original location in the event of a catastrophe.</p><p>For the moment, we will <a id="id209" class="indexterm"/>look at how to delete this snapshot. Later in this chapter, we will see how to use snapper as a simple mechanism in order to manage snapshots on LVM and <code class="literal">btrfs</code> systems:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume delete /data/simple/backup</strong></span>
</pre></div></div>
<div class="section" title="Optimizing btrfs for solid state drives" id="aid-1ENBI1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Optimizing btrfs for solid state drives</h1></div></div></div><p>When creating <a id="id210" class="indexterm"/>a <code class="literal">btrfs</code> filesystem on multiple SSDs, using the single <code class="literal">-m</code> option will ensure that the metadata is not duplicated. On an SSD, duplicating metadata is thought of as a waste of space and has an overhead that can lessen the life of the disk, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkfs.btrfs -m single /dev/sdb</strong></span>
</pre></div><p>The second way is to use the <code class="literal">ssd</code> mount option. This option will set a few performance options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Allows large metadata clusters</li><li class="listitem">Allows more sequential data allocation</li><li class="listitem">Disables leaf writing to match key and block order in the b-tree database</li><li class="listitem">Commits b-tree log fragments without batching multiple processes</li></ul></div></div>
<div class="section" title="Managing snapshots with snapper" id="aid-1FLS41"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Managing snapshots with snapper</h1></div></div></div><p>The snapshot <a id="id211" class="indexterm"/>command is included on RHEL 7 and can <a id="id212" class="indexterm"/>be used to manage snapshots and view their differences with the original data easily. It can be employed along with LVM or btrfs <code class="literal">systems.h</code>.</p><p>To install snapper, we fall back to RHEL's package management:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># yum install snapper</strong></span>
</pre></div><p>Currently, there seems to be a bug or feature on SELinux that prevents snapper from working if SELinux is enforced. We could allow the correct SELinux access to our resources by creating a new policy or simply set <code class="literal">snapperd_t</code> to a permissive domain. In this way, we can still use the power and security of SELinx, but just have it disabled for snapper as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># semanage permissive -a snapperd_t</strong></span>
</pre></div><p>At a later date, you can use the <code class="literal">-d</code> option to delete the enabled snapper and the SELinux support:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># semanage permissive -d snapperd_t</strong></span>
</pre></div><p>For the moment, we will leave snapper in the permissive mode and proceed to create a configuration for snapper and our <code class="literal">/data/simple/vol1</code> data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper -c simple_data create-config -f btrfs /data/simple/</strong></span>
</pre></div><p>Using the following <a id="id213" class="indexterm"/>command, we can list the configurations <a id="id214" class="indexterm"/>that we have:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper list-configs</strong></span>
</pre></div><p>The following screenshot shows the creation of the configuration and the listing command:</p><div class="mediaobject"><img src="../Images/image00250.jpeg" alt="Managing snapshots with snapper"/></div><p style="clear:both; height: 1em;"> </p><p>Creating the configuration will create a hidden directory <code class="literal">.snapshots</code> at the root <code class="literal">/data/simple/vol1</code> directory. The configuration itself is stored in <code class="literal">/etc/snapper/configs</code>; a log file exits from troubleshooting located at <code class="literal">/var/log/snapper.log</code>.</p><p>Now that we have the foundation created, we will create the snapshot:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper --config simple_data create --description "Start"</strong></span>
</pre></div><p>We can see that the process is very easy, quick, and saves us a lot of effort. If we check the subvolumes that now exits after <code class="literal">/data/simple</code>, we will see<code class="literal">.snapshots</code> and the numbered subvolume after this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># btrfs subvolume list /data/simple</strong></span>
</pre></div><p>The output is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00251.jpeg" alt="Managing snapshots with snapper"/></div><p style="clear:both; height: 1em;"> </p><p>More easily and normally though, we use snapper entirely to manage this, and we should view snapshots with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper --config simple_data list </strong></span>
</pre></div><p>To show how we can view the difference in data, we will delete a PDF file from the original <code class="literal">vol1</code> location:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># rm /data/simple/vol1/tutorial.pdf</strong></span>
</pre></div><p>With this file removed, we will now have a difference between the original data and the snapshot. The CoW system will have the deleted file written to the snapshot location as the deletion occurred. We can view the difference in the data using the following command, where <code class="literal">0</code> is the original data and <code class="literal">1</code> is the snapshot:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper -c simple_data status 0..1</strong></span>
</pre></div><p>The output of the <a id="id215" class="indexterm"/>command is shown in the following <a id="id216" class="indexterm"/>screenshot, which indicates that the snapshot has the extra file now:</p><div class="mediaobject"><img src="../Images/image00252.jpeg" alt="Managing snapshots with snapper"/></div><p style="clear:both; height: 1em;"> </p><p>To restore the deleted file, we will use the <code class="literal">undochange</code> command; note that we need to display the effect from the snapshot, to the original or <code class="literal">1..0</code>, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># snapper -c simple_data undochange 1..0</strong></span>
</pre></div><p>We now have the <code class="literal">tutorial.pdf</code> file returned to us in the <code class="literal">vol1</code> directory as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ls /data/simple/vol1/tutorial.pdf</strong></span>
</pre></div><p>From the following <a id="id217" class="indexterm"/>screenshot, you will be able to see the file <a id="id218" class="indexterm"/>restore command and the listing of the returned file:</p><div class="mediaobject"><img src="../Images/image00253.jpeg" alt="Managing snapshots with snapper"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Summary" id="aid-1GKCM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Summary</h1></div></div></div><p>In this chapter, we saw the power that can be unleashed with the <code class="literal">btrfs</code> filesystem and the time we can save using it compared with other Linux logical volume systems such as LVM. We also saw how to implement software RAID and then combined the file management, logical volume management, and RAID management to a single command.</p><p>Using snapper to help manage snapshots works well for us on LVM and <code class="literal">btrfs</code> systems. We used snapper with the <code class="literal">btrfs</code> filesystem in this chapter.</p><p>In the next chapter, we will see how to share files on the network using <span class="strong"><strong>NFS</strong></span> (<span class="strong"><strong>Network File System</strong></span>), the traditional UNIX way to share file resources on your network.</p></div></body></html>