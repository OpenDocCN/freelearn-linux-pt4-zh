- en: Chapter 11. CoreOS and Containers – Production Considerations
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章。CoreOS 和容器 - 生产考虑因素
- en: There is a big difference between running applications and containers in development
    versus production environments. Production environments pose a special set of
    challenges. The challenges mainly lie in scalability, high availability, security,
    and automation. CoreOS and Docker have solved significant challenges in taking
    applications from development to production. In this chapter, we will cover the
    production considerations for microservice infrastructure, including deployment,
    automation, and security.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发环境和生产环境中运行应用程序和容器之间存在很大差异。生产环境面临一系列特殊挑战，主要涉及可伸缩性、高可用性、安全性和自动化。CoreOS 和 Docker
    已解决了将应用程序从开发转移到生产中的重要挑战。本章将介绍微服务基础设施的生产考虑因素，包括部署、自动化和安全性。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: CoreOS cluster design considerations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS 集群设计考虑因素
- en: Distributed infrastructure design consideration – Service discovery, deployment
    patterns, PaaS, and stateful and stateless Containers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式基础设施设计考虑因素 - 服务发现、部署模式、PaaS 和有状态与无状态容器
- en: Security considerations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全考虑
- en: Deployment and automation – CI/CD approaches and using Ansible for automation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署与自动化 - CI/CD 方法及使用 Ansible 进行自动化
- en: CoreOS and the Docker roadmap
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS 和 Docker 路线图
- en: Microservice infrastructure – platform choices and solution providers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微服务基础设施 - 平台选择和解决方案提供者
- en: CoreOS cluster design considerations
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 集群设计考虑因素
- en: The cluster size and update strategy are important design considerations for
    a CoreOS cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 集群规模和更新策略是 CoreOS 集群的重要设计考虑因素。
- en: The update strategy
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 更新策略
- en: The CoreOS automatic update feature keeps the nodes in the cluster secure and
    up-to-date. CoreOS provides you with various update mechanisms to control updates,
    and the user can select an approach based on their needs. We covered details of
    update strategies in [Chapter 3](index_split_075.html#filepos216260), CoreOS AutoUpdate.
    Some customers prefer doing the update only in the maintenance window and CoreOS
    gives control to do this.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 的自动更新功能可保持集群节点安全和最新。CoreOS 提供多种更新机制以控制更新，用户可以根据自己的需求选择方法。我们在《第3章》（index_split_075.html#filepos216260），CoreOS
    自动更新中详细介绍了更新策略的细节。一些客户更喜欢仅在维护窗口中进行更新，CoreOS 允许用户进行控制。
- en: Cluster considerations
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集群考虑因素
- en: The following are some considerations that need to be taken into account when
    choosing the CoreOS cluster. We have covered these individual topics in earlier
    chapters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 CoreOS 集群时需要考虑以下因素。我们在早期章节中已经涵盖了这些个别主题。
- en: 'Cluster size: A bigger cluster size provides better redundancy but updates
    take a little longer.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群规模：更大的集群规模提供更好的冗余性，但更新需要稍长时间。
- en: 'Cluster architecture: We need to choose the architecture based on whether the
    cluster is used for development or production. For a production cluster, the preferable
    scheme is to have a small master cluster to run critical services such as Etcd
    and Fleet and have worker nodes point to the master cluster. Worker nodes should
    be used only to run application Containers.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群架构：我们需要根据集群用于开发还是生产选择架构。对于生产集群，首选方案是拥有一个小型主节点集群来运行关键服务，如 Etcd 和 Fleet，并且让工作节点指向主节点集群。工作节点仅用于运行应用容器。
- en: 'Etcd heartbeat and timeout tuning: These parameter values need to be tuned
    depending on whether the cluster is local or geographically distributed.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd 心跳和超时调整：这些参数值需要根据集群是本地还是地理分布式进行调整。
- en: 'Node backup and restore: Nodes can go bad. It is necessary to take periodic
    backups.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点备份与恢复：节点可能出现故障。需要定期备份。
- en: 'Adding and removing nodes in the cluster: CoreOS provides mechanisms to add
    and remove nodes in the Etcd cluster dynamically without data loss. This can be
    used to grow the cluster size organically.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中添加和移除节点：CoreOS 提供机制，可以动态添加和移除 Etcd 集群中的节点而不会丢失数据。这可以有机地扩展集群规模。
- en: Distributed infrastructure design considerations
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式基础架构设计考虑因素
- en: In this section, we will cover some miscellaneous infrastructure design considerations
    that were not covered in earlier chapters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖一些未在早期章节中涉及的杂项基础设施设计考虑因素。
- en: Service discovery
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现
- en: 'Microservices are dynamic and Service discovery refers to how microservices
    can find each other dynamically. Service discovery has three components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务是动态的，服务发现指的是微服务如何动态地找到彼此。服务发现有三个组件：
- en: It discovers services automatically as they come up and accesses a service by
    the service name using DNS
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会在服务启动时自动发现服务，并通过DNS使用服务名称访问服务
- en: It maintains a shared database of services along with their access details that
    can be accessed from multiple hosts
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它维护一个共享的服务数据库及其访问详情，可以从多个主机访问。
- en: It accesses services using a load balancer and handles service failures automatically
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过负载均衡器访问服务，并自动处理服务故障
- en: Service discovery is automatically taken care of when using a Container orchestration
    system such as Kubernetes. For smaller deployments, when there is no Orchestration
    system, we can do this manually using standalone tools.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器编排系统（如Kubernetes）时，服务发现会自动处理。对于没有编排系统的小型部署，我们可以通过使用独立工具手动进行。
- en: We covered Service discovery in [Chapter 4](index_split_093.html#filepos245712),
    CoreOS Core Services – Etcd, Systemd, Fleet in the Service discovery section using
    the Sidekick service and Etcd. This approach did not provide DNS lookup. The following
    approach is another way of doing service discovery with integrated DNS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](index_split_093.html#filepos245712)中涵盖了服务发现部分，使用了Sidekick服务和Etcd来介绍CoreOS核心服务—Etcd、Systemd、Fleet。此方法未提供DNS查找。以下方法是另一种通过集成DNS来实现服务发现的方式。
- en: Service discovery using Registrator and Consul
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Registrator和Consul进行服务发现
- en: Consul ([https://consul.io/](https://consul.io/)) and the Gliderlabs registrator
    ([https://github.com/gliderlabs/docker-consul/tree/consul-0.4](https://github.com/gliderlabs/docker-consul/tree/consul-0.4))
    in combination provide automatic service discovery and a service database.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Consul（[https://consul.io/](https://consul.io/)）和Gliderlabs注册器（[https://github.com/gliderlabs/docker-consul/tree/consul-0.4](https://github.com/gliderlabs/docker-consul/tree/consul-0.4)）结合使用提供自动服务发现和服务数据库。
- en: 'The following figure shows the model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了该模型：
- en: '![](img/00218.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00218.jpg)'
- en: 'The following points show you how this works:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几点展示了它是如何工作的：
- en: Consul provides service discovery, shared key-value storage, DNS-based service
    lookup and service health monitoring
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Consul提供服务发现、共享键值存储、基于DNS的服务查找以及服务健康监控
- en: Gliderlabs registrator monitors the Docker socket for service creation and informs
    Consul about registration
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gliderlabs注册器监视Docker套接字，监听服务创建，并将注册信息通知Consul
- en: As DNS is integrated with Consul, services can be accessed by the service name
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于DNS与Consul集成，服务可以通过服务名称访问
- en: 'The following are the steps necessary to try this out in a Ubuntu Linux machine:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在Ubuntu Linux机器上尝试此方法所需的步骤：
- en: Set the Docker daemon to use the Docker bridge IP as one of the DNS lookup servers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Docker守护进程使用Docker桥接IP作为其中一个DNS查找服务器。
- en: 'Add this line to `/etc/default/docker`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一行添加到`/etc/default/docker`中：
- en: '`DOCKER_OPTS="--dns 172.18.0.1 --dns 8.8.8.8 --dns-search service.consul"`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`DOCKER_OPTS="--dns 172.18.0.1 --dns 8.8.8.8 --dns-search service.consul"`'
- en: 'Restart the Docker daemon:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重启Docker守护进程：
- en: '`Sudo service docker restart`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sudo service docker restart`'
- en: 'Start the Consul server:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Consul服务器：
- en: '`docker run -d -p 8400:8400 -p 8500:8500 -p 172.18.0.1:53:8600/udp -h node1 gliderlabs/consul-server -server –bootstrap`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d -p 8400:8400 -p 8500:8500 -p 172.18.0.1:53:8600/udp -h node1
    gliderlabs/consul-server -server --bootstrap`'
- en: The preceding command exposes port `8400` for rpc, `8500` for UI, and `8600`
    for DNS. We mapped DNS to the Docker bridge IP address (`172.18.0.1`), and this
    allows us to access service names directly from inside Containers. In the preceding
    step, we made the Docker bridge IP as one of the DNS lookup servers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将端口`8400`暴露用于rpc，`8500`用于UI，`8600`用于DNS。我们将DNS映射到Docker桥接IP地址（`172.18.0.1`），这样就可以直接从容器内部访问服务名称。在上一步中，我们将Docker桥接IP设置为DNS查找服务器之一。
- en: 'Start the Gliderlabs registrator:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Gliderlabs注册器：
- en: '`docker run -d \``    --name=registrator \``    --net=host \``    --volume=/var/run/docker.sock:/tmp/docker.sock \``    gliderlabs/registrator:latest \``      consul://localhost:8500`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d \`` --name=registrator \`` --net=host \`` --volume=/var/run/docker.sock:/tmp/docker.sock
    \`` gliderlabs/registrator:latest \`` consul://localhost:8500`'
- en: In the preceding command, we also specified the location of Consul so that the
    registrator can register services to Consul.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，我们还指定了Consul的位置，以便注册器能够将服务注册到Consul。
- en: 'Now, let''s start a few Containers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们启动一些容器：
- en: '`docker run -d -P --name=nginx nginx``docker run --name mysql -e MYSQL_ROOT_PASSWORD=mysql -d mysql``docker run --name wordpress --link mysql:mysql -d -P wordpress``docker run --name wordpress1 --link mysql:mysql -d -P wordpress`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d -P --name=nginx nginx``docker run --name mysql -e MYSQL_ROOT_PASSWORD=mysql
    -d mysql``docker run --name wordpress --link mysql:mysql -d -P wordpress``docker
    run --name wordpress1 --link mysql:mysql -d -P wordpress`'
- en: 'Following output shows the running Containers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示运行中的容器：
- en: '![](img/00415.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00415.jpg)'
- en: 'We can look at the Consul UI to check whether the Services are registered.
    The Consul, NGINX, and WordPress Containers are seen in the following output along
    with their IP addresses and port numbers:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看 Consul UI 来检查服务是否已注册。以下输出中可以看到 Consul、NGINX 和 WordPress 容器以及它们的 IP 地址和端口号：
- en: '![](img/00473.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00473.jpg)'
- en: 'We can check whether the service lookup by the DNS name is working by accessing
    the service across Containers. The following output shows that the NGINX container
    is able to access the WordPress container by the service name, `wordpress`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过跨容器访问服务，检查通过 DNS 名称查找服务是否正常工作。以下输出显示 NGINX 容器能够通过服务名称 `wordpress` 访问 WordPress
    容器：
- en: '![](img/00299.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00299.jpg)'
- en: Dynamic load balancing
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态负载均衡
- en: As part of Service discovery, a load balancer should be able to automatically
    find out active services and load balance among the active instances of the service.
    For example, when three instances of a web service are started, and if one of
    the instances dies, the load balancer should automatically be able to remove the
    inactive instance from the load balance list.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为服务发现的一部分，负载均衡器应该能够自动找到活动的服务，并在活动实例之间进行负载均衡。例如，当启动了三个 Web 服务实例时，如果其中一个实例宕机，负载均衡器应该能够自动将该不活动实例从负载均衡列表中移除。
- en: I found the following two approaches to be useful to achieve this.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现以下两种方法对于实现这一目标非常有用。
- en: Load balancing with confd and nginx
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 confd 和 nginx 进行负载均衡
- en: 'In the approach at [https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos](https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos),
    the following is a list of the steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos](https://www.digitalocean.com/community/tutorials/how-to-use-confd-and-etcd-to-dynamically-reconfigure-services-in-coreos)
    这一方法中，以下是步骤列表：
- en: The Sidekick service registers service details with `etcd`.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sidekick 服务将服务详情注册到 `etcd`。
- en: Confd listens for etcd changes and updates `nginx.conf`.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Confd 监听 etctd 变更并更新 `nginx.conf`。
- en: The Nginx load balancer does the load balancing based on entries in `nginx.conf`.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nginx 负载均衡器根据 `nginx.conf` 中的条目进行负载均衡。
- en: 'The following diagram illustrates the load balancing with ETCD, HA DISCOVER,
    and HAProxy:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了使用 ETCD、HA DISCOVER 和 HAProxy 进行负载均衡：
- en: '![](img/00402.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00402.jpg)'
- en: Load balancing with HAdiscover and HAproxy
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 HAdiscover 和 HAproxy 进行负载均衡
- en: 'In the approach at [http://adetante.github.io/articles/service-discovery-haproxy/](http://adetante.github.io/articles/service-discovery-haproxy/),
    the following is a list of the steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [http://adetante.github.io/articles/service-discovery-haproxy/](http://adetante.github.io/articles/service-discovery-haproxy/)
    这一方法中，以下是步骤列表：
- en: The registrator registers the service details with `etcd`
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Registrator 将服务详情注册到 `etcd`。
- en: HAdiscover listens for changes to etcd and updates `haproxy.conf`
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAdiscover 监听 etctd 变更并更新 `haproxy.conf`。
- en: HAproxy does the load balancing based on the HAproxy configuration
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAproxy 基于 HAproxy 配置执行负载均衡。
- en: 'The following diagram illustrates the load balancing with ETCD, HA DISCOVER,
    and NGINX:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了使用 ETCD、HA DISCOVER 和 NGINX 进行负载均衡：
- en: '![](img/00024.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpg)'
- en: Deployment patterns
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模式
- en: We covered the advantages of microservices in the first chapter. Designing a
    microservice-based application is similar to object-oriented programming, where
    the Container image can be compared to a class and Containers can be compared
    to objects. There are many design patterns in object-oriented programming that
    specify how to split a monolithic application into classes and how classes can
    work together with other classes. Some of the object-oriented design principles
    also apply to microservices.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一章中讨论了微服务的优势。设计基于微服务的应用程序类似于面向对象编程，其中容器镜像可以比作类，容器可以比作对象。面向对象编程中有许多设计模式，指定如何将一个单体应用程序拆分为类，以及类如何与其他类协作。一些面向对象设计原则同样适用于微服务。
- en: In [Chapter 8](index_split_181.html#filepos585742), Container Orchestration,
    we covered Kubernetes Pods and how closely related containers can be grouped together
    in a single Pod. Design patterns such as Sidecar, ambassador, and adapter are
    pretty widely used to create Pods. Even though these design patterns are mentioned
    in the context of the Kubernetes pod, these can also be used in a non-Kubernetes-based
    system as well.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 8 章](index_split_181.html#filepos585742)，容器编排中，我们讲解了 Kubernetes Pods 以及如何将紧密相关的容器分组到同一个
    Pod 中。设计模式如侧车、代理和适配器模式被广泛用于创建 Pods。尽管这些设计模式是在 Kubernetes Pod 的上下文中提到的，但也可以在非 Kubernetes
    系统中使用。
- en: This link ([http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html](http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html))
    talks about common Kubernetes composite patterns.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个链接（[http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html](http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html)）讲述了常见的
    Kubernetes 组合模式。
- en: The following are more details on common Kubernetes composite patterns.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见 Kubernetes 组合模式的更多细节。
- en: The Sidecar pattern
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 侧车模式
- en: In the Sidecar pattern, there are two dependent Containers accomplishing a single
    task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在侧车模式中，两个依赖的容器完成一个单一的任务。
- en: 'In the following diagram, the health check container monitors the web container
    and updates the results in a shared storage, such as ETCD, which can be used by
    the load balancer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，健康检查容器监控 Web 容器并将结果更新到共享存储中，例如 ETCD，负载均衡器可以使用这些数据：
- en: '![](img/00032.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpg)'
- en: 'In the following diagram, the Git sync container updates data volume from the
    Git server, which is used by the web container to update the web page:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，Git 同步容器从 Git 服务器更新数据卷，Web 容器使用该数据卷来更新网页：
- en: '![](img/00037.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpg)'
- en: 'In the following diagram, the Web container updates the log volume that is
    read by the logging container to update the central log server:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，Web 容器更新日志卷，日志容器读取该卷来更新中央日志服务器：
- en: '![](img/00385.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00385.jpg)'
- en: The Ambassador pattern
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大使模式
- en: The Ambassador pattern is used when there is a need to access different types
    of services from a client container and it is not efficient to modify the client
    container for each type of service. A proxy container will take care of accessing
    different types of service and the client container needs to talk only to the
    proxy container. For example, the redis proxy takes care of talking to a single
    redis master scenario or a scenario with a redis master and multiple redis slaves
    without the redis client being aware of the type of the redis service.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 大使模式用于当客户端容器需要访问不同类型的服务，而修改每个服务的客户端容器不高效时。代理容器将负责访问不同类型的服务，客户端容器只需要与代理容器通信。例如，redis
    代理负责处理单个 redis 主节点场景，或有一个 redis 主节点和多个 redis 从节点的场景，而 redis 客户端则不需要了解 redis 服务的类型。
- en: 'The following diagram shows the redis client with the redis ambassador accessing
    the redis service:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 redis 客户端通过 redis 代理访问 redis 服务：
- en: '![](img/00047.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00047.jpg)'
- en: The Adapter pattern
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器模式
- en: The Adapter pattern is the inverse of the Ambassador pattern. An example of
    the Adapter pattern is a service container exposing a common interface independent
    of the application residing in the service. For example, a monitoring or logging
    application wants a common interface to gather inputs irrespective of the application
    type. An adapter container takes care of converting the data to a standard format
    expected by the monitoring or logging application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器模式是大使模式的反向。适配器模式的一个例子是服务容器暴露一个独立于服务中应用的公共接口。例如，一个监控或日志应用希望有一个公共接口来收集输入，而不考虑应用的类型。适配器容器负责将数据转换为监控或日志应用所期望的标准格式。
- en: 'The following example shows a monitoring/logging application accessing two
    different container applications, each with their own adapters:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了一个监控/日志应用访问两个不同容器应用，每个应用都有自己的适配器：
- en: '![](img/00322.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00322.jpg)'
- en: Rolling updates with the Canary pattern
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新与金丝雀模式
- en: This is an upgrade approach used when an application runs as a Container across
    a cluster of servers behind a load balancer. In this approach, the application
    upgrade is done on a few servers, and based on the preliminary feedback from customers,
    the upgrade can either be continued or reverted.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是当应用程序作为容器在多个服务器后面的负载均衡器后运行时使用的升级方式。在这种方法中，应用程序的升级会先在少数服务器上进行，然后根据客户的初步反馈，升级可以继续进行或撤回。
- en: Kubernetes supports a rolling upgrade with the Canary pattern. In the following
    example, we will demonstrate the Canary pattern with Kubernetes running on a CoreOS
    cluster in AWS. Here, we will upgrade `hello1-controller` with three replicas
    of the `hello:v1` container to `hello2-controller`, which also has three replicas
    of the `hello:v2` container.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes支持使用金丝雀模式的滚动升级。在下面的示例中，我们将演示在AWS上的CoreOS集群上运行的Kubernetes如何使用金丝雀模式。在这里，我们将`hello1-controller`（其包含三个`hello:v1`容器副本）升级为`hello2-controller`，后者也包含三个`hello:v2`容器副本。
- en: For this example, we need a three-node Kubernetes CoreOS cluster. Installation
    instructions can be found in [Chapter 8](index_split_181.html#filepos585742),
    Container Orchestration.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本示例，我们需要一个包含三个节点的Kubernetes CoreOS集群。安装说明可以在[第8章](index_split_181.html#filepos585742)《容器编排》中找到。
- en: 'The following is a three-node cluster with one master and two worker nodes:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个包含一个主节点和两个工作节点的三节点集群：
- en: '![](img/00230.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00230.jpg)'
- en: 'The following is the replication controller, `hello1-controller.json`, with
    the `hello1` container image and three replicas:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是具有`hello1`容器镜像和三个副本的复制控制器`hello1-controller.json`：
- en: '`apiVersion: v1 kind: ReplicationController metadata:   name: hello1   labels:
        name: hello spec:   replicas: 3   selector:     name: hello     version: v1
      template:     metadata:       labels:         name: hello         version: v1
        spec:       containers:       - name: hello         image: quay.io/kelseyhightower/hello:1.0.0
            ports:         - containerPort: 80`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ReplicationController metadata:   name: hello1   labels:     name:
    hello spec:   replicas: 3   selector:     name: hello     version: v1   template:     metadata:       labels:         name:
    hello         version: v1     spec:       containers:       - name: hello         image:
    quay.io/kelseyhightower/hello:1.0.0         ports:       - containerPort: 80`'
- en: 'The following is the `hello-s.json` service using the `hello1` replication
    controller:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`hello1`复制控制器的`hello-s.json`服务：
- en: '`apiVersion: v1 kind: Service metadata:   name: hello   labels:     name: hello
    spec:   # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the hello service.   type: NodePort   ports:
        # the port that this service should serve on     - port: 80   selector:     name: hello`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: hello   labels:     name: hello
    spec:   # 如果你的集群支持，请取消注释以下内容以自动创建   # 一个外部负载均衡IP给hello服务。   type: NodePort   ports:     #
    该服务应提供的端口     - port: 80   selector:     name: hello`'
- en: 'Let''s start the replication controller and service:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动复制控制器和服务：
- en: '`kubectl create -f hello1-controller.json ``kubectl create -f hello-s.json`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl create -f hello1-controller.json  kubectl create -f hello-s.json`'
- en: 'Let''s look at the running services and pods:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来查看正在运行的服务和Pod：
- en: '![](img/00271.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00271.jpg)'
- en: 'Let''s create a new replication controller and perform a Canary pattern rolling
    upgrade. The following is the new replication `controller`, `hello2-controller.json`,
    using the `hello:2.0.0` container image:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建一个新的复制控制器，并执行金丝雀模式的滚动升级。以下是新的复制`controller`，`hello2-controller.json`，使用`hello:2.0.0`容器镜像：
- en: '`apiVersion: v1 kind: ReplicationController metadata:   name: hello2   labels:
        name: hello spec:   replicas: 3   selector:     name: hello     version: v2
      template:     metadata:       labels:         name: hello         version: v2
        spec:       containers:       - name: hello         image: quay.io/kelseyhightower/hello:2.0.0
            ports:         - containerPort: 80`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ReplicationController metadata:   name: hello2   labels:     name:
    hello spec:   replicas: 3   selector:     name: hello     version: v2   template:     metadata:       labels:         name:
    hello         version: v2     spec:       containers:       - name: hello         image:
    quay.io/kelseyhightower/hello:2.0.0         ports:       - containerPort: 80`'
- en: 'The following command does the rolling upgrade to `hello2`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行滚动升级到`hello2`：
- en: '`kubectl rolling-update hello1 --update-period=10s -f hello2-controller.json`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl rolling-update hello1 --update-period=10s -f hello2-controller.json`'
- en: The `update-period` parameter specifies the time interval between the upgrade
    of each pod.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`update-period`参数指定每个Pod升级之间的时间间隔。'
- en: 'The following output shows you how each pod gets upgraded from `hello1` to
    `hello2`. At the end, the `hello1` replication controller is deleted:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了每个 pod 如何从 `hello1` 升级到 `hello2`。最终，`hello1` 复制控制器被删除：
- en: '![](img/00307.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00307.jpg)'
- en: 'Let''s look at the running replication controllers now. As we can see in the
    following output, `hello2` RC is running and `hello1` RC has been deleted:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下正在运行的复制控制器。正如我们在以下输出中看到的，`hello2` RC 正在运行，而 `hello1` RC 已被删除：
- en: '![](img/00335.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00335.jpg)'
- en: Kubernetes also supports the rollback option. In case a problem is detected
    as part of a rolling upgrade, the rolling upgrade can be stopped and rollback
    can be done using the `--rollback` option.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 还支持回滚选项。如果在滚动升级过程中检测到问题，可以停止滚动升级，并使用 `--rollback` 选项进行回滚。
- en: Containers and PaaS
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 容器与PaaS
- en: 'Traditionally, the Services architecture has three types:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，服务架构有三种类型：
- en: IaaS (Infrastructure as a service)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IaaS（基础设施即服务）
- en: PaaS (Platform as a service)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaaS（平台即服务）
- en: SaaS (Software as a service)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SaaS（软件即服务）
- en: With the advent of Docker, the PaaS layer has become a little difficult to define.
    PaaS vendors have used Containers as their underlying technology from the beginning.
    In fact, Docker came from the Dotcloud Company, which was providing a PaaS service.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Docker 的出现，PaaS 层变得有些难以定义。PaaS 服务商从一开始就将容器作为其基础技术。事实上，Docker 来源于 Dotcloud
    公司，该公司曾提供 PaaS 服务。
- en: 'The following figure describes the new PaaS models and how they tie in to the
    traditional PaaS models and IaaS:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示描述了新型 PaaS 模式以及它们如何与传统 PaaS 模式和 IaaS 相结合：
- en: '![](img/00384.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00384.jpg)'
- en: 'The following are some notes on the preceding diagram as well as how newer
    PaaS models are being developed:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于前述图示的一些注释，以及新型 PaaS 模式如何发展：
- en: PaaS is typically used to simplify application deployment, which allows application
    developers to just develop the application, and PaaS provides necessary infrastructure
    services such as HA, scalability, and networking. PaaS is typically used for web
    applications.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaaS 通常用于简化应用程序部署，它允许应用程序开发人员仅专注于开发应用程序，而 PaaS 提供所需的基础设施服务，如高可用性、可扩展性和网络连接。PaaS
    通常用于 Web 应用程序。
- en: PaaS is typically deployed internally as Containers though users of PaaS need
    not be aware of this.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaaS 通常以内部分部署为容器，尽管 PaaS 用户不需要了解这一点。
- en: Even though PaaS makes deploying applications faster, flexibility gets lost
    with PaaS.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管PaaS加速了应用程序的部署，但PaaS失去了灵活性。
- en: Examples of traditional PaaS systems are AWS Elastic beanstalk, Google GAE,
    Openshift, and Cloudfoundry.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统 PaaS 系统的示例包括 AWS Elastic Beanstalk、Google GAE、Openshift 和 Cloudfoundry。
- en: There is a new class of Micro-PaaS, where every service runs as a Docker Container,
    and this gives a little more flexibility than traditional PaaS. Examples are Deis,
    Flynn, and Tutum. Tutum was recently acquired by Docker.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一种新型的微型 PaaS，每个服务都以 Docker 容器的形式运行，这比传统的 PaaS 提供了更多的灵活性。示例有 Deis、Flynn 和 Tutum。Tutum
    最近被 Docker 收购。
- en: With Docker containers, Container orchestration systems such as Kubernetes,
    and Container OSes such as CoreOS, it becomes easier for customers to build a
    PaaS system by themselves, which gives them maximum flexibility. Both Amazon and
    Google have launched Container services where users can run their Containers.
    Users have the option to build Container services on top of their own infrastructure
    as well.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 容器、如 Kubernetes 这样的容器编排系统和 CoreOS 等容器操作系统，客户可以更轻松地自行构建 PaaS 系统，从而获得最大的灵活性。亚马逊和谷歌都推出了容器服务，用户可以在其中运行自己的容器。用户也可以选择在自己的基础设施上构建容器服务。
- en: Stateful and Stateless Containers
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有状态与无状态容器
- en: 'Stateless containers are typically web applications such as NGINX, Node.js,
    and others. These follow the 12-factor application development ([http://12factor.net/](http://12factor.net/))
    methodology. These containers can be horizontally scaled. Stateful containers
    are used to store data like databases as data volumes in the host machine. Examples
    of stateful containers are Redis, MySQL, and MongoDB. We covered options for Container
    data persistence in [Chapter 6](index_split_147.html#filepos425088), CoreOS Storage
    Management. When stateful containers are migrated, it is necessary to migrate
    the data associated with the stateful containers. The following options are available
    to migrate stateful containers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态容器通常是 Web 应用程序，如 NGINX、Node.js 等。这些容器遵循 12-factor 应用开发方法论（[http://12factor.net/](http://12factor.net/)）。这些容器可以进行水平扩展。有状态容器用于存储数据，如数据库在主机中的数据卷。常见的有状态容器包括
    Redis、MySQL 和 MongoDB。在 [第六章](index_split_147.html#filepos425088)，《CoreOS 存储管理》中，我们讨论了容器数据持久化的选项。当有状态容器被迁移时，必须迁移与容器相关的数据。以下是迁移有状态容器的几种选项：
- en: Using tools such as Flocker, which takes care of the volume and data migration
    when a Container moves across hosts
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用类似 Flocker 的工具，当容器在主机之间迁移时，它负责处理数据卷和数据的迁移
- en: Using a cluster-file system or NFS so that the same data volume can be seen
    across multiple hosts
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群文件系统或 NFS，以便在多个主机之间共享相同的数据卷
- en: If implementing stateful containers is difficult, the other option for storage
    is to keep databases separate from application containers and run them on special
    systems.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果实现有状态容器较为困难，另一种存储选择是将数据库与应用容器分离，并在专门的系统上运行。
- en: Security
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: The following are some approaches to secure the CoreOS cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些确保 CoreOS 集群安全的方法。
- en: Secure the external daemons
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 确保外部守护进程的安全
- en: Services such as Etcd, Fleet, and Docker can be reached externally. We can secure
    the client and server side using TLS and client and server certificates. We covered
    some of these details in earlier chapters when individual services were covered.
    If we are using Container orchestration such as Kubernetes, we need to make sure
    that the Kubernetes API server is using the TLS mechanism.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 服务如 Etcd、Fleet 和 Docker 可以在外部访问。我们可以使用 TLS 以及客户端和服务器证书来保障客户端和服务器端的安全。在之前的章节中，我们讨论过一些关于单个服务的细节。如果我们使用
    Kubernetes 这样的容器编排工具，需要确保 Kubernetes API 服务器采用 TLS 机制。
- en: SELinux
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: SELinux
- en: SELinux is a Linux kernel feature that allows Container isolation even in case
    of a kernel bug that can cause the hacker to escape the Container namespace. SELinux
    integration is available from CoreOS 808.0 release. CoreOS disables SELinux by
    default. It can be enabled using the procedure at [https://coreos.com/os/docs/latest/selinux.html](https://coreos.com/os/docs/latest/selinux.html).
    There are some limitations like not being able to run SELinux with the btrfs filesystem
    and with Containers sharing volumes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: SELinux 是 Linux 内核的一项特性，它允许在内核出现漏洞并导致黑客逃逸容器命名空间时，仍然能够隔离容器。CoreOS 808.0 版本开始支持
    SELinux 集成。CoreOS 默认禁用 SELinux，但可以通过 [https://coreos.com/os/docs/latest/selinux.html](https://coreos.com/os/docs/latest/selinux.html)
    中的过程启用它。存在一些限制，例如无法与 btrfs 文件系统以及共享数据卷的容器一起使用 SELinux。
- en: Container image signing
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像签名
- en: Docker supports Container signing using the Docker content trust. Rkt supports
    image signing using GPG. Using these approaches, we can validate that Containers
    running on CoreOS come from reliable sources and the Container image is not tampered
    in the middle. Container image signing was covered in detail in [Chapter 7](index_split_164.html#filepos509414),
    Container Integration with CoreOS – Docker and Rkt.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 支持使用 Docker 内容信任进行容器签名。Rkt 支持使用 GPG 进行镜像签名。通过这些方法，我们可以验证在 CoreOS 上运行的容器来自可靠的来源，并且容器镜像在传输过程中未被篡改。容器镜像签名的详细内容请参见
    [第七章](index_split_164.html#filepos509414)，《与 CoreOS 的容器集成 – Docker 和 Rkt》。
- en: Deployment and automation
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 部署与自动化
- en: Containers make it easy to package and ship software and guarantee that the
    same Container can work in development as well as production environments. Combining
    Containers with good deployment and automation techniques will aid in faster software
    deployment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 容器使得打包和运输软件变得简单，并且能够保证同一个容器在开发环境和生产环境中都能正常运行。将容器与良好的部署和自动化技术结合，将有助于更快速地部署软件。
- en: Continuous Integration and Continuous Delivery
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成与持续交付
- en: 'The traditional approach of releasing software has the following problems:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的软件发布方法存在以下问题：
- en: Software release cycles were spaced apart, which caused new features taking
    a longer time to reach the customers
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件发布周期间隔较长，导致新特性需要更长的时间才能到达客户。
- en: Majority of the processes from the development stage to production were manual
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从开发阶段到生产阶段的大部分过程都是手动的。
- en: Considering the different deployment scenarios, it was difficult to guarantee
    that software worked in all environments and configurations
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到不同的部署场景，保证软件在所有环境和配置中都能正常运行是非常困难的。
- en: Containers have tried to mitigate some of these problems. Using microservices
    and the Container approach, it is guaranteed that the application will behave
    similarly in the development and production stages. Process automation and appropriate
    testing are still necessary for a Container-based environment.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 容器已尝试缓解这些问题。通过使用微服务和容器方法，确保应用程序在开发和生产阶段的行为类似。对于基于容器的环境，过程自动化和适当的测试仍然是必要的。
- en: Continuous Integration (CI) refers to the process of making an executable or
    Container image automatically after a developer has done the Unit testing and
    code commit..
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成（CI）指的是开发者完成单元测试和代码提交后，自动生成可执行文件或容器镜像的过程。
- en: Continuous Delivery (CD) refers to the process of taking the developer's built
    image, setting up the staging environment to test the image, and deploying it
    successfully for production.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付（CD）指的是将开发者构建的镜像，设置预发布环境以测试镜像，并成功地部署到生产环境的过程。
- en: 'The following figure illustrates the steps for CI/CD:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 CI/CD 的步骤：
- en: '![](img/00430.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00430.jpg)'
- en: 'The following are some notes on the preceding diagram:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是上述图的一些注释：
- en: The first row in the preceding diagram captures the steps for CI and the second
    row is the steps for CD.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述图中的第一行捕捉了 CI 的步骤，第二行则是 CD 的步骤。
- en: The CI process starts when developers commit the code after their basic UT.
    Typically, GitHub or Bitbucket is used as an image repository.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CI 流程从开发人员在完成基本单元测试后提交代码开始。通常，GitHub 或 Bitbucket 被用作镜像仓库。
- en: There are hooks provided from the image repository to build system to automatically
    trigger the build after committing. The build system could be something such as
    Jenkins, which integrates with different code repositories. For Container images,
    `Dockerfile` and `docker build` will be used to build the Container image.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像仓库和构建系统之间提供了钩子，可以在提交后自动触发构建。构建系统可以是像 Jenkins 这样的工具，它与不同的代码仓库集成。对于容器镜像，`Dockerfile`
    和 `docker build` 将用于构建容器镜像。
- en: Automatic UT suites can be kicked in if necessary before the image is committed
    to the image repository.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，在将镜像提交到镜像仓库之前，可以触发自动单元测试套件。
- en: The build itself needs to be done inside Containers in order to eliminate dependency
    on the host system.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建本身需要在容器内进行，以消除对主机系统的依赖。
- en: An image repository could be something such as the Docker hub or Docker trusted
    registry for Containers. CoreOS support the Quay repository for Container images.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像仓库可以是 Docker hub 或 Docker trusted registry 等容器镜像仓库。CoreOS 支持用于容器镜像的 Quay 仓库。
- en: Once the image is pushed to a repository, the start of CD is automatically triggered.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦镜像推送到仓库，CD 过程就会自动触发。
- en: The staging environment needs to be set up with different Containers, storage,
    and other non-container software if necessary.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要根据需要在预发布环境中设置不同的容器、存储和其他非容器软件。
- en: QA tests are done in the staging environment. It is necessary that the staging
    environment be as close as possible to production.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QA 测试是在预发布环境中进行的。预发布环境必须尽可能接近生产环境。
- en: Once the QA tests are successful, images are deployed in production, such as
    AWS or GCE. If it's a PaaS application, it can be deployed to Cloudfoundry, among
    others.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦 QA 测试成功，镜像就会部署到生产环境，如 AWS 或 GCE。如果是 PaaS 应用程序，则可以部署到 Cloudfoundry 等平台。
- en: There are companies that provide integrated CI/CD solutions, such as Codeship,
    CircleCI, Shippable, and others. Docker has released an enterprise product called
    Universal Control Plane (UCP), which targets the CD part. Jenkins has Docker plugins
    to build images in Containers and also provides integration with the Docker hub.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些公司提供集成的 CI/CD 解决方案，如 Codeship、CircleCI、Shippable 等。Docker 已发布一款名为 Universal
    Control Plane（UCP）的企业产品，专门针对 CD 部分。Jenkins 有 Docker 插件，用于在容器中构建镜像，并提供与 Docker
    hub 的集成。
- en: There are different deployment patterns to do the upgrade. We covered the Canary
    deployment pattern in an earlier section.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有不同的部署模式来进行升级。我们在早期章节中介绍了Canary部署模式。
- en: Ansible integration with CoreOS and Docker
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible与CoreOS和Docker的集成
- en: 'Ansible is a configuration management and automation tool. Ansible is a very
    popular DevOps tool and serves similar purposes as Puppet or Chef. Ansible has
    a unique feature that there is no need to install an agent on the device side
    and this makes it very popular. There is active work ongoing to integrate Ansible
    with CoreOS and Docker. The following are some integration possibilities:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible是一个配置管理和自动化工具。Ansible是一个非常流行的DevOps工具，与Puppet或Chef类似。Ansible有一个独特的特性，即无需在设备端安装代理程序，这使其非常受欢迎。正在积极进行的工作是将Ansible与CoreOS和Docker集成。以下是一些集成可能性：
- en: Manage the CoreOS system with Ansible. As CoreOS does not come with Python installed
    and the fact that packages cannot be installed directly, there are some workarounds
    necessary to get Ansible to manage a CoreOS system.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ansible管理CoreOS系统。由于CoreOS不预装Python且不能直接安装软件包，因此需要一些变通方法来使Ansible能够管理CoreOS系统。
- en: Ansible has a Docker module that simplifies Container management such as starting
    and stopping containers and controlling Container properties.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible有一个Docker模块，简化了容器管理，如启动和停止容器以及控制容器属性。
- en: The Docker installation can be automated with Ansible. Other than automating
    the Docker installation, Ansible can also manage other host infrastructure such
    as logging, storage, and networking.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ansible可以自动化Docker安装。除了自动化Docker安装外，Ansible还可以管理其他主机基础设施，如日志记录、存储和网络。
- en: Ansible can be used to build Docker images instead of using Dockerfile. There
    is a `docker_image` module ([http://docs.ansible.com/ansible/docker_image_module.html](http://docs.ansible.com/ansible/docker_image_module.html)),
    but it is advised not to use it as its idempotent nature causes the Docker image
    to not be built in certain cases, which is a problem.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ansible可以用来构建Docker镜像，而不是使用Dockerfile。有一个`docker_image`模块（[http://docs.ansible.com/ansible/docker_image_module.html](http://docs.ansible.com/ansible/docker_image_module.html)），但建议不要使用它，因为其幂等性可能导致在某些情况下Docker镜像无法构建，这是一个问题。
- en: Using Ansible to manage CoreOS
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ansible管理CoreOS
- en: I followed the procedure at [https://coreos.com/blog/managing-coreos-with-ansible/](https://coreos.com/blog/managing-coreos-with-ansible/)
    to manage CoreOS with Ansible. As there is no package manager in CoreOS, Python
    cannot be installed directly. An approach at [https://github.com/defunctzombie/ansible-coreos-bootstrap](https://github.com/defunctzombie/ansible-coreos-bootstrap)
    that is being used is to install PyPy, which is a minimal Python interpreter in
    CoreOS in the user directory and get Ansible to use this. The following example
    prepares the CoreOS node to be managed by Ansible and starts Etcd and Fleet service
    in the node using Ansible.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我按照[https://coreos.com/blog/managing-coreos-with-ansible/](https://coreos.com/blog/managing-coreos-with-ansible/)的步骤来使用Ansible管理CoreOS。由于CoreOS中没有包管理器，因此不能直接安装Python。在[https://github.com/defunctzombie/ansible-coreos-bootstrap](https://github.com/defunctzombie/ansible-coreos-bootstrap)中使用的方法是安装PyPy，在用户目录中为CoreOS安装了一个简化的Python解释器，并让Ansible使用它。以下示例准备了CoreOS节点以由Ansible管理，并使用Ansible在节点中启动Etcd和Fleet服务。
- en: 'The following are the steps:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤：
- en: Install Ansible in the host machine. In my case, I am running Ansible 1.9 version
    in my Ubuntu 14.04 machine.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主机上安装Ansible。在我的情况下，我在Ubuntu 14.04机器上运行的是Ansible 1.9版本。
- en: Create a CoreOS cluster.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建CoreOS集群。
- en: Run the CoreOS bootstrap role to install the Python interpreter in CoreOS and
    update the system PATH to use it. Ansible roles create an abstraction over playbooks
    for specific tasks.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行CoreOS引导角色，在CoreOS中安装Python解释器并更新系统PATH以使用它。Ansible角色为特定任务创建playbooks的抽象。
- en: Run Ansible playbooks to start CoreOS services. Playbook is an Ansible task
    list.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Ansible playbook以启动CoreOS服务。Playbook是一个Ansible任务列表。
- en: 'Set up a CoreOS cluster:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 设置CoreOS集群：
- en: 'The following commands set up the CoreOS cluster. In this case, a single-node
    cluster is created:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令设置CoreOS集群。在本例中，创建了一个单节点集群：
- en: '`git clone https://github.com/defunctzombie/coreos-ansible-example.git``cd coreos-ansible-example``vagrant up`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/defunctzombie/coreos-ansible-example.git``cd coreos-ansible-example``vagrant up`'
- en: 'Set up passwordless SSH access:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 设置无密码SSH访问：
- en: Use the following command to set up passwordless SSH access. Ansible needs passwordless
    SSH access.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令设置无密码SSH访问。Ansible需要无密码SSH访问。
- en: '`./bin/generate_ssh_config`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`./bin/generate_ssh_config`'
- en: 'Run the CoreOS bootstrap role:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 CoreOS 引导角色：
- en: 'The following command sets up a CoreOS node with Python using the Ansible role,
    `defunctzombie.coreos-bootstrap`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令使用 Ansible 角色 `defunctzombie.coreos-bootstrap` 设置一个包含 Python 的 CoreOS 节点：
- en: '`ansible-galaxy install defunctzombie.coreos-bootstrap -p ./roles``ansible-playbook -i inventory/vagrant bootstrap.yml`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`ansible-galaxy install defunctzombie.coreos-bootstrap -p ./roles``ansible-playbook -i inventory/vagrant bootstrap.yml`'
- en: 'I created the following playbook to start CoreOS services, Etcd2, and Fleet:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了以下 playbook 来启动 CoreOS 服务、Etcd2 和 Fleet：
- en: '`//Coreos_services.yml: - name: CoreOS services   hosts: web   tasks:     - name: Start etcd2
          service: name=etcd2.service state=started       sudo: true       sudo_user: root      - name: Start fleet
          service: name=fleet.service state=started       sudo: true       sudo_user: root`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`//Coreos_services.yml: - name: CoreOS services   hosts: web   tasks:     - name: Start etcd2
          service: name=etcd2.service state=started       sudo: true       sudo_user: root      - name: Start fleet
          service: name=fleet.service state=started       sudo: true       sudo_user: root`'
- en: In the preceding playbook, we have used the Ansible `service` module. Ansible
    modules are functions to do specific tasks. Ansible ships with a number of default
    modules and users can extend or write their own modules.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 playbook 中，我们使用了 Ansible `service` 模块。Ansible 模块是执行特定任务的函数。Ansible 自带多个默认模块，用户可以扩展或编写自己的模块。
- en: 'The following is the output when I started the playbook for the first time.
    The inventory file contains details of the single CoreOS node:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我第一次启动 playbook 时的输出。清单文件包含了单个 CoreOS 节点的详细信息：
- en: '`ansible-playbook –i inventory/vagrant coreos-services.yml`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`ansible-playbook –i inventory/vagrant  coreos-services.yml`'
- en: '![](img/00125.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpg)'
- en: .
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'The following is the output when I ran the same playbook one more time:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我再次运行相同 playbook 时的输出：
- en: '![](img/00133.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00133.jpg)'
- en: As we can see, services don't get restarted as they have already started and
    the `changed` variable is not set.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，服务没有被重启，因为它们已经启动，并且 `changed` 变量未被设置。
- en: 'The following output shows the running Etcd2 and Fleet services in the CoreOS
    node:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了在 CoreOS 节点上运行的 Etcd2 和 Fleet 服务：
- en: '![](img/00143.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00143.jpg)'
- en: Using Ansible to manage Docker Containers
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ansible 管理 Docker 容器
- en: Ansible provides you with a Docker module ([http://docs.ansible.com/ansible/docker_module.html](http://docs.ansible.com/ansible/docker_module.html))
    to manage Docker Containers. The Docker module can manage the Container life cycle,
    which includes the starting and stopping of Containers. As Ansible modules are
    idempotent, we can use this functionality to pull Docker images only if necessary
    and restart Containers only if the base image has changed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible 为您提供了一个 Docker 模块（[http://docs.ansible.com/ansible/docker_module.html](http://docs.ansible.com/ansible/docker_module.html)），用于管理
    Docker 容器。Docker 模块可以管理容器的生命周期，包括容器的启动和停止。由于 Ansible 模块是幂等的，我们可以在必要时仅拉取 Docker
    镜像，并且仅在基础镜像发生变化时才重启容器。
- en: 'The following is a playbook that is executed on the same CoreOS node where
    we had run the CoreOS services playbook in the previous section. This will install
    the Docker module in the remote host and start the NGINX Container and a WordPress
    service having the WordPress and MySQL Containers:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个在我们之前运行 CoreOS 服务 playbook 的同一 CoreOS 节点上执行的 playbook。此 playbook 将在远程主机上安装
    Docker 模块，并启动 NGINX 容器和包含 WordPress 和 MySQL 容器的 WordPress 服务：
- en: '`//Coreos_containers.yml: - name: CoreOS Container   hosts: web   tasks:     - name: Install docker-py
          pip: name=docker-py version=1.1.0      - name: pull container       raw: docker pull nginx      - name: launch nginx container
          docker:         image: "nginx"         name: "example-nginx"         ports: "8080:80"
            net: bridge         state: reloaded      - name: launch mysql container
          docker:         image: mysql         name: mysql         pull: always         net: bridge
            state: reloaded         env:             MYSQL_ROOT_PASSWORD: mysql      - name: launch wordpress container
          docker:         image: wordpress         name: wordpress         pull: always
            ports: 8000:80         net: bridge         state: reloaded         links:
            - "mysql:mysql"`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`//Coreos_containers.yml: - name: CoreOS Container   hosts: web   tasks:     - name: Install docker-py
          pip: name=docker-py version=1.1.0      - name: pull container       raw: docker pull nginx      - name: launch nginx container
          docker:         image: "nginx"         name: "example-nginx"         ports: "8080:80"
            net: bridge         state: reloaded      - name: launch mysql container
          docker:         image: mysql         name: mysql         pull: always         net: bridge
            state: reloaded         env:             MYSQL_ROOT_PASSWORD: mysql      - name: launch wordpress container
          docker:         image: wordpress         name: wordpress         pull: always
            ports: 8000:80         net: bridge         state: reloaded         links:
            - "mysql:mysql"`'
- en: 'The following shows the output when the playbook is started for the first time:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是第一次启动 playbook 时的输出：
- en: '![](img/00152.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00152.jpg)'
- en: 'The following screenshot shows the output when the same playbook is run again.
    As we can see, the `changed` flag is not set as all the Containers are running
    and there is no configuration change necessary:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了相同的 playbook 再次运行时的输出。正如我们所见，`changed` 标志未设置，因为所有容器都在运行，并且没有必要进行配置更改：
- en: '![](img/00162.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00162.jpg)'
- en: 'The following output shows the running Containers in the CoreOS node:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示 CoreOS 节点中正在运行的容器：
- en: '![](img/00172.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00172.jpg)'
- en: Note
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: The `reloaded` flag in the Ansible Docker module should restart containers
    only if the base image is changed or configuration flags have changed. I hit a
    bug where Containers were restarted always. The link here ([https://github.com/ansible/ansible-modules-core/issues/1251](https://github.com/ansible/ansible-modules-core/issues/1251))
    describes this bug. Its workaround is to specify the `net` parameter as I have
    done in the preceding playbook.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Ansible Docker 模块中的 `reloaded` 标志应仅在基础镜像或配置标志发生变化时重启容器。我遇到了一个 bug，容器总是被重启。这里的链接
    ([https://github.com/ansible/ansible-modules-core/issues/1251](https://github.com/ansible/ansible-modules-core/issues/1251))
    说明了这个 bug。它的解决方法是指定 `net` 参数，正如我在前面的 playbook 中所做的那样。
- en: The `reloaded` and `pull` flags are available from Ansible 1.9.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`reloaded` 和 `pull` 标志从 Ansible 1.9 版本开始可用。'
- en: Ansible as a Container
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible 作为容器
- en: 'Public Container images with Ansible preinstalled are available. This link,
    [https://hub.docker.com/r/ansible/ubuntu14.04-ansible/](https://hub.docker.com/r/ansible/ubuntu14.04-ansible/),
    is an example Container image with Ansible preinstalled. The following output
    shows the Ansible version in the running Container:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了预安装了 Ansible 的公共容器镜像。这个链接，[https://hub.docker.com/r/ansible/ubuntu14.04-ansible/](https://hub.docker.com/r/ansible/ubuntu14.04-ansible/)，是一个预安装了
    Ansible 的容器镜像示例。以下输出显示了正在运行的容器中的 Ansible 版本：
- en: '![](img/00181.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00181.jpg)'
- en: Using Ansible to install Docker
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ansible 安装 Docker
- en: Ansible has this concept of Roles that gives a good abstraction to share a list
    of playbooks that accomplish a single task. Ansible Roles are available to install
    Docker on the Linux host. Ansible Roles are maintained in a central repository
    called Ansible Galaxy, which can be shared across users. Ansible Galaxy is similar
    to the Docker hub for Ansible roles.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Ansible 有一个角色（Roles）的概念，提供了一个良好的抽象来共享执行单一任务的 playbook 列表。Ansible 角色可以用于在 Linux
    主机上安装 Docker。Ansible 角色保存在一个名为 Ansible Galaxy 的中央仓库中，可以供用户共享。Ansible Galaxy 类似于
    Docker Hub，用于分享 Ansible 角色。
- en: 'The following are the steps necessary:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要执行的步骤：
- en: Install the Ansible role locally from Ansible Galaxy.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Ansible Galaxy 本地安装 Ansible 角色。
- en: Create a playbook with this role and run it.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含此角色的 playbook 并运行它。
- en: I used this Galaxy role ([https://github.com/jamesdbloom/ansible-install-docker](https://github.com/jamesdbloom/ansible-install-docker))
    to install Docker on my Ubuntu node. There are a few other roles in Galaxy accomplishing
    the same task.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用这个 Galaxy 角色 ([https://github.com/jamesdbloom/ansible-install-docker](https://github.com/jamesdbloom/ansible-install-docker))
    在我的 Ubuntu 节点上安装 Docker。Galaxy 中还有一些其他角色也能完成相同的任务。
- en: 'Use the following command to install the role:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令安装角色：
- en: '`ansible-galaxy install jamesdbloom.install-docker -p ./roles`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`ansible-galaxy install jamesdbloom.install-docker -p ./roles`'
- en: 'Create the `install_docker1.yml` playbook with the role:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含角色的 `install_docker1.yml` playbook：
- en: '`- name: install docker   hosts: ubuntu   gather_facts: True   sudo: true   roles:
        - jamesdbloom.install-docker`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`- name: install docker  hosts: ubuntu  gather_facts: True  sudo: true  roles:   -
    jamesdbloom.install-docker`'
- en: 'Run the playbook as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式运行 playbook：
- en: '`ansible-playbook -i inventory/vagrant install_docker1.yml`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`ansible-playbook -i inventory/vagrant install_docker1.yml`'
- en: 'The following is my inventory file:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我的 inventory 文件：
- en: '`## inventory file for vagrant machines``ubuntu-01 ansible_ssh_host=172.13.8.101``[ubuntu]``ubuntu-01``[ubuntu:vars]``ansible_ssh_user=vagrant`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`## inventory 文件用于 vagrant 虚拟机``ubuntu-01 ansible_ssh_host=172.13.8.101``[ubuntu]``ubuntu-01``[ubuntu:vars]``ansible_ssh_user=vagrant`'
- en: 'The following output shows the playbook output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 playbook 的输出：
- en: '`Ansible-playbook –i inventory/vagrant install_docker1.yml`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`Ansible-playbook –i inventory/vagrant install_docker1.yml`'
- en: '![](img/00190.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00190.jpg)'
- en: 'The following output shows Docker installed on my Ubuntu host using the preceding
    playbook:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了使用前面的 playbook 在我的 Ubuntu 主机上安装的 Docker：
- en: '![](img/00321.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00321.jpg)'
- en: Note
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: I faced an issue with restarting the Docker service. I was able to solve
    it using the procedure at [https://github.com/ansible/ansible-modules-core/issues/1170](https://github.com/ansible/ansible-modules-core/issues/1170),
    where the init file has to be removed. I faced this issue with Ansible 1.9.1;
    however, this is fixed in later Ansible versions.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我遇到了重启 Docker 服务的问题。我通过 [https://github.com/ansible/ansible-modules-core/issues/1170](https://github.com/ansible/ansible-modules-core/issues/1170)
    中的过程解决了此问题，其中需要删除 init 文件。我在 Ansible 1.9.1 版本中遇到此问题，但在后来的 Ansible 版本中已经修复。
- en: The CoreOS roadmap
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 路线图
- en: Ignition
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Ignition
- en: 'The Ignition ([https://github.com/coreos/ignition](https://github.com/coreos/ignition))
    project is being developed to setup initial CoreoS filesystem and it overcomes
    some of the issues with `coreos-cloudinit`. The `coreos-cloudinit` program is
    used to set up an initial CoreOS system configuration. The following are some
    known issues with `coreos-cloudinit`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Ignition ([https://github.com/coreos/ignition](https://github.com/coreos/ignition))
    项目正在开发中，用于设置初始的 CoreOS 文件系统，并解决了 `coreos-cloudinit` 一些问题。`coreos-cloudinit` 程序用于设置初始的
    CoreOS 系统配置。以下是 `coreos-cloudinit` 一些已知的问题：
- en: It is difficult to feed in dynamic environment variables. This makes it difficult
    to run CoreOS in Openstack environments and other environments where it is difficult
    to determine the IP address. This link, [https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4](https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4),
    describes the case where IP addresses don't get set because of which `cloud-config`
    services fail in Openstack.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态环境变量的传递比较困难。这使得在 Openstack 环境和其他难以确定 IP 地址的环境中运行 CoreOS 变得困难。这个链接， [https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4](https://groups.google.com/forum/#!topic/coreos-user/STmEU6FGRB4)，描述了由于
    IP 地址未被设置，`cloud-config` 服务在 Openstack 中无法正常工作的问题。
- en: The `cloud-config` service is processed serially and we cannot specify dependencies.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud-config` 服务是串行处理的，我们不能指定依赖关系。'
- en: Ignition is run once on initial system bring-up and it writes the necessary
    files like service files and configuration data. On the first boot, Ignition reads
    the configuration from a specific location that's specified in the bootloader.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Ignition 在系统首次启动时运行一次，并写入必要的文件，如服务文件和配置数据。在第一次启动时，Ignition 从启动加载程序指定的特定位置读取配置。
- en: Systemd, as part of a running provider metadata service file, will create `coreos-metadata.target`,
    which will contain necessary environment variables that service files can use.
    Service files will specify this target file as a dependency and systemd will take
    care of this dependency.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Systemd 作为运行中的提供程序元数据服务文件的一部分，将创建 `coreos-metadata.target`，该文件将包含服务文件可以使用的必要环境变量。服务文件将指定此目标文件作为依赖关系，systemd
    将处理这个依赖关系。
- en: 'The following is a sample `etcd2.service` file, which specifies `coreos-metadata.service`
    as a dependency. The `/run/metadata/coreos` environment file will contain `COREOS_IPV4_PUBLIC`,
    and this will be generated by `coreos-metadata.service`:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例 `etcd2.service` 文件，其中指定了 `coreos-metadata.service` 作为依赖关系。`/run/metadata/coreos`
    环境文件将包含 `COREOS_IPV4_PUBLIC`，该变量将由 `coreos-metadata.service` 生成：
- en: '`[Unit] Requires=coreos-metadata.service After=coreos-metadata.service  [Service]
    EnvironmentFile=/run/metadata/coreos ExecStart= ExecStart=/usr/bin/etcd2 \     --advertise-client-urls=http://${COREOS_IPV4_PUBLIC}:2379 \
        --initial-advertise-peer-urls=http://${COREOS_IPV4_LOCAL}:2380 \     --listen-client-urls=http://0.0.0.0:2379 \
        --listen-peer-urls=http://${COREOS_IPV4_LOCAL}:2380 \     --initial-cluster=${ETCD_NAME}=http://${COREOS_IPV4_LOCAL}:2380`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Requires=coreos-metadata.service After=coreos-metadata.service  [Service]
    EnvironmentFile=/run/metadata/coreos ExecStart= ExecStart=/usr/bin/etcd2  \     --advertise-client-urls=http://${COREOS_IPV4_PUBLIC}:2379  \
        --initial-advertise-peer-urls=http://${COREOS_IPV4_LOCAL}:2380  \     --listen-client-urls=http://0.0.0.0:2379  \
        --listen-peer-urls=http://${COREOS_IPV4_LOCAL}:2380  \     --initial-cluster=${ETCD_NAME}=http://${COREOS_IPV4_LOCAL}:2380`'
- en: Ignition will be backward-compatible with cloudinit. Ignition has not yet been
    officially released.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Ignition 将与 cloudinit 向后兼容。Ignition 还没有正式发布。
- en: DEX
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: DEX
- en: 'DEX is an open source project started by CoreOS for identity management, including
    authentication and authorization. The following are some properties of DEX:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: DEX 是一个由 CoreOS 启动的开源项目，用于身份管理，包括认证和授权。以下是 DEX 的一些特性：
- en: DEX uses the OpenID connect (OIDC) ([http://openid.net/connect/](http://openid.net/connect/))
    standard, which is built on OAuth 2.0\. OAuth 2.0 is used by Google to sign in
    to their services such as Gmail.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEX 使用 OpenID Connect (OIDC) ([http://openid.net/connect/](http://openid.net/connect/))
    标准，构建在 OAuth 2.0 之上。OAuth 2.0 被 Google 用于登录他们的服务，如 Gmail。
- en: DEX supports multiple identity providers using the Connectors module. Currently,
    DEX supports the local connector using local servers and a OIDC connector such
    as Google.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEX 使用连接器模块支持多个身份提供者。目前，DEX 支持使用本地服务器的本地连接器和像 Google 这样的 OIDC 连接器。
- en: There is a plan to add authorization, user management, and multiple other connectors
    such as LDAP and GitHub.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有计划增加授权、用户管理以及多个其他连接器，如 LDAP 和 GitHub。
- en: DEX is used as an identity provider in the Tectonic project.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEX 在 Tectonic 项目中用作身份提供者。
- en: DEX is still in its early stages and under active development.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: DEX 仍处于初期阶段，正在积极开发中。
- en: Clair
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Clair
- en: 'Clair is an open source project started by CoreOS to detect Container vulnerabilities.
    The following are some properties of Clair:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Clair 是一个由 CoreOS 启动的开源项目，用于检测容器漏洞。以下是 Clair 的一些特点：
- en: Clair scans Container images stored in the Quay Container repository for vulnerabilities
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clair 扫描存储在 Quay 容器仓库中的容器镜像，检测其中的漏洞。
- en: Each Container layer contains information about packages installed in that layer
    and this is provided by the corresponding Linux package manager
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个容器层包含该层中安装的包的信息，这些信息由相应的 Linux 包管理器提供。
- en: Clair analyzes each Container layer by querying the package manager-related
    files and compares them against the vulnerability database available in the particular
    Linux distribution to check whether the particular Container layer is vulnerable
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clair 通过查询与包管理器相关的文件，分析每个容器层，并将其与特定 Linux 发行版中的漏洞数据库进行比较，以检查该容器层是否存在漏洞。
- en: Clair makes an index-directed graph of each Container layer, and this speeds
    up the analysis of a lot of Container images sharing layers
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clair 会为每个容器层创建一个索引导向图，这加速了对共享层的多个容器镜像的分析。
- en: Clair currently supports CentOS, Ubuntu, and Debian Linux distributions
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clair 当前支持 CentOS、Ubuntu 和 Debian Linux 发行版。
- en: Clair is still in its early stages and under active development.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Clair 仍处于初期阶段，正在积极开发中。
- en: The Docker roadmap
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 路线图
- en: Docker has transitioned from providing Container runtime to a Container platform.
    Docker provides both open source solutions as well as commercial products around
    Containers.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 已从提供容器运行时转变为容器平台。Docker 提供了围绕容器的开源解决方案和商业产品。
- en: 'The following diagram shows different Docker products around Core Docker, Security,
    Orchestration, Registry, and Deployment as of November 2015:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了截至 2015 年 11 月，围绕核心 Docker、安全性、编排、注册表和部署的不同 Docker 产品：
- en: '![](img/00210.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00210.jpg)'
- en: The following are some new projects announced recently by Docker.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Docker 最近宣布的一些新项目。
- en: Tutum
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Tutum
- en: Tutum makes it easy to build, deploy, and manage Containerized applications
    and is available as a SaaS application. An application can be a single- or multi-container
    application. Tutum integrates well with the Docker hub.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Tutum 使得构建、部署和管理容器化应用程序变得更加简单，并作为 SaaS 应用提供。一个应用程序可以是单容器应用或多容器应用。Tutum 与 Docker
    Hub 配合得很好。
- en: UCP
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: UCP
- en: UCP is Docker's commercial offering to provide on-premise Container deployment
    solutions. UCP integrates with the Docker trusted registry as well as with enterprise
    services such as LDAP and Role-based access control (RBAC). UCP also integrates
    with all other Docker services such as Networking, Compose, and Swarm. UCP is
    in the beta phase currently.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: UCP 是 Docker 提供的商业解决方案，用于提供本地容器部署解决方案。UCP 与 Docker 信任的注册表以及企业服务（如 LDAP 和基于角色的访问控制
    RBAC）集成。UCP 还与 Docker 的所有其他服务（如网络、Compose 和 Swarm）集成。目前 UCP 正处于测试阶段。
- en: Nautilus
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Nautilus
- en: This project is targeted towards Container vulnerability detection. This is
    similar to the Clair project from CoreOS. Nautilus is still in the very early
    stages.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目针对容器漏洞检测，类似于 CoreOS 的 Clair 项目。Nautilus 仍处于初期阶段。
- en: Microservices infrastructure
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务基础设施
- en: In this section, we will cover an overview of microservice infrastructure components
    and examples of a few solution providers.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述微服务基础设施组件，并举例说明一些解决方案提供商。
- en: Platform choices
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 平台选择
- en: The following are some design decisions/platform choices that customers who
    are developing and deploying microservices need to make. The following examples
    are just a sample set and do not cover all the providers.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些微服务开发和部署过程中，客户需要做出的设计决策/平台选择。以下示例只是一个样本集，并未涵盖所有提供商。
- en: 'IaaS vs PaaS: This choice applies for local data centers as well as for Cloud
    providers. In the earlier section, we covered the comparison between Container
    and PaaS models. Here, the trade-offs are flexibility versus time-to-market.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: IaaS vs PaaS：这个选择适用于本地数据中心以及云服务提供商。在前面的章节中，我们已经比较了容器和PaaS模型。在这里，权衡是灵活性与上市时间。
- en: 'Local data center versus cloud providers: This is mostly a cost versus time
    trade-off.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本地数据中心与云服务提供商：这主要是成本与时间的权衡。
- en: 'Base OS: The choice here is either going with Container-optimized OSes such
    as CoreOS, Rancher, or Atomic or traditional OSes such as Ubuntu or Fedora. For
    pure microservice architecture, Container-optimized OSes are definitely worth
    pursuing.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 基础操作系统：这里的选择是选择容器优化的操作系统，如 CoreOS、Rancher 或 Atomic，或传统的操作系统，如 Ubuntu 或 Fedora。对于纯微服务架构来说，容器优化的操作系统绝对值得追求。
- en: 'VM Orchestration: VMs and Containers have different use cases and will continue
    to live together. There will be scenarios where VMs will be used standalone or
    Containers will run on top of VMs. There are open source solutions such as Openstack
    and commercial solutions from VMWare for VM Orchestration.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: VM编排：VM和容器有不同的用例，并将继续共存。在某些情况下，VM将独立使用，或者容器将运行在VM之上。有一些开源解决方案，如Openstack，以及来自VMWare的商业解决方案，用于VM编排。
- en: 'Container runtime: Choices here are Docker, Rkt, or LXC.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时：这里的选择有 Docker、Rkt 或 LXC。
- en: 'Networking: Container orchestration systems such as Kubernetes typically integrate
    networking. As networking support is provided as plugins, it can be swapped with
    a different implementation if necessary. Some examples of networking plugins include
    Weave, Calico, and Contiv.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 网络：容器编排系统（如Kubernetes）通常集成了网络功能。由于网络支持是作为插件提供的，如果需要，可以用不同的实现进行替换。一些网络插件的例子包括
    Weave、Calico 和 Contiv。
- en: 'Storage: We need to evaluate between dedicated storage versus stateful Containers.
    Choices for stateful Containers are GlusterFS, Ceph, or Flocker.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 存储：我们需要评估专用存储与有状态容器之间的选择。有状态容器的选择包括 GlusterFS、Ceph 或 Flocker。
- en: 'Container Orchestration: Choices here are Kubernetes, Docker Swarm, Mesos,
    and so on.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排：这里的选择包括 Kubernetes、Docker Swarm、Mesos 等。
- en: 'Service discovery and DNS: This can be built manually using building blocks
    mentioned in previous sections, or if we choose a container orchestration system
    such as Kubernetes, it''s already integrated with this.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 服务发现和DNS：可以手动构建，使用前面部分提到的构建模块，或者选择诸如Kubernetes这样的容器编排系统，它已经集成了这一功能。
- en: 'CI and CD: This can be manually built or we can use packaged solutions from
    Codeship, CircleCI, or Shippable.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: CI和CD：可以手动构建，或者使用来自Codeship、CircleCI或Shippable的打包解决方案。
- en: 'Monitoring and logging system: Examples are Sysdig or Logentries. We covered
    more details on Monitoring and logging in [Chapter 10](index_split_219.html#filepos708963),
    CoreOS and Containers - Troubleshooting and Debugging.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和日志系统：例如 Sysdig 或 Logentries。我们在[第10章](index_split_219.html#filepos708963)，CoreOS和容器
    - 故障排除和调试中详细介绍了监控和日志。
- en: Solution providers
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案提供商
- en: 'As we have seen throughout this book, there are many hardware and software
    components that comprise the infrastructure to create and deploy microservices.
    We can think of each component as a LEGO block and there are numerous ways of
    bringing these LEGO blocks together. Customers have the following three choices:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所看到的，有许多硬件和软件组件构成了创建和部署微服务的基础设施。我们可以把每个组件看作是乐高积木，有许多方法将这些乐高积木组合在一起。客户有以下三种选择：
- en: Integrating all the infrastructure components by themselves
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自行集成所有基础设施组件
- en: Going with solution providers who integrate these components and give an opinionated
    architecture
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择集成这些组件并给出倾向性架构的解决方案提供商
- en: Choosing a hybrid solution between the previous two options, where we can choose
    reference architecture and replace a few components based on specific needs
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前两个选项之间选择混合解决方案，我们可以选择参考架构，并根据特定需求替换几个组件
- en: 'The following are some commercial and open source integrated solutions available.
    The list is not extensive and some of these solutions do not integrate all the
    components:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些商业和开源集成解决方案。此列表不全面，其中一些解决方案并未集成所有组件：
- en: Tectonic Enterprise from CoreOS.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自CoreOS的Tectonic Enterprise。
- en: Google Container service
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 容器服务
- en: AWS Container service
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 容器服务
- en: Cisco's Mantl project
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cisco 的 Mantl 项目
- en: Openstack Magnum
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Openstack Magnum
- en: Summary
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered some of the production considerations in deploying
    microservice-based distributed infrastructure, and this includes CoreOS, Docker,
    and the associated ecosystem. Cloud companies such as Google, Amazon, and Facebook
    have used microservices and Container-based technologies for quite a long time
    and they have learned the best practices and pitfalls based on their experience.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们介绍了部署基于微服务的分布式基础设施的一些生产考量，包括CoreOS、Docker及其相关生态系统。谷歌、亚马逊和Facebook等云公司已经使用微服务和基于容器的技术相当长时间，并且他们根据自己的经验总结了最佳实践和坑点。
- en: The issue till now has been the replication of approaches and not having a common
    standard/approach. The trend in the last few years has been that these companies
    as well as many start-ups such as CoreOS and Docker are willing to develop technologies
    and work together in an open manner that helps the entire industry. A big contributor
    to this is open source software development, and many big companies are willing
    to develop software in the open now. Obviously, commercial solutions around open
    source technologies will continue to thrive as the industry still needs to make
    money to survive.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止的问题在于方法的重复以及缺乏共同的标准/方法。近年来的趋势是，这些公司以及许多初创公司（如CoreOS和Docker）愿意开发技术并以开放的方式合作，帮助整个行业。开放源代码软件开发是这一趋势的重要推动力，许多大公司现在愿意以开放的方式开发软件。显然，围绕开源技术的商业解决方案将继续蓬勃发展，因为行业仍然需要通过盈利来生存。
- en: Container technology and microservices are the biggest trends in the software
    industry currently. Customers have many options and this includes both open source
    and commercial solutions. At this point, there is a need to put together different
    technologies/products to create a complete solution for microservices infrastructure.
    As these technologies mature, integrated open solutions with a pluggable architecture
    will win over the long term.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术和微服务是当前软件行业中最大的趋势。客户有很多选择，包括开源和商业解决方案。此时，需要将不同的技术/产品结合起来，创建一个完整的微服务基础设施解决方案。随着这些技术的成熟，具有可插拔架构的集成开源解决方案将在长期中占据优势。
- en: References
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Registrator: [http://gliderlabs.com/registrator/latest/user/quickstart/](http://gliderlabs.com/registrator/latest/user/quickstart/)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Registrator: [http://gliderlabs.com/registrator/latest/user/quickstart/](http://gliderlabs.com/registrator/latest/user/quickstart/)'
- en: 'Ansible reference: [https://docs.ansible.com/](https://docs.ansible.com/)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ansible 参考资料: [https://docs.ansible.com/](https://docs.ansible.com/)'
- en: 'Managing CoreOS with Ansible: [https://coreos.com/blog/managing-coreos-with-ansible/](https://coreos.com/blog/managing-coreos-with-ansible/)
    and [https://github.com/defunctzombie/ansible-coreos-bootstrap](https://github.com/defunctzombie/ansible-coreos-bootstrap)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 Ansible 管理 CoreOS: [https://coreos.com/blog/managing-coreos-with-ansible/](https://coreos.com/blog/managing-coreos-with-ansible/)
    和 [https://github.com/defunctzombie/ansible-coreos-bootstrap](https://github.com/defunctzombie/ansible-coreos-bootstrap)'
- en: 'Ansible Docker module: [http://docs.ansible.com/ansible/docker_module.html](http://docs.ansible.com/ansible/docker_module.html)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ansible Docker 模块: [http://docs.ansible.com/ansible/docker_module.html](http://docs.ansible.com/ansible/docker_module.html)'
- en: 'CoreOS and Docker: [https://developer.rackspace.com/blog/ansible-and-docker/](https://developer.rackspace.com/blog/ansible-and-docker/)
    and [http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html](http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CoreOS 和 Docker: [https://developer.rackspace.com/blog/ansible-and-docker/](https://developer.rackspace.com/blog/ansible-and-docker/)
    和 [http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html](http://opensolitude.com/2015/05/26/building-docker-images-with-ansible.html)'
- en: 'CI pipeline with Docker: [https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf](https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 Docker 的 CI 流水线: [https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf](https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf)'
- en: 'Containers and PaaS: [http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/](http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/)
    and [http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/](http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '容器与PaaS: [http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/](http://cloudtweaks.com/2014/12/paas-vs-docker-heated-debate/)
    和 [http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/](http://thenewstack.io/docker-is-driving-a-new-breed-of-paas/)'
- en: 'Container security with SELinux and CoreOS: [https://coreos.com/blog/container-security-selinux-coreos/](https://coreos.com/blog/container-security-selinux-coreos/)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 SELinux 和 CoreOS 进行容器安全管理: [https://coreos.com/blog/container-security-selinux-coreos/](https://coreos.com/blog/container-security-selinux-coreos/)'
- en: 'CoreOS Ignition: [https://github.com/coreos/ignition](https://github.com/coreos/ignition)
    and [https://coreos.com/ignition/docs/latest/examples.html](https://coreos.com/ignition/docs/latest/examples.html)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CoreOS Ignition: [https://github.com/coreos/ignition](https://github.com/coreos/ignition)
    和 [https://coreos.com/ignition/docs/latest/examples.html](https://coreos.com/ignition/docs/latest/examples.html)'
- en: 'CoreOS DEX: [https://github.com/coreos/dex](https://github.com/coreos/dex),
    [https://coreos.com/blog/announcing-dex/](https://coreos.com/blog/announcing-dex/),
    and [https://www.youtube.com/watch?v=QZgkJQiI_gE](https://www.youtube.com/watch?v=QZgkJQiI_gE)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CoreOS DEX: [https://github.com/coreos/dex](https://github.com/coreos/dex),
    [https://coreos.com/blog/announcing-dex/](https://coreos.com/blog/announcing-dex/)，以及
    [https://www.youtube.com/watch?v=QZgkJQiI_gE](https://www.youtube.com/watch?v=QZgkJQiI_gE)'
- en: 'Clair for Container vulnerability analysis: [https://coreos.com/blog/vulnerability-analysis-for-containers/](https://coreos.com/blog/vulnerability-analysis-for-containers/)
    and [https://github.com/coreos/clair](https://github.com/coreos/clair)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clair 用于容器漏洞分析: [https://coreos.com/blog/vulnerability-analysis-for-containers/](https://coreos.com/blog/vulnerability-analysis-for-containers/)
    和 [https://github.com/coreos/clair](https://github.com/coreos/clair)'
- en: 'Docker Tutum and UCP: [https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/](https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/),
    [https://www.docker.com/tutum](https://www.docker.com/tutum), and [https://www.docker.com/universal-control-plane](https://www.docker.com/universal-control-plane)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Tutum 和 UCP: [https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/](https://blog.docker.com/2015/11/dockercon-eu-2015-docker-universal-control-plane/),
    [https://www.docker.com/tutum](https://www.docker.com/tutum)，以及 [https://www.docker.com/universal-control-plane](https://www.docker.com/universal-control-plane)'
- en: 'Mantl project: [https://github.com/CiscoCloud/microservices-infrastructure](https://github.com/CiscoCloud/microservices-infrastructure)
    and [http://mantl.io/](http://mantl.io/)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mantl 项目: [https://github.com/CiscoCloud/microservices-infrastructure](https://github.com/CiscoCloud/microservices-infrastructure)
    和 [http://mantl.io/](http://mantl.io/)'
- en: Further reading and tutorials
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 深入阅读和教程
- en: 'Service discovery: [http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '服务发现: [http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/](http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/)'
- en: 'Ansible with Docker on Rancher: [http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/](http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 Docker 在 Rancher 上的 Ansible: [http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/](http://rancher.com/using-ansible-with-docker-to-deploy-a-wordpress-service-on-rancher/)'
- en: 'Stateful Containers: [http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/](http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '有状态容器: [http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/](http://techcrunch.com/2015/11/21/i-want-to-run-stateful-containers-too/)'
- en: 'Codeship, Shippable, and CircleCI: [https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci](https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci),
    [https://circleci.com/docs/docker](https://circleci.com/docs/docker), [https://blog.codeship.com/continuous-integration-and-delivery-with-docker/](https://blog.codeship.com/continuous-integration-and-delivery-with-docker/),
    and [http://docs.shippable.com/](http://docs.shippable.com/)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Codeship、Shippable 和 CircleCI: [https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci](https://scotch.io/tutorials/speed-up-your-deployment-workflow-with-codeship-and-parallelci),
    [https://circleci.com/docs/docker](https://circleci.com/docs/docker), [https://blog.codeship.com/continuous-integration-and-delivery-with-docker/](https://blog.codeship.com/continuous-integration-and-delivery-with-docker/)，以及
    [http://docs.shippable.com/](http://docs.shippable.com/)'
- en: 'Comparing CI/CD solutions: [https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker](https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '比较 CI/CD 解决方案: [https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker](https://www.quora.com/What-is-the-difference-between-Bamboo-CircleCI-CIsimple-Ship-io-Codeship-Jenkins-Hudson-Semaphoreapp-Shippable-Solano-CI-TravisCI-and-Wercker)'
- en: 'Containers and PaaS: [https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/](https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/)
    and [https://www.youtube.com/watch?v=YydhEEgOoDg](https://www.youtube.com/watch?v=YydhEEgOoDg)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '容器和 PaaS: [https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/](https://labs.ctl.io/flynn-vs-deis-the-tale-of-two-docker-micro-paas-technologies/)
    和 [https://www.youtube.com/watch?v=YydhEEgOoDg](https://www.youtube.com/watch?v=YydhEEgOoDg)'
- en: 'Ignition presentation: [https://www.youtube.com/watch?v=ly3uwn0HzBI](https://www.youtube.com/watch?v=ly3uwn0HzBI)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ignition 演示：[https://www.youtube.com/watch?v=ly3uwn0HzBI](https://www.youtube.com/watch?v=ly3uwn0HzBI)
- en: 'Jenkins Docker plugin: [https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins Docker 插件：[https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin](https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin)
- en: 'Continuous delivery with Docker and Jenkins: [https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf](https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf)
    and [https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf](https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 和 Jenkins 的持续交付：[https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf](https://www.docker.com/sites/default/files/UseCase/RA_CI%20with%20Docker_08.25.2015.pdf)
    和 [https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf](https://pages.cloudbees.com/rs/083-PKZ-512/images/Docker-Jenkins-Continuous-Delivery.pdf)
