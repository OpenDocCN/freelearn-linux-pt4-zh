<html><head></head><body>
<p id="filepos245712" class="calibre_"><span class="calibre1"><span class="bold">Chapter 4. CoreOS Primary Services – Etcd, Systemd, and Fleet</span></span></p><p class="calibre_8">This chapter will cover the internals of CoreOS' critical services—Etcd, Systemd, and Fleet. For each of the services, we will cover installation, configuration, and their applications. CoreOS ships with Etcd, Systemd, and Fleet by default. They can also be installed as standalone components in any Linux system. The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Etcd—installation, access methods, configuration options, use cases, tuning, cluster management, security, authentication, and debugging</li><li value="2" class="calibre_13">Systemd—unit types, specifiers, templates, and special units</li><li value="3" class="calibre_13">Fleet—installation, access methods, templates, scheduling, HA, and debugging</li><li value="4" class="calibre_13">Service discovery options using Etcd and Fleet</li></ul><div class="mbp_pagebreak" id="calibre_pb_93"/>


<p id="filepos246906" class="calibre_14"><span class="calibre1"><span class="bold">Etcd</span></span></p><p class="calibre_8">Etcd is a distributed key-value<a/> store used by all the machines in the CoreOS cluster to read/write and exchange data. An overview of etcd is provided in <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>. This section will cover the internals of etcd.</p><div class="mbp_pagebreak" id="calibre_pb_94"/>


<p id="filepos247327" class="calibre_9"><span class="calibre3"><span class="bold">Versions</span></span></p><p class="calibre_8">Etcd is under <a/>continuous development, and frequent releases are done to add enhancements as well as fix bugs. The following are some major updates from recent etcd releases:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Version 2.0 is the first stable release and was released in January 2015. Pre-version 2.0 is available as etcd and post-version 2.0 is available as etcd2 in CoreOS nodes.</li><a/><li value="2" class="calibre_13">Version 2.0 added IANA-assigned ports <tt class="calibre2">2379</tt> for client-to-server communication and <tt class="calibre2">2380</tt> for server-to-server communication. Previously, port <tt class="calibre2">4001</tt> was used for client-to-server communication and port <tt class="calibre2">7001</tt> was used for server-to-server communication.</li><li value="3" class="calibre_13">Version 2.1 introduced authentication and metrics collection features and these are in experimental mode.</li><a/><li value="4" class="calibre_13">The latest release as of September 2015 is 2.2.0.</li><li value="5" class="calibre_13">An experimental v3 API (some examples are multikey reads, range reads, and binary keys) is available now as a preview and will be available officially in version 2.3.0 scheduled at the end of October 2015.</li></ul><p class="calibre_8">All examples in this <a/>chapter are based on etcd version 2.1.0 and above.</p><div class="mbp_pagebreak" id="calibre_pb_95"/>


<p id="filepos248957" class="calibre_9"><span class="calibre3"><span class="bold">Installation</span></span></p><p class="calibre_8">CoreOS ships<a/> with etcd. Both the etcd and etcd2 versions are available in the base CoreOS image. The following are the etcd versions available in the CoreOS alpha image 779.0.0:</p><p class="calibre_9"><img src="images/00049.jpg" class="calibre_106"/></p><p class="calibre_8">
</p><p id="filepos249433" class="calibre_9"><span class="calibre3"><span class="bold">Standalone installation</span></span></p><p class="calibre_8">Etcd can also be<a/> installed on any Linux machine. The following is the installation command tried out on Ubuntu 14.04 to install etcd version 2.2:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl -L  https://github.com/coreos/etcd/releases/download/v2.2.0/etcd-v2.2.0-linux-amd64.tar.gz -o etcd-v2.2.0-linux-amd64.tar.gz</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">tar xzvf etcd-v2.2.0-linux-amd64.tar.gz</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following example shows you how to try out etcd in the standalone mode. To start the server run the following command:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd -name etcdtest</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Now, check whether we can connect to the etcd server using some basic commands:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl cluster-health</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00053.jpg" class="calibre_107"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is an example of a simple set and get operation using the curl interface:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl –L –X PUT http://127.0.0.1:2379/v2/keys/message -d value="hello"</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl –L http://127.0.0.1:2379/v2/keys/message</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00055.jpg" class="calibre_108"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_96"/>


<p id="filepos251451" class="calibre_9"><span class="calibre3"><span class="bold">Accessing etcd</span></span></p><p class="calibre_8">Etcd can be accessed using either etcdctl CLI or REST API. This applies to both the standalone etcd as<a/> well as etcd in CoreOS. The following figure shows you the different ways to access etcd:</p><p class="calibre_9"><img src="images/00058.jpg" class="calibre_109"/></p><p class="calibre_8">
</p><p id="filepos251944" class="calibre_9"><span class="calibre3"><span class="bold">REST</span></span></p><p class="calibre_8">The etcd database can be accessed and modified through the REST API. The etcd database can be accessed<a/> either locally or remotely using this approach.</p><p class="calibre_8">The following example shows the <tt class="calibre2">curl</tt> method to access the CoreOS node to get all the keys:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl –L http://localhost:2379/v2/keys/?recursive=true</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00060.jpg" class="calibre_110"/></p><p class="calibre_8">
</p><p class="calibre_8">The following example shows the <tt class="calibre2">curl</tt> method to access the remote CoreOS node to get all the keys:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl –L http://172.17.8.101:2379/v2/keys/?recursive=true</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following <a/>screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00063.jpg" class="calibre_111"/></p><p class="calibre_8">
</p><p id="filepos253342" class="calibre_9"><span class="calibre3"><span class="bold">Etcdctl</span></span></p><p class="calibre_8">Etcdctl is a CLI wrapper on<a/> top of the REST interface. Etcdctl can be used for local or remote access.</p><p class="calibre_8">The following example shows etcdctl method to access the CoreOS node to get all the keys:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl ls / --recursive</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00067.jpg" class="calibre_112"/></p><p class="calibre_8">
</p><p class="calibre_8">The following example shows etcdctl method to access the remote CoreOS node to get all the keys:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl --peers=http://172.17.8.101:2379 ls / --recursive</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00070.jpg" class="calibre_113"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_97"/>


<p id="filepos254669" class="calibre_9"><span class="calibre3"><span class="bold">Etcd configuration</span></span></p><p class="calibre_8">Etcd configuration <a/>parameters can be used to modify the etcd member property or cluster-wide property. Etcd options can be set either in the command line or using environment variables. The command line will override the environment variables. The following are the broad categories and their critical configuration parameters/environment variables:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Member: Name, data-dir, and heartbeat interval</li><li value="2" class="calibre_13">Cluster: Discovery token and initial cluster nodes</li><li value="3" class="calibre_13">Proxy: Proxy on/off and proxy intervals</li><li value="4" class="calibre_13">Security: Certificate and key</li><li value="5" class="calibre_13">Logging: Enable/disable logging and logging levels</li><li value="6" class="calibre_13">Experimental</li></ul><p class="calibre_8">The following is an etcd invocation example, where we use some of the preceding configuration parameters:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd -name infra0 -data-dir infra0  --cacert=~/.etcd-ca/ca.crt -cert-file=/home/smakam14/infra0.crt -key-file=/home/smakam14/infra0.key.insecure  -advertise-client-urls=https://192.168.56.104:2379 -listen-client-urls=https://192.168.56.104:2379</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Etcd environment variables can also be specified in <tt class="calibre2">cloud-config</tt>. The following is a <tt class="calibre2">cloud-config</tt> example to specify etcd environment variables:</p><p class="calibre_8"><tt class="calibre2">etcd2:<br class="calibre4"/>    #generate a new token for each unique cluster from https://discovery.etcd.io/new<br class="calibre4"/>    discovery: https://discovery.etcd.io/d93c8c02eedadddd3cf14828f9bec01c<br class="calibre4"/>    # multi-region and multi-cloud deployments need to use $public_ipv4<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001</tt></p><p class="calibre_8">Etcd2 environment variables from cloud-config are stored in the following directory: <tt class="calibre2">/run/systemd/system/etcd2.service.d</tt>.</p><p class="calibre_8">The etcd2 service needs to be restarted if the environment variables are changed.</p><p class="calibre_8">A complete list <a/>of configuration parameters and environment variables for<a/> etcd can be found at <a href="https://coreos.com/etcd/docs/latest/configuration.html">https://coreos.com/etcd/docs/latest/configuration.html</a>.</p><div class="mbp_pagebreak" id="calibre_pb_98"/>


<p id="filepos257818" class="calibre_9"><span class="calibre3"><span class="bold">Etcd operations</span></span></p><p class="calibre_8">The following are <a/>some examples of major operations that can be done using etcd:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Set, get, and delete operations of a key-value pair</li><li value="2" class="calibre_13">Set a key with timeout where the key expires automatically</li><li value="3" class="calibre_13">Set a key based on the atomic condition check</li><li value="4" class="calibre_13">Hidden keys</li><li value="5" class="calibre_13">Watching and waiting for key changes</li><li value="6" class="calibre_13">Creating in-order keys</li></ul><p class="calibre_8">Using these operations, etcd can be used for a variety of distributed application use cases. The following is an example TTL use case where we check for the liveliness of the Apache service and update service details such as the IP address and port number in etcd, which other applications can use to determine if the service is running or not. If the Apache service dies, the etcd key-value pair will be deleted after 30 seconds in this case:</p><p class="calibre_8"><tt class="calibre2">## Test whether service is accessible and then register useful information like IP address, port<br class="calibre4"/>ExecStart=/bin/bash -c '\<br class="calibre4"/>  while true; do \<br class="calibre4"/>    curl -f ${COREOS_PUBLIC_IPV4}:%i; \<br class="calibre4"/>    if [ $? -eq 0 ]; then \<br class="calibre4"/>      etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \'{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\' --ttl 30; \<br class="calibre4"/>    fi; \<br class="calibre4"/>    sleep 20; \<br class="calibre4"/>  done'</tt></p><p class="calibre_8">We can find statistics about the etcd node as well as the key-related operations.</p><p class="calibre_8">The following output shows the etcd node statistics:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl http://127.0.0.1:2379/v2/stats/self | jq .</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following <a/>screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00072.jpg" class="calibre_114"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the etcd key statistics:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl http://127.0.0.1:2379/v2/stats/store | jq .</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following screenshot is the output of the preceding command:</p><p class="calibre_9"><img src="images/00076.jpg" class="calibre_115"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_99"/>


<p id="filepos260876" class="calibre_9"><span class="calibre3"><span class="bold">Etcd tuning</span></span></p><p class="calibre_8">The following are some etcd parameters that can be tuned to achieve optimum cluster performance based on the operating environment:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Cluster size</span>: A bigger cluster size provides you with better redundancy. The disadvantage with big cluster sizes is that updates can take a long time. In <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>, we saw the failure tolerance limit with different cluster sizes.</li><a/><li value="2" class="calibre_13"><span class="bold">Heartbeat interval</span>: This is the time interval at which the master node sends a heartbeat message to its followers. The default heartbeat interval is 100 ms. It is necessary to choose a heartbeat interval based on the average round-trip time taken for the ping between nodes. If the nodes are geographically distributed, then the round-trip time will be longer. The suggested heartbeat interval is 0.5-1.5 x the average round-trip time. If we choose a small heartbeat interval, the overhead will be a higher number of packets. If we choose a large heartbeat interval, it will take a longer time to detect leader failure. The heartbeat interval can be set using the <tt class="calibre2">heartbeat-interval</tt> parameter in the etcd command line or the <tt class="calibre2">ETCD_HEARTBEAT_INTERVAL</tt> environment variable.</li><a/><li value="3" class="calibre_13"><span class="bold">Election timeout</span>: When the follower nodes fail to get a heartbeat message for the election timeout value, they become the leader node. The default election timeout is 1,000 ms. The suggested value for election timeout is 10 times the heartbeat interval. Keeping the election timeout too low can cause false leader election. The election timeout can be set using the <tt class="calibre2">election-timeout</tt> parameter in the etcd command line or the <tt class="calibre2">ETCD_ELECTION_TIMEOUT</tt> environment variable.</li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_100"/>


<p id="filepos262992" class="calibre_14"><span class="calibre3"><span class="bold">Etcd proxy</span></span></p><p class="calibre_8">An etcd proxy is used when <a/>worker nodes want to use the master node or master cluster to provide etcd service. In this case, all etcd requests from the worker node are proxied to the master node and the master node replies to the worker node.</p><p class="calibre_8">Let's say that we have a working three-node master cluster as follows:</p><p class="calibre_9"><img src="images/00080.jpg" class="calibre_116"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>example shows the <tt class="calibre2">cloud-config</tt> for the fourth node that is a worker node and acting as a proxy. Here, the master cluster members are mentioned statically:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    proxy: on<br class="calibre4"/>    listen-client-urls: http://localhost:2379<br class="calibre4"/>    initial-cluster: etcdserver=http://172.17.8.101:2380, http://172.17.8.102:2380, http://172.17.8.103:2380<br class="calibre4"/>  fleet:<br class="calibre4"/>    etcd_servers: "http://localhost:2379"<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">In the preceding <span class="italic">Etcd configuration</span> section, we have turned on the proxy and pointed to the <tt class="calibre2">etcd_server</tt> cluster. The fourth node needs to be started with the preceding <tt class="calibre2">cloud-config</tt>.</p><p class="calibre_8">The following example shows the <tt class="calibre2">cloud-config</tt> for the fourth node that is acting as a proxy and using a discovery token. We need to use the same discovery token as we did for the three-node cluster:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    proxy: on<br class="calibre4"/>    # use the same discovery token as for master, these nodes will proxy to master<br class="calibre4"/>    discovery: &lt;your token&gt;<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>  fleet:<br class="calibre4"/>    etcd_servers: "http://localhost:2379"<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">The following is the<a/> etcd member output in the new node. As we can see, the etcd cluster is composed of only three nodes and the new node is proxying to the master etcd cluster:</p><p class="calibre_9"><img src="images/00085.jpg" class="calibre_117"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is the Fleet machine's output in the new node. As we can see, there are four nodes and this includes the fourth worker node and the three-node etcd cluster:</p><p class="calibre_9"><img src="images/00482.jpg" class="calibre_118"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_101"/>


<p id="filepos266506" class="calibre_9"><span class="calibre3"><span class="bold">Adding and removing nodes from a cluster</span></span></p><p class="calibre_8">There will be scenarios<a/> where we need to add and remove nodes from <a/>a working etcd cluster. This<a/> section illustrates how to add and remove nodes in a working<a/> etcd cluster.</p><p class="calibre_8">Let's say that we have a three-node working cluster and we want to add a fourth node to the cluster. The following command can be executed in one of the three working nodes to add the fourth node <a/>detail:</p><p class="calibre_9"><img src="images/00093.jpg" class="calibre_119"/></p><p class="calibre_8">
</p><p class="calibre_8">The<a/> following<a/>
<tt class="calibre2">cloud-config</tt> can be used to start the new<a/> fourth node:</p><p class="calibre_8"><tt class="calibre2">#cloud-config<br class="calibre4"/>coreos:<br class="calibre4"/>  etcd2:<br class="calibre4"/>    name: core-04<br class="calibre4"/>    initial_cluster: "core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380,core-04=http://172.17.8.104:2380"<br class="calibre4"/>    initial_cluster_state: existing<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    # legacy ports can be omitted if your application doesn't depend on them<br class="calibre4"/>    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/>  fleet:<br class="calibre4"/>    public-ip: $public_ipv4<br class="calibre4"/>  units:<br class="calibre4"/>    # Note: this requires a release that contains etcd2<br class="calibre4"/>    - name: etcd2.service<br class="calibre4"/>      command: start<br class="calibre4"/>    - name: fleet.service<br class="calibre4"/>      command: start</tt></p><p class="calibre_8">From the following output, we can see that the new member has been successfully added:</p><p class="calibre_9"><img src="images/00095.jpg" class="calibre_120"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>command can be used to remove the fourth number<a/> that we added before:</p><p class="calibre_9"><img src="images/00138.jpg" class="calibre_121"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's check the member list<a/> and cluster health now. We can see that the three nodes are part of the cluster and that the fourth node has been removed:</p><p class="calibre_9"><img src="images/00103.jpg" class="calibre_122"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_102"/>


<p id="filepos269555" class="calibre_9"><span class="calibre3"><span class="bold">Node migration and backup</span></span></p><p class="calibre_8">Node migration is <a/>necessary to handle failure of the node and cluster and also<a/> to replicate the cluster to a different location.</p><p class="calibre_8">To take a backup of the etcd database, we can perform the following:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo etcdctl backup --data-dir=/var/lib/etcd2 --backup-dir=/tmp/etcd2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This approach allows us to reuse the backed-up etcd data in another cluster. In this approach, <tt class="calibre2">nodeid</tt> and <tt class="calibre2">clusterid</tt> are overwritten in the backup directory to prevent unintentional addition of a new node to the old cluster.</p><p class="calibre_8">To preserve the node<a/> ID and cluster ID, we have to manually make a copy, and the copy can be used to restart the service.</p><p class="calibre_8">The following <a/>are the steps to move the etcd2 data directory:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Stop the service:<p class="calibre_"><img src="images/00106.jpg" class="calibre_123"/></p><p class="calibre_8">
</p></li><li value="2" class="calibre_13">Make a copy of the <tt class="calibre2">/var/lib/etcd2</tt> etcd data directory in <tt class="calibre2">/tmp/etcd2_backup</tt>:<p class="calibre_"><img src="images/00196.jpg" class="calibre_124"/></p><p class="calibre_8">
</p></li><li value="3" class="calibre_13">Start etcd2 manually using the new data directory, <tt class="calibre2">/tmp/etcd2_backup</tt>:<p class="calibre_"><img src="images/00115.jpg" class="calibre_125"/></p><p class="calibre_8">
</p></li></ol><p class="calibre_8">There are two approaches to handle the migration:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Add a new member and remove the old member. We can use <tt class="calibre2">etcdctl member add</tt> and <tt class="calibre2">etcdctl member remove</tt>.</li><li value="2" class="calibre_13">Make a copy of the etcd database, move it to the new node, and update it.</li></ul><p class="calibre_8">With the first approach, the new member has a different identity. With the second approach, we can have the new node retain the same old identity. With the first approach, there is no need to stop the etcd service, while we need to stop the etcd service before taking the backup in the second approach.</p><div class="mbp_pagebreak" id="calibre_pb_103"/>


<p id="filepos272411" class="calibre_9"><span class="calibre3"><span class="bold">Etcd security</span></span></p><p class="calibre_8">A secure etcd is needed to<a/> ensure that the client-to-server communication and server-to-server communication are secure. The following figure shows you the different components involved in providing etcd security. Certificate authority is used to <a/>provide and verify certificates for the etcd client-to-server and server-to-server communication:</p><p class="calibre_9"><img src="images/00117.jpg" class="calibre_126"/></p><p class="calibre_8">
</p><p id="filepos273062" class="calibre_9"><span class="calibre3"><span class="bold">Certificate authority – etcd-ca</span></span></p><p class="calibre_8">Certificate authority is <a/>a trusted source that issues certificates<a/> to a trusted server. Other than using standard <a/>
<span class="bold">certificate authorities</span> (<span class="bold">CA</span>), etcd allows for a custom CA. Etcd-ca (<a href="https://github.com/coreos/etcd-ca">https://github.com/coreos/etcd-ca</a>) is a <a/>GO application that can be used as a CA for testing purposes. Recently, etcd <a/>has migrated to CFSSL (<a href="https://github.com/cloudflare/cfssl">https://github.com/cloudflare/cfssl</a>) as the official tool for certificates.</p><p id="filepos273759" class="calibre_9"><span class="calibre3"><span class="bold">Installing etcd-ca</span></span></p><p class="calibre_8">I installed etcd-ca<a/> in my Linux VM running Ubuntu 14.04 using the following steps:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">git clone https://github.com/coreos/etcd-ca</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">cd etcd-ca</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">./build</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: The GO application needs to be installed before the etcd-ca installation.</p><p class="calibre_8">Following three steps are needed to setup etcd-ca:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Creating a CA using etcd-ca.</li><li value="2" class="calibre_13">Creating server keys.</li><li value="3" class="calibre_13">Creating client keys.</li></ol><p class="calibre_8">The following command, <tt class="calibre2">etcd-ca init</tt>, is used to create a CA. This is a one-time procedure. The following screenshot shows you the output when creating a CA:</p><p class="calibre_9"><img src="images/00317.jpg" class="calibre_127"/></p><p class="calibre_8">
</p><p class="calibre_8">The following commands can be used to create a server certificate:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd-ca new-cert -ip  172.17.8.101 core-01</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca sign core-01</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca chain core-01</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca export --insecure core-01 | tar xvf –</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">In the preceding command, <tt class="calibre2">172.17.8.101</tt> is the CoreOS node IP and <tt class="calibre2">core-01</tt> is the node name. These steps will create <tt class="calibre2">core-01.crt</tt> and <tt class="calibre2">core-01.key.insecure</tt>.</p><p class="calibre_8">The following commands can be used to create a client certificate:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd-ca new-cert -ip 192.168.56.104 client</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca sign client</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca chain client</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">etcd-ca export --insecure client | tar xvf -</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">In the preceding command, <tt class="calibre2">192.168.56.104</tt> is the client node IP. These steps will create <tt class="calibre2">client.crt</tt> and<a/>
<tt class="calibre2">client.key.insecure</tt>.</p><p id="filepos276467" class="calibre_9"><span class="calibre3"><span class="bold">Etcd secure client-to-server communication using a server certificate</span></span></p><p class="calibre_8">A server certificate is<a/> used by the client to ensure the server's identity. The following command starts the etcd server using a server certificate and the key that was generated in the previous section:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd2 -name core-01 --cert-file=/home/core/core-01.crt --key-file=/home/core/core-01.key  --advertise-client-urls=https://172.17.8.101:2379 --listen-client-urls=https://172.17.8.101:2379</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is an example to set a key and retrieve it using a secure mechanism:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl --cacert /home/smakam14/.etcd-ca/ca.crt https://172.17.8.101:2379/v2/keys/foo</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following example uses etcdctl to do the key retrieval:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --peers https://172.17.8.101:2379 get /foo</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos277853" class="calibre_9"><span class="calibre3"><span class="bold">Etcd secure client-to-server communication using server certificate and client certificate</span></span></p><p class="calibre_8">In the previous <a/>example, only the server had a certificate. In this example, we will generate a client certificate so that the server can verify the client's identity. The following command starts the etcd server using a server certificate and key and enabling client authentication. The server certificate and keys are the same as generated in the previous section:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcd2 -name core-01 --data-dir=core-01 -client-cert-auth -trusted-ca-file=/home/core/ca.crt -cert-file=/home/core/key.crt  -key-file=/home/core/key.key -advertise-client-urls https://172.17.8.101:2379 -listen-client-urls https://172.17.8.101:2379</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is an example to set a key and retrieve it using a secure client and server mechanism. The client certificate and key are the same as generated in the previous section:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure -L https://172.17.8.101:2379/v2/keys/foo -XPUT -d value=bar -v</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl --cacert /home/smakam14/.etcd-ca/ca.crt --cert /home/smakam14/client.crt --key /home/smakam14/client.key.insecure https://172.17.8.101:2379/v2/keys/foo</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The<a/> following example uses etcdctl to do the key retrieval:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">etcdctl --ca-file /home/smakam14/.etcd-ca/ca.crt --cert-file /home/smakam14/client.crt --key-file /home/smakam14/client.key.insecure --peers https://172.17.8.101:2379 get /foo</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos279843" class="calibre_9"><span class="calibre3"><span class="bold">A secure cloud-config</span></span></p><p class="calibre_8">The following<a/> is a sample <tt class="calibre2">cloud-config</tt> that sets up the etcd security environment variables as well as the necessary certificate and keys:</p><p class="calibre_8"><tt class="calibre2">cloud-config<br class="calibre4"/>write_files:<br class="calibre4"/><br class="calibre4"/> - path: /run/systemd/system/etcd2.service.d/30-configuration.conf<br class="calibre4"/>   permissions: 0644<br class="calibre4"/>   content: |<br class="calibre4"/>     [Service]<br class="calibre4"/>     Environment=ETCD_NAME=core-01<br class="calibre4"/>     Environment=ETCD_VERBOSE=1<br class="calibre4"/>     # Encryption<br class="calibre4"/>     Environment=ETCD_CLIENT_CERT_AUTH=1<br class="calibre4"/>     Environment=ETCD_TRUSTED_CA_FILE=/home/core/ca.crt<br class="calibre4"/>     Environment=ETCD_CERT_FILE=/home/core/server.crt<br class="calibre4"/>     Environment=ETCD_KEY_FILE=/home/core/server.key<br class="calibre4"/>    <br class="calibre4"/> - path: /home/core/ca.crt<br class="calibre4"/>   permissions: 0644<br class="calibre4"/>   content: |<br class="calibre4"/>     -----BEGIN CERTIFICATE-----<br class="calibre4"/>      -----END CERTIFICATE-----<br class="calibre4"/>   <br class="calibre4"/> - path: /home/core/server.crt<br class="calibre4"/>   permissions: 0644<br class="calibre4"/>   content: |<br class="calibre4"/>     -----BEGIN CERTIFICATE-----<br class="calibre4"/>     -----END CERTIFICATE-----<br class="calibre4"/><br class="calibre4"/> - path: /home/core/server.key<br class="calibre4"/>   permissions: 0644<br class="calibre4"/>   content: |<br class="calibre4"/>     -----BEGIN RSA PRIVATE KEY-----<br class="calibre4"/>     -----END RSA PRIVATE KEY-----<br class="calibre4"/><br class="calibre4"/>coreos:<br class="calibre4"/> etcd2:<br class="calibre4"/>    # Static cluster<br class="calibre4"/>    initial-cluster-token: etcd-cluster-1<br class="calibre4"/>    initial-cluster: core-01=http://$private_ipv4:2380<br class="calibre4"/>    initial-cluster-state: new<br class="calibre4"/>    advertise-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    initial-advertise-peer-urls: http://$private_ipv4:2380<br class="calibre4"/>    # listen on both the official ports and the legacy ports<br class="calibre4"/>    # legacy ports can be omitted if your application doesn't depend on them<br class="calibre4"/>    listen-client-urls: http://$public_ipv4:2379<br class="calibre4"/>    listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001<br class="calibre4"/> units:<br class="calibre4"/>   - name: etcd2.service<br class="calibre4"/>     command: start</tt></p><div class="mbp_pagebreak" id="calibre_pb_104"/>


<p id="filepos282243" class="calibre_9"><span class="calibre3"><span class="bold">Authentication</span></span></p><p class="calibre_8">Before the introduction of the authentication feature, there were no restrictions on access to the etcd<a/> database. The authentication feature was introduced as an experimental feature in etcd 2.1.0 and allows access to a specific set of keys based on the <a/>username and password.</p><p class="calibre_8">There are two entities associated with authentication:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Users</span>: Users can be created with a username and password. Before enabling the authentication feature, a root user needs to be created. The root user has substantially more privileges/permissions to add users and roles and assign role permissions.</li><a/><li value="2" class="calibre_13"><span class="bold">Roles</span>: Roles can be used to restrict access to a specific key or directory that holds multiple keys. Roles are assigned to users, and manipulations of the keys can be done based on the username.</li><a/></ul><p class="calibre_8">To get started with authentication, we need to first create a root user and then enable authentication.</p><p class="calibre_8">Create a root user first, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00424.jpg" class="calibre_128"/></p><p class="calibre_8">
</p><p class="calibre_8">Enable authentication as follows:</p><p class="calibre_9"><img src="images/00428.jpg" class="calibre_129"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>example illustrates the etcd authentication.</p><p class="calibre_8">Let's create a <a/>sample keyset, user, and role:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create <tt class="calibre2">/dir1/key1</tt> and <tt class="calibre2">/dir2/key2</tt> keys.</li><li value="2" class="calibre_13">Create a <tt class="calibre2">role_dir1</tt> role that has access to <tt class="calibre2">/dir1/*</tt> only.</li><li value="3" class="calibre_13">Create a <tt class="calibre2">role_dir2</tt> role that has access to <tt class="calibre2">/dir2/*</tt> only.</li><li value="4" class="calibre_13">Create <tt class="calibre2">user1</tt> and grant the <tt class="calibre2">role_dir1</tt> role.</li><li value="5" class="calibre_13">Create <tt class="calibre2">user2</tt> and grant the <tt class="calibre2">role_dir2</tt> role.</li></ol><p class="calibre_8">At this point, <tt class="calibre2">user1</tt> will be able to access <tt class="calibre2">/dir1/*</tt> only and <tt class="calibre2">user2</tt> will be able to access <tt class="calibre2">/dir2/*</tt> only.</p><p class="calibre_8">The following is a breakdown of the steps:</p><p class="calibre_8">Create <tt class="calibre2">/dir1/key1</tt> and <tt class="calibre2">/dir2/key2</tt> keys:</p><p class="calibre_9"><img src="images/00432.jpg" class="calibre_130"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00435.jpg" class="calibre_131"/></p><p class="calibre_8">
</p><p class="calibre_8">Create<a/> a <tt class="calibre2">role_dir1</tt> role that has access to <tt class="calibre2">/dir1/*</tt> only:</p><p class="calibre_9"><img src="images/00438.jpg" class="calibre_132"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00443.jpg" class="calibre_33"/></p><p class="calibre_8">
</p><p class="calibre_8">Create a <tt class="calibre2">role_dir2</tt> role that has<a/> access to <tt class="calibre2">/dir2/*</tt> only:</p><p class="calibre_9"><img src="images/00446.jpg" class="calibre_133"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00450.jpg" class="calibre_134"/></p><p class="calibre_8">
</p><p class="calibre_8">Create <tt class="calibre2">user1</tt> and grant the <tt class="calibre2">role_dir1</tt> role:</p><p class="calibre_9"><img src="images/00453.jpg" class="calibre_135"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00456.jpg" class="calibre_136"/></p><p class="calibre_8">
</p><p class="calibre_8">Create <tt class="calibre2">user2</tt> and grant the <tt class="calibre2">role_dir2</tt> role:</p><p class="calibre_9"><img src="images/00459.jpg" class="calibre_137"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00463.jpg" class="calibre_138"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can <a/>verify that <tt class="calibre2">user1</tt> has access only to <tt class="calibre2">/dir1/key1</tt>. As shown in the following screenshot, <tt class="calibre2">user1</tt> is not able to access <tt class="calibre2">/dir2/key1</tt>:</p><p class="calibre_9"><img src="images/00466.jpg" class="calibre_139"/></p><p class="calibre_8">
</p><p class="calibre_8">Similarly, <tt class="calibre2">user2</tt> has<a/> access only to <tt class="calibre2">/dir2/key1</tt>:</p><p class="calibre_9"><img src="images/00471.jpg" class="calibre_140"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_105"/>


<p id="filepos288069" class="calibre_9"><span class="calibre3"><span class="bold">Etcd debugging</span></span></p><p class="calibre_8">Etcd log files can be <a/>checked using the following command:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Journalctl –u etcd2.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Default logging is set to <tt class="calibre2">INFO</tt>. For more elaborate logging, we can set <tt class="calibre2">ETCD_DEBUG=1</tt> in the environment file or use the <tt class="calibre2">-debug</tt> command-line option.</p><p class="calibre_8">Sometimes, it's useful to<a/> check the curl command associated with the etcdctl CLI command. This can be achieved using the <tt class="calibre2">--debug</tt> option. The following is an example:</p><p class="calibre_9"><img src="images/00475.jpg" class="calibre_39"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_106"/>


<p id="filepos288998" class="calibre_"><span class="calibre1"><span class="bold">Systemd</span></span></p><p class="calibre_8">An overview of<a/> Systemd was provided in <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>. Systemd is the init system used by CoreOS and is always on by default. In this section, we will walk through some of the Systemd internals.</p><div class="mbp_pagebreak" id="calibre_pb_107"/>


<p id="filepos289401" class="calibre_9"><span class="calibre3"><span class="bold">Unit types</span></span></p><p class="calibre_8">Units describe a particular task <a/>along with its dependencies and the execution order. Some units are started on the CoreOS system by default. CoreOS users can also start their own units. System-started units are at <tt class="calibre2">/usr/lib64/systemd/system</tt> and user-started units are at <tt class="calibre2">/etc/systemd/system</tt>.</p><p class="calibre_8">The following are some of the common unit types:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Service unit</span>: This is used to start a particular daemon or process. Examples are <tt class="calibre2">sshd.service</tt> and <tt class="calibre2">docker.service</tt>. The <tt class="calibre2">sshd.service</tt> unit starts the SSH service, and <tt class="calibre2">docker.service</tt> starts the docker daemon.</li><a/><li value="2" class="calibre_13"><span class="bold">Socket unit</span>: This is used for local IPC or network communication. Examples are <tt class="calibre2">systemd-journald.socket</tt> and <tt class="calibre2">docker.socket</tt>. There is a corresponding service associated with a socket that manages the socket. For example, <tt class="calibre2">docker.service</tt> manages <tt class="calibre2">docker.socket</tt>. In <tt class="calibre2">docker.service</tt>, <tt class="calibre2">docker.socket</tt> is mentioned as a dependency. <tt class="calibre2">Docker.socket</tt> provides remote connectivity to the docker engine.</li><a/><li value="3" class="calibre_13"><span class="bold">Target unit</span>: This is used mainly to group related units so that they can be started together. All user-created services are in <tt class="calibre2">multi-user.target</tt>.</li><a/><li value="4" class="calibre_13"><span class="bold">Mount unit</span>: This is used to mount disks to the filesystem. Examples are <tt class="calibre2">tmp.mount</tt> and <tt class="calibre2">usr-share-oem.mount</tt>. The following is a relevant section of <tt class="calibre2">usr-share-oem.mount</tt> that mounts <tt class="calibre2">/usr/share/oem</tt>:<p class="calibre_"><img src="images/00479.jpg" class="calibre_141"/></p><p class="calibre_8">
</p></li><a/><li value="5" class="calibre_13"><span class="bold">Timer unit</span>: These are units that are started periodically based on the interval specified. Examples are <tt class="calibre2">update-engine-stub.timer</tt> and <tt class="calibre2">logrotate.timer</tt>. The following is a relevant section of <tt class="calibre2">update-engine-stub.timer</tt>, where <tt class="calibre2">update-engine-stub.service</tt> is invoked every <tt class="calibre2">41 minutes</tt> to check for CoreOS updates:<p class="calibre_"><img src="images/00483.jpg" class="calibre_142"/></p><p class="calibre_8">
</p></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_108"/>


<p id="filepos292123" class="calibre_14"><span class="calibre3"><span class="bold">Unit specifiers</span></span></p><p class="calibre_8">When writing systemd <a/>units, it is useful to access system environment variables such as hostname, username, IP address, and so on so that we can avoid hardcoding<a/> and use the same systemd unit across systems. For this, systemd provides you with unit specifiers, which are shortcuts to get to the system environment.</p><p class="calibre_8">The following are some common unit specifiers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">%H</tt>: Hostname</li><li value="2" class="calibre_13"><tt class="calibre2">%m</tt>: Machine ID</li><li value="3" class="calibre_13"><tt class="calibre2">%u</tt>: Username</li></ul><p class="calibre_8">A complete list of <a/>unit specifiers is specified at <a href="http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers">http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers</a>.</p><p class="calibre_8">The following<a/> service example illustrates the usage of unit specifiers. In this example, we are setting the key-value pair associated with different specifiers in etcd in ExecStartPre. In ExecStartPost, we are getting the key-value and then cleaning up in the end:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=My Service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>KillMode=none<br class="calibre4"/>ExecStartPre=/usr/bin/etcdctl set hostname %H ; /usr/bin/etcdctl set machinename %m ; /usr/bin/etcdctl set bootid %b ; /usr/bin/etcdctl set unitname %n ; /usr/bin/etcdctl set username %u<br class="calibre4"/>ExecStart=/bin/echo hello, set done, will echo and remove<br class="calibre4"/>ExecStartPost=/usr/bin/etcdctl get hostname ; /usr/bin/etcdctl get machinename ; /usr/bin/etcdctl get bootid ; /usr/bin/etcdctl get unitname ; /usr/bin/etcdctl get username ;<br class="calibre4"/>ExecStartPost=/usr/bin/etcdctl rm hostname ; /usr/bin/etcdctl rm machinename ; /usr/bin/etcdctl rm bootid ; /usr/bin/etcdctl rm unitname ; /usr/bin/etcdctl rm username ;<br class="calibre4"/><br class="calibre4"/>[Install]<br class="calibre4"/>WantedBy=multi-user.target</tt></p><p class="calibre_8">To execute<a/> this service, it is necessary to execute all the following operations with sudo:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create the <tt class="calibre2">unitspec.service</tt> file in <tt class="calibre2">/etc/systemd/system</tt> with the preceding content.</li><li value="2" class="calibre_13">Enable the service with <tt class="calibre2">systemctl enable unitspec.service</tt>.</li><li value="3" class="calibre_13">Start the service with <tt class="calibre2">systemctl start unitspec.service</tt>.</li><li value="4" class="calibre_13">If we change the service after this, we need to execute command <tt class="calibre2">systemctl daemon-reload</tt> before starting the service.</li></ol><p class="calibre_8">The following are the <tt class="calibre2">journalctl</tt> logs associated with the service where we can see the key being set and <a/>retrieved and the corresponding unit specifier value:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">journalctl –u unitspec.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00487.jpg" class="calibre_143"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_109"/>


<p id="filepos295761" class="calibre_9"><span class="calibre3"><span class="bold">Unit templates</span></span></p><p class="calibre_8">Systemd units can be created as a template, and the same template unit can be used to instantiate multiple units<a/> based on the invocation of templates.</p><p class="calibre_8">Templates are <a/>created as <tt class="calibre2">unitname@.service</tt>. The invocation of templates can be done using <tt class="calibre2">unitname@instanceid.service</tt>. In the unit file, the <tt class="calibre2">unit name</tt> can be accessed with <tt class="calibre2">%p</tt> and <tt class="calibre2">instanceid</tt> can be accessed using <tt class="calibre2">%i</tt>.</p><p class="calibre_8">The following is an example template file, <tt class="calibre2">unitspec@.service</tt>:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=My Service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>ExecStartPre=/usr/bin/etcdctl set instance%i %i ; /usr/bin/etcdctl set prefix %p<br class="calibre4"/>ExecStart=/bin/echo Demonstrate systemd template<br class="calibre4"/><br class="calibre4"/>[Install]<br class="calibre4"/>WantedBy=multi-user.target</tt></p><p class="calibre_8">To execute this service, it is necessary to execute all the following operations with sudo:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create the <tt class="calibre2">unitspec@.service</tt> file in <tt class="calibre2">/etc/systemd/system</tt>.</li><li value="2" class="calibre_13">Enable the service with <tt class="calibre2">systemctl enable unitspec@.service</tt>.</li><li value="3" class="calibre_13">Start multiple services with <tt class="calibre2">systemctl start unitspec@1.service</tt> and <tt class="calibre2">systemctl start unitspec@2.service</tt>.</li></ol><p class="calibre_8">If we look at the<a/> etcd content, we can see that the instance value gets updated based on the <tt class="calibre2">%i</tt> argument supplied in the unit name and creates the <tt class="calibre2">instance1</tt> and <tt class="calibre2">instance2</tt> keys:</p><p class="calibre_9"><img src="images/00490.jpg" class="calibre_144"/></p><p class="calibre_8">
</p><p class="calibre_8">The following example gives a more practical example of instantiated units. It uses a template<a/> nginx service, <tt class="calibre2">nginx@.service</tt>, where the port number of the web service is passed dynamically:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=Apache web server service<br class="calibre4"/>After=etcd.service<br class="calibre4"/>After=docker.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/>Restart=always<br class="calibre4"/>EnvironmentFile=/etc/environment<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill nginx%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm nginx%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull nginx<br class="calibre4"/>ExecStart=/usr/bin/docker run --name nginx%i -p ${COREOS_PUBLIC_IPV4}:%i:80 nginx<br class="calibre4"/>ExecStop=/usr/bin/docker stop nginx%i<br class="calibre4"/><br class="calibre4"/>[Install]<br class="calibre4"/>WantedBy=multi-user.target</tt></p><p class="calibre_8">There are two <a/>service options used in the preceding code:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">Timeoutstartsec</tt>: This specifies the time taken to start the service, and if the service is not started by this time, it gets killed. The <tt class="calibre2">none</tt> parameter disables this option and is useful when downloading big containers.</li><li value="2" class="calibre_13"><tt class="calibre2">Restart</tt>: This controls the restartability of the service. Here we have specified <tt class="calibre2">always</tt> to restart the service in case there is a failure associated with this service.</li></ul><p class="calibre_8">Let's create two instances of this service using the following commands:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo systemctl enable nginx@.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Sudo systemctl start nginx@8080.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Sudo systemctl start nginx@8081.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">This creates two docker containers with nginx service; the first one exposing port <tt class="calibre2">8080</tt> and the second one exposing port <tt class="calibre2">8081</tt>.</p><p class="calibre_8">Let's look at <tt class="calibre2">docker ps</tt>:</p><p class="calibre_9"><img src="images/00494.jpg" class="calibre_145"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the <a/>status of the two units. As we can see in the following screenshot, the units are in an active (running) state:</p><p class="calibre_9"><img src="images/00497.jpg" class="calibre_146"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00258.jpg" class="calibre_104"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_110"/>


<p id="filepos300814" class="calibre_9"><span class="calibre3"><span class="bold">Drop-in units</span></span></p><p class="calibre_8">Drop-in units are <a/>useful to change system unit properties at runtime. There are four <a/>ways of creating drop-in units.</p><p id="filepos301076" class="calibre_9"><span class="calibre3"><span class="bold">Default cloud-config drop-in units</span></span></p><p class="calibre_8">Parameters<a/> specified in the <tt class="calibre2">cloud-config</tt> user data will automatically be configured as drop-in units. For example, let's look at the <tt class="calibre2">etcd2.service cloud-config</tt>:</p><p class="calibre_9"><img src="images/00260.jpg" class="calibre_147"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the <tt class="calibre2">etcd2.service</tt> status:</p><p class="calibre_9"><img src="images/00262.jpg" class="calibre_148"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see in the preceding output, the default drop-in unit is <tt class="calibre2">20-cloudinit.conf</tt>.</p><p class="calibre_8"><tt class="calibre2">20-cloudinit.conf</tt> will <a/>contain the parameters specified in <tt class="calibre2">etcd2 cloud-config</tt> as environment variables, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00265.jpg" class="calibre_148"/></p><p class="calibre_8">
</p><p id="filepos302343" class="calibre_9"><span class="calibre3"><span class="bold">Cloud-config custom drop-in units</span></span></p><p class="calibre_8">We can specify the drop-in<a/> unit as part of the <tt class="calibre2">cloud-config</tt>. The following is an example of the <tt class="calibre2">fleet.service</tt> drop-in unit, where we change the default <tt class="calibre2">Restart</tt> parameter from <tt class="calibre2">Always</tt> to <tt class="calibre2">No</tt>:</p><p class="calibre_9"><img src="images/00267.jpg" class="calibre_149"/></p><p class="calibre_8">
</p><p class="calibre_8">When we use this <tt class="calibre2">cloud-config</tt>, the <tt class="calibre2">norestart.conf</tt> drop-in file gets automatically created as can be seen from the <tt class="calibre2">fleet.service</tt> status:</p><p class="calibre_9"><img src="images/00269.jpg" class="calibre_150"/></p><p class="calibre_8">
</p><p class="calibre_8">This <a/>configuration change will keep <tt class="calibre2">fleet.service</tt> non-restartable.</p><p id="filepos303392" class="calibre_9"><span class="calibre3"><span class="bold">Runtime drop-in unit – specific parameters</span></span></p><p class="calibre_8">We can change<a/> specific properties of the service using the drop-in configuration file. The following is the service section of <tt class="calibre2">fleet.service</tt>, which shows the default parameters:</p><p class="calibre_9"><img src="images/00272.jpg" class="calibre_151"/></p><p class="calibre_8">
</p><p class="calibre_8">This specifies that the service needs to be started in 10 seconds in case the service dies because of some error. Let's check whether the restart works by killing the Fleet service.</p><p class="calibre_8">We can kill the service as follows:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo kill -9 &lt;fleet pid&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is a log showing the Fleet service restarting in 10 seconds, which is due to <tt class="calibre2">Restartsec</tt> specified in the service configuration:</p><p class="calibre_9"><img src="images/00274.jpg" class="calibre_152"/></p><p class="calibre_8">
</p><p class="calibre_8">To prove the runtime drop-in configuration change, let's create a configuration file where we disable the<a/> restart for the Fleet service.</p><p class="calibre_8">Create <tt class="calibre2">norestart.conf</tt> under <tt class="calibre2">/etc/systemd.system/system/fleet.service.d</tt>:</p><p class="calibre_9"><img src="images/00276.jpg" class="calibre_153"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, let's restart the systemd configuration:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo systemd daemon-reload</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's check the status of <tt class="calibre2">fleet.service</tt> now:</p><p class="calibre_9"><img src="images/00279.jpg" class="calibre_154"/></p><p class="calibre_8">
</p><p class="calibre_8">We can see that other than <tt class="calibre2">20-cloudinit.conf</tt>, we also have a <tt class="calibre2">norestart.conf</tt> drop-in unit.</p><p class="calibre_8">Now, if we kill the Fleet service, it does not get restarted as the restart option has been disabled by the <tt class="calibre2">restart.conf</tt> drop-in unit. <tt class="calibre2">Fleet.service</tt> stays in a failed state, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00281.jpg" class="calibre_155"/></p><p class="calibre_8">
</p><p id="filepos306212" class="calibre_9"><span class="calibre3"><span class="bold">Runtime drop-in unit – full service</span></span></p><p class="calibre_8">In this approach, we <a/>can replace the complete system service using our own service. Let's change the restart option by creating this <tt class="calibre2">fleet.service</tt> file in <tt class="calibre2">/etc/systemd/system</tt>:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=fleet daemon<br class="calibre4"/><br class="calibre4"/>After=etcd.service<br class="calibre4"/>After=etcd2.service<br class="calibre4"/><br class="calibre4"/>Wants=fleet.socket<br class="calibre4"/>After=fleet.socket<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>ExecStart=/usr/bin/fleetd<br class="calibre4"/>Restart=no<br class="calibre4"/><br class="calibre4"/>[Install]<br class="calibre4"/>WantedBy=multi-user.target</tt></p><p class="calibre_8">We can start the <tt class="calibre2">fleet.service</tt> as follows:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo systemctl start fleet.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's see the status of <tt class="calibre2">fleet.service</tt>:</p><p class="calibre_9"><img src="images/00283.jpg" class="calibre_156"/></p><p class="calibre_8">
</p><p class="calibre_8">From the preceding output, we can see that <tt class="calibre2">fleet.service</tt> is picked up from <tt class="calibre2">/etc/systemd/system</tt>.</p><p class="calibre_8">If we compare this option (a drop-in unit with a complete service change) with the previous option (a drop-in unit with a specific parameter change), the previous option gives the flexibility to change specific parameters and not touch the original set. This makes it easier to handle upgrades when new versions of the service allow additional options.</p><div class="mbp_pagebreak" id="calibre_pb_111"/>


<p id="filepos307983" class="calibre_9"><span class="calibre3"><span class="bold">Network units</span></span></p><p class="calibre_8">The <tt class="calibre2">systemd-networkd</tt>
<a/> service manages networks. System-configured networks are specified in <tt class="calibre2">/usr/lib64/systemd/network</tt> and user-configured networks are specified in <tt class="calibre2">/etcd/systemd/network</tt>. The following is a sample Vagrant configured systemd-network file to configure the <tt class="calibre2">eth1</tt> IP address:</p><p class="calibre_9"><img src="images/00286.jpg" class="calibre_157"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">ifconfig</tt> output associated with <tt class="calibre2">eth1</tt> shows the IP address that Vagrant configured, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00288.jpg" class="calibre_158"/></p><p class="calibre_8">
</p><p class="calibre_8">As an example, let's try to change the <tt class="calibre2">eth1</tt> IP address. There are three steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Stop <tt class="calibre2">systemd-networkd.service</tt>.</li><li value="2" class="calibre_13">Flush the IP address.</li><li value="3" class="calibre_13">Set a new IP address.</li></ol><p class="calibre_8">Let's create a service file to flush the <tt class="calibre2">eth1</tt> IP address and another network file specifying the new IP address for <tt class="calibre2">eth1</tt>.</p><p class="calibre_8">Create a service file to flush the <tt class="calibre2">eth1</tt> IP address as follows. We need to place this in <tt class="calibre2">/etc/systemd/system/down-eth1.service</tt>.</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=eth1 flush<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>Type=oneshot<br class="calibre4"/>ExecStart=/usr/bin/ip link set eth1 down<br class="calibre4"/>ExecStart=/usr/bin/ip addr flush dev eth1<br class="calibre4"/><br class="calibre4"/>[Install]<br class="calibre4"/>WantedBy=multi-user.target</tt></p><p class="calibre_8">The following is the network file to specify the <tt class="calibre2">eth1</tt> new address. We need to place this in <tt class="calibre2">/etc/systemd/network/40-eth1.network</tt>:</p><p class="calibre_8"><tt class="calibre2">[Match]<br class="calibre4"/>Name=eth1<br class="calibre4"/><br class="calibre4"/>[Network]<br class="calibre4"/>Address=172.17.8.110/24<br class="calibre4"/>Gateway=172.17.8.1</tt></p><p class="calibre_8">The steps to change the IP address are as follows:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Stop the system-networkd service by <tt class="calibre2">sudo systemctl stop systemd-networkd.service</tt>.</li><li value="2" class="calibre_13">Flush the <tt class="calibre2">eth1</tt> IP address by <tt class="calibre2">sudo systemctl start down-eth1.service</tt>.</li><li value="3" class="calibre_13">Start <tt class="calibre2">systemd-networkd.service</tt> by <tt class="calibre2">sudo systemctl start systemd-networkd.service</tt>.</li></ol><p class="calibre_8">If we look at the ifconfig output now, we should see the new IP address <tt class="calibre2">172.17.8.110</tt>:</p><p class="calibre_9"><img src="images/00290.jpg" class="calibre_159"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_112"/>


<p id="filepos311348" class="calibre_"><span class="calibre1"><span class="bold">Fleet</span></span></p><p class="calibre_8">Fleet is a cluster manager/scheduler that <a/>controls service creation at the CoreOS cluster level. We can think of Fleet as Systemd for the cluster. For an overview of Fleet, refer to <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>. Fleet is used mainly for the orchestration of critical system services, while other orchestration solutions such as Kubernetes are used for application service orchestration. Fleet is not under active development and is mostly under the maintenance mode.</p><div class="mbp_pagebreak" id="calibre_pb_113"/>


<p id="filepos312007" class="calibre_9"><span class="calibre3"><span class="bold">Installation</span></span></p><p class="calibre_8">Fleet is installed and <a/>started by default in CoreOS. The following is the Fleet version in the CoreOS stable 766.3.0 release:</p><p class="calibre_9"><img src="images/00293.jpg" class="calibre_160"/></p><p class="calibre_8">
</p><p class="calibre_8">Fleet can also<a/> be installed in a standalone Linux machine. Fleet releases can be found at <a href="https://github.com/coreos/fleet/releases">https://github.com/coreos/fleet/releases</a>.</p><div class="mbp_pagebreak" id="calibre_pb_114"/>


<p id="filepos312700" class="calibre_9"><span class="calibre3"><span class="bold">Accessing Fleet</span></span></p><p class="calibre_8">The following <a/>are different approaches<a/> to access Fleet.</p><p id="filepos312903" class="calibre_9"><span class="calibre3"><span class="bold">Local fleetctl</span></span></p><p class="calibre_8">The <tt class="calibre2">fleetctl</tt> command is present in <a/>each CoreOS node and can be used to control Fleet services.</p><p id="filepos313146" class="calibre_9"><span class="calibre3"><span class="bold">Remote fleetctl</span></span></p><p class="calibre_8">The <tt class="calibre2">fleetctl</tt> command can be used to access non-local CoreOS nodes by specifying an endpoint argument. The following is an example:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Fleetctl --endpoint=http://172.17.8.101:2379 list-machines</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00295.jpg" class="calibre_64"/></p><p class="calibre_8">
</p><p id="filepos313719" class="calibre_9"><span class="calibre3"><span class="bold">Remote fleetctl with an SSH tunnel</span></span></p><p class="calibre_8">The previous <a/>example did not use any authentication. To make access to fleetctl secure, we can use the SSH authentication scheme. It is necessary to add the CoreOS node private key to the local SSH authentication agent for this mode. For a Vagrant CoreOS cluster, the private key is stored in <tt class="calibre2">~/.vagrant.d/insecure_private_key</tt>. For an AWS CoreOS cluster, the private key can be downloaded as part of the initial key creation.</p><p class="calibre_8">To add a private key to the authentication agent:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">eval `ssh-agent -s`</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh-add &lt;private key&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00298.jpg" class="calibre_161"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00301.jpg" class="calibre_162"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we can use fleetctl to <a/>use a secure SSH to access the CoreOS cluster:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Fleetctl --tunnel=http://172.17.8.101 list-unit-files</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00303.jpg" class="calibre_163"/></p><p class="calibre_8">
</p><p id="filepos315311" class="calibre_9"><span class="calibre3"><span class="bold">Remote HTTP</span></span></p><p class="calibre_8">Remote<a/> HTTP Fleet API access is disabled by default.</p><p class="calibre_8">To enable remote access, create a <tt class="calibre2">.socket</tt> file to expose the Fleet API port. The following is an example Fleet configuration file to expose port <tt class="calibre2">49153</tt> for external API access:</p><p class="calibre_9"><img src="images/00305.jpg" class="calibre_164"/></p><p class="calibre_8">
</p><p class="calibre_8">It is necessary to restart the systemd, <tt class="calibre2">fleet.socket</tt>, and <tt class="calibre2">fleet.service</tt> after creating the remote API configuration file for it to take effect:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Sudo systemctl daemon-reload</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Sudo systemctl restart fleet.socket</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Sudo systemctl restart fleet.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Now, we can access the remote API. The following is an example using <tt class="calibre2">fleetctl</tt> and <tt class="calibre2">curl</tt>:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Fleetctl --endpoint=http://172.17.8.101:49153 list-units</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00356.jpg" class="calibre_165"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows you the unit list using the Fleet HTTP API. The following curl output is<a/> truncated to show partial output:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">Curl –s http://172.17.8.101:49153/fleet/v1/units | jq .</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_9"><img src="images/00310.jpg" class="calibre_166"/></p><p class="calibre_8">
</p><p id="filepos317292" class="calibre_9"><span class="calibre3"><span class="bold">Using etcd security</span></span></p><p class="calibre_8">We can also<a/> use the secure etcd approach to access Fleet. Setting up a secure etcd is covered in the section on <span class="italic">Etcd security</span>. The following example shows the <tt class="calibre2">fleetctl</tt> command with a server certificate:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">fleetctl --debug --ca-file ca.crt  --endpoint=https://172.17.8.101:2379 list-machines</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><div class="mbp_pagebreak" id="calibre_pb_115"/>


<p id="filepos317856" class="calibre_9"><span class="calibre3"><span class="bold">Templates, scheduling, and HA</span></span></p><p class="calibre_8">Fleet supports unit specifiers and templates similar to systemd. A unit specifier provides <a/>you with shortcuts within a service file, and templates provide reusable service files. The earlier section on systemd covered details on unit specifiers <a/>and templates. <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span> covered the basics of Fleet scheduling and HA.</p><p class="calibre_8">Fleet metadata for a node can be specified in the <span class="italic">Fleet</span> section of <tt class="calibre2">cloud-config</tt>. The following example sets the Fleet node metadata for <tt class="calibre2">role</tt> as <tt class="calibre2">web</tt>. Metadata can be used in Fleet service files to control scheduling:</p><p class="calibre_8"><tt class="calibre2"><span class="bold">metadata: "role=services"</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Fleet uses a pretty simple scheduling algorithm, and X-fleet options are used to specify constraints while scheduling the service. The following are the available X-fleet options:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">MachineMetaData</tt>: Service gets scheduled based on matching metadata.</li><li value="2" class="calibre_13"><tt class="calibre2">MachineId</tt>: Service gets scheduled based on the specified <tt class="calibre2">MachineId</tt>.</li><li value="3" class="calibre_13"><tt class="calibre2">MachineOf</tt>: Service gets scheduled based on other services running in the same node. This can be used to schedule tightly coupled services in the same node.</li><li value="4" class="calibre_13"><tt class="calibre2">Conflict</tt>: This option can be used to avoid scheduling conflicting services in the same node. </li><li value="5" class="calibre_13"><tt class="calibre2">Global</tt>: The same service gets scheduled in all the nodes of the cluster.</li></ul><p class="calibre_8">The following example uses unit specifiers and templates and illustrates Fleet scheduling and HA. The following are some details of the application:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">An application consists of a WordPress container and MySQL container</li><li value="2" class="calibre_13">The WordPress container uses the database from the MySQL container and is linked using Docker container linking</li><li value="3" class="calibre_13">Linking across containers is done using the <tt class="calibre2">--link</tt> option, and it works only if both containers are on the same host</li><li value="4" class="calibre_13">Fleet's template feature will be used to launch multiple services using a single WordPress and MySQL template, and Fleet's X-fleet constraint feature will be used to launch the related containers on the same host</li><a/><li value="5" class="calibre_13">When one of the nodes in the cluster dies, Fleet's HA mechanism will take care of rescheduling the failed units, and we will see it working in this example</li></ul><p class="calibre_8">The MySQL template service is as follows:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=app-mysql<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>Restart=always<br class="calibre4"/>RestartSec=5<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill mysql%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm mysql%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull mysql<br class="calibre4"/>ExecStart=/usr/bin/docker run --name mysql%i -e MYSQL_ROOT_PASSWORD=mysql mysql<br class="calibre4"/>ExecStop=/usr/bin/docker stop mysql%i</tt></p><p class="calibre_8">The WordPress template service is as follows:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=wordpress<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>Restart=always<br class="calibre4"/>RestartSec=15<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill wordpress%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm wordpress%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull wordpress<br class="calibre4"/>ExecStart=/usr/bin/docker run --name wordpress%i --link mysql%i:mysql wordpress<br class="calibre4"/>ExecStop=/usr/bin/docker stop wordpress%i<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>MachineOf=mysql@%i.service</tt></p><p class="calibre_8">The following are<a/> some notes on the service:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">We have used <tt class="calibre2">%i</tt> as an instance specifier</li><a/><li value="2" class="calibre_13">WordPress has an X-fleet constraint to schedule the corresponding MySQL container in the same node</li></ul><p class="calibre_8">The first step is to <a/>submit the services:</p><p class="calibre_9"><img src="images/00311.jpg" class="calibre_167"/></p><p class="calibre_8">
</p><p class="calibre_8">The next step is to load each instance of the service:</p><p class="calibre_9"><img src="images/00425.jpg" class="calibre_168"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's check whether all the services are running. As can be seen in the following screenshot, we have three instances of the WordPress application and the associated MySQL database:</p><p class="calibre_9"><img src="images/00313.jpg" class="calibre_169"/></p><p class="calibre_8">
</p><p class="calibre_8">To demonstrate HA, let's kill CoreOS <tt class="calibre2">node2</tt>. This can be done by shutting down the node.</p><p class="calibre_8">As we can see, there<a/> are only two nodes in the cluster now:</p><p class="calibre_9"><img src="images/00467.jpg" class="calibre_170"/></p><p class="calibre_8">
</p><p class="calibre_8">From the following<a/> new service output, we can see that the services running on the old <tt class="calibre2">node2</tt> have<a/> been moved to <tt class="calibre2">node3</tt> now as <tt class="calibre2">node2</tt> is not available:</p><p class="calibre_9"><img src="images/00316.jpg" class="calibre_169"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_116"/>


<p id="filepos324402" class="calibre_9"><span class="calibre3"><span class="bold">Debugging</span></span></p><p class="calibre_8">The status of the Fleet<a/> service can be checked using <tt class="calibre2">fleetctl status</tt>. The following is an example:</p><p class="calibre_9"><img src="images/00011.jpg" class="calibre_171"/></p><p class="calibre_8">
</p><p class="calibre_8">Logs of the Fleet service can be checked using <tt class="calibre2">fleetctl journal</tt>. The following is an example:</p><p class="calibre_9"><img src="images/00319.jpg" class="calibre_98"/></p><p class="calibre_8">
</p><p class="calibre_8">For debugging<a/> and to get the REST API corresponding to the <tt class="calibre2">fleetctl</tt> command, we can use the <tt class="calibre2">--debug</tt> option as follows:</p><p class="calibre_9"><img src="images/00040.jpg" class="calibre_172"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_117"/>


<p id="filepos325494" class="calibre_9"><span class="calibre3"><span class="bold">Service discovery</span></span></p><p class="calibre_8">Microservices <a/>are dynamic, and it is important for services to discover <a/>other services dynamically to find the IP address, port number, and metadata about the services. There are multiple schemes available to discover services, and in this section, we will cover a few schemes using etcd and Fleet for service discovery. In the later chapters of the book, we will cover advanced service discovery options.</p><p id="filepos326049" class="calibre_9"><span class="calibre3"><span class="bold">Simple etcd-based discovery</span></span></p><p class="calibre_8">The following<a/> figure shows you the simplest possible service discovery mechanism, where a service updates etcd with service-related details that other services can access from etcd:</p><p class="calibre_9"><img src="images/00323.jpg" class="calibre_173"/></p><p class="calibre_8">
</p><p class="calibre_8">The following is an example Apache service, <tt class="calibre2">apacheupdateetcd@.service</tt>, that updates the hostname and port number in etcd when the service is started:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=My Advanced Service<br class="calibre4"/>After=etcd2.service<br class="calibre4"/>After=docker.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill apache%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm apache%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull coreos/apache<br class="calibre4"/>ExecStart=/usr/bin/docker run --name apache%i -p %i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND<br class="calibre4"/>ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running<br class="calibre4"/>ExecStop=/usr/bin/docker stop apache%i<br class="calibre4"/>ExecStopPost=/usr/bin/etcdctl rm /domains/example.com/%H:%i<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/># Don't schedule on the same machine as other Apache instances<br class="calibre4"/>X-Conflicts=apache*@*.service</tt></p><p class="calibre_8">Let's start the service and create two instances:</p><p class="calibre_9"><img src="images/00324.jpg" class="calibre_174"/></p><p class="calibre_8">
</p><p class="calibre_9"><img src="images/00327.jpg" class="calibre_175"/></p><p class="calibre_8">
</p><p class="calibre_8">Now, we <a/>can verify that etcd gets updated with the service details of the two services:</p><p class="calibre_9"><img src="images/00330.jpg" class="calibre_176"/></p><p class="calibre_8">
</p><p id="filepos328304" class="calibre_9"><span class="calibre3"><span class="bold">Sidekick discovery</span></span></p><p class="calibre_8">In the preceding scheme, there<a/> is no way to know if the service is alive and running after it has been started. The following figure shows you a slightly advanced service discovery scheme where a sidekick service updates etcd with the details of the service:</p><p class="calibre_9"><img src="images/00333.jpg" class="calibre_177"/></p><p class="calibre_8">
</p><p class="calibre_8">The purpose of the<a/> Side kick container is to monitor the main service and update etcd only if the Service is active. It is important to run the Side kick container in the same node as the main service that Side kick is monitoring.</p><p class="calibre_8">The following is a Sidekick example using the Apache service and a sidekick for the Apache service.</p><p class="calibre_8">Following is the <tt class="calibre2">Apache.service</tt> unit file:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=Apache web server service on port %i<br class="calibre4"/><br class="calibre4"/># Requirements<br class="calibre4"/>Requires=etcd2.service<br class="calibre4"/>Requires=docker.service<br class="calibre4"/>Requires=apachet-discovery@%i.service<br class="calibre4"/><br class="calibre4"/># Dependency ordering<br class="calibre4"/>After=etcd2.service<br class="calibre4"/>After=docker.service<br class="calibre4"/>Before=apachet-discovery@%i.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/># Let processes take awhile to start up (for first run Docker containers)<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/><br class="calibre4"/># Change killmode from "control-group" to "none" to let Docker remove<br class="calibre4"/># work correctly.<br class="calibre4"/>KillMode=none<br class="calibre4"/><br class="calibre4"/># Get CoreOS environmental variables<br class="calibre4"/>EnvironmentFile=/etc/environment<br class="calibre4"/><br class="calibre4"/># Pre-start and Start<br class="calibre4"/>## Directives with "=-" are allowed to fail without consequence<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill apachet.%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm apachet.%i<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull coreos/apache<br class="calibre4"/>ExecStart=/usr/bin/docker run --name apachet.%i -p ${COREOS_PUBLIC_IPV4}:%i:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND<br class="calibre4"/><br class="calibre4"/># Stop<br class="calibre4"/>ExecStop=/usr/bin/docker stop apachet.%i</tt></p><p class="calibre_8">Following is the Apache sidekick service unit file:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=Apache Sidekick<br class="calibre4"/><br class="calibre4"/># Requirements<br class="calibre4"/>Requires=etcd2.service<br class="calibre4"/>Requires=apachet@%i.service<br class="calibre4"/><br class="calibre4"/># Dependency ordering and binding<br class="calibre4"/>After=etcd2.service<br class="calibre4"/>After=apachet@%i.service<br class="calibre4"/>BindsTo=apachet@%i.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/><br class="calibre4"/># Get CoreOS environmental variables<br class="calibre4"/>EnvironmentFile=/etc/environment<br class="calibre4"/><br class="calibre4"/># Start<br class="calibre4"/>## Test whether service is accessible and then register useful information<br class="calibre4"/>ExecStart=/bin/bash -c '\<br class="calibre4"/>  while true; do \<br class="calibre4"/>    curl -f ${COREOS_PUBLIC_IPV4}:%i; \<br class="calibre4"/>    if [ $? -eq 0 ]; then \<br class="calibre4"/>      etcdctl set /services/apachet/${COREOS_PUBLIC_IPV4} \'{"host": "%H", "ipv4_addr": ${COREOS_PUBLIC_IPV4}, "port": %i}\' --ttl 30; \<br class="calibre4"/>    else \<br class="calibre4"/>      etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}; \<br class="calibre4"/>    fi; \<br class="calibre4"/>    sleep 20; \<br class="calibre4"/>  done'<br class="calibre4"/><br class="calibre4"/># Stop<br class="calibre4"/>ExecStop=/usr/bin/etcdctl rm /services/apachet/${COREOS_PUBLIC_IPV4}<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/># Schedule on the same machine as the associated Apache service<br class="calibre4"/>X-ConditionMachineOf=apachet@%i.service</tt></p><p class="calibre_8">The preceding <span class="bold">Side kick</span> container<a/> service does<a/> a periodic ping to the main service and updates the <span class="bold">etcd</span> output. If the main service is not reachable, the service-related details are removed from <span class="bold">etcd</span>.</p><p class="calibre_8">Let's start two instances of the service:</p><p class="calibre_9"><img src="images/00337.jpg" class="calibre_119"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's see the etcd output. As shown in the following screenshot, etcd reflects the two nodes where Apache is running:</p><p class="calibre_9"><img src="images/00341.jpg" class="calibre_178"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's see the docker output in node1:</p><p class="calibre_9"><img src="images/00345.jpg" class="calibre_98"/></p><p class="calibre_8">
</p><p class="calibre_8">To demonstrate the<a/> sidekick service, let's stop the docker container and check whether the sidekick service updates etcd in order to remove the appropriate service:</p><p class="calibre_9"><img src="images/00349.jpg" class="calibre_179"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's check the status of the units. As can be seen below, <tt class="calibre2">apachet@2.service</tt> has failed and the associated sidekick service <tt class="calibre2">apachet-discovery@2.service</tt> is inactive.</p><p class="calibre_9"><img src="images/00352.jpg" class="calibre_180"/></p><p class="calibre_8">
</p><p class="calibre_8">From the following output, we can see that the <tt class="calibre2">apachet@2.service</tt> details are removed from etcd:</p><p class="calibre_9"><img src="images/00355.jpg" class="calibre_181"/></p><p class="calibre_8">
</p><p id="filepos334324" class="calibre_9"><span class="calibre3"><span class="bold">ELB service discovery</span></span></p><p class="calibre_8">This is a variation<a/> of the Sidekick discovery in which, instead of Sidekick updating etcd, Sidekick updates the IP address to the load balancer and the load balancer redirects the web query to the active nodes. In this example, we will use the AWS Elastic load balancer and CoreOS elb-presence container available in the Quay repository. The elb-presence container takes care of checking the health of the nginx container and updates AWS ELB with the container's IP address.</p><p class="calibre_8">The following figure shows you a high-level architecture of this approach:</p><p class="calibre_9"><img src="images/00264.jpg" class="calibre_182"/></p><p class="calibre_8">
</p><p class="calibre_8">The first step is to create ELB in AWS, as shown in the following screenshot. Here we have used AWS CLI to create ELB, <tt class="calibre2">testlb</tt>:</p><p class="calibre_9"><img src="images/00124.jpg" class="calibre_183"/></p><p class="calibre_8">
</p><p class="calibre_8">We need to use <tt class="calibre2">testlb</tt> created in the preceding screenshot in the Sidekick service.</p><p class="calibre_8">Following is the <tt class="calibre2">nginx.service</tt> unit file:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=nginx<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill nginx-%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm nginx-%i<br class="calibre4"/>ExecStart=/usr/bin/docker run --rm --name nginx-%i -p 80:80 nginx<br class="calibre4"/>ExecStop=/usr/bin/docker stop nginx-%i<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>Conflicts=nginx@*.service</tt></p><p class="calibre_8">Following is the nginx sidekick service that updates AWS ELB based on the health of <tt class="calibre2">nginx.service</tt>:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=nginx presence service<br class="calibre4"/>BindsTo=nginx@%i.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill nginx-presence-%i<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm nginx-presence-%i<br class="calibre4"/>ExecStart=/usr/bin/docker run --rm --name nginx-presence-%i -e AWS_ACCESS_KEY=&lt;key&gt; -e AWS_SECRET_KEY=&lt;secretkey&gt; -e AWS_REGION=us-west-2 -e ELB_NAME=testlb quay.io/coreos/elb-presence<br class="calibre4"/>ExecStop=/usr/bin/docker stop nginx-presence-%i<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>MachineOf=nginx@%i.service</tt></p><p class="calibre_8">The following is the Fleet status after creating two instances of the service:</p><p class="calibre_9"><img src="images/00128.jpg" class="calibre_184"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can see in the following screenshot, AWS ELB has both the instances registered, and it will load-balance between these two instances:</p><p class="calibre_9"><img src="images/00130.jpg" class="calibre_185"/></p><p class="calibre_8">
</p><p class="calibre_8">At this point, if we<a/> stop any instance of the nginx service, the Sidekick service will take care of removing this instance from ELB.</p><div class="mbp_pagebreak" id="calibre_pb_118"/>


<p id="filepos337816" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered the internals of Etcd, Systemd, and Fleet with sufficient hands-on examples, which will allow you to get comfortable with configuring and using these services. By keeping the development of the critical services open source, CoreOS has encouraged the usage of these services outside CoreOS as well. We also covered the basic service discovery options using Etcd, Systemd, and Fleet. In the next chapter, we will cover container networking and Flannel.</p><div class="mbp_pagebreak" id="calibre_pb_119"/>


<p id="filepos338437" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Etcd docs: <a href="https://coreos.com/etcd/docs/latest/">https://coreos.com/etcd/docs/latest/</a></li><a/><li value="2" class="calibre_13">Fleet docs: <a href="https://coreos.com/fleet/docs/latest/">https://coreos.com/fleet/docs/latest/</a></li><a/><li value="3" class="calibre_13">Systemd docs: <a href="http://www.freedesktop.org/wiki/Software/systemd/">http://www.freedesktop.org/wiki/Software/systemd/</a></li><a/><li value="4" class="calibre_13">Fleet service discovery: <a href="https://coreos.com/fleet/docs/latest/examples/service-discovery.html">https://coreos.com/fleet/docs/latest/examples/service-discovery.html</a></li><a/><li value="5" class="calibre_13">Etcd-ca: <a href="https://github.com/coreos/etcd-ca">https://github.com/coreos/etcd-ca</a></li><li value="6" class="calibre_13">Etcd security: <a href="https://github.com/coreos/etcd/blob/master/Documentation/security.md">https://github.com/coreos/etcd/blob/master/Documentation/security.md</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_120"/>


<p id="filepos339730" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Etcd security and authentication: <a href="http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/">http://thepracticalsysadmin.com/etcd-2-1-1-encryption-and-authentication/</a></li><a/><li value="2" class="calibre_13">Etcd administration: <a href="https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md">https://github.com/coreos/etcd/blob/master/Documentation/admin_guide.md</a></li><a/><li value="3" class="calibre_13">Why systemd: <a href="http://blog.jorgenschaefer.de/2014/07/why-systemd.html">http://blog.jorgenschaefer.de/2014/07/why-systemd.html</a></li><a/><li value="4" class="calibre_13">Comparing init systems: <a href="http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html">http://centos-vn.blogspot.in/2014/06/daemon-showdown-upstart-vs-runit-vs.html</a></li><a/><li value="5" class="calibre_13">Systemd talk by the Systemd creator: <a href="https://www.youtube.com/watch?v=VIPonFvPlAs">https://www.youtube.com/watch?v=VIPonFvPlAs</a></li><a/><li value="6" class="calibre_13">Service discovery overview: <a href="http://www.gomicro.services/articles/service-discovery-overview">http://www.gomicro.services/articles/service-discovery-overview</a> and <a href="http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/">http://progrium.com/blog/2014/07/29/understanding-modern-service-discovery-with-docker/</a></li><a/><li value="7" class="calibre_13">Highly available Docker services using CoreOS and Consul: <a href="http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/">http://blog.xebia.com/2015/03/24/a-high-available-docker-container-platform-using-coreos-and-consul/</a> and <a href="http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/">http://blog.xebia.com/2015/04/23/how-to-deploy-high-available-persistent-docker-services-using-coreos-and-consul/</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_121"/>
</body></html>