<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;NSX Manager Installation and Configuration"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. NSX Manager Installation and Configuration</h1></div></div></div><p>This chapter explains the list of prerequisites and installation steps for NSX Manager installation. The following are the key points that we will discuss:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">NSX Manager requirements</li><li class="listitem" style="list-style-type: disc">NSX Manager installation</li><li class="listitem" style="list-style-type: disc">NSX Manager design considerations</li><li class="listitem" style="list-style-type: disc">Controller requirements</li><li class="listitem" style="list-style-type: disc">NSX Controller deployments design considerations</li><li class="listitem" style="list-style-type: disc">NSX data plane installation</li></ul></div><div class="section" title="NSX Manager requirements"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>NSX Manager requirements</h1></div></div></div><p>VMware NSX Manager is a preconfigured virtual appliance which we can download from the VMware website just like any other VMware software. This preconfigured virtual machine comes with 16 GB of memory, 4 VCPUs, and 60 GB of storage space.</p><p>Let's have a quick look at the prerequisites for NSX Manager 6.2 installation:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VMware vCenter server 6.0</li><li class="listitem" style="list-style-type: disc">VMware ESXi 6.0</li><li class="listitem" style="list-style-type: disc">Host clusters prepared with NSX 6.2</li><li class="listitem" style="list-style-type: disc">vSphere distributed switch which is supported with the respective version of host and virtual center</li><li class="listitem" style="list-style-type: disc">Ensure we have shared data stores through which we can leverage vSphere HA/DRS features to prevent downtime for NSX Manager VM</li><li class="listitem" style="list-style-type: disc">Confirm whether we are using a dual stack or IPV4/IPV6 only networks</li><li class="listitem" style="list-style-type: disc">Collect the Gateway, DNS, Syslog, and NTP server configuration</li><li class="listitem" style="list-style-type: disc">All port requirements are updated in the VMware public knowledge base article at <a class="ulink" href="https://kb.vmware.com/kb/2079386">https://kb.vmware.com/kb/2079386</a>.</li></ul></div><p>After downloading the NSX Manager OVA, we will follow the four-step process for installation and configuration of the manager software as listed in the following:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Deploy the NSX Manager OVA file</li><li class="listitem">Log in to NSX Manager</li><li class="listitem">Establish the NSX Manager and vCenter Server connection</li><li class="listitem">Configure backup options</li></ol></div><p>With that said, let's get started with the installation process. From my experience, I can confidently comment that deploying any appliances is a very easy task. However, when things start getting weird, we are forced to go back and review the deployment process and we find that the majority of the issues are something that we totally missed during the installation process wherein we just clicked the <span class="strong"><strong>Next</strong></span>, and <span class="strong"><strong>Finish</strong></span> buttons.</p></div></div>
<div class="section" title="NSX Manager installation"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>NSX Manager installation</h1></div></div></div><p>For installing NSX Manager, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open vCenter via VMware web client.</li><li class="listitem">Select <span class="strong"><strong>VMs and Templates</strong></span>, right-click your data center, and select <span class="strong"><strong>Deploy OVF Template</strong></span>.</li><li class="listitem">Paste the VMware download URL or click <span class="strong"><strong>Browse</strong></span> to select the file on your computer.</li><li class="listitem">Click in the checkbox <span class="strong"><strong>Accept extra configuration options</strong></span>.<p>This allows you to set IPv4 and IPv6 addresses, default gateway, DNS, NTP, and SSH properties during the installation. If we do not set these configurations during deployment, we can always set them after the deployment.</p></li><li class="listitem">Accept the VMware license agreements.</li><li class="listitem">Edit the NSX Manager name (if required). Select the location for the deployed NSX Manager.</li><li class="listitem">This name will appear in the vCenter Server inventory.</li><li class="listitem">The folder you select will be used to apply permissions to the NSX Manager.</li><li class="listitem">Select a host or cluster on which to deploy the NSX Manager appliance. I would prefer selecting a cluster and letting DRS decide the best host for placing the appliance.</li><li class="listitem">Change the virtual disk format to <span class="strong"><strong>Thick Provision</strong></span>, and select the destination data store for the virtual machine configuration files and the virtual disks.</li><li class="listitem">Select the management port group for NSX Manager.</li></ol></div><p>Finally, review the screen after all the options are configured and we can see that this is an IPV4 only deployment. So stay focused and review the screen once more and if any corrections are required, we should go back to previous steps and correct it.</p><p>Now that we have successfully deployed NSX Manager, it is worth double-checking all the configurations are intact and the manager appliance can ping DNS, NTP, Gateway, ESXi hosts, and VC. This is a very important step as any communication issues between manager and these components will have a direct impact on the functionality/deployment of VMware NSX features.</p><p>The following figure shows NSX Manager OVF details:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_001.jpg" alt="NSX Manager installation"/></div><p>
</p><p>Let's make a note of the key configuration details from the preceding image and I will explain the design decisions for choosing it.</p><div class="section" title="Understanding the key configuration details"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec11"/>Understanding the key configuration details</h2></div></div></div><p>As I said earlier, there are a few key design factors which we need to take care of while deploying NSX Manager appliances. Let's talk more about them.</p><div class="section" title="Target - Management and Edge cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec1"/>Target - Management and Edge cluster</h3></div></div></div><p>The management and edge cluster is a dedicated cluster in vSphere and it is always recommended to have a unique cluster for deploying all management components, which will ease any upgrade activity on the cluster without impacting the compute cluster, which is explicitly used for deploying end user virtual machines. In this example, we have a preconfigured vSphere cluster. And we leverage this cluster for deploying the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">NSX Manager</li><li class="listitem" style="list-style-type: disc">NSX Controller</li><li class="listitem" style="list-style-type: disc">Vcenter Server</li><li class="listitem" style="list-style-type: disc">Any other third-party or VMware management software</li></ul></div><p>But remember that NSX can be integrated with a wide range of VMware products, such as Horizon View, SRM, vCloud Director, VRA, VIO, and VCO. So, based on the type of product integration and data center design, situations might demand having multiple management clusters for isolation purposes. For example, if we have two vCenter Servers in the primary site, one for vSphere with NSX integration and a second vCenter Server for SRM integration, it is okay to create two separate management clusters. Again, this is a design choice, whether we want all our eggs in one basket or we are okay to place them in unique baskets (cluster, rack).</p></div><div class="section" title="Network mapping"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec2"/>Network mapping</h3></div></div></div><p>We have connected NSX Manager to a vSphere distributed switch port group called <span class="strong"><strong>Mgmt_VDS_MGT</strong></span>. These are preconfigured port groups which will be ideally configured with a VLAN port group. Either we can have a separate distributed switch for the management and edge cluster or we can have one single distributed switch which spans across multiple clusters. The preferred method of deployment would be having a unique distributed switch, as that would remain a VMotion boundary for management virtual machines. Yes, we don't want a <span class="strong"><strong>Distributed Resource Scheduler</strong></span> (<span class="strong"><strong>DRS</strong></span>) or manual migration movement for those virtual machines to any other compute cluster, as this would defeat the purpose of having a unique cluster for management and compute machines. Added to that, based on the physical network design, let's assume, as shown in the following screenshot, that we have a top-of-rack switch for the management and edge cluster which is running on a single rack. Any network-level changes in TOR switches have no impact on the compute cluster. Are we planning for LACP, static EtherChannel, or virtual PortChannel like configurations? It is important to note that the LACP configuration should be consistent across all port groups which are from the same distributed switch. The following diagram depicts a typical NSX enterprise vSphere design with separate compute and management and edge clusters:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_002.jpg" alt="Network mapping"/><div class="caption"><p>Compute and edge cluster with top of rack design</p></div></div><p>
</p><p>After confirming all our initial configurations are intact, we can log in to NSX Manager via a supported web browser, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_003.jpg" alt="Network mapping"/><div class="caption"><p>NSX Manager initial screen</p></div></div><p>
</p><p>From the preceding image, we can see the NSX Manager initial page and for me there is a significant difference between vCloud networking security and the NSX Manager initial page: all the options are well arranged with a straightforward explanation. By default, we can log in to NSX Manager using the following credentials. The default username and password for NSX Manager is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">User: <code class="literal">admin</code></li><li class="listitem" style="list-style-type: disc">Password: <code class="literal">default</code></li></ul></div><p>For security-hardening purposes, we can always change the password and create multiple users for management access. We can also see from the preceding image that on the right-hand side top corner, we have the initial configuration for NSX Manager along with the version of NSX, which is 6.2, and the build number.</p></div></div><div class="section" title="NSX Manager virtual appliance management"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec12"/>NSX Manager virtual appliance management</h2></div></div></div><p>Let's discuss different options that are listed in the preceding GUI. All these are key NSX appliance configuration and integration options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>View summary</strong></span>: The view summary page gives us the complete configuration summary of NSX manager. This includes Virtual Appliance DNS, IP, version, and uptime/current time, along with common components and management services as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_03_004.jpg" alt="NSX Manager virtual appliance management"/><div class="caption"><p>NSX Manager summary</p></div></div><p>
</p><p>One look at the preceding screenshot and we decipher that vPostgres, RabbitMQ, and management services should be up and running. <span class="strong"><strong>RabbitMQ</strong></span> Server is a process which is hosted on NSX Manager and they interact with the <span class="strong"><strong>Firewall daemon (vsfwd</strong></span>), which is running on the ESXi host via the message bus. We will discuss vsfwd in more detail in Chapter 6, <span class="emphasis"><em>Configuring and Managing NSX Network Services</em></span>; however, as of now we have to understand the importance of this service and how they communicate in the NSX world. Postgres is the NSX Manager database which comes along with the appliance. Hence, it is very important to note that any editing or table changes in the database through any methods will have a direct impact and it is highly recommended to perform such practices with the help of the VMware support team.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Manage appliance settings</strong></span>: Manage settings will be extremely helpful to make any configuration changes against NSX Manager. For example, there is a new Syslog Server configured and we would like to leverage a new Syslog Server to be configured with the current NSX Manager. We can easily go ahead and update the changes by editing the <span class="strong"><strong>Syslog Server</strong></span> tab. In earlier versions of NSX, SSL was disabled by default. However, starting from the NSX 6.1 release, SSL is enabled by default. So let's take a look at the following screenshot, which shows the <span class="strong"><strong>General</strong></span> settings of NSX Manager:<p>
</p><div class="mediaobject"><img src="graphics/B03244_03_05-1024x586.jpg" alt="NSX Manager virtual appliance management"/><div class="caption"><p>Time Settings, Syslog Server and Locale settings of NSX Manager</p></div></div><p>
</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Manage vCenter registration</strong></span>: NSX Manager and vCenter Server have a one-to-one relationship. With cross VCenter Server NSX installation, this is one of the most confused topics as we believe multiple VC is supported with a single NSX instance, which is totally wrong. NSX 6.2 allows you to manage multiple vCenter NSX environments from a single primary NSX Manager. Even in a Cross vCenter Server NSX installation, the NSX Manager to vCenter Server relationship is still one-to-one. More about cross VC installation will be discussed in <a class="link" href="ch07.html" title="Chapter 7. NSX Cross vCenter">Chapter 7</a>, <span class="emphasis"><em>NSX Cross vCenter</em></span>, NSX-vCross-vCenter feature and design decisions.</li></ul></div><div class="section" title="Register vCenter Server with NSX Manager"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec3"/>Register vCenter Server with NSX Manager</h3></div></div></div><p>We will quickly go ahead and register NSX Manager with vCenter Server by following steps. The procedure for NSX Manager registration:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Firstly, log in to the NSX Manager virtual appliance.</li><li class="listitem">Under <span class="strong"><strong>Appliance Management</strong></span>, click <span class="strong"><strong>Manage Appliance Settings</strong></span>.</li><li class="listitem">We need to type the IP address of the vCenter Server.</li><li class="listitem">Type the vCenter Server username and password.</li><li class="listitem">Click <code class="literal">OK</code>.</li><li class="listitem">Wait for few seconds for successful connection to the vCenter Server.</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/B03244_03_06.jpg" alt="Register vCenter Server with NSX Manager"/></div><p>
</p></div><div class="section" title="Register SSO with NSX Manager"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec4"/>Register SSO with NSX Manager</h3></div></div></div><p>For <span class="strong"><strong>single sign-on</strong></span> (<span class="strong"><strong>SSO</strong></span>) service integration with NSX, we need to configure the lookup service, which will improve the security of user authentication for vCenter users and enables NSX to authenticate from identity services such as AD, NIS, and LDA.</p><p>Procedure for LookUp service registration:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to the NSX Manager virtual appliance.</li><li class="listitem">Under <span class="strong"><strong>Appliance Management</strong></span>, click <span class="strong"><strong>Manage Settings</strong></span>.</li><li class="listitem">Click <span class="strong"><strong>NSX Management Service</strong></span>.</li><li class="listitem">Click <span class="strong"><strong>Edit</strong></span> next to <span class="strong"><strong>Lookup Service</strong></span>.</li><li class="listitem">Type the name or IP address of the host that has the lookup service.</li><li class="listitem">Change the port number if required. The default port is <span class="strong"><strong>7444</strong></span>.</li><li class="listitem">The lookup service URL is displayed based on the specified host and port. Type the vCenter administrator user name and password.</li><li class="listitem">This enables NSX Manager to register itself with the Security Token Service server.</li><li class="listitem">Click <span class="strong"><strong>OK</strong></span>.</li><li class="listitem">Confirm that the lookup service status is connected.</li></ol></div><p>The following figure shows successful registration of the lookup service and vCenter Server registration for NSX Manager:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_03_07-1024x628.jpg" alt="Register SSO with NSX Manager"/></div><p>
</p><p>It's time to explore other NSX Manager settings, such as Tech Support Logs, upgrading, and restoring the management appliance:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Download Tech Support Log</strong></span>: For diagnostic purposes, we can go ahead and download the NSX Manager logs by clicking the <span class="strong"><strong>download</strong></span> button under <span class="strong"><strong>NSX Manager virtual appliance management</strong></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Backup and Restore</strong></span>: NSX Manager data, including system configuration, events, and audit log tables (stored in the Postgres DB), can be backed up at any time by performing an on-demand backup from the NSX Manager GUI. It is also possible to schedule periodic backups to be performed (hourly, daily or weekly).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Upgrade</strong></span>: Based on the version of NSX Manager or vCloud network security solution, we will be in need of upgrading the management plane. Once we download the upgrade bundle, we need to upload the same to the <span class="strong"><strong>NSX Manager-Upgrade-Upload New Bundle</strong></span> tab and click on <span class="strong"><strong>Upgrade</strong></span>. Please note, upgrading NSX Managers won't upgrade control plane or data plane components.</li></ul></div><p>Keep in mind that we are using an administrator role account to register the vCenter Server with NSX. Also note that on ASCII characters in the password will create synchronization issues with NSX Manager. A successful registration of NSX Manager with vCenter Server will let us manage NSX Manager via VMware web client and we will see <span class="strong"><strong>Networking &amp; Security</strong></span> solution in the web client inventory as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_03_08.jpg" alt="Register SSO with NSX Manager"/></div><p>
</p><p>Lastly, let's go ahead and license NSX Manager.</p><p>Log in to the vCenter Server with the vSphere web client and perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the middle pane, click the <span class="strong"><strong>Solutions</strong></span> tab.</li><li class="listitem">Select the <span class="strong"><strong>NSX</strong></span> for vSphere solution.</li><li class="listitem">Click the <span class="strong"><strong>Assign License Key</strong></span> link.</li><li class="listitem">In the <span class="strong"><strong>Assign License Key</strong></span> panel, select <span class="strong"><strong>Assign a new license key</strong></span> from the drop-down menu.</li><li class="listitem">In the <span class="strong"><strong>License key</strong></span> textbox, enter or paste your NSX for vSphere license key.</li><li class="listitem">Click <span class="strong"><strong>OK</strong></span>.</li></ol></div><p>The VMware NSX for multi-hypervisor license key may also be used to license VMware NSX for vSphere.</p></div></div></div>
<div class="section" title="NSX Manager deployment consideration"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>NSX Manager deployment consideration</h1></div></div></div><p>Due to the critical role NSX plays in a vSphere environment, it is extremely important to know and implement NSX management, control plane and data plane features. NSX management is NSX Manager, which provides a single point for configuring all NSX features and in addition we can leverage REST-API calls for deployment, configuration, and other tasks. So let's talk about communication channels from the management plane to other components.</p><div class="section" title="The communication path"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec13"/>The communication path</h2></div></div></div><p>The following list shows the communication path between NSX Manager and various components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NSX Manager to VCenter Server</strong></span>: Communication between manager and VC is via vSphere API</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NSX Manager to Controllers</strong></span>: Communication between manager and controllers is via HTTPs</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NSX Manager to ESXi hosts</strong></span>: Communication between manager and underlying ESXi hosts would be via message bus.</li></ul></div></div><div class="section" title="Network and port requirements"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec14"/>Network and port requirements</h2></div></div></div><p>NSX Manager virtual machines being part of the management plane, typically the NSX Manager and vCenter are placed on a single management network (vSphere PortGroup). I know most of the architects would be wondering having isolated networks (different subnets) for NSX Manager and vCenter Server will remain supported? The answer is <span class="emphasis"><em>yes</em></span>, they can reside in different networks and also in different VLANs.</p><p>NSX for vSphere protocol and port requirements are updated as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_03_09.jpg" alt="Network and port requirements"/></div><p>
</p><p>Ensure that all these ports and protocols are allowed in the network.</p></div><div class="section" title="User roles and permissions"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec15"/>User roles and permissions</h2></div></div></div><p>Firstly, NSX roles and permissions are totally different from vCenter Server roles and permissions. Hence, it is important to secure user access. Using <span class="strong"><strong>Role Based Access Control</strong></span> (<span class="strong"><strong>RBAC</strong></span>), we can secure a user access. Adding to that, we can also leverage vCenter SSO identity source users and groups once after properly configuring a lookup service with NSX Manager. NSX provides scope to restrict the area that a user can access in the NSX system:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Global</strong></span>: The user has access to all areas of NSX</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Limited access</strong></span>: The user has access to only the NSX areas defined in the user profile</li></ul></div><p>Various user roles are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Enterprise Administrator: </strong></span>NSX operations and security</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NSX Administrator</strong></span>: NSX operations only, for example, install virtual appliances, configure port groups, and so on</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Security Administrator</strong></span>: Read and write access to NSX security area, such as defining data security policies, creating port groups, and creating reports for NSX modules, and has read-only access to other areas</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Auditor</strong></span>: Has read-only access to all areas</li></ul></div><p>A user cannot be defined without a role. After a role is assigned to users, the role can be changed. All these roles are extremely important when giving the permissions to NSX users and ensure we are giving limited access to non-enterprise administrator accounts.</p></div></div>
<div class="section" title="Controller requirements"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Controller requirements</h1></div></div></div><p>Now that we are clear about manager deployment and design decisions, let's discuss controller requirements. NSX Controllers are also deployed as virtual appliances with default compute resources per controllers. Since we have already registered NSX Manager with virtual center and ensured that we have ports and protocols opened, let me re-emphasize once again why controllers are required:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VXLAN unicast and hybrid mode replication</li><li class="listitem" style="list-style-type: disc">Distributed logical routing</li></ul></div><p>Since NSX Controllers are virtual machines with control plane intelligence, from a network requirement perspective, they need to have an IP address. However, we don't stick with the traditional method of manual or DHCP discovery processes for IP assignments. Prior to controller deployment, let's configure IP pools.</p><div class="section" title="The procedure for controller IP pool creation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec16"/>The procedure for controller IP pool creation</h2></div></div></div><p>IP pools are used for assigning IP addresses to controllers and <span class="strong"><strong>VXLAN Tunnel Endpoints</strong></span> (<span class="strong"><strong>VTEP</strong></span>). I certainly love this feature, which is a less manual process. All we need is to create an IP pool and the controller will pick an IP from the pool while it is getting created; also, it will release an IP during the deletion time:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">From <span class="strong"><strong>vCenter Server Web Client</strong></span>, navigate to <span class="strong"><strong>Networking Security</strong></span> and under <span class="strong"><strong>Manage</strong></span> select <span class="strong"><strong>Grouping Objects</strong></span> as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/B03244_03_10-1024x280.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p></li><li class="listitem">Click the <span class="strong"><strong>+ </strong></span>icon and we need to configure a static IP pool so that individual controllers can pick one IP from this pool as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/B03244_03_11.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p></li><li class="listitem">Now that we have an IP pool ready, we can switch back to the NSX home installation page and click on the <span class="strong"><strong>+</strong></span> sign under <span class="strong"><strong>NSX Controller Nodes</strong></span> as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/B03244_03_12-1024x624.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p></li><li class="listitem">Select and update respectively <span class="strong"><strong>vCenter Datacenter</strong></span>, <span class="strong"><strong>Cluster/Resource Pool</strong></span>, shared <span class="strong"><strong>Datastore</strong></span> location, vSphere PortGroup for connectivity, and lastly the name of <span class="strong"><strong>IP pool</strong></span> which we created in step 2. After accomplishing the task, you'll see the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_03_013.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p></li><li class="listitem">After the first controller is deployed, we can deploy an additional two more controllers by following the same steps since the three node control cluster is mandatory.</li><li class="listitem">Successful deployment of controllers will have a <span class="strong"><strong>Normal</strong></span> status and a green check mark as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_03_014.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p></li></ol></div><p>Let's make a note of all three controller IPs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 1</strong></span>: <span class="strong"><strong>192.168.110.201</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 2</strong></span>: <span class="strong"><strong>192.168.110.202</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 3</strong></span>: <span class="strong"><strong>192.168.110.203</strong></span></li></ul></div><p>Even though we have controllers deployed and the status is green, it is important to check the control cluster connections and their status from the command line, which would give granular-level details. SSH to all three controllers and issue the following command:</p><pre class="programlisting">
<span class="strong"><strong>    Show Control-cluster status&#13;
    # Command to Check Controller Status and ID&#13;
</strong></span>
</pre><p>The controller types and their status are explained as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Join status</code>: Verify the controller node is reporting join complete</li><li class="listitem" style="list-style-type: disc"><code class="literal">Majority status</code>: Verify the controller is connected to the cluster majority</li><li class="listitem" style="list-style-type: disc"><code class="literal">Cluster ID</code>: All the controller nodes in a cluster should have the same cluster ID</li></ul></div><p>Remember the controller roles that we discussed in <a class="link" href="ch02.html" title="Chapter 2.  NSX Architecture">Chapter 2</a>, <span class="emphasis"><em>NSX Architecture</em></span>? Yes, those are the five roles which are populated here, as shown in the following screenshot - <code class="literal">api_provider</code>, <code class="literal">persistence_server</code>, <code class="literal">switch_manager</code>, <code class="literal">logical_manager</code>, and <code class="literal">directory_server</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_015.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p><p>Okay, this is all really great, but I know we have a few questions left. Let's have a look at those questions one by one.</p><p>How do we check which controller is master for those roles?</p><p>The following command will display that output for us:</p><pre class="programlisting">
<span class="strong"><strong>Show Control-Cluster roles</strong></span>
</pre><p>Let's do a random check on one of those controllers, in this example, <code class="literal">Controller 2</code>, which is <code class="literal">192.168.110.202</code>. As we can see from the following screenshot, except for the persistence server, all the roles are master in controller <code class="literal">192.168.110.202</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_016.jpg" alt="The procedure for controller IP pool creation"/><div class="caption"><p>SSH session NSX Controller</p></div></div><p>
</p><p>The controller cluster majority leader listens on port <code class="literal">2878</code> and will have a <code class="literal">Y</code> in the <code class="literal">listening</code> column. To check that, let's issue the following command and check on the same controller <code class="literal">192.168.110.202</code> that we have checked during step 2:</p><pre class="programlisting">
<span class="strong"><strong>    S<span class="strong"><strong>how Control-Cluster Connections</strong></span>
</strong></span>
</pre><p>We got the following output:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_017.jpg" alt="The procedure for controller IP pool creation"/></div><p>
</p><p>Does that look bit weird?</p><p>We know that the controller cluster majority leader listens on port <code class="literal">2878</code> and would have a <code class="literal">
<span class="strong"><strong>- </strong></span>
</code>in the <code class="literal">listening</code> column for persistence server for controller <code class="literal">192.168.110.202</code> . No rocket science here; as per the <code class="literal">Show Control-Cluster roles</code> output which we have tested in step 7, except for the persistence server role, for the rest of the roles, controller cluster 2 was master, hence <code class="literal">Show Control-Cluster Connections</code> is reporting <code class="literal">-</code> for the persistence server. I hope we now have some basic understanding of controller roles. We will discuss a few troubleshooting scenarios in <a class="link" href="ch08.html" title="Chapter 8.  NSX Troubleshooting">Chapter 8</a>, <span class="emphasis"><em>NSX Troubleshooting</em></span>.</p></div></div>
<div class="section" title="NSX Controller design consideration"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>NSX Controller design consideration</h1></div></div></div><p>NSX Controller virtual machines are the DNA of the control plane, hence it is important to take decisions on where to install and connect the controller. Lastly, we don't want the controller to get exposed to users who are leveraging NSX features; basically, no control plane attack.</p><div class="section" title="Communication path"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Communication path</h2></div></div></div><p>It's good to know the communication protocol used between NSX Manager, controllers and NSX Edges:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Communication between controller and NSX Manager - HTTPS</li><li class="listitem" style="list-style-type: disc">Communication between Edge and controller - HTTPS</li></ul></div></div><div class="section" title="Network and port requirements"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>Network and port requirements</h2></div></div></div><p>Ensure that the port requirements mentioned in the following screenshot are met for controller communication:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_018.jpg" alt="Network and port requirements"/></div><p>
</p></div><div class="section" title="Controller deployment consideration"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Controller deployment consideration</h2></div></div></div><p>I know, I keep telling you this: the real power of NSX is all about controllers. How we deploy our controllers, what best practices are implemented, all makes a vital difference in NSX design. You know by now, because of overlay networks, there will be a whole bunch of design best practices that we might need to do in both the physical and virtual worlds. But here, we will discuss controller deployment considerations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First and foremost, NSX Controllers should be deployed in the same vCenter where NSX Manager is registered. The only exception would be while leveraging cross-VC NSX design, which we will discuss in <a class="link" href="ch07.html" title="Chapter 7. NSX Cross vCenter">Chapter 7</a>, <span class="emphasis"><em>NSX Cross vCenter</em></span>.</li><li class="listitem" style="list-style-type: disc">While deploying the controllers, don't make any other configuration changes or deploy any other NSX features. The rule is the same even if we are deleting a controller and deploying a new one during break fix scenarios.</li><li class="listitem" style="list-style-type: disc">Use a separate vSphere Edge cluster for controller deployments. For small-scale deployments, it is okay to deploy controllers in a management cluster.</li><li class="listitem" style="list-style-type: disc">NSX Controllers should be reachable with all ESXi host vmkernel networks and NSX Manager management networks.</li><li class="listitem" style="list-style-type: disc">The controller being a VM, most of the enterprise environments will have the vSphere <span class="strong"><strong>Distributed Resource Schedular</strong></span> (<span class="strong"><strong>DRS</strong></span>) feature and it will consider it as a normal virtual machine and will migrate or place it on same ESXi host based on the placement algorithm. To ensure controllers are not getting deployed or migrated to the same host and if there is a host failure this will have a direct impact on controllers and the entire control plane will be down. To avoid such situations, we will have to leverage vSphere anti affinity rules to avoid deploying more than one controller on the same ESXi host. Adding to that, I would highly recommend starting with more than three host clusters and later scale accordingly. This way, we can easily place controllers on separate ESXi hosts and scale accordingly. Don't get my message wrong, we are deploying three controllers on three different hosts and not leaving the rest of the host as a spare one. In any environment, we will be doing maintenance activity, sometimes as a part of a software upgrade or maybe adding or removing hardware devices from the server. For such scenarios, when we take a downtime for one of the hosts, we are still left with more than three ESXi hosts and controllers would be placed on them based on the anti affinity rules.</li><li class="listitem" style="list-style-type: disc">All the hosts in the cluster should have automatic VM startup/shutdown enabled.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note5"/>Note</h3><p>The first host where the controllers are deployed will have automatic VM startup/shutdown enabled by default.</p></div></div><p>That summarizes controller deployment and key design aspects. With that, let's move to data plane preparation. Time to memorize what we did so far? Yes, let's do it. We have deployed NSX Manager (management plane) and registered the solution with vCenter Server. Later, we deployed NSX Controllers (control plane) and, finally, we are in the last phase of installation, which is the data plane.</p></div></div>
<div class="section" title="The NSX data plane"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>The NSX data plane</h1></div></div></div><p>The NSX data plane consists of <span class="strong"><strong>vSphere Distributed Switch</strong></span> (<span class="strong"><strong>VDS</strong></span>), kernel modules, User World Agents, NSX Edge, and Distributed routing/firewall and bridging modules. Firstly, let's discuss preparing ESXi clusters for NSX.</p><p>What is host preparation? All it does is install the kernel modules and builds a management and control plane domain. The kernel modules that we refer to here are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Distributed routing</li><li class="listitem" style="list-style-type: disc">Distributed firewall</li><li class="listitem" style="list-style-type: disc">VXLAN bridging</li></ul></div><p>In a multi-cluster environment, we need to perform this installation per cluster level. Based on the vSphere design, we can always introduce new hosts to the cluster and preparation is automated for newly added hosts, which makes life easier for architects.</p><div class="section" title="The host preparation procedure"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>The host preparation procedure</h2></div></div></div><p>Let's discuss how an ESXi host is prepared to push the kernel modules:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In vCenter, navigate to <span class="strong"><strong>Home</strong></span> | <span class="strong"><strong>Networking &amp; Security</strong></span> | <span class="strong"><strong>Installation</strong></span> and select the <span class="strong"><strong>Host Preparation</strong></span> tab.</li><li class="listitem">Select the respective cluster and click the gear icon and click <span class="strong"><strong>Install</strong></span> as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_03_019.jpg" alt="The host preparation procedure"/></div><p>
</p></li><li class="listitem">Monitor the installation. Repeat the same steps for other clusters.<p>Now, one common question from all vSphere folks: do we need to reboot the host since VIBS got installed? The answer is a <span class="strong"><strong>BIG FAT NO</strong></span>! Only after uninstallation scenarios do we need a host reboot. We humans tend to forget things easily, don't we? No problem, there would be a message populated near our ESXi host icon notifying <span class="emphasis"><em>Reboot Required</em></span>.</p></li><li class="listitem">For each vSphere cluster, we will go ahead and configure <span class="strong"><strong>
<span class="strong"><strong>VXLAN</strong></span>
</strong></span> networking prerequisites.</li></ol></div><p>Firstly, we will configure a <span class="strong"><strong>static pool</strong></span> for the <span class="strong"><strong>VTEP IP</strong></span> assignment, which is similar to the controller <span class="strong"><strong>IP pool</strong></span> configuration that we did earlier. The <span class="strong"><strong>DHCP</strong></span> pool assignment is also possible; however, in this case, I'm showcasing the IP assignment with static pools.</p><p>From NSX Manager, navigate to manage <span class="strong"><strong>IP pools </strong></span>and click on the <span class="strong"><strong>+</strong></span> sign. Update the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VTEP <span class="strong"><strong>Name</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Gateway</strong></span> address</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Prefix Length</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Static IP Pool </strong></span>for VTEP IP assignments.</li></ul></div><p>Have a look at the following screenshot for reference:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_020.jpg" alt="The host preparation procedure"/></div><p>
</p><p>Select one of the vSphere clusters and click the configure link provided in the VXLAN column:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select the vSphere distributed switch.</li><li class="listitem">Update the <span class="strong"><strong>VLAN</strong></span> number. Enter <code class="literal">0</code> if you're not using a VLAN, which will pass along untagged traffic.</li><li class="listitem">Ensure <span class="strong"><strong>MTU</strong></span> is <code class="literal">
<span class="strong"><strong>1600</strong></span>
</code> (VXLAN overhead)</li><li class="listitem">For VMKnic IP addressing, we need to make use of the earlier VTEP IP pool that we configured.</li></ol></div><p>
<span class="strong"><strong>VMKNic Teaming Policy</strong></span> method is used for bonding the vmnics (physical NICs) for use with the VTEP port group. I have selected for fail over. The other options are Static EtherChannel, LACP (Active), LACP (Passive), Load Balance by Source ID, Load Balance by Source MAC, and Enhanced LACP. The following screenshot is updated with a list of supported VXLAN NIC teaming policies:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_021.jpg" alt="The host preparation procedure"/></div><p>
</p><p>VTEP NIC teaming design is extremely critical in NSX environments. Most customers would go with single VTEP configuration primarily because of the simplicity in the design. However, if we have more than 10G VXLAN traffic, LACP or Static EtherChannel would be the preferred load balancing policy.</p><p>VTEP Value is the number of VTEPs per host. Is there any specific reason why we would go for multi VTEP configuration? Well, if we have more than one physical link that we would like to use for VXLAN traffic and the upstream switches do not support LACP the use of multiple VTEPs allows us to balance the traffic between physical links.</p><p>The following screenshot depicts the preceding step:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_022.jpg" alt="The host preparation procedure"/></div><p>
</p><p>Based on the requirement, we can repeat the same step for other clusters as well, and each such configuration will create a VXLAN distributed port group in vSphere.</p><p>Successful installation of VXLAN modules will show as <span class="strong"><strong>Configured</strong></span> as highlighted in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_023.jpg" alt="The host preparation procedure"/></div><p>
</p><p>I know most us will have a few design-related queries on VXLAN and how it works. Stay focused: we are going in the right direction and will discuss that in upcoming chapters.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Summary</h1></div></div></div><p>We started this chapter with an introduction to NSX Manager requirements and we covered all design aspects of the management plane. Later, we discussed NSX Controller requirements and key design decisions. Finally, we moved to data plane installation. In the next chapter, we will discuss managing and deploying NSX logical networks.</p></div></body></html>