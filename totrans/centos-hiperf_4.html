<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;4.&#xA0;Real-world Implementations of Clustering" id="SJGS1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Real-world Implementations of Clustering</h1></div></div></div><p class="calibre8">In this chapter, you will learn how to use your cluster in real-life scenarios by deploying a web server and a database server. Before we do this, we will need to review some fundamental concepts related to these key components, configure replicated storage so that files are kept in sync between nodes, and then finally, populate our database with sample data, which we will then query using a simple PHP application.</p><p class="calibre8">Since the programming side of things is out of the scope of this book, feel free to use some other programming language of your choice if you want to do so. I have chosen PHP for simplicity. Keep in mind that this book is not aimed at teaching you how to build web-based applications for use in a CentOS 7 cluster, but rather how to use it in order to provide high availability for those applications.</p><p class="calibre8">During the course of this chapter, you will notice that we will rely on the concepts introduced and the services configured in previous chapters as we dive into taking advantage of the cluster architecture that we have already put in place.</p></div>

<div class="book" title="Chapter&#xA0;4.&#xA0;Real-world Implementations of Clustering" id="SJGS1-1d2c6d19b9d242db82da724021b51ea0">
<div class="book" title="Setting up storage"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec24" class="calibre1"/>Setting up storage</h1></div></div></div><p class="calibre8">When we started discussing the fundamental concepts of clustering, we mentioned that high availability<a id="id148" class="calibre1"/> clusters aim, in simple terms, to minimize downtime of services by providing failover capabilities. As we begin the journey of installing a web server and a database server in our cluster, we can't help but wonder how will we synchronize between nodes the content that those services should make available to us. We need to find a way for nodes to share a piece of common storage where data will be saved. If one node fails to provide access to it, the other node will take client requests from then on.</p><p class="calibre8">In Linux, a common and cost-free method of dealing with this question is an open source technology <a id="id149" class="calibre1"/>known as <span class="strong"><strong class="calibre2">Distributed Replicated Block Device</strong></span> (<span class="strong"><strong class="calibre2">DRBD</strong></span>), which makes it possible to mirror or replicate individual storage devices (such as hard disks or partitions) from one node to the other(s) over a network connection. In a somewhat high-level explanation, you can think of the functionality<a id="id150" class="calibre1"/> offered by DRBD as a network-based RAID-1. Its basic structure and data flow are illustrated in the following figure:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="Setting up storage" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre8">All replicated data sets, such as a shared storage device, are called a resource in DRBD and should not be confused with a PCS resource, as discussed in previous chapters.</p></div><p class="calibre8">In order to install DRBD, you will need to enable the ELRepo repository on both nodes, because this <a id="id151" class="calibre1"/>software package is not distributed through the standard CentOS repositories. Here is a brief explanation of the purpose and contents of the ELRepo repository:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">The first step consists of importing the GPG key that is used to sign the <code class="email">rpm</code> package, which represents the foundation to the repository. Should you try to install the package using rpm before importing the key, the installation will fail as a security measure.</li><li class="listitem" value="2">Run the following commands on both nodes:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org</strong></span>
<span class="strong"><strong class="calibre2">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm</strong></span></pre></div></li><li class="listitem" value="3">You can verify that ELRepo has been added to your configured repositories with the <a id="id152" class="calibre1"/>following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">yum repolist | grep elrepo</strong></span></pre></div><p class="calibre15">The output should be similar to the one shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00034.jpeg" alt="Setting up storage" class="calibre10"/></div><p class="calibre13"> </p><div class="note" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre8">Alternatively, you can explicitly disable ELRepo after installing the <code class="email">rpm</code> packages that add it to your system and enable it only to install the necessary packages (for precaution, make sure you make a copy of the original repository configuration file first):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cp /etc/yum.repos.d/elrepo.repo /etc/yum.repos.d/elrepo.repo.ORG </strong></span>
<span class="strong"><strong class="calibre2">sed -i "s/enabled=1/enabled=0/g" /etc/yum.repos.d/elrepo.repo</strong></span>
<span class="strong"><strong class="calibre2">yum --enablerepo elrepo update</strong></span>
<span class="strong"><strong class="calibre2">yum --enablerepo elrepo install -y drbd84-utils kmod-drbd84</strong></span></pre></div></div></li><li class="listitem" value="4">Then, use the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">yum update &amp;&amp; yum install drbd84-utils kmod-drbd84</strong></span></pre></div><p class="calibre15">It will install the necessary management utilities, along with the corresponding kernel module for DRBD. Once this process is complete, you will need to check whether the module is loaded, using this command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">lsmod | grep -i drbd</strong></span></pre></div><p class="calibre15">If it is <a id="id153" class="calibre1"/>not loaded automatically, you can load the module to the kernel on both nodes, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">modprobe drbd</strong></span></pre></div></li></ol><div class="calibre14"/></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note15" class="calibre1"/>Note</h3><p class="calibre8">Note that <code class="email">modprobe</code> command will take care of loading the kernel module for the time<a id="id154" class="calibre1"/> being on your current session. However, in order for it to be loaded during boot, you have to make use of the systemd-modules-load service by creating a file inside <code class="email">/etc/modules-load.d/</code> so that the DRBD module is loaded properly each time the system boots:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">echo drbd &gt;/etc/modules-load.d/drbd.conf</strong></span></pre></div></div></div></div>
<div class="book" title="ELRepo repository and DRBD availability" id="TI1E1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec25" class="calibre1"/>ELRepo repository and DRBD availability</h1></div></div></div><p class="calibre8">ELRepo is a community repository for Linux distributions that are compatible with Red Hat Enterprise Linux, which CentOS and Scientific Linux are derivatives of. ELRepo has hardware-related packages (especially drivers) as the primary focus in order to enhance or provide<a id="id155" class="calibre1"/> functionality that is not present in the current kernel. Thus, by installing the corresponding<a id="id156" class="calibre1"/> package, you save yourself from the pain of having to recompile the kernel only to add a certain feature, or having to wait for it to be supported by upstream repositories, or for the feature to be included in a later kernel release. The ELRepo repository is maintained by active members of the related distributions (RHEL, CentOS, and Scientific Linux).</p><p class="calibre8">DBRD, as made available by ELRepo, is intended primarily to evaluate and get experience with DRBD on RHEL-based platforms, but is not officially supported by Red Hat and LINBIT, the creators of DRBD. However, following the procedures outlined in this chapter and throughout the rest of this book, you can ensure that all of the necessary functionality will be available in your cluster.</p><p class="calibre8">Once we have installed the packages mentioned earlier, we need to allocate the physical space that will be used to store the replicated contents on both servers. With scalability in mind, we will use the <span class="strong"><strong class="calibre2">Logical Volume Manager</strong></span> (<span class="strong"><strong class="calibre2">LVM</strong></span>) technology to create dynamic hard disk partitions<a id="id157" class="calibre1"/> that are easily resizable down the road if we need to.</p><p class="calibre8">To begin with, we will add a 2 GB hard disk to each node. The purpose of this hard disk is to serve as the underlying filesystem for a PHP application accessed by the Apache web server.</p><p class="calibre8">I chose this size because it will be enough to store all the necessary files to be replicated, and because Virtualbox allows you to pick arbitrary sizes for storage disks. If you happen to be using real hardware as you follow along with this book, you may want to choose a different size accordingly.</p><p class="calibre8">To add a <a id="id158" class="calibre1"/>virtual hard disk to an existing virtual machine in Virtualbox, follow these steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Turn off the <span class="strong"><strong class="calibre2">VM</strong></span></li><li class="listitem" value="2">Right-click on it in Virtualbox's initial screen</li><li class="listitem" value="3">From the contextual menu, choose <span class="strong"><strong class="calibre2">Settings</strong></span> and then <span class="strong"><strong class="calibre2">Storage</strong></span></li><li class="listitem" value="4">Select <span class="strong"><strong class="calibre2">Controller: SATA</strong></span>, and click on <span class="strong"><strong class="calibre2">Add hard disk</strong></span> and then click on <span class="strong"><strong class="calibre2">Create new disk</strong></span></li><li class="listitem" value="5">Choose <span class="strong"><strong class="calibre2">Virtual Disk Image (VDI)</strong></span> and <span class="strong"><strong class="calibre2">Dynamically Allocated</strong></span> and proceed to next step</li><li class="listitem" value="6">Finally, assign a name for the device and choose 2 GB as size</li></ol><div class="calibre14"/></div><p class="calibre8">After starting <a id="id159" class="calibre1"/>and booting up<a id="id160" class="calibre1"/> each node, we should issue the following command in order to identify the newly added disk (the new disk will be, in our case, the one that is not partitioned yet):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">ls -l /dev | grep -Ei sd[a-z]</strong></span></pre></div><p class="calibre8">We will identify the newly added disk with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">dmesg | grep sdb</strong></span></pre></div><p class="calibre8">Here, <code class="email">/dev/sdb</code> is the new disk ID, as returned by listing the contents of the <code class="email">/dev</code> directory earlier:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node02 ~]# dmesg | grep sdb</strong></span>
<span class="strong"><strong class="calibre2">[  2.484257] sd 3:0:0:0: [sdb] 4194304 512-byte logical blocks: (2.14 GB/2.00 GiB)</strong></span>
<span class="strong"><strong class="calibre2">[  2.484258] sd 3:0:0:0: [sdb] Write Protect is off</strong></span>
<span class="strong"><strong class="calibre2">[  2.484258] sd 3:0:0:0: [sdb] Mode Sense: 00 3a 00 00</strong></span>
<span class="strong"><strong class="calibre2">[  2.484258] sd 3:0:0:0: [sdb] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA</strong></span>
<span class="strong"><strong class="calibre2">[  2.487361]  sdb: unknown partition table</strong></span>
<span class="strong"><strong class="calibre2">[  2.498564] sd 3:0:0:0: [sdb] Attached SCSI disk</strong></span>
<span class="strong"><strong class="calibre2">[root@node02 ~]#</strong></span></pre></div><p class="calibre8">Now, let's create a partition on the disk, the corresponding physical volume, a volume group (<code class="email">drbd_vg</code>), and finally, a logical volume (<code class="email">drbd_vol</code>) on top. Make sure you repeat these steps on each node, changing the device (<code class="email">dev/sdX</code>) as needed:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">parted /dev/sdb mkl</strong></span><span class="strong"><strong class="calibre2">abel msdos</strong></span>
<span class="strong"><strong class="calibre2">parted /dev/sdb mkpart</strong></span><span class="strong"><strong class="calibre2"> p 0% 100%</strong></span>
<span class="strong"><strong class="calibre2">pvcreate /dev/sdb1</strong></span>
<span class="strong"><strong class="calibre2">vgcreate drbd_vg /dev/sdb1</strong></span>
<span class="strong"><strong class="calibre2">lvcreate -n drbd_vol -l 100%FREE drbd_vg</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note16" class="calibre1"/>Note</h3><p class="calibre8">You can<a id="id161" class="calibre1"/> check the <a id="id162" class="calibre1"/>status of the newly created logical volume with <code class="email">lvdisplay /dev/drbd_vg/drbd_vol</code>.</p></div></div>
<div class="book" title="Configuring DRBD"><div class="book" id="UGI02-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec26" class="calibre1"/>Configuring DRBD</h1></div></div></div><p class="calibre8">After having successfully created and partitioned our DRBD disks on each node, the main<a id="id163" class="calibre1"/> configuration file for DRBD is located in <code class="email">/etc/drbd.conf</code>, which consists only of the following two lines:</p><div class="informalexample"><pre class="programlisting">include "drbd.d/global_common.conf";
include "drbd.d/*.res";</pre></div><p class="calibre8">Both lines include relative paths, starting at <code class="email">/etc/</code>, of the actual configuration files. In the <code class="email">global_common.conf</code> file, you will find the global settings for your DRBD installation, along with the common section (which defines those settings that should be inherited by every resource) of the DRBD configuration. On the other hand, in the <code class="email">.res</code> files, you will find the specific configuration for each DRBD resource.</p><p class="calibre8">We will now rename the existing <code class="email">global_common.conf</code> file as <code class="email">global_common.conf.orig</code> (as a backup copy of the original settings) with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mv /etc/drbd.d/global_common.conf /etc/drbd.d/global_common.conf.orig</strong></span></pre></div><p class="calibre8">We will then create a new <code class="email">global_common.conf</code> file with the following contents by opening the file with your preferred text editor:</p><div class="informalexample"><pre class="programlisting">global {
 usage-count no;
}
common {
 net {
  protocol C;
 }
}</pre></div><p class="calibre8">Once you created the preceding file on one node (say, <code class="email">node01</code>), you can easily copy it to the another node, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">ssh node02 mv /etc/drbd.d/global_common.conf /etc/drbd.d/global_common.conf.orig</strong></span>
<span class="strong"><strong class="calibre2">scp /etc/drbd.d/global_common.conf node02:/etc/drbd.d/</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note17" class="calibre1"/>Note</h3><p class="calibre8">You should make it a habit to make backup copies of the original configuration files so that you can roll back to previous settings, should something go wrong at any time.</p></div><p class="calibre8">The <code class="email">usage-count no</code> line in the global section skips sending a notice to the DRBD team each time a <a id="id164" class="calibre1"/>new version of the software is installed in your system. You could change it to <code class="email">yes</code> if you want to submit information from your system. Alternatively, you could change it to <code class="email">ask</code> if you want to be prompted for a decision each time you do an upgrade. Either way, you should know that they use this information for statistical analysis only, and their reports are always available<a id="id165" class="calibre1"/> to the public at <a class="calibre1" href="http://usage.drbd.org/cgi-bin/show_usage.pl">http://usage.drbd.org/cgi-bin/show_usage.pl</a>.</p><p class="calibre8">The <code class="email">protocol C</code> line tells the DRBD resource to use a fully synchronous replication, which means that local write operations on the node that is functioning as primary are considered completed only after both the local and remote disk writes have been confirmed. Thus, if you run into the loss of a single node, that should not lead to any data loss under normal circumstances, unless both nodes (or their storage subsystems) are irreversibly destroyed at the same time.</p><p class="calibre8">Next, we will need to create a specific new configuration file file (called <code class="email">/etc/drbd.d/drbd0.res</code>) for our resource, which we will name <code class="email">drbd0</code>, with the following contents (where <code class="email">192.168.0.2</code> and <code class="email">192.168.0.3</code> are the IP addresses of our two nodes, and <code class="email">7789</code> is the port used for communication):</p><div class="informalexample"><pre class="programlisting">resource drbd0 {
     disk /dev/drbd_vg/drbd_vol;
     device /dev/drbd0;
     meta-disk internal;
     on node01 {
             address 192.168.0.2:7789;
     }
     on node02 {
             address 192.168.0.3:7789;
     }
}</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note18" class="calibre1"/>Note</h3><p class="calibre8">You can look up the meaning of each directive (and the rest as well) in the resource <a id="id166" class="calibre1"/>configuration file at Linbit's website at <a class="calibre1" href="http://drbd.linbit.com/users-guide-8.4/">http://drbd.linbit.com/users-guide-8.4/</a>.</p><p class="calibre8">TCP port <code class="email">7789</code> is the<a id="id167" class="calibre1"/> typical port number used in most DRBD installations. However, the official documentation states that DRBD (by convention) uses TCP ports from <code class="email">7788</code> upwards, with every resource listening on a separate port. In this chapter, since we are dealing with only one resource, we will only use port <code class="email">7789</code>—both in the only resource configuration file and in the firewall settings on both nodes. It is essential that you remember to <a id="id168" class="calibre1"/>open this port in the firewall, because otherwise, the resources will not be able to synchronize later.</p><p class="calibre8">To open the <code class="email">7789</code> TCP port in the firewall configuration, execute the following commands on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 7789 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">service iptables save</strong></span></pre></div></div><p class="calibre8">Again, you <a id="id169" class="calibre1"/>can copy this file to the other node, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">scp /etc/drbd.d/drbd0.res node02:/etc/drbd.d/</strong></span></pre></div><p class="calibre8">When we installed DRBD earlier, a utility called <code class="email">drbdadm</code> was installed as well, which, as you will be able to guess from its name, is intended to be used for the administration of DRBD resources, such as our newly configured volume The first step in starting and bringing a DRBD resource online is to initialize its metadata (you may need to change the resource name if you set a different name in the configuration file previously). Note that the <code class="email">/var/lib/drbd</code> directory is needed beforehand. If it was not created previously when you installed DRBD, create it manually before proceeding, using the following lines of code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mkdir /var/lib/drbd</strong></span>
<span class="strong"><strong class="calibre2">drbdadm create-md drbd0</strong></span></pre></div><p class="calibre8">These lines should result in the following output, with the corresponding confirmation message that indicates a successful creation of the metadata for the device:</p><div class="mediaobject"><img src="../images/00035.jpeg" alt="Configuring DRBD" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre8">The word "metadata" has been defined as data about the data. In the context of DRBD resources, the metadata of a resource consists of several pieces of information about the device and the data that is kept in it. The <code class="email">drbdadm create-md [drbd resource]</code> command will return useful debugging information if something does not work as expected.</p></div><p class="calibre8">The next <a id="id170" class="calibre1"/>step consists of enabling <code class="email">drbd0</code> in order to finish allocating both disk and network resources for its operation:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm up drbd0</strong></span></pre></div><p class="calibre8">You can verify the status of the resource by taking a look at the <code class="email">/proc</code> virtual filesystem, which allows you to view the system's resources as the kernel sees them, as you can see in the following screenshot. However, make sure you have followed the instructions outlined earlier on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cat /proc/drbd</strong></span></pre></div><p class="calibre8">Take a look at the following screenshot:</p><div class="mediaobject"><img src="../images/00036.jpeg" alt="Configuring DRBD" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note that the status of the device shows as unknown and inconsistent since we haven't indicated yet which of the DRBD devices (one in each node) will act as a primary device and which one as a secondary device. At this point, given our current scenario where we have set up two DRBD devices from scratch, it does not matter which one you choose to be primary. However, if we had used one device with data already residing in it, it is crucial that you select that one device as the primary resource. Otherwise, you run the serious risk of losing your data.</p><p class="calibre8">Run this command in order to mark one device as primary and to perform the initial synchronization. You only need to do this in the node that has the primary resource (in our example, this means <code class="email">node01</code>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm primary --force drbd0</strong></span></pre></div><p class="calibre8">As you did earlier, you can check the current status of the synchronization while it's being performed. The cat <code class="email">/proc/drbd</code> command displays the creation and synchronization progress of the resource, as shown here:</p><div class="mediaobject"><img src="../images/00037.jpeg" alt="Configuring DRBD" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, with the help of <code class="email">drbd-overview</code> command, as its name implies, you can see an overview <a id="id171" class="calibre1"/>of the currently configured DRBD resources. In this case, you should see that <code class="email">node01</code> is acting as primary and <code class="email">node02</code> as secondary, as indicated by running the command on both nodes (which can also be seen in the following screenshot):</p><p class="calibre8">In <code class="email">node01</code> : the drbd-overview command should return:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">0:drbd0/0  Connected Primary/Secondary UpToDate/UpToDate</strong></span></pre></div><p class="calibre8">Whereas in <code class="email">node02</code> you should see:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">0:drbd0/0  Connected Secondary/Primary UpToDate/UpToDate</strong></span></pre></div><div class="mediaobject"><img src="../images/00038.jpeg" alt="Configuring DRBD" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Finally, we need to create a filesystem on <code class="email">/dev/drbd0</code> in <code class="email">node01</code>. You can choose whatever suits your needs or requirements, if any. <code class="email">Ext4</code> is a good choice if you have not decided which one to use. XFS is the default filesystem for CentOS 7 out of the box. However, it is not possible to resize it if we need to do so at a later time, should we run into a more complex setup for the underlying storage needed for the operation of the web and database servers.</p><p class="calibre8">Run the following command on the primary node to create an <code class="email">ext4</code> filesystem on <code class="email">/dev/drbd0</code> and wait until it completes, as shown in the following screenshot:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mkfs.ext4 /dev/drbd0</strong></span></pre></div><div class="mediaobject"><img src="../images/00039.jpeg" alt="Configuring DRBD" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, your<a id="id172" class="calibre1"/> DRBD resource is ready to be used as usual. You can now mount it and start saving files to it. However, we still need to add it as a cluster resource before we can start using it as a highly available and fail-safe component. This is what we will do in the next section.</p><p class="calibre8">It is very important that you create the filesystem on the resource from <code class="email">node01</code>, our primary node. Otherwise, you will run into a mounting issue that is caused when you try to add a filesystem from a node that is not the primary member of the cluster.</p></div>
<div class="book" title="Adding DRBD as a PCS cluster resource"><div class="book" id="VF2I2-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec27" class="calibre1"/>Adding DRBD as a PCS cluster resource</h1></div></div></div><p class="calibre8">You will recall how in <a class="calibre1" title="Chapter 2. Installing Cluster Services and Configuring Network Components" href="part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0">Chapter 2</a>, <span class="strong"><em class="calibre9">Installing Cluster Services and Configuring Network Components</em></span>, we added a virtual IP address to the cluster. Now, it's time to do the same <a id="id173" class="calibre1"/>with the DRBD resource that we have just created and configured.</p><p class="calibre8">Before doing<a id="id174" class="calibre1"/> that, however, we must point out that one of the most distinguishing features of the PCS command-line tool that we first introduced back in <a class="calibre1" title="Chapter 2. Installing Cluster Services and Configuring Network Components" href="part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0">Chapter 2</a>, <span class="strong"><em class="calibre9">Installing Cluster Services and Configuring Network Components</em></span>, is its ability to save the current cluster configuration to a file, to which you can add further settings using command-line tools. Then, you can use the resulting file to update the running cluster configuration.</p><p class="calibre8">To retrieve the cluster<a id="id175" class="calibre1"/> configuration from the <span class="strong"><strong class="calibre2">Cluster Information Base</strong></span> (<span class="strong"><strong class="calibre2">CIB</strong></span>) and save it to a file named <code class="email">drbd0_conf</code> in the current working directory, use the following command to make sure you started the cluster first:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster start --all</strong></span></pre></div><p class="calibre8">Then save the cluster configuration to the file mentioned earlier (<code class="email">drbd0_conf</code> will be created automatically):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster cib drbd0_conf</strong></span></pre></div><p class="calibre8">Next, we will add the DRBD device as a PCS cluster resource. Note the <code class="email">-f</code> switch, which indicates that changes resulting from the following command should be appended to the <code class="email">drbd0_conf</code> file. The following command must be executed from the same directory as the previous command (meaning the directory where the <code class="email">drbd0_conf</code> file is located):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f drbd0_conf resource create web_drbd ocf:linbit:drbd drbd_resource=drbd0 op monitor interval=60s</strong></span></pre></div><p class="calibre8">Finally, we need to make sure that the resource will run on both nodes simultaneously by adding a clone resource (a special type of resource that should be active on multiple hosts at the same time) for that purpose:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f drbd0_conf resource master web_drbd_clone web_drbd master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true</strong></span></pre></div><p class="calibre8">At this point, we can update the cluster configuration using the <code class="email">drbd0_conf</code> file. However, a quick inspection of the cluster status and its resources will allow us to better visualize the changes if we run <code class="email">pcs status</code> command before and after updating the global configuration, in that order:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status</strong></span>
<span class="strong"><strong class="calibre2">pcs cluster cib-push drbd0_conf</strong></span></pre></div><p class="calibre8">The last command should result in the following message if the update was successful:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">CIB updated</strong></span></pre></div><p class="calibre8">Now, let's check the current cluster configuration again:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status</strong></span></pre></div><p class="calibre8">In the case the last PCS status indicates some failure event (most likely related to SELinux policies and less likely with regular file permissions), you should inspect the <code class="email">/var/log/audit/audit.log</code> file to start your troubleshooting. Lines starting with AVC will point out the places where you need to look first. Here is an example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">type=AVC msg=audit(1429116572.153:295): avc:  denied  { read write } for  pid=24192 comm="drbdsetup-84" name="drbd-147-0" dev="tmpfs" ino=20373 scontext=system_u:system_r:drbd_t:s0 tcontext=unconfined_u:object_r:var_lock_t:s0 tclass=file</strong></span></pre></div><p class="calibre8">The<a id="id176" class="calibre1"/> preceding<a id="id177" class="calibre1"/> error message seems to indicate that SELinux is denying the <code class="email">drbdsetup-84</code> executable read/write access to the temporary <code class="email">tmpfs</code> filesystem. Its corresponding denied system call supports this theory:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">type=SYSCALL msg=audit(1429116572.153:295): arch=c000003e syscall=2 success=no exit=-13 a0=125e080 a1=42 a2=180 a3=7fff42b39f80 items=0 ppid=24191 pid=24192 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm="drbdsetup-84" exe="/usr/lib/drbd/drbdsetup-84" subj=system_u:system_r:drbd_t:s0 key=(null)</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre8">NSA Security-Enhanced Linux (SELinux) is an implementation of a flexible mandatory access control architecture in Linux. You can disable it to perform the following steps (but it is strongly recommended that you don't) if you experience several issues with it at first. If you choose to disable SELinux by editing <code class="email">/etc/sysconfig/selinux</code>, do not forget to clean the resource error count with <code class="email">pcs resource cleanup [resource_id]</code>, where <code class="email">resource_id</code> is the name of the resource as returned by <code class="email">pcs resource show</code>.</p></div><p class="calibre8">To clear all doubts, install the <code class="email">policycoreutils-python</code> package (which contains the management tools used to manage an SELinux environment):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">yum update &amp;&amp; yum install policycoreutils-python</strong></span></pre></div><p class="calibre8">Use the <code class="email">audit2allow</code> utility included in it to view the reason of access denied in human-readable form and then generate an SELinux policy-allow rule based on logs of denied operations. The following command will output the last line in the <code class="email">audit.log</code> file where the word AVC appears and then pipe it to <code class="email">audit2allow</code> to produce the result in human-readable form:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cat /var/log/audit/audit.log | grep AVC | tail -1 | audit2allow -w -a</strong></span></pre></div><p class="calibre8">As shown in the following screenshot, we can confirm that access was denied due to a missing type enforcement rule:</p><div class="mediaobject"><img src="../images/00040.jpeg" alt="Adding DRBD as a PCS cluster resource" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now <a id="id178" class="calibre1"/>that we know what is causing the problem, let's create a policy package in order to implement the necessary type enforcement rule into a module whose name is specified<a id="id179" class="calibre1"/> in the command line:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">cat /var/log/audit/audit.log | grep AVC | tail -1 | audit2allow -a -M drbd0_access_0</strong></span></pre></div><p class="calibre8">If you do <code class="email">ls -l</code> in your current working directory, you will find that the preceding command created a type enforcement file (<code class="email">drbd_access_0.te</code>) and compiled it into a policy package (<code class="email">drbd_access_0.pp</code>), which you will need to activate with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">semodule -i drbd0_access_0.pp</strong></span></pre></div><p class="calibre8">The preceding command can take about a minute to complete, so do not worry if this is the case for you, as you can see in the following screenshot, no output means a successful operation:</p><div class="mediaobject"><img src="../images/00041.jpeg" alt="Adding DRBD as a PCS cluster resource" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, we need to copy the module to <code class="email">node02</code> and install it there. This is one of the reasons why we set up key-based authentication between nodes in <a class="calibre1" title="Chapter 1. Cluster Basics and Installation on CentOS 7" href="part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0">Chapter 1</a>, <span class="strong"><em class="calibre9">Cluster Basics and Installation on CentOS 7</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">scp drbd0_access_0.pp node02:~</strong></span></pre></div><p class="calibre8">Then, run the following command in <code class="email">node02</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">semodule -i drbd0_access_0.pp</strong></span></pre></div><p class="calibre8">Alternatively, you can execute the following command in <code class="email">node01</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">ssh node02 semodule -i drbd0_access_0.pp</strong></span></pre></div><p class="calibre8">In<a id="id180" class="calibre1"/> addition, the<a id="id181" class="calibre1"/> SELinux <code class="email">daemons_enable_cluster_mode</code> policy should be set to true on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">setsebool -P daemons_enable_cluster_mode 1</strong></span></pre></div><p class="calibre8">Then, you may need to repeat this process more than once if the output of <code class="email">pcs status</code> shows further errors. If you find that you have to repeat it several times, you may want to consider setting SELinux to permissive so that it will still issue warnings instead of blocking the cluster resource. Then, you can continue with the setup for the time being and debug later.</p><p class="calibre8">We can see that both nodes are online, and the cluster resources are properly started, as shown here:</p><div class="mediaobject"><img src="../images/00042.jpeg" alt="Adding DRBD as a PCS cluster resource" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, let's <a id="id182" class="calibre1"/>give DRBD a rest for a brief moment, and let's focus on the installation of the web<a id="id183" class="calibre1"/> and database servers. Note that we will also revisit this topic in <a class="calibre1" title="Chapter 5. Monitoring the Cluster Health" href="part0041_split_000.html#173721-1d2c6d19b9d242db82da724021b51ea0">Chapter 5</a>, <span class="strong"><em class="calibre9">Monitoring the Cluster Health</em></span>, where we will simulate and troubleshoot issues. Note that if you reboot a node or both of them, nodes may detect a split-brain situation at this point, which we will fix manually (as that is the method that is recommended by LINBIT) later during the next chapter, when we troubleshoot the most common issues that may come up during the cluster operation.</p></div>
<div class="book" title="Installing the web and database servers" id="10DJ41-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec28" class="calibre1"/>Installing the web and database servers</h1></div></div></div><p class="calibre8">As of the time <a id="id184" class="calibre1"/>of writing this book, the Apache HTTP server (or just<a id="id185" class="calibre1"/> Apache for short) remains the world's most widely used web server<a id="id186" class="calibre1"/> and is often used within what is called a <span class="strong"><strong class="calibre2">LAMP stack</strong></span>. In this stack, a Linux distribution is used as the operating system, Apache as the web server, MySQL/MariaDB as the database server, and PHP as the server-side programming language for applications. Each one of these components is free, and these technologies are widely spread and thus easy to learn/get help on.</p><p class="calibre8">To install the Apache and MariaDB (a free and open source fork of MySQL) servers, run the following commands on each node. Note that this will install PHP as well:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">yum update &amp;&amp; yum install httpd mariadb mariadb-server php</strong></span></pre></div><p class="calibre8">Upon successful installation, we will proceed as we did earlier. To begin, let's enable and start the web server on both nodes:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl enable httpd</strong></span>
<span class="strong"><strong class="calibre2">systemctl start httpd</strong></span></pre></div><p class="calibre8">Don't forget to make sure that Apache is running:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl status httpd</strong></span></pre></div><p class="calibre8">Allow traffic through TCP port <code class="email">80</code> in the firewall:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">service iptables save</strong></span></pre></div><p class="calibre8">At this point, you can fire up a web browser and point it to the individual IP addresses of the nodes (remember that we haven't added Apache as a cluster resource, and thus, we can't access the web server on the virtual IP that is common to both nodes). You should see Apache's welcome page, as shown in the following figure, where we can see that web server is running correctly on <code class="email">node02</code> (<code class="email">192.168.0.3</code> as per our initial setup):</p><div class="mediaobject"><img src="../images/00043.jpeg" alt="Installing the web and database servers" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, it is <a id="id187" class="calibre1"/>time to take a small step back. We will disable and<a id="id188" class="calibre1"/> stop Apache on both nodes so that the cluster will manage it when PCS is moving forward:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl disable httpd</strong></span>
<span class="strong"><strong class="calibre2">systemctl stop httpd</strong></span></pre></div><p class="calibre8">In order for Apache to listen on the virtual IP (to which we assigned <code class="email">192.168.0.4</code> as the IP address) and the loopback address (we will see why in just a minute), we need to modify the main configuration file <code class="email">(/etc/httpd/conf/httpd.conf</code>), as follows (you may want to make a backup of this file first):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2"># Listen: Allows you to bind Apache to specific IP addresses and/or</strong></span>
<span class="strong"><strong class="calibre2"># ports, instead of the default. See also the &lt;VirtualHost&gt;</strong></span>
<span class="strong"><strong class="calibre2"># directive.</strong></span>
<span class="strong"><strong class="calibre2">#</strong></span>
<span class="strong"><strong class="calibre2"># Change this to Listen on specific IP addresses as shown below to</strong></span>
<span class="strong"><strong class="calibre2"># prevent Apache from glomming onto all bound IP addresses.</strong></span>
<span class="strong"><strong class="calibre2">#</strong></span>
<span class="strong"><strong class="calibre2">#Listen 12.34.56.78:80</strong></span>
<span class="strong"><strong class="calibre2">Listen 192.168.0.4:80</strong></span>
<span class="strong"><strong class="calibre2">Listen 127.0.0.1</strong></span></pre></div><p class="calibre8">Then, restart Apache:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">systemctl restart httpd</strong></span></pre></div><p class="calibre8">Note that while restarting the web server in the second node, an error is to be expected since there is<a id="id189" class="calibre1"/> already a service running in that socket. However, that<a id="id190" class="calibre1"/> is normal, and now, you should be able to access the Apache welcome page by pointing your browser to the virtual IP.</p><p class="calibre8">The fun part is finding out which is the node in which the virtual IP was started, as shown in the following screenshot. If you get an error here instead, make sure <code class="email">virtual_ip</code> is started by PCS first:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs status | grep virtual_ip</strong></span></pre></div><div class="mediaobject"><img src="../images/00044.jpeg" alt="Installing the web and database servers" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, let's stop the cluster in that node, using the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource show virtual_ip</strong></span></pre></div><p class="calibre8">Then, on the other node, it should still indicate that the resource is active.</p><p class="calibre8">However, even when the virtual IP is failed over to <code class="email">node02</code>, the web server is not accessible through that resource because it wasn't started there in the first place. For this reason, we still need to configure Apache as a cluster resource so that it can be managed as such.</p></div>
<div class="book" title="Configuring the web server as a cluster resource"><div class="book" id="11C3M2-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec29" class="calibre1"/>Configuring the web server as a cluster resource</h1></div></div></div><p class="calibre8">You will recall from when we configured the virtual IP in <a class="calibre1" title="Chapter 2. Installing Cluster Services and Configuring Network Components" href="part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0">Chapter 2</a>, <span class="strong"><em class="calibre9">Installing Cluster Services and Configuring Network Components</em></span>, and when we added replicated <a id="id191" class="calibre1"/>storage earlier during this chapter that <a id="id192" class="calibre1"/>we must indicate a way for PCS to check on a periodic basis whether the resource is available or not.</p><p class="calibre8">In this case, we will use the<a id="id193" class="calibre1"/> server status page (<code class="email">http://node0[1-2]/server-status</code>), which is the preferred Apache web page as it provides information about how well the server will be performing PCS will query this page once per minute. This is accomplished by creating a file named <code class="email">status.conf</code> inside <code class="email">/etc/httpd/conf.d</code> on both nodes:</p><div class="informalexample"><pre class="programlisting">&lt;Location /server-status&gt;
  SetHandler server-status
  Order deny,allow
  Deny from all
  Allow from 127.0.0.1
&lt;/Location&gt;</pre></div><p class="calibre8">Then, with the following command, we will add Apache as a cluster resource. The status of the resource will be checked by PCS once every minute:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource create webserver ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" op monitor interval=1min</strong></span></pre></div><p class="calibre8">By default, pacemaker will try to balance the resource usage over the cluster. However, at certain<a id="id194" class="calibre1"/> times, our setup will require that two <a id="id195" class="calibre1"/>related resources (as it is in the case of the web server and the virtual IP) need to run on the same host.</p><p class="calibre8">The web server should always run on the host on which the virtual IP is active. This also means that if the virtual IP resource is not active on any node, the web server should not run at all. In addition, since we need the web server to listen on the virtual IP address as well as on the loopback device on each host, it goes without saying that</p><p class="calibre8">We must ensure that the virtual IP resource is started before the web server resource.</p><p class="calibre8">We can accomplish both requirements through the use of the following constraints:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs constraint colocation add webserver with virtual_ip INFINITY</strong></span>
<span class="strong"><strong class="calibre2">pcs constraint order virtual_ip then webserver</strong></span></pre></div><p class="calibre8">After running the second command, you should see the following message on your screen. Note that starting the virtual IP resource before the web server is a mandatory requirement:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">Adding virtual_ip webserver (kind: Mandatory) (Options: first-action=start then-action=start)</strong></span></pre></div><p class="calibre8">Now, let's check the status of the cluster and focus on its assigned resources, as shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00045.jpeg" alt="Configuring the web server as a cluster resource" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">You can<a id="id196" class="calibre1"/> now simulate a failover by forcing <code class="email">node01</code> to<a id="id197" class="calibre1"/> go offline. To do so, you can run the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster stop</strong></span></pre></div><p class="calibre8">The resources should be automatically started on <code class="email">node02</code>, as indicated in the following screenshot:</p><div class="mediaobject"><img src="../images/00046.jpeg" alt="Configuring the web server as a cluster resource" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The last step <a id="id198" class="calibre1"/>consists of mounting the<a id="id199" class="calibre1"/> DRBD resource on the <code class="email">/var/html/www directory</code> and adding in it a simple PHP page to display the PHP configuration of the cluster. You will then be able to build on that simple example to add more sophisticated applications.</p><p class="calibre8">Before attempting to use <code class="email">/dev/drbd0</code>, we should check its status on both nodes with <code class="email">drbd-overview</code>. If the output shows StandAlone or WFConnection, we are looking at a split-brain situation, which can be confirmed in the output of the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">dmesg | grep -i brain </strong></span></pre></div><p class="calibre8">This will result in a <code class="email">Split-Brain detected, dropping connection!</code> error message.</p><p class="calibre8">Linbit recommends to manually resolve such cases by choosing a node whose modifications will be discarded and then issuing the following commands in it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm secondary [resource name] </strong></span>
<span class="strong"><strong class="calibre2">drbdadm connect --discard-my-data [resource name] </strong></span></pre></div><p class="calibre8">Then connect the DRBD resource on the other node:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm connect [resource name]</strong></span></pre></div><p class="calibre8">You can<a id="id200" class="calibre1"/> also start or stop DRBD and get an<a id="id201" class="calibre1"/> overview with the following commands in <code class="email">node01</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm up drbd0</strong></span>
<span class="strong"><strong class="calibre2">drbdadm down drbd0</strong></span>
<span class="strong"><strong class="calibre2">drbd-overview</strong></span>
<span class="strong"><strong class="calibre2">ssh node02 drbdadm up drbd0</strong></span>
<span class="strong"><strong class="calibre2">ssh node02 drbdadm down drbd0</strong></span>
<span class="strong"><strong class="calibre2">ssh node02 drbd-overview</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre8">Review the DRBD documentation carefully before choosing a recovery method after a split-brain situation. Since there is no one-size-fits-all answer to this issue, I have chosen to cover the recommended method in this book.</p></div></div>
<div class="book" title="Mounting the DRBD resource and using it with Apache" id="12AK81-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec30" class="calibre1"/>Mounting the DRBD resource and using it with Apache</h1></div></div></div><p class="calibre8">Before using<a id="id202" class="calibre1"/> the DRBD resource, you must define a filesystem on it<a id="id203" class="calibre1"/> and mount it on a local directory. We <a id="id204" class="calibre1"/>will use Apache's document root directory (<code class="email">/var/www/html</code>), but given the case, you could use a virtual host directory as well. As we did earlier, we will add these changes in a configuration file, step by step, and we will push it to the running CIB later on <code class="email">node01</code> (or whatever the DC is).</p><p class="calibre8">To begin, create a new configuration file named <code class="email">fs_dbrd0_cfg</code> (feel free to change the name if you want):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster cib fs_drbd0_cfg</strong></span></pre></div><p class="calibre8">Next, we'll create the filesystem resource itself (again, change the variable values if needed). This is another special type of resource provided out of the box:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg resource create web_fs Filesystem device="/dev/drbd0" directory="/var/www/html" fstype="ext4"</strong></span></pre></div><p class="calibre8">It indicates that the filesystem should always be available on the master DRBD resource:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg constraint colocation add web_fs with web_drbd_clone INFINITY with-rsc-role=Master</strong></span></pre></div><p class="calibre8">Note that in order for the filesystem to be started properly, <code class="email">/dev/drbd0</code> must be started first, so we will have to add a constraint for this purpose:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg constraint order promote web_drbd_clone then start web_fs</strong></span></pre></div><p class="calibre8">Finally, ensure that Apache needs to run on the same node as the filesystem resource, which also <a id="id205" class="calibre1"/>needs to come online before the web server <a id="id206" class="calibre1"/>resource can be started:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg constraint colocation add webserver with web_fs INFINITY</strong></span>
<span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg constraint order web_fs then webserver </strong></span></pre></div><p class="calibre8">You can <a id="id207" class="calibre1"/>review the configuration with the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs -f fs_drbd0_cfg constraint</strong></span></pre></div><p class="calibre8">The output is shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00047.jpeg" alt="Mounting the DRBD resource and using it with Apache" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If everything is correct, then push it to the running CIB with this command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster cib-push fs_drbd0_cfg </strong></span></pre></div><p class="calibre8">The preceding command should show CIB updated on successful completion.</p><p class="calibre8">If you now run <code class="email">pcs status</code>, you should see the newly added resources, as you can see in the following screenshot:</p><div class="mediaobject"><img src="../images/00048.jpeg" alt="Mounting the DRBD resource and using it with Apache" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, you<a id="id208" class="calibre1"/> don't need to manually mount <code class="email">/dev/drbd0</code> in <code class="email">/var/www/html</code>, because the cluster will take care of it. You can verify that the<a id="id209" class="calibre1"/> DRBD device has been mounted in <code class="email">/var/www/html</code> using<a id="id210" class="calibre1"/> this command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mount | grep drbd0</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre8">Remember <a id="id211" class="calibre1"/>that any original contents present in <code class="email">/var/html/www</code> will not be available while <code class="email">/dev/drbd0</code> is mounted.</p></div></div>
<div class="book" title="Testing the DRBD resource along with Apache" id="1394Q1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec31" class="calibre1"/>Testing the DRBD resource along with Apache</h1></div></div></div><p class="calibre8">As a <a id="id212" class="calibre1"/>simple test, we will display the information<a id="id213" class="calibre1"/> about the PHP installation. Create a file named <code class="email">info.php</code> inside <code class="email">/var/www/html</code> on <code class="email">node01</code> with the following contents:</p><div class="informalexample"><pre class="programlisting">&lt;?php
phpinfo();
?&gt;</pre></div><p class="calibre8">Now, point your browser to <code class="email">192.168.0.4/info.php</code> and verify that the output is similar to the one shown here:</p><div class="mediaobject"><img src="../images/00049.jpeg" alt="Testing the DRBD resource along with Apache" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Then, stop the cluster (<code class="email">pcs cluster stop</code>) on <code class="email">node01</code> or put it into the standby mode (<code class="email">pcs cluster standby node01</code>) and refresh the browser. The only thing that should change on the output is the system name, as shown in the following screenshot, since the <code class="email">phinfo()</code> PHP function returns the local hostname along with the information about the PHP installation:</p><div class="mediaobject"><img src="../images/00050.jpeg" alt="Testing the DRBD resource along with Apache" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In addition, if <a id="id214" class="calibre1"/>you list the contents of <code class="email">/var/www/html</code> on <code class="email">node02</code>, you will see that the <code class="email">info.php</code> file that was created originally<a id="id215" class="calibre1"/> on <code class="email">node01</code> now shows on <code class="email">node02</code> as well, as indicated in this screenshot:</p><div class="mediaobject"><img src="../images/00051.jpeg" alt="Testing the DRBD resource along with Apache" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Before proceeding, remember to return <code class="email">node01</code> to normal mode:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs cluster unstandby node01</strong></span></pre></div></div>
<div class="book" title="Setting up a high-availability database with replicated storage"><div class="book" id="147LC2-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec32" class="calibre1"/>Setting up a high-availability database with replicated storage</h1></div></div></div><p class="calibre8">The last <a id="id216" class="calibre1"/>part of this chapter <a id="id217" class="calibre1"/>focuses on setting up a HA MariaDB database with replicated storage. To begin, we will have to set up another DRBD resource as we did earlier. We will review the necessary steps here for clarity:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Add another virtual disk to each virtual machine (a 2 GB disk will do).</li><li class="listitem" value="2">Create a partition on the newly added disk and then go through the process of creating<a id="id218" class="calibre1"/> a <span class="strong"><strong class="calibre2">Physical Volume</strong></span> (<span class="strong"><strong class="calibre2">PV</strong></span>) on <code class="email">/dev/</code><code class="email">sdc1</code>, a <a id="id219" class="calibre1"/><span class="strong"><strong class="calibre2">Volume Group</strong></span> (<span class="strong"><strong class="calibre2">VG</strong></span>, named <code class="email">drbd_db_vg</code>), and <a id="id220" class="calibre1"/>finally a <span class="strong"><strong class="calibre2">Logical Volume</strong></span> (<span class="strong"><strong class="calibre2">LV</strong></span>, <code class="email">drbd_db_vol</code>):<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">parted /dev/sdc mklabel msdos</strong></span>
<span class="strong"><strong class="calibre2">parted /dev/sdc mkpart p 0% 100%</strong></span>
<span class="strong"><strong class="calibre2">pvcreate /dev/sdc1</strong></span>
<span class="strong"><strong class="calibre2">vgcreate drbd_db_vg /dev/sdc1</strong></span>
<span class="strong"><strong class="calibre2">lvcreate -n drbd_db_vol -l 100%FREE drbd_db_</strong></span><span class="strong"><strong class="calibre2">vg</strong></span></pre></div></li><li class="listitem" value="3">Create<a id="id221" class="calibre1"/> a<a id="id222" class="calibre1"/> configuration file (<code class="email">/etc/drbd.d/drbd1.res</code>) for the new DRBD resource (<code class="email">drbd1</code>), and based on the configuration file for the first replicated storage resource, edit the settings accordingly and use a different port:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">resource drbd1 {</strong></span>
<span class="strong"><strong class="calibre2">        disk /dev/drbd_db_vg/drbd_db_vol;</strong></span>
<span class="strong"><strong class="calibre2">        device /dev/drbd1;</strong></span>
<span class="strong"><strong class="calibre2">        meta-disk internal;</strong></span>
<span class="strong"><strong class="calibre2">        on node01 {</strong></span>
<span class="strong"><strong class="calibre2">                address 192.168.0.2:7790;</strong></span>
<span class="strong"><strong class="calibre2">        }</strong></span>
<span class="strong"><strong class="calibre2">        on node02 {</strong></span>
<span class="strong"><strong class="calibre2">                address 192.168.0.3:7790;</strong></span>
<span class="strong"><strong class="calibre2">        }</strong></span>
<span class="strong"><strong class="calibre2">}</strong></span></pre></div><p class="calibre15">The, add a firewall rule to allow traffic:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 7790 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">service iptables save</strong></span></pre></div></li><li class="listitem" value="4">Repeat the previous steps on the second node. Initialize the metadata for the new DRBD resource on both nodes:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm create-md dbrd1</strong></span></pre></div></li><li class="listitem" value="5">Enable the replicated storage resource in order to allocate disk and network resources for its operation:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm up drbd1</strong></span></pre></div></li><li class="listitem" value="6">Mark the DRBD device on the DC node as primary:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">drbdadm primary --force drbd1</strong></span></pre></div></li><li class="listitem" value="7">Add the new DRBD device as cluster resource:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mkdir -p /var/lib/mariadb_drbd1/data</strong></span>
<span class="strong"><strong class="calibre2">pcs cluster cib drbd1_conf</strong></span>
<span class="strong"><strong class="calibre2">pcs -f drbd1_conf resource create db_drbd ocf:linbit:drbd drbd_resource=drbd1 op monitor interval=60s</strong></span>
<span class="strong"><strong class="calibre2">pcs -f drbd1_conf resource master db_drbd_clone db_drbd master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true</strong></span>
<span class="strong"><strong class="calibre2">pcs -f fs_drbd1_cfg resource create db_fs Filesystem device="/dev/drbd1" directory="/var/lib/mariadb_drbd1" fstype="ext4"</strong></span>
<span class="strong"><strong class="calibre2">pcs cluster cib-push drbd1_conf</strong></span></pre></div></li></ol><div class="calibre14"/></div><p class="calibre8">When<a id="id223" class="calibre1"/> this process is<a id="id224" class="calibre1"/> complete, the overview of all configured DRBD resources up until this point should look like this:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# cat /proc/drbd</strong></span>
<span class="strong"><strong class="calibre2">version: 8.4.6 (api:1/proto:86-101)</strong></span>
<span class="strong"><strong class="calibre2">GIT-hash: 833d830e0152d1e457fa7856e71e11248ccf3f70 build by phil@Build64R7, 2015-04-10 05:13:52</strong></span>
<span class="strong"><strong class="calibre2"> 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----</strong></span>
<span class="strong"><strong class="calibre2">    ns:98324 nr:0 dw:32888 dr:66457 al:11 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0</strong></span>
<span class="strong"><strong class="calibre2"> 1: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r-----</strong></span>
<span class="strong"><strong class="calibre2">    ns:2092956 nr:0 dw:33996 dr:2094412 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]# drbd-overview</strong></span>
<span class="strong"><strong class="calibre2"> 0:drbd0/0  Connected Primary/Secondary UpToDate/UpToDate /var/www/html ext4 2.0G 6.1M 1.9G 1%</strong></span>
<span class="strong"><strong class="calibre2"> 1:drbd1/0  Connected Primary/Secondary UpToDate/UpToDate</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]#</strong></span></pre></div><p class="calibre8">In addition, the cluster should now include the new DRBD resource and its clone (<code class="email">db_drbd</code> and <code class="email">db_drbd_clone</code>, respectively) as well as the filesystem resource, as you can see in this screenshot:</p><div class="mediaobject"><img src="../images/00052.jpeg" alt="Setting up a high-availability database with replicated storage" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can <a id="id225" class="calibre1"/>now divide the <a id="id226" class="calibre1"/>MariaDB files into two separate sections:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Binaries, socket, and <code class="email">.pid</code> files will be placed inside a directory on a regular partition, independent on each node <code class="email">(/var/lib/mysql</code> by default). These are files we don't need to be highly available or fail-safe.</li><li class="listitem">Database and configuration files (<code class="email">my.cnf</code>) will be stored in a DRBD resource, which will be mounted under <code class="email">/var/lib/mariadb_drbd1</code>, inside a directory named data.</li></ul></div><p class="calibre8">Next, we need to add the database server as a cluster resource:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource create dbserver ocf:heartbeat:mysql config="/var/lib/mariadb_drbd1/my.cnf" datadir="/var/lib/mariadb_drbd1/data" op monitor interval="30s" op start interval="0" timeout="60s" op stop interval="0" timeout="60s"</strong></span></pre></div><p class="calibre8">This we will add the same constraints that we did with Apache:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs constraint colocation add dbserver with virtual_ip INFINITY</strong></span>
<span class="strong"><strong class="calibre2">pcs constraint order virtual_ip then dbserver</strong></span>
<span class="strong"><strong class="calibre2">pcs constraint colocation add db_drbd_clone with virtual_ip INFINITY</strong></span>
<span class="strong"><strong class="calibre2">pcs constraint order virtual_ip then db_drbd_clone</strong></span></pre></div><p class="calibre8">Next, we will add a firewall rule to allow traffic:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport 3306 -j ACCEPT</strong></span>
<span class="strong"><strong class="calibre2">service iptables save</strong></span></pre></div><p class="calibre8">We will begin by creating an <code class="email">ext4</code> filesystem on <code class="email">drbd1</code> and mounting it in the directory that was created previously. Only perform this step on the DC:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mkfs.ext4 /dev/drb</strong></span><span class="strong"><strong class="calibre2">d1</strong></span>
<span class="strong"><strong class="calibre2">mount /dev/dr</strong></span><span class="strong"><strong class="calibre2">bd1 /var/lib/mariadb_drbd1</strong></span></pre></div><p class="calibre8">Next, we<a id="id227" class="calibre1"/> need to move the <a id="id228" class="calibre1"/>database server configuration file to the mount point of <code class="email">drbd1</code> (perform all of the following steps on both nodes):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mv /etc/my.cnf /var/lib/mariadb_drbd1/my.cnf</strong></span></pre></div><p class="calibre8">Edit it so that the <code class="email">datadir</code> variable will point to the right directory inside the mount point of the DRBD resource and at the same time, specify that the database server should listen for TCP connections on a defined address (in this case, the IP address of our virtual IP resource):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">dat</strong></span><span class="strong"><strong class="calibre2">adir=/var/lib/mariadb_drbd1/</strong></span><span class="strong"><strong class="calibre2">data</strong></span>
<span class="strong"><strong class="calibre2">bind-address=192.168.0.4</strong></span></pre></div><p class="calibre8">Next, we need to initialize the database data directory:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mysql_install_db --no-defaults --datadir=/var/lib/mariadb_drbd1/data</strong></span></pre></div><p class="calibre8">Finally, log on to the database server:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">mysql –h 192.168.0.4 –u root –p</strong></span></pre></div><p class="calibre8">Then, grant all permissions to the root user identified by the defined password:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">GRANT ALL ON *.* TO 'root'@'%' IDENTIFIED BY 'MyDBpassword'; </strong></span>
<span class="strong"><strong class="calibre2">FLUSH PRIVILEGES;</strong></span></pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre8">This permission set is only for testing and should be modified with the necessary security parameters before moving the cluster to a production environment.</p></div><p class="calibre8">Alternatively, we can create an empty database:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">CREATE DATABASE cluster_db;</strong></span></pre></div><p class="calibre8">Finally, make sure the <code class="email">mysql</code> user can access the <code class="email">/var/lib/mariadb_drbd1</code> directory:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">chown -R mysql:mysql /var/lib/mariadb_drbd1/</strong></span></pre></div><p class="calibre8">If we now failover, from the active node to the passive one, the actual database files within <code class="email">datadir</code> will be replicated by DRBD to the same directory on the other node.</p></div>
<div class="book" title="Troubleshooting" id="1565U1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec33" class="calibre1"/>Troubleshooting</h1></div></div></div><p class="calibre8">As explained previously, the output of <code class="email">pcs status</code> under <code class="email">Failed actions</code> will show you whether there are problems with the cluster resources and provide information as to what you<a id="id229" class="calibre1"/> should do in order to fix them.</p><p class="calibre8">Here is an example:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">exit-reason='Config /var/lib/mariadb_drbd1/my.cnf doesn't exist'</code>: Make sure the configuration file for MariaDB exists and is identical on both nodes.</li><li class="listitem"><code class="email">exit-reason='Couldn't find device [/dev/drbd1]. Expected /dev/??? to exist'</code>: The DRBD device was not created correctly. Review the instructions and try to create it.</li></ul></div><p class="calibre8">As you can see, the exit reason will give you valuable information to troubleshoot and fix the issues you may have. If, after verifying the conditions outlined in the error messages, you are still experiencing issues with a particular resource, it is useful to clean up the operation history of a resource and redetect its current state:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">pcs resource cleanup [resource name]</strong></span></pre></div><p class="calibre8">From Kamran, a real world problem scenario, which happens when the reader follows (or gets lost following) instructions in this chapter:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong class="calibre2">[root@node01 ~]# pcs status</strong></span>
<span class="strong"><strong class="calibre2">Cluster name: MyCluster</strong></span>
<span class="strong"><strong class="calibre2">Last updated: Tue May 12 17:07:04 2015</strong></span>
<span class="strong"><strong class="calibre2">Last change: Tue May 12 16:54:03 2015</strong></span>
<span class="strong"><strong class="calibre2">Stack: corosync</strong></span>
<span class="strong"><strong class="calibre2">Current DC: node01 (1) - partition with quorum</strong></span>
<span class="strong"><strong class="calibre2">Version: 1.1.12-a14efad</strong></span>
<span class="strong"><strong class="calibre2">2 nodes configured</strong></span>
<span class="strong"><strong class="calibre2">9 resources configured</strong></span>


<span class="strong"><strong class="calibre2">Online: [ node01 node02 ]</strong></span>

<span class="strong"><strong class="calibre2">Full list of resources:</strong></span>

<span class="strong"><strong class="calibre2"> virtual_ip  (ocf::heartbeat:IPaddr2):  Started node02 </strong></span>
<span class="strong"><strong class="calibre2"> Master/Slave Set: web_drbd_clone [web_drbd]</strong></span>
<span class="strong"><strong class="calibre2">     Masters: [ node01 ]</strong></span>
<span class="strong"><strong class="calibre2">     Slaves: [ node02 ]</strong></span>
<span class="strong"><strong class="calibre2"> webserver  (ocf::heartbeat:apache):  Stopped </strong></span>
<span class="strong"><strong class="calibre2"> web_fs  (ocf::heartbeat:Filesystem):  Started node01 </strong></span>
<span class="strong"><strong class="calibre2"> dbserver  (ocf::heartbeat:mysql):  Stopped </strong></span>
<span class="strong"><strong class="calibre2"> Master/Slave Set: db_drbd_clone [db_drbd]</strong></span>
<span class="strong"><strong class="calibre2">     Masters: [ node02 ]</strong></span>
<span class="strong"><strong class="calibre2">     Stopped: [ node01 ]</strong></span>
<span class="strong"><strong class="calibre2"> db_fs  (ocf::heartbeat:Filesystem):  Stopped </strong></span>

<span class="strong"><strong class="calibre2">Failed actions:</strong></span>
<span class="strong"><strong class="calibre2">    dbserver_start_0 on node01 'not installed' (5): call=36, status=complete, exit-reason='Config /var/lib/mariadb_drbd1/my.cnf doesn't exist', last-rc-change='Tue May 12 17:01:09 2015', queued=0ms, exec=66ms</strong></span>
<span class="strong"><strong class="calibre2">    db_fs_start_0 on node01 'not installed' (5): call=41, status=complete, exit-reason='Couldn't find device [/dev/drbd1]. Expected /dev/??? to exist', last-rc-change='Tue May 12 17:01:09 2015', queued=0ms, exec=38ms</strong></span>
<span class="strong"><strong class="calibre2">    dbserver_start_0 on node02 'not installed' (5): call=41, status=complete, exit-reason='Config /var/lib/mariadb_drbd1/my.cnf doesn't exist', last-rc-change='Tue May 12 17:01:09 2015', queued=0ms, exec=91ms</strong></span>
<span class="strong"><strong class="calibre2">    db_fs_start_0 on node02 'not installed' (5): call=32, status=complete, exit-reason='Couldn't find device [/dev/drbd1]. Expected /dev/??? to exist', last-rc-change='Tue May 12 17:01:08 2015', queued=0ms, exec=39ms</strong></span>


<span class="strong"><strong class="calibre2">PCSD Status:</strong></span>
<span class="strong"><strong class="calibre2">  node01: Online</strong></span>
<span class="strong"><strong class="calibre2">  node02: Online</strong></span>

<span class="strong"><strong class="calibre2">Daemon Status:</strong></span>
<span class="strong"><strong class="calibre2">  corosync: active/enabled</strong></span>
<span class="strong"><strong class="calibre2">  pacemaker: active/enabled</strong></span>
<span class="strong"><strong class="calibre2">  pcsd: active/enabled</strong></span>
<span class="strong"><strong class="calibre2">[root@node01 ~]# </strong></span></pre></div></div>
<div class="book" title="Summary" id="164MG1-1d2c6d19b9d242db82da724021b51ea0"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec34" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we explained how to set up real-world applications of clusters: a database server and a web server. Both applications build upon a replicated storage device in a setup that increases availability by providing failover storage for regular and database files.</p><p class="calibre8">In the next two chapters, we will build upon the concepts and resources that we introduced here, troubleshoot common issues in cluster-based web and database servers, and prevent common bottlenecks in order to ensure the high availability of applications.</p></div></body></html>