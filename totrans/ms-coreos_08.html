<html><head></head><body>
<p id="filepos585742" class="calibre_"><span class="calibre1"><span class="bold">Chapter 8. Container Orchestration</span></span></p><p class="calibre_8">As Containers became the basis of modern application development and deployment, it is necessary to deploy <a/>hundreds or thousands of Containers to a single data center cluster or data center clusters. The cluster could be an on-premises cluster or in a cloud. It is necessary to have a good Container orchestration system to deploy and manage Containers at scale.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The basics of modern application deployment</li><li value="2" class="calibre_13">Container orchestration with Kubernetes, Docker Swarm, and Mesos and their core concepts, installation, and deployment</li><li value="3" class="calibre_13">Comparison of popular orchestration solutions</li><li value="4" class="calibre_13">Application definition with Docker Compose</li><li value="5" class="calibre_13">Packaged Container Orchestration solutions—the AWS container service, Google container engine, and CoreOS Tectonic</li></ul><div class="mbp_pagebreak" id="calibre_pb_181"/>


<p id="filepos587102" class="calibre_14"><span class="calibre1"><span class="bold">Modern application deployment</span></span></p><p class="calibre_8">We <a/>covered the basics of Microservices in <a href="index_split_023.html#filepos77735">Chapter 1</a>, <span class="italic">CoreOS Overview</span>. In cloud-based application development, infrastructure is treated as cattle rather than pet (<a href="http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story">http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story</a>). What this means is that the infrastructure is commonly a commodity hardware that can easily go bad and high availability needs to be handled at either the application layer or application Orchestration layer. High availability can be taken care of by having a combination of the load balancer and Orchestration system that monitors the health of services taking necessary actions such as respawning the service if it dies. Containers have the nice property of isolation and packaging that allows independent teams to develop individual components as Containers.</p><p class="calibre_8">Companies can adopt a pay-as-you-grow model where they can scale their Containers as they grow. It is necessary to manage hundreds or thousands of Containers at scale. To do this efficiently, we need a Container Orchestration system. The following are some <a/>characteristics of a Container Orchestration system:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It treats disparate infrastructure hardware as a collection and represents it as one single resource to the application</li><li value="2" class="calibre_13">It schedules Containers based on user constraints and uses the infrastructure in the most efficient manner</li><li value="3" class="calibre_13">It scales out containers dynamically</li><li value="4" class="calibre_13">It maintains high availability of services</li></ul><p class="calibre_8">There is a <a/>close relation between the application definition and Container Orchestration. The application definition is typically a manifest file describing the Containers that are part of the application and the services that the Container exposes. Container Orchestration is done based on the application definition. The Container Orchestrator operates on resources that could be a VM or bare metal. Typically, the nodes where Containers run are installed with Container-optimized OSes, such as CoreOS, DCOS, and Atomic. The following image shows you the relationship between the application definition, Container Orchestration, and Container-optimized nodes along with some examples of solutions in each category:</p><p class="calibre_9"><img src="images/00458.jpg" class="calibre_341"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_182"/>


<p id="filepos590075" class="calibre_"><span class="calibre1"><span class="bold">Container Orchestration</span></span></p><p class="calibre_8">A basic requirement of <a/>Container orchestration is to efficiently deploy <span class="italic">M</span> containers into <span class="italic">N</span> compute resources.</p><p class="calibre_8">The following are <a/>some problems that a Container Orchestration system should solve:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It should schedule containers efficiently, giving enough control to the user to tweak scheduling parameters based on their need</li><li value="2" class="calibre_13">It should provide Container networking across the cluster</li><li value="3" class="calibre_13">Services should be able to discover each other dynamically</li><li value="4" class="calibre_13">Orchestration system should be able to handle service failures</li></ul><p class="calibre_8">We will cover <a/>Kubernetes, Docker Swarm, and Mesos in the following sections. Fleet is used internally by CoreOS for Container orchestration. Fleet has very minimal capabilities and works well for the deployment of critical system services in CoreOS. For very small deployments, Fleet can be used for Container orchestration, if necessary.</p><div class="mbp_pagebreak" id="calibre_pb_183"/>


<p id="filepos591494" class="calibre_9"><span class="calibre3"><span class="bold">Kubernetes</span></span></p><p class="calibre_8">Kubernetes is<a/> an open source platform for Container Orchestration. This<a/> was initially started by Google and now multiple vendors are working together in this open source project. Google has used Containers to develop and deploy applications in their internal data center and they had a system called <a/>Borg (<a href="http://research.google.com/pubs/pub43438.html">http://research.google.com/pubs/pub43438.html</a>) for cluster management. Kubernetes uses a lot of the concepts from Borg combined with modern technologies available now. Kubernetes is lightweight, works across almost all environments, and has a lot of industry traction currently.</p><p id="filepos592288" class="calibre_9"><span class="calibre3"><span class="bold">Concepts of Kubernetes</span></span></p><p class="calibre_8">Kubernetes has <a/>some unique concepts, and it will be good to understand them before diving deep into the architecture of Kubernetes.</p><p id="filepos592567" class="calibre_9"><span class="bold">Pods</span></p><p class="calibre_8">Pods <a/>are a set of Containers that are scheduled together in a single node and need <a/>to work closely with each other. All containers in a Pod share the IPC namespace, network namespace, UTS namespace, and PID namespace. By sharing the IPC namespace, Containers can use IPC mechanisms to talk to each other. </p><p class="calibre_8">By sharing the network namespace, Containers can use sockets to talk to each other, and all Containers in a Pod share a single IP address. By sharing the UTS namespace, volumes can be mounted to a Pod and all Containers can see these volumes. The following are some common <a/>application deployment patterns with Pods:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Sidecar pattern</span>: An example is an application container and logging container or application synchronizer container such as a Git synchronizer.</li><a/><li value="2" class="calibre_13"><span class="bold">Ambassador pattern</span>: In this pattern, the application container and proxy container work together. When the application container changes, external services can still talk to the proxy container as before. An example is a redis application container with the redis proxy.</li><a/><li value="3" class="calibre_13"><span class="bold">Adapter pattern</span>: In this pattern, there is an application container and adapter container that adapts to different environments. An example is a logging container that works as an adapter and changes with different cloud providers but the interface to the adapter container remains the same.</li><a/></ul><p class="calibre_8">The smallest<a/> unit in Kubernetes is a Pod and Kubernetes takes care of scheduling the Pods.</p><p class="calibre_8">The following is a Pod definition example with the NGINX Container and Git helper container:</p><p class="calibre_8"><tt class="calibre2">apiVersion: v1<br class="calibre4"/>kind: Pod<br class="calibre4"/>metadata:<br class="calibre4"/>  name: www<br class="calibre4"/>spec:<br class="calibre4"/>  containers:<br class="calibre4"/>  - name: nginx<br class="calibre4"/>    image: nginx<br class="calibre4"/>  - name: git-monitor<br class="calibre4"/>    image: kubernetes/git-monitor<br class="calibre4"/>    env:<br class="calibre4"/>    - name: GIT_REPO<br class="calibre4"/>      value: http://github.com/some/repo.git</tt></p><p id="filepos595030" class="calibre_9"><span class="bold">Networking</span></p><p class="calibre_8">Kubernetes <a/>has the one IP per Pod approach. This approach was taken to avoid the pains associated with NAT to access Container services when Containers shared the host IP address. All Containers in a pod share the same IP address. Pods across nodes can talk to each other using different techniques such as cloud-based routing from cloud providers, Flannel, Weave, Calico, and others. The end goal is to have Networking as a plugin within Kubernetes and the user can choose the plugin based on their needs.</p><p id="filepos595650" class="calibre_9"><span class="bold">Services</span></p><p class="calibre_8">Services are an abstraction that Kubernetes provides to logically combine Pods that provide similar functionality. Typically, Labels are used as selectors to create services from Pods. As Pods are ephemeral, Kubernetes creates a service object with its own IP address that always remains permanent. Kubernetes takes care of load <a/>balancing for multiple pods.</p><p class="calibre_8">The following is an example service:</p><p class="calibre_8"><tt class="calibre2">{<br class="calibre4"/>    "kind": "Service",<br class="calibre4"/>    "apiVersion": "v1",<br class="calibre4"/>    "metadata": {<br class="calibre4"/>        "name": "my-service"<br class="calibre4"/>    },<br class="calibre4"/>    "spec": {<br class="calibre4"/>        "selector": {<br class="calibre4"/>            "app": "MyApp"<br class="calibre4"/>        },<br class="calibre4"/>        "ports": [<br class="calibre4"/>            {<br class="calibre4"/>                "protocol": "TCP",<br class="calibre4"/>                "port": 80,<br class="calibre4"/>                "targetPort": 9376<br class="calibre4"/>            }<br class="calibre4"/>        ]<br class="calibre4"/>    }<br class="calibre4"/>}</tt></p><p class="calibre_8">In the preceding example, we created a <tt class="calibre2">my-service</tt> service that groups all pods with a <tt class="calibre2">Myapp</tt> label. Any request to the <tt class="calibre2">my-service</tt> service's IP address and port number <tt class="calibre2">80</tt> will be load balanced to all the pods with the <tt class="calibre2">Myapp</tt> label and redirected to port number <tt class="calibre2">9376</tt>.</p><p class="calibre_8">Services need to be discovered internally or externally based on the type of service. An example of internal discovery is a web service needing to talk to a database service. An example of external discovery is a web service that gets exposed to the outside world.</p><p class="calibre_8">For internal service discovery, Kubernetes provides two options:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Environment variable</span>: When a new Pod is created, environment variables from older services can be imported. This allows services to talk to each other. This approach enforces ordering in service creation.</li><a/><li value="2" class="calibre_13"><span class="bold">DNS</span>: Every service registers to the DNS service; using this, new services can find and talk to other services. Kubernetes provides the <tt class="calibre2">kube-dns</tt> service for this.</li><a/></ul><p class="calibre_8">For external service discovery, Kubernetes provides two options:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">NodePort</span>: In this method, Kubernetes exposes the service through special ports (30000-32767) of the node IP address.</li><a/><li value="2" class="calibre_13"><span class="bold">Loadbalancer</span>: In this method, Kubernetes interacts with the cloud provider to create a load balancer that redirects the traffic to the Pods. This approach is currently available with GCE.</li><a/></ul><p id="filepos598862" class="calibre_14"><span class="calibre3"><span class="bold">Kubernetes architecture</span></span></p><p class="calibre_8">The <a/>following diagram shows you the different software components of the Kubernetes architecture and how they interact with each other:</p><p class="calibre_9"><img src="images/00460.jpg" class="calibre_342"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>are some notes on the Kubernetes architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The master node hosts the Kubernetes control services. Slave nodes run the pods and are managed by master nodes. There can be multiple master nodes for redundancy purposes and to scale master services.</li><li value="2" class="calibre_13">Master nodes run the critical services such as the Scheduler, Replication controller, and API server. Slave nodes run the critical services such as Kubelet and Kube-proxy.</li><li value="3" class="calibre_13">User interaction with Kubernetes is through Kubectl, which uses standard Kubernetes-exposed APIs.</li><a/><li value="4" class="calibre_13">The Kubernetes scheduler takes care of scheduling the pods in the nodes based on the constraints specified in the Pod manifest.</li><li value="5" class="calibre_13">The replication controller is necessary to maintain high availability of Pods and create multiple instances of pods as specified in the replication controller manifest.</li><li value="6" class="calibre_13">The API server in the master node talks to Kubelet of each slave node to provision the pods.</li><li value="7" class="calibre_13">Kube-proxy takes care of service redirection and load balancing the traffic to the Pods.</li><li value="8" class="calibre_13">Etcd is used as a shared data repository for all nodes to communicate with each other.</li><li value="9" class="calibre_13">DNS is used for service discovery.</li></ul><p id="filepos601061" class="calibre_14"><span class="calibre3"><span class="bold">Kubernetes installation</span></span></p><p class="calibre_8">Kubernetes <a/>can be installed on baremetal, VM, or in cloud providers such as AWS, GCE, and Azure. We can decide on the choice of the host OS on any of these<a/> systems. In this chapter, all the examples will use CoreOS as the host OS. As Kubernetes consists of multiple components such as the API server, scheduler, replication controller, kubectl, and kubeproxy spread between master and slave nodes, the manual installation of the individual components would be complicated. There are scripts provided by Kubernetes and its users that automate some of the node setup and software installation. The latest stable version of Kubernetes as of October 2015 is 1.0.7 and all examples in this chapter are based on the 1.0.7 version.</p><p id="filepos601941" class="calibre_9"><span class="bold">Non-Coreos Kubernetes installation</span></p><p class="calibre_8">For <a/>non-Coreos-based Kubernetes installation, the procedure is straightforward:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Find the Kubernetes release necessary from <a href="https://github.com/kubernetes/kubernetes/releases">https://github.com/kubernetes/kubernetes/releases</a>.</li><li value="2" class="calibre_13">Download <tt class="calibre2">kubernetes.tar.gz</tt> for the appropriate version and unzip them.</li><li value="3" class="calibre_13">Set <tt class="calibre2">KUBERNETES_PROVIDER</tt> as one of these (AWS, GCE, Vagrant, and so on)</li><li value="4" class="calibre_13">Change the cluster size and any other configuration parameter in the <tt class="calibre2">cluster</tt> directory.</li><a/><li value="5" class="calibre_13">Run <tt class="calibre2">cluster/kube-up.sh</tt>.</li></ol><p id="filepos602951" class="calibre_14"><span class="bold">Kubectl installation</span></p><p class="calibre_8">Kubectl is <a/>the CLI client to interact with Kubernetes. Kubectl is not installed by default. Kubectl can be installed in either the client machine or the kubernetes master node.</p><p class="calibre_8">The following command can be used to install kubectl. It is needed to match kubectl version with Kubernetes version:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ARCH=linux; wget https://storage.googleapis.com/kubernetes-release/release/v1.0.7/bin/$ARCH/amd64/kubectl</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">If Kubectl is installed in the client machine, we can use the following command to proxy the request to the Kubernetes master node:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ssh -f -nNT -L 8080:127.0.0.1:8080 core@&lt;control-external-ip&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p id="filepos603961" class="calibre_9"><span class="bold">Vagrant installation</span></p><p class="calibre_8">I used the <a/>procedure at <a href="https://github.com/pires/kubernetes-vagrant-coreos-cluster">https://github.com/pires/kubernetes-vagrant-coreos-cluster</a> to create a Kubernetes cluster running on the Vagrant environment with CoreOS. I initially tried this in Windows. As I faced the issue mentioned in <a href="https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158">https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158</a>, I moved to the Vagrant environment running on Ubuntu Linux.</p><p class="calibre_8">The following are the commands:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Git clone https://github.com/pires/kubernetes-vagrant-coreos-cluster.git</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Cd coreos-container-platform-as-a-service/vagrant</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Vagrant up</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows the two running Kubernetes nodes:</p><p class="calibre_9"><img src="images/00462.jpg" class="calibre_343"/></p><p class="calibre_8">
</p><p id="filepos605244" class="calibre_9"><span class="bold">GCE installation</span></p><p class="calibre_8">I used the <a/>procedure at <a href="https://github.com/rimusz/coreos-multi-node-k8s-gce">https://github.com/rimusz/coreos-multi-node-k8s-gce</a> to create a Kubernetes cluster running in GCE with CoreOS.</p><p class="calibre_8">The following are the commands:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">git clone https://github.com/rimusz/coreos-multi-node-k8s-gce</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">cd coreos-multi-node-k8s-gce</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">In the <tt class="calibre2">settings</tt> file, change <tt class="calibre2">project</tt>, <tt class="calibre2">zone</tt>, <tt class="calibre2">node count</tt>, and any other necessary changes.</p><p class="calibre_8">Run the following three scripts in the same order:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">1-bootstrap_cluster.sh</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">2-get_k8s_fleet_etcd.sh</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">3-install_k8s_fleet_units.sh</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows the cluster composed of three nodes:</p><p class="calibre_9"><img src="images/00464.jpg" class="calibre_263"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the Kubernetes client and server versions:</p><p class="calibre_9"><img src="images/00465.jpg" class="calibre_344"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the Kubernetes services running in master and slave nodes:</p><p class="calibre_9"><img src="images/00468.jpg" class="calibre_345"/></p><p class="calibre_8">
</p><p class="calibre_8">The script <a/>used in this example uses Fleet to orchestrate Kubernetes services. As we can see in the preceding image, the API server, controller, and scheduler run in the master node and kubelet and proxy run in the slave nodes. There are three copies of kubelet and kube-proxy, one each for every slave node.</p><p id="filepos607560" class="calibre_9"><span class="bold">AWS installation</span></p><p class="calibre_8">I used <a/>the procedure at <a href="https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html">https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html</a> to create the Kubernetes CoreOS cluster running on AWS.</p><p class="calibre_8">The first step is to install the kube-aws tool:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Git clone https://github.com/coreos/coreos-kubernetes/releases/download/v0.1.0/kube-aws-linux-amd64.tar.gz</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Unzip and copy kube-aws to an executable path. Make sure that <tt class="calibre2">~/.aws/credentials</tt> is updated with your credentials.</p><p class="calibre_8">Create a default <tt class="calibre2">cluster.yaml</tt> file:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl --silent --location https://raw.githubusercontent.com/coreos/coreos-kubernetes/master/multi-node/aws/cluster.yaml.example &gt; cluster.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Modify <tt class="calibre2">cluster.yaml</tt> with your <tt class="calibre2">keyname</tt>, <tt class="calibre2">region</tt>, and <tt class="calibre2">externaldnsname</tt>; <tt class="calibre2">externaldnsname</tt> matters for external access only.</p><p class="calibre_8">To deploy the cluster, we can perform the following:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Kube-aws up</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows the two nodes that are part of the Kubernetes cluster:</p><p class="calibre_9"><img src="images/00470.jpg" class="calibre_346"/></p><p class="calibre_8">
</p><p id="filepos609403" class="calibre_9"><span class="calibre3"><span class="bold">An example of a Kubernetes application</span></span></p><p class="calibre_8">The<a/> following diagram illustrates the guestbook example that we will use to illustrate the different Kubernetes concepts discussed in the previous sections. This example is based on the reference at <a href="http://kubernetes.io/v1.1/examples/guestbook/README.html">http://kubernetes.io/v1.1/examples/guestbook/README.html</a>:</p><p class="calibre_9"><img src="images/00472.jpg" class="calibre_347"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some notes on this guestbook application:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">This application uses the php frontend with the redis master and slave backend to store the guestbook database</li><li value="2" class="calibre_13">Frontend RC creates three instances of the <tt class="calibre2">kubernetes/example-guestbook-php-redis</tt> container</li><li value="3" class="calibre_13">Redis-master RC creates one instance of the redis container</li><li value="4" class="calibre_13">Redis-slave RC creates two instances of the <tt class="calibre2">kubernetes/redis-slave</tt> container</li></ul><p class="calibre_8">For this <a/>example, I used the cluster created in the previous section with Kubernetes running on AWS with CoreOS. There is one master node and two slave nodes.</p><p class="calibre_8">Let's look at the nodes:</p><p class="calibre_9"><img src="images/00474.jpg" class="calibre_145"/></p><p class="calibre_8">
</p><p class="calibre_8">In this example, the Kubernetes cluster uses flannel to communicate across pods. The following output shows the flannel subnet allocated to each node in the cluster:</p><p class="calibre_9"><img src="images/00477.jpg" class="calibre_348"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the commands necessary to start the application:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create -f redis-master-controller.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create --validate=false -f redis-master-service.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create -f redis-slave-controller.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create --validate=false -f redis-slave-service.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create -f frontend-controller.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">kubectl create --validate=false -f frontend-service.yaml</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the list of pods:</p><p class="calibre_9"><img src="images/00478.jpg" class="calibre_349"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding <a/>output shows three instances of the php frontend, one instance of the redis master, and two instances of the redis slave.</p><p class="calibre_8">Let's look at the list of RC:</p><p class="calibre_9"><img src="images/00480.jpg" class="calibre_146"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding output shows the replication count per pod. Frontend has three replicas, <tt class="calibre2">redis-master</tt> has one replica, and <tt class="calibre2">redis-slave</tt> has two replicas, as we requested.</p><p class="calibre_8">Let's look at the list of services:</p><p class="calibre_9"><img src="images/00481.jpg" class="calibre_350"/></p><p class="calibre_8">
</p><p class="calibre_8">In the preceding output, we can see the three services comprising the guestbook application.</p><p class="calibre_8">For internal service discovery, this example uses <tt class="calibre2">kube-dns</tt>. The following output shows the <tt class="calibre2">kube-dns</tt> RC running:</p><p class="calibre_9"><img src="images/00484.jpg" class="calibre_95"/></p><p class="calibre_8">
</p><p class="calibre_8">For external <a/>service discovery, I modified the example to use the <tt class="calibre2">NodePort</tt> mechanism, where one of the internal ports gets exposed. The following is the new <tt class="calibre2">frontend-service.yaml</tt> file:</p><p class="calibre_8"><tt class="calibre2">apiVersion: v1<br class="calibre4"/>kind: Service<br class="calibre4"/>metadata:<br class="calibre4"/>  name: frontend<br class="calibre4"/>  labels:<br class="calibre4"/>    name: frontend<br class="calibre4"/>spec:<br class="calibre4"/>  # if your cluster supports it, uncomment the following to automatically create<br class="calibre4"/>  # an external load-balanced IP for the frontend service.<br class="calibre4"/>  type: NodePort<br class="calibre4"/>  ports:<br class="calibre4"/>    # the port that this service should serve on<br class="calibre4"/>  - port: 80<br class="calibre4"/>  selector:<br class="calibre4"/>    name: frontend</tt></p><p class="calibre_8">The following is the output when we start the frontend service with the <tt class="calibre2">NodePort</tt> type. The output shows that the service is exposed using port <tt class="calibre2">30193</tt>:</p><p class="calibre_9"><img src="images/00486.jpg" class="calibre_351"/></p><p class="calibre_8">
</p><p class="calibre_8">Once we expose port <tt class="calibre2">30193</tt> using the AWS firewall, we can access the guestbook application as follows:</p><p class="calibre_9"><img src="images/00488.jpg" class="calibre_352"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at <a/>the application containers in <tt class="calibre2">Node1</tt>:</p><p class="calibre_9"><img src="images/00489.jpg" class="calibre_353"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the application containers in <tt class="calibre2">Node2</tt>:</p><p class="calibre_9"><img src="images/00492.jpg" class="calibre_154"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding output accounts for three instances of frontend, one instance of redis master, and two instances of redis slave.</p><p class="calibre_8">To illustrate how the replication controller maintains the pod replication count, I went and stopped the <a/>guestbook frontend Docker Container in one of the nodes, as shown in the following image:</p><p class="calibre_9"><img src="images/00493.jpg" class="calibre_354"/></p><p class="calibre_8">
</p><p class="calibre_8">Kubernetes RC detects that the Pod is not running and restarts the Pod. This can be seen in the restart count for one of the guestbook pods, as shown in the following image:</p><p class="calibre_9"><img src="images/00495.jpg" class="calibre_355"/></p><p class="calibre_8">
</p><p class="calibre_8">To do some basic debugging, we can log in to the pods or containers themselves. The following example shows you how we can get inside the Pod:</p><p class="calibre_9"><img src="images/00496.jpg" class="calibre_248"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding output shows the IP address in the guestbook pod, which agrees with the flannel subnet allocated to that node, as shown in the Flannel output in the beginning of this example.</p><p class="calibre_8">Another command that's useful for the debugging is <tt class="calibre2">kubectl logs</tt> as follows:</p><p class="calibre_9"><img src="images/00476.jpg" class="calibre_42"/></p><p class="calibre_8">
</p><p id="filepos617810" class="calibre_9"><span class="calibre3"><span class="bold">Kubernetes with Rkt</span></span></p><p class="calibre_8">By default, Kubernetes <a/>works with Container runtime Docker. The architecture of Kubernetes allows other Container runtime such as Rkt to work with Kubernetes. There is active work going on (<a href="https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt">https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt</a>) to integrate Kubernetes with Rkt and CoreOS.</p><p id="filepos618374" class="calibre_9"><span class="calibre3"><span class="bold">Kubernetes 1.1 update</span></span></p><p class="calibre_8">Kubernetes <a/>released 1.1 version (<a href="http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html">http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html</a>) in November 2015. Significant additions in 1.1 are increased performance, auto-scaling, and job objects for the batching tasks.</p><div class="mbp_pagebreak" id="calibre_pb_184"/>


<p id="filepos618959" class="calibre_9"><span class="calibre3"><span class="bold">Docker Swarm</span></span></p><p class="calibre_8">Swarm is <a/>Docker's native Orchestration solution. The <a/>following are some <a/>properties of Docker Swarm:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Rather than managing individual Docker nodes, the cluster can be managed as a single entity.</li><li value="2" class="calibre_13">Swarm has a built-in scheduler that decides the placement of Containers in the cluster. Swarm uses user-specific constraints and affinities (<a href="https://docs.docker.com/swarm/scheduler/filter/">https://docs.docker.com/swarm/scheduler/filter/</a>) to decide the Container placement. Constraints could be CPU and memory, and affinity are parameters to group related Containers together. Swarm also has the provision to take its scheduler out and work with other schedulers such as Kubernetes.</li></ul><p class="calibre_8">The following image <a/>shows the Docker Swarm architecture:</p><p class="calibre_9"><img src="images/00188.jpg" class="calibre_356"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are <a/>some notes on the Docker Swarm architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The <span class="bold">Swarm Master</span> takes care of scheduling Docker Containers based on the scheduling algorithm, constraints, and affinities. Supported algorithms are spread, <tt class="calibre2">binpack</tt>, and <tt class="calibre2">random</tt>. The default algorithm is spread. Multiple Swarm masters can be run in parallel to provide high availability. The Spread scheduling is used to distribute workloads evenly. The binpack scheduling is used to utilize each node fully before scheduling on another node.</li><a/><li value="2" class="calibre_13">The <span class="bold">Swarm Agent</span> runs in each node and communicates to the <span class="bold">Swarm Master</span>.</li><a/><li value="3" class="calibre_13">There are different approaches available for Swarm worker nodes to discover the <span class="bold">Swarm Master</span>. Discovery is necessary because the <span class="bold">Swarm Master</span> and agents run on different nodes and Swarm agents are not started by the <span class="bold">Swarm Master</span>. It is necessary for Swarm agents and the <span class="bold">Swarm Master</span> to discover each other in order to understand that they are part of the same cluster. Available discovery mechanisms are Docker hub, Etcd, Consul, and others.</li><li value="4" class="calibre_13">Docker Swarm integrates with the Docker machine to ease the creation of Docker nodes. Docker Swarm integrates with Docker compose for multicontainer application orchestration.</li><li value="5" class="calibre_13">With the Docker 1.9 release, Docker Swarm integrates with multi-host Docker networking that allows Containers scheduled across hosts to talk to each other.</li></ul><p id="filepos622060" class="calibre_14"><span class="calibre3"><span class="bold">The Docker Swarm installation</span></span></p><p class="calibre_8">A <a/>prerequisite for this example is to install Docker 1.8.1 and Docker-machine 0.5.0. I used the procedure at <a href="https://docs.docker.com/swarm/install-w-machine/">https://docs.docker.com/swarm/install-w-machine/</a> to create a single Docker Swarm master with two Docker Swarm agent nodes. The following are the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a discovery token.</li><li value="2" class="calibre_13">Create a Swarm master node with the created discovery token.</li><li value="3" class="calibre_13">Create two Swarm agent nodes with the created discovery token.</li></ol><p class="calibre_8">By setting the environment variable to <tt class="calibre2">swarm-master</tt>, as shown in the following command we can control the Docker swarm cluster using regular Docker commands:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">eval $(docker-machine env --swarm swarm-master)</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Let's look at the <tt class="calibre2">docker info</tt> output on the Swarm cluster:</p><p class="calibre_9"><img src="images/00056.jpg" class="calibre_357"/></p><p class="calibre_8">
</p><p class="calibre_8">The preceding output shows that three nodes (one master and two agents) are in the cluster and that four containers are running in the cluster. The <tt class="calibre2">swarm-master</tt> node has two containers and the <tt class="calibre2">swarm-agent</tt> node has one container each. These containers are used to manage the Swarm service. Application containers are scheduled only in Swarm agent nodes.</p><p class="calibre_8">Let's look at<a/> the individual containers in the master node. This shows the master and agent services running:</p><p class="calibre_9"><img src="images/00191.jpg" class="calibre_71"/></p><p class="calibre_8">
</p><p class="calibre_8">Let's look at the container running in the agent node. This shows the <tt class="calibre2">swarm agent</tt> running:</p><p class="calibre_9"><img src="images/00193.jpg" class="calibre_68"/></p><p class="calibre_8">
</p><p id="filepos624590" class="calibre_9"><span class="calibre3"><span class="bold">An example of Docker Swarm</span></span></p><p class="calibre_8">To <a/>illustrate the Docker Swarm Container orchestration, let's start four <tt class="calibre2">nginx</tt> containers:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name nginx1 nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name nginx2 nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name nginx3 nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker run -d --name nginx4 nginx</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">From the following output, we can see that the four containers are spread equally between <tt class="calibre2">swarm-agent-00</tt> and <tt class="calibre2">swarm-agent-01</tt>. The default <tt class="calibre2">spread</tt> scheduling strategy has been used here:</p><p class="calibre_9"><img src="images/00195.jpg" class="calibre_358"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>output shows the overall Container count across the cluster that includes the master and two agent nodes. The container count eight includes Swarm service containers as well as nginx application containers:</p><p class="calibre_9"><img src="images/00252.jpg" class="calibre_359"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_185"/>


<p id="filepos626074" class="calibre_9"><span class="calibre3"><span class="bold">Mesos</span></span></p><p class="calibre_8">Apache <a/>Mesos is <a/>an open source clustering software. Mesosphere's DCOS is the commercial version of Apache Mesos. Mesos combines the Clustering OS and Cluster manager. Clustering OS is responsible for representing resources from multiple disparate computers in one single resource over which applications can be scheduled. The cluster manager is responsible for scheduling the jobs in the cluster. The same cluster can be used for different workloads such as Hadoop and Spark. There is a two-level scheduling within Mesos. The first-level scheduling does resource allocation among frameworks and the framework takes care of scheduling the jobs within that particular framework. Each framework is an application category such as Hadoop, Spark, and others. For general purpose applications, the best framework available is Marathon. Marathon is a distributed INIT and HA system to schedule containers. The Chronos framework is like a Cron job and this framework is suitable to run shorter workloads that need to be run periodically. The Aurora framework provides you with a much more fine-grained control for complex jobs.</p><p class="calibre_8">The following image shows you the different layers in the <a/>Mesos architecture:</p><p class="calibre_9"><img src="images/00198.jpg" class="calibre_360"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_186"/>


<p id="filepos627652" class="calibre_9"><span class="calibre3"><span class="bold">Comparing Kubernetes, Docker Swarm, and Mesos</span></span></p><p class="calibre_8">Even <a/>though all these solutions (Kubernetes, Docker Swarm, and Mesos) do application Orchestration, there are many differences in their approach and use cases. I have tried to summarize the differences based on their latest available release. All these Orchestration solutions are under active development, so the feature set can vary going forward. This table is updated as of October 2015:</p><table border="1" valign="top" class="calibre_16"><tr valign="top" class="calibre_17"><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Feature</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Kubernetes</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Docker Swarm</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Mesos</span></p></th></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Deployment unit</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Pods</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Container</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Process or Container</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Container runtime</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker and Rkt.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker; there is some discussion ongoing on Mesos with Rkt.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Networking</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Each container has an IP address, and can use external network plugins.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Initially, this did Port forwarding with a common agent IP address. With Docker 1.9, it uses Overlay networking and per container IP address. It can use external network plugins.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Initially, this did Port forwarding with a common agent IP address. Currently, it works on per Container IP. integration with Calico.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Workloads</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Homogenous workload. With namespaces, multiple virtual clusters can be created.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Homogenous workload.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Multiple frameworks such as Marathon, Aurora, and Hadoop can be run in parallel.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Service discovery</p></td><td valign="top" class="calibre_19"><p class="calibre_8">This can use either environment variable-based discovery or Kube-dns for dynamic discovery.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Static with modification of <tt class="calibre2">/etc/hosts</tt>. A DNS approach is planned in the future.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">DNS-based approach to discover services dynamically.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">High availability</p></td><td valign="top" class="calibre_19"><p class="calibre_8">With a replication controller, services are highly available. Service scaling can be done easily.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Service high availability is not yet implemented.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Frameworks take care of service high availability. For example, Marathon has the <tt class="calibre2">Init.d</tt> system to run containers.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Maturity</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Relatively new. The first production release was done a few months before.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Relatively new. The first production release was done a few months before.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Pretty stable and used in big production environments.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Complexity</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Easy.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Easy.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">This is a little difficult to set up.</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Use case</p></td><td valign="top" class="calibre_19"><p class="calibre_8">This is more suitable for homogenous workloads.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Presenting Docker frontend makes it attractive for Docker users not needing to learn any new management interface.</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Suitable for heterogeneous workloads.</p></td></tr></table><p class="calibre_8">Kubernetes <a/>can be run as a framework on top of Mesos. In this case, Mesos does the first-level scheduling for Kubernetes and Kubernetes schedules and manages applications scheduled. This project (<a href="https://github.com/mesosphere/kubernetes-mesos">https://github.com/mesosphere/kubernetes-mesos</a>) is dedicated to running Kubernetes on top of Mesos.</p><p class="calibre_8">There is work ongoing to integrate Docker Swarm with Kubernetes so that Kubernetes can be used as a scheduler and process manager for the cluster while users can still use the Docker interface to manage containers using Docker Swarm.</p><div class="mbp_pagebreak" id="calibre_pb_187"/>


<p id="filepos634313" class="calibre_"><span class="calibre1"><span class="bold">Application definition</span></span></p><p class="calibre_8">When an application<a/> is composed of multiple Containers, it is useful to represent each Container property along with its dependencies in a single JSON or YAML file so that the application can be instantiated as a whole rather than instantiating each Container of the application separately. The application definition file takes care of defining the multicontainer application. Docker-compose defines both the application file and runtime to instantiate containers based on the application file.</p><div class="mbp_pagebreak" id="calibre_pb_188"/>


<p id="filepos634987" class="calibre_9"><span class="calibre3"><span class="bold">Docker-compose</span></span></p><p class="calibre_8">Docker-compose <a/>provides you with an application definition<a/> format, and when we run the tool, Docker-compose takes care of parsing the application definition file and instantiating the Containers taking care of all the dependencies.</p><p class="calibre_8">Docker-compose has the following <a/>advantages and <a/>use cases:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It gives a simple approach to specify an application's manifest that contains multiple containers along with their constraints and affinities</li><li value="2" class="calibre_13">It integrates well with Dockerfile, Docker Swarm, and multihost networking</li><li value="3" class="calibre_13">The same compose file can be adapted to different environments using environment variables</li></ul><p id="filepos636016" class="calibre_14"><span class="calibre3"><span class="bold">A single-node application</span></span></p><p class="calibre_8">The <a/>following example shows you how to build a multicontainer WordPress application with a WordPress and MySQL container.</p><p class="calibre_8">The following is the <tt class="calibre2">docker-compose.yml</tt> file defining the Containers and their properties:</p><p class="calibre_8"><tt class="calibre2">wordpress:<br class="calibre4"/>  image: wordpress<br class="calibre4"/>  ports:<br class="calibre4"/>   - "8080:80"<br class="calibre4"/>  environment:<br class="calibre4"/>    WORDPRESS_DB_HOST: "composeword_mysql_1:3306"<br class="calibre4"/>    WORDPRESS_DB_PASSWORD: mysql<br class="calibre4"/>mysql:<br class="calibre4"/>  image: mysql<br class="calibre4"/>  environment:<br class="calibre4"/>    MYSQL_ROOT_PASSWORD: mysql</tt></p><p class="calibre_8">The following command shows you how to start the application using <tt class="calibre2">docker-compose</tt>:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker-compose –p composeword –f docker-compose.yml up -d</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the output of the preceding command:</p><p class="calibre_9"><img src="images/00200.jpg" class="calibre_234"/></p><p class="calibre_8">
</p><p class="calibre_8">Containers are prefixed with a keyword specified in the <tt class="calibre2">-p</tt> option. In the preceding example, we have used <tt class="calibre2">composeword_mysql_1</tt> as the hostname, and the IP address is derived dynamically from the container using this and updated in <tt class="calibre2">/etc/hosts</tt>.</p><p class="calibre_8">The following output shows the running containers of the wordpress application:</p><p class="calibre_9"><img src="images/00202.jpg" class="calibre_146"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>output is the <tt class="calibre2">/etc/hosts</tt> output in the wordpress container; the one which shows that the IP address of the MySQL container is dynamically updated:</p><p class="calibre_9"><img src="images/00220.jpg" class="calibre_361"/></p><p class="calibre_8">
</p><p id="filepos638353" class="calibre_9"><span class="calibre3"><span class="bold">A multinode application</span></span></p><p class="calibre_8">I used <a/>the example at <a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/">https://docs.docker.com/engine/userguide/networking/get-started-overlay/</a> to create a web application spanning multiple nodes using <tt class="calibre2">docker-compose</tt>. In this case, <tt class="calibre2">docker-compose</tt> is integrated with Docker Swarm and Docker multihost networking.</p><p class="calibre_8">The prerequisite for this example is to have a working Docker Swarm cluster and Docker version 1.9+.</p><p class="calibre_8">The following command creates the multihost counter application. This application has a web container as the frontend and a mongo container as the backend. These commands have to be executed against the Swarm cluster:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">docker-compose –p counter –x-networking up -d</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the output of the preceding command:</p><p class="calibre_9"><img src="images/00204.jpg" class="calibre_362"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the overlay network <tt class="calibre2">counter</tt> created as part of this application:</p><p class="calibre_9"><img src="images/00206.jpg" class="calibre_363"/></p><p class="calibre_8">
</p><p class="calibre_8">The following <a/>output shows the running Containers in the Swarm cluster:</p><p class="calibre_9"><img src="images/00207.jpg" class="calibre_116"/></p><p class="calibre_8">
</p><p class="calibre_8">The following output shows the Swarm cluster information. There are in total five containers—three of them are Swarm service containers and two of them are the preceding application containers:</p><p class="calibre_9"><img src="images/00209.jpg" class="calibre_364"/></p><p class="calibre_8">
</p><p class="calibre_8">The following<a/> output shows the working web application:</p><p class="calibre_9"><img src="images/00211.jpg" class="calibre_365"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_189"/>


<p id="filepos640962" class="calibre_"><span class="calibre1"><span class="bold">Packaged Container Orchestration solutions</span></span></p><p class="calibre_8">There are many <a/>components necessary for the deployment of a distributed microservice application at scale. The following are some of the important <a/>components:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">An infrastructure cluster</li><li value="2" class="calibre_13">A Container-optimized OS</li><li value="3" class="calibre_13">A Container orchestrator with a built-in scheduler, service discovery, and networking</li><li value="4" class="calibre_13">Storage integration</li><li value="5" class="calibre_13">Multitenant capability with authentication</li><li value="6" class="calibre_13">An API at all layers to ease management</li></ul><p class="calibre_8">Cloud providers <a/>such as Amazon and Google already have an ecosystem to manage VMs, and their approach has been to integrate Containers and Container orchestration into their IaaS offering so that Containers play well with their other tools. The AWS Container service and Google Container engine fall in this category. The focus of CoreOS has been to develop a secure Container-optimized OS and open source tools for distributed application development. CoreOS realized that integrating their offering with Kubernetes would give their customers an integrated solution, and Tectonic provides this integrated solution.</p><p class="calibre_8">There are a few other projects such as OpenStack Magnum (<a href="https://github.com/openstack/magnum">https://github.com/openstack/magnum</a>) and Cisco's Mantl (<a href="https://mantl.io/">https://mantl.io/</a>) that falls under this category of managed Container Orchestration. We have not covered these in this chapter.</p><div class="mbp_pagebreak" id="calibre_pb_190"/>


<p id="filepos642995" class="calibre_9"><span class="calibre3"><span class="bold">The AWS Container service</span></span></p><p class="calibre_8">The <a/>AWS <span class="bold">EC2 Container Service</span> (<span class="bold">ECS</span>) is a <a/>Container Orchestration service from AWS. The following are some of the key characteristics of this service:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">ECS creates and manages the node cluster where containers are launched. The user needs to specify only the cluster size.</li><li value="2" class="calibre_13">Container health is monitored by container agents running on the node. The Container agent communicates to the master node that makes all service-related decisions. This allows for high availability of Containers.</li><li value="3" class="calibre_13">ECS takes care of scheduling the containers across the cluster. A scheduler API is implemented as a plugin and this allows integration with other schedulers such as Marathon and Kubernetes.</li><li value="4" class="calibre_13">ECS integrates well with other AWS services such as Cloudformation, ELB, logging, Volume management, and others.</li></ul><p id="filepos644236" class="calibre_14"><span class="calibre3"><span class="bold">Installing ECS and an example</span></span></p><p class="calibre_8">ECS can be controlled <a/>from the AWS console or using the AWS CLI or ECS CLI. For the following example, I have used the ECS CLI, which can be installed using the procedure in this link (<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html</a>).</p><p class="calibre_8">I used the <a/>following example (<a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html">http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html</a>) to create a WordPress application with two containers (WordPress and MySQL) using the compose YML file.</p><p class="calibre_8">The following are the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create an ECS cluster.</li><li value="2" class="calibre_13">Deploy the application as a service over the cluster.</li><li value="3" class="calibre_13">The cluster size or service size can be dynamically changed later based on the requirements.</li></ol><p class="calibre_8">The following <a/>command shows the running containers of the WordPress application:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ecs-cli ps</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the output of the preceding command:</p><p class="calibre_9"><img src="images/00213.jpg" class="calibre_102"/></p><p class="calibre_8">
</p><p class="calibre_8">We can scale the application using the <tt class="calibre2">ecs-cli</tt> command. The following command scales each container to two:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">ecs-cli compose --file hello-world.yaml scale 2</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following output shows the running containers at this point. As we can see, containers have scaled to two:</p><p class="calibre_9"><img src="images/00214.jpg" class="calibre_366"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: The MySQL container is scaled typically using a single master and multiple slaves.</p><p class="calibre_8">We can also log <a/>in to each AWS node and look at the running containers. The following output shows three containers in one of the AWS nodes. Two<a/> of them are application containers, and the third one is the ECS agent container that does container monitoring and talks to the master node:</p><p class="calibre_9"><img src="images/00216.jpg" class="calibre_37"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">Note: To log in to each node, we need to use <tt class="calibre2">ec2-user</tt> as the username along with the private key used while creating the cluster.</p><p class="calibre_8">To demonstrate HA, I tried stopping containers or nodes. Containers got rescheduled because the Container agent monitors containers in each node.</p><div class="mbp_pagebreak" id="calibre_pb_191"/>


<p id="filepos647918" class="calibre_9"><span class="calibre3"><span class="bold">Google Container Engine</span></span></p><p class="calibre_8">Google Container Engine is<a/> the cluster manager and container orchestration solution from Google that is built on top of Kubernetes. The following are the differences or benefits that we get from GCE compared to running a container cluster using Kubernetes as specified in the <span class="italic">Kubernetes installation</span> section:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">A node cluster is created automatically by Google Container engine. The user needs to specify only the cluster size and the CPU and memory requirement.</li><li value="2" class="calibre_13">Kubernetes is composed of multiple individual services such as an API server, scheduler, and agents that need to be installed for the Kubernetes system to work. Google Container engine takes care of creating the Kubernetes master with appropriate services and installing other Kubernetes services in agent nodes.</li><li value="3" class="calibre_13">Google Container engine integrates well with other Google services such as VPC networking, Logging, autoscaling, load balancing, and so on.</li><li value="4" class="calibre_13">The Docker hub, Google container registry, or on-premise registry can be used to store Container images.</li><a/></ul><p id="filepos649381" class="calibre_14"><span class="calibre3"><span class="bold">Installing GCE and an example</span></span></p><p class="calibre_8">The<a/> procedure at <a href="https://cloud.google.com/container-engine/docs/before-you-begin">https://cloud.google.com/container-engine/docs/before-you-begin</a> can be used to install the gcloud container components and kubectl. Containers can also be managed using the GCE dashboard.</p><p class="calibre_8">I used the procedure at <a href="https://cloud.google.com/container-engine/docs/tutorials/guestbook">https://cloud.google.com/container-engine/docs/tutorials/guestbook</a> to create a guestbook application containing three services. This application is the same as the one used in the <span class="italic">An example of Kubernetes application</span> section specified earlier.</p><p class="calibre_8">The following are the steps:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Create a node cluster with the required cluster size. This will automatically create a Kubernetes master and appropriate agent services will be installed in the nodes.</li><li value="2" class="calibre_13">Deploy the application using a replication controller and service files.</li><li value="3" class="calibre_13">The cluster can be dynamically resized later based on the need.</li></ol><p class="calibre_8">The following is the cluster that I created. There are four nodes in the cluster as specified by <tt class="calibre2">NUM_NODES</tt>:</p><p class="calibre_9"><img src="images/00217.jpg" class="calibre_234"/></p><p class="calibre_8">
</p><p class="calibre_8">The following command shows the running services that consist of frontend, redis-master, and redis-slave. The Kubernetes service is also running in the master node:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Kubectl get services</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the output of the preceding command:</p><p class="calibre_9"><img src="images/00342.jpg" class="calibre_367"/></p><p class="calibre_8">
</p><p class="calibre_8">As the frontend <a/>service is integrated with the GCE load balancer, there is also an external IP address. Using the external IP address, guestbook service can be accessed. The following command shows the list of endpoints associated with the load balancer:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">Kubectl describe services frontend</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is the output of the preceding command:</p><p class="calibre_9"><img src="images/00222.jpg" class="calibre_368"/></p><p class="calibre_8">
</p><p class="calibre_8">To resize the cluster, we need to first find the instance group associated with the cluster and resize it. The following command shows the instance group associated with the guestbook:</p><p class="calibre_9"><img src="images/00224.jpg" class="calibre_309"/></p><p class="calibre_8">
</p><p class="calibre_8">Using the instance group, we can resize the cluster as follows:</p><p class="calibre_9"><img src="images/00225.jpg" class="calibre_98"/></p><p class="calibre_8">
</p><p class="calibre_8">The initial <a/>set of outputs that show the cluster size as four were done after the resizing of the cluster.</p><p class="calibre_8">We can log in to the individual nodes and see the containers launched in the node using regular Docker commands. In the following output, we see one instance of <tt class="calibre2">redis-slave</tt> and one instance of front end running in this node. Other Containers are Kubernetes infrastructure containers:</p><p class="calibre_9"><img src="images/00227.jpg" class="calibre_369"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_192"/>


<p id="filepos653778" class="calibre_9"><span class="calibre3"><span class="bold">CoreOS Tectonic</span></span></p><p class="calibre_8">Tectonic is the <a/>commercial offering from CoreOS where they have integrated CoreOS and the open source components of CoreOS (Etcd, Fleet, Flannel, Rkt, and Dex) along with Kubernetes. With Tectonic, CoreOS is integrating their other commercial offerings such as CoreUpdate, Quay repository, and Enterprise CoreOS into Tectonic. </p><p class="calibre_8">The plan is to expose the Kubernetes API as it is in Tectonic. Development in CoreOS open <a/>source projects will continue as it is, and the latest software will be updated to Tectonic.</p><p class="calibre_8">The following diagram illustrates the different <a/>components of Tectonic:</p><p class="calibre_9"><img src="images/00228.jpg" class="calibre_370"/></p><p class="calibre_8">
</p><p class="calibre_8">Tectonic provides you with <a/>
<span class="bold">Distributed Trusted Computing</span> (<span class="bold">DTM</span>), where security is provided at all layers <a/>including hardware and software. The following are some unique differentiators:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">At the firmware level, the customer key can be embedded, and this allows customers to verify all the software running in the system.</li><li value="2" class="calibre_13">Secure keys embedded in the firmware can verify the bootloader as well as CoreOS.</li><li value="3" class="calibre_13">Containers such as Rkt can be verified with their image signature.</li><li value="4" class="calibre_13">Logs can be made tamper-proof using the TPM hardware module embedded in the CPU motherboard.</li></ul><div class="mbp_pagebreak" id="calibre_pb_193"/>


<p id="filepos655696" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered the importance of Container Orchestration along with the internals of popular container orchestration solutions, such as Kubernetes, Docker Swarm, and Mesos. There are many companies offering integrated Container orchestration solutions, and we covered a few popular ones such as the AWS Container service, Google Container Engine, and CoreOS Tectonic. For all the technologies covered in this chapter, installation and examples have been provided so that you can try them out. Customers have a choice of picking between integrated Container Orchestration solutions and manually integrating the Orchestration solution in their infrastructure. The factors affecting the choice would be flexibility, integration with in-house solutions, and cost. In the next chapter, we will cover OpenStack integration with Containers and CoreOS.</p><div class="mbp_pagebreak" id="calibre_pb_194"/>


<p id="filepos656695" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Kubernetes page: <a href="http://kubernetes.io/">http://kubernetes.io/</a></li><li value="2" class="calibre_13">Mesos: <a href="http://mesos.apache.org/">http://mesos.apache.org/</a> and <a href="https://mesosphere.com/">https://mesosphere.com/</a></li><li value="3" class="calibre_13">Docker Swarm: <a href="https://docs.docker.com/swarm/">https://docs.docker.com/swarm/</a></li><li value="4" class="calibre_13">Kubernetes on CoreOS: <a href="https://coreos.com/kubernetes/docs/latest/">https://coreos.com/kubernetes/docs/latest/</a></li><li value="5" class="calibre_13">Google Container Engine: <a href="https://cloud.google.com/container-engine/">https://cloud.google.com/container-engine/</a></li><li value="6" class="calibre_13">AWS ECS: <a href="https://aws.amazon.com/ecs/">https://aws.amazon.com/ecs/</a></li><li value="7" class="calibre_13">Docker Compose: <a href="https://docs.docker.com/compose">https://docs.docker.com/compose</a></li><li value="8" class="calibre_13">Docker machine: <a href="https://docs.docker.com/machine/">https://docs.docker.com/machine/</a></li><li value="9" class="calibre_13">Tectonic: <a href="https://tectonic.com">https://tectonic.com</a></li><li value="10" class="calibre_13">Tectonic Distributed Trusted Computing: <a href="https://tectonic.com/blog/announcing-distributed-trusted-computing/">https://tectonic.com/blog/announcing-distributed-trusted-computing/</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_195"/>


<p id="filepos658492" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Container Orchestration with Kubernetes and CoreOS: <a href="https://www.youtube.com/watch?v=tA8XNVPZM2w">https://www.youtube.com/watch?v=tA8XNVPZM2w</a></li><li value="2" class="calibre_13">Comparing Orchestration solutions: <a href="http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html">http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html</a>, <a href="http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1">http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1</a>, and <a href="https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca">https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca</a></li><li value="3" class="calibre_13">Mesosphere introduction: <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere">https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere</a></li><li value="4" class="calibre_13">Docker and AWS ECS: 0<a href="https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga">https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga</a></li></ul><div class="mbp_pagebreak" id="calibre_pb_196"/>
</body></html>