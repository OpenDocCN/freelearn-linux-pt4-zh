<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Managing Nginx"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Managing Nginx</h1></div></div></div><p>In a web server running at full scale, thousands of events are occurring each second. Micromanaging these events is obviously not possible, yet even small glitches are able to cause serious deterioration of quality of service and affect user experience.</p><p>To prevent theses glitches from happening, a dedicated webmaster or site reliability engineer must be able to understand and properly manage the processes behind the scenes.</p><p>In this chapter, you will learn how to manage an Nginx instance in operation, and we will discuss the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Starting and stopping Nginx</li><li class="listitem" style="list-style-type: disc">Reloading and reconfiguring processes</li><li class="listitem" style="list-style-type: disc">Allocating worker processes</li><li class="listitem" style="list-style-type: disc">Other management questions</li></ul></div><div class="section" title="The Nginx connection processing architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec11"/>The Nginx connection processing architecture</h1></div></div></div><p>Before<a id="id97" class="indexterm"/> you study the management procedures of Nginx, you need to get some idea of how Nginx processes connections. In the full-scale<a id="id98" class="indexterm"/> mode, a single Nginx instance consists of the <span class="strong"><strong>master process</strong></span> <a id="id99" class="indexterm"/>and <span class="strong"><strong>worker processes</strong></span>, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B04282_02_01.jpg" alt="The Nginx connection processing architecture"/></div><p>The <a id="id100" class="indexterm"/>master process spawns worker processes and controls them by sending and forwarding signals and listening for quit notifications from them. Worker processes wait on listening sockets and accept incoming connections. The operating system distributes incoming connections among worker processes in a round-robin fashion.</p><p>The master process is<a id="id101" class="indexterm"/> responsible for all startup, shutdown, and maintenance tasks such as the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reading and re-reading configuration files</li><li class="listitem" style="list-style-type: disc">Opening and reopening log files</li><li class="listitem" style="list-style-type: disc">Creating listening sockets</li><li class="listitem" style="list-style-type: disc">Starting and restarting worker processes</li><li class="listitem" style="list-style-type: disc">Forwarding signals to the worker processes</li><li class="listitem" style="list-style-type: disc">Starting a new binary</li></ul></div><p>The master process thus ensures continuous operation of an Nginx instance in the face of various changes in the environment and occasional crashes of worker processes.</p><p>Worker processes are responsible for serving connections and accepting new ones. Worker <a id="id102" class="indexterm"/>processes can run certain maintenance tasks as well. For instance, they reopen log files on their own after the master process has ensured that this operation is safe. Each worker process handles multiple connections. This is achieved by running an event loop that pulls events that occurred on open sockets from the operating system via a special system call, and quickly processing all pulled events by reading from and writing to active sockets. Resources required to maintain a connection are allocated when a worker process starts. The maximum number of connections that a worker process can handle simultaneously is configured by the <code class="literal">worker_connections</code> directive and defaults to 512.</p><p>In a <span class="strong"><strong>clustered setup</strong></span>, a<a id="id103" class="indexterm"/> special routing device such as a load balancer or another Nginx instance is used to balance incoming connections among a set of identical Nginx instances, each of them consisting of a master process and a collection of worker processes. This is shown in the following figure:</p><div class="mediaobject"><img src="graphics/B04282_02_02.jpg" alt="The Nginx connection processing architecture"/></div><p>In this setup, the load balancer routes connections only to those instances that are listening for incoming connections. The load balancer ensures that each of the active instances gets an approximately equal amount of traffic, and routes traffic away from an instance if it shows any connectivity problems.</p><p>Because <a id="id104" class="indexterm"/>of the difference in the architecture, the management procedures for a clustered setup are slightly different than for<a id="id105" class="indexterm"/> a <span class="strong"><strong>standalone instance</strong></span>. We will discuss these differences soon.</p></div></div>
<div class="section" title="Starting and stopping Nginx"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec12"/>Starting and stopping Nginx</h1></div></div></div><p>In the<a id="id106" class="indexterm"/> previous chapter, you learned a bit about how to start your Nginx<a id="id107" class="indexterm"/> instance. On Ubuntu, Debian, or Redhat-like systems you can run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># service nginx start</strong></span>
</pre></div><p>In the absence of startup scripts, you can simply run the binary using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># sbin/nginx</strong></span>
</pre></div><p>Nginx will read and parse the configuration file, create a PID file (a file containing its process ID), open log files, create listening sockets, and start worker processes. Once worker processes have started, a Nginx instance is able to respond to incoming connections. This is what a running Nginx instance looks like in the process list:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      2324     1  0 15:30 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  2325  2324  0 15:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  2326  2324  0 15:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  2327  2324  0 15:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  2328  2324  0 15:30 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>Every Nginx process sets its process title such that it conveniently reflects the role of the process. Here, for example, you see the master process of the instance with process ID <code class="literal">2324</code> and four worker processes with process IDs <code class="literal">2325</code>, <code class="literal">2326</code>, <code class="literal">2327</code>, and <code class="literal">2328</code>. Note how<a id="id108" class="indexterm"/> the <span class="strong"><strong>parent process ID</strong></span> (<span class="strong"><strong>PPID</strong></span>) column points at the master process. We will refer to the ID of the master process further in this section.</p><p>If you can't find your instance in the process list or you see an error message on the console upon startup, something is preventing Nginx from starting. The following table lists<a id="id109" class="indexterm"/> potential<a id="id110" class="indexterm"/> issues and their solutions:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Message</p>
</th><th style="text-align: left" valign="bottom">
<p>Issue</p>
</th><th style="text-align: left" valign="bottom">
<p>Resolution</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">[emerg] bind() to x.x.x.x:x failed (98: Address already in use)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Conflicting listening endpoint</p>
</td><td style="text-align: left" valign="top">
<p>Make sure endpoints specified by the <code class="literal">listen</code> directive do not conflict with other services</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">[emerg] open() "&lt;path to file&gt;" failed (2: No such file or directory)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Invalid path to a file</p>
</td><td style="text-align: left" valign="top">
<p>Make sure all paths in your configuration point to existing directories</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">[emerg] open() "&lt;path to file&gt;" failed (13: Permission denied)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Insufficient privileges</p>
</td><td style="text-align: left" valign="top">
<p>Make sure all paths in your configuration point to directories that Nginx has access to</p>
</td></tr></tbody></table></div><p>To stop Nginx, you can run the following command if a startup script is available:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># service nginx stop</strong></span>
</pre></div><p>Alternatively, you can send the <code class="literal">TERM</code> or <code class="literal">INT</code> signal to the master process of your instance to trigger a fast shutdown or the <code class="literal">QUIT</code> signal to trigger a graceful shutdown, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -QUIT 2324</strong></span>
</pre></div><p>The <a id="id111" class="indexterm"/>preceding<a id="id112" class="indexterm"/> command will trigger the graceful shutdown procedure on the instance and all processes will eventually quit. Here, we refer to the process ID of the master process from the preceding process list.</p></div>
<div class="section" title="Control signals and their usage"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Control signals and their usage</h1></div></div></div><p>Nginx, like <a id="id113" class="indexterm"/>any other Unix background service, is controlled by<a id="id114" class="indexterm"/> signals. Signals are asynchronous events that interrupt normal execution of a process and activate certain functions. The following table lists all signals that Nginx supports and the functions that they trigger:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Signal</p>
</th><th style="text-align: left" valign="bottom">
<p>Function</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">TERM</code>, <code class="literal">INT</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Fast <a id="id115" class="indexterm"/>shutdown</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">QUIT</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Graceful <a id="id116" class="indexterm"/>shutdown</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">HUP</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Reconfiguration </p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">USR1</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Log<a id="id117" class="indexterm"/> file<a id="id118" class="indexterm"/> reopening</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">USR2</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Nginx <a id="id119" class="indexterm"/>binary upgrade</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">WINCH</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Graceful <a id="id120" class="indexterm"/>worker shutdown</p>
</td></tr></tbody></table></div><p>All signals must be sent to the master process of an instance. The master process of an instance can be located by looking it up in the process list:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      4754  3201  0 11:10 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  4755  4754  0 11:10 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  4756  4754  0 11:10 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  4757  4754  0 11:10 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  4758  4754  0 11:10 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>In this listing, the <a id="id121" class="indexterm"/>master process has a process ID <code class="literal">4754</code> and four worker processes. The process ID of the master process can be also obtained by examining the content of the PID file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># cat /var/run/nginx.pid</strong></span>
<span class="strong"><strong>4754</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>
<span class="strong"><strong>Note</strong></span>: The path of <code class="literal">nginx.pid</code> might vary in different systems. You can use the <code class="literal">/usr/sbin/nginx -V</code> command to find out the exact path.</p></div></div><p>To send a signal to an instance, use the <code class="literal">kill</code> command and specify the process ID of the master process as the last argument:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -HUP 4754</strong></span>
</pre></div><p>Alternatively, you can use command substitution to take the process ID of the master process directly from the PID file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -HUP `cat /var/run/nginx.pid`</strong></span>
</pre></div><p>You can also use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill - HUP $(cat /var/run/nginx.pid)</strong></span>
</pre></div><p>The preceding three commands will trigger reconfiguration of the instance. We will now discuss each of the functions that signals trigger in Nginx.</p><div class="section" title="Fast shutdown"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Fast shutdown</h2></div></div></div><p>The <code class="literal">TERM</code> and <code class="literal">INT</code> signals <a id="id122" class="indexterm"/>are sent to the master<a id="id123" class="indexterm"/> process of an Nginx instance to trigger the fast shutdown procedure. All resources such as connections, open files and log files that each worker process is in possession of are immediately closed. After that, each worker process quits and the master process gets notified. Once all worker processes quit, the master process quits and shutdown is completed.</p><p>A fast shutdown obviously causes visible service outage. Therefore, it must be used either in<a id="id124" class="indexterm"/> emergency situations or when you are absolutely<a id="id125" class="indexterm"/> sure that nobody is using your instance.</p></div><div class="section" title="Graceful shutdown"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Graceful shutdown</h2></div></div></div><p>Once Nginx receives the <code class="literal">QUIT</code> signal, it enters graceful shutdown mode. Nginx closes listening <a id="id126" class="indexterm"/>sockets and accepts no new connections from then on. Existing connections are still served until no longer needed. Therefore, graceful shutdown might take a long time to complete, especially if some of the connections are in the middle of a long download or upload.</p><p>After you have signaled graceful shutdown to Nginx, you can monitor your process list to see which Nginx worker processes are still running and keep track of the progress of your shutdown procedure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      5813  3201  0 12:07 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  5814  5813 11 12:07 ?        00:00:01 nginx: worker process is shutting down</strong></span>
</pre></div><p>In this listing, you can see an instance after a graceful shutdown has been triggered. A single worker process has an <code class="literal">is shutting down</code> label and its process title is marking a process that is currently shutting down.</p><p>Once all connections handled by a worker are closed, the worker process quits and the master process gets notified. Once all worker processes quit, the master process quits and shutdown is completed.</p><p>In a clustered or load-balanced setup, graceful shutdown is a typical way of putting an instance out of operation. Using graceful shutdown ensures that there are no visible outages of your service due to server reconfiguration or maintenance.</p><p>In a single instance, graceful shutdown can only make sure that existing connections are not closed abruptly. Once graceful shutdown is triggered on a single instance, the service will immediately be unavailable for new visitors. To ensure continuous availability on a single instance, use maintenance procedures such as reconfiguration, log file reopening, and Nginx binary update.</p></div><div class="section" title="Reconfiguration"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Reconfiguration</h2></div></div></div><p>The <code class="literal">HUP</code> signal can<a id="id127" class="indexterm"/> be used to signal Nginx to reread the <a id="id128" class="indexterm"/>configuration files and restart worker processes. This procedure cannot be performed without restarting worker processes, as configuration data structures cannot be changed while a worker process is running.</p><p>Once the master process receives the <code class="literal">HUP</code> signals, it tries to reread the configuration files. If the configuration files can be parsed and contain no errors, the master process signals all the existing worker process to gracefully shut down. After signaling, it starts new worker processes with the new configuration.</p><p>As with graceful shutdown, the reconfiguration procedure might take a long time to complete. After you have signaled the reconfiguration to Nginx, you can monitor your process list to see which old Nginx worker processes are still running and keep track of the progress of your reconfiguration.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>If another reconfiguration is triggered during a running reconfiguration procedure, Nginx will start a new collection of worker processes—even though worker processes from the past two rounds have not finished. This, in principle, might lead to excessive process table usage, so it's recommended that you wait until the current reconfiguration procedure is finished before starting a new one.</p></div></div><p>Here is an example of a reconfiguration procedure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      5887  3201  0 12:14 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  5888  5887  0 12:14 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5889  5887  0 12:14 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5890  5887  0 12:14 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5891  5887  0 12:14 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>This listing shows an operating Nginx instance. The master process has a process ID of <code class="literal">5887</code>. Let's send an <code class="literal">HUP</code> signal to the master process of the instance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -HUP 5887</strong></span>
</pre></div><p>The instance will change in the following way:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      5887  3201  0 12:14 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  5888  5887  5 12:14 ?        00:00:07 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data  5889  5887  0 12:14 ?        00:00:01 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data  5890  5887  0 12:14 ?        00:00:00 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data  5891  5887  0 12:14 ?        00:00:00 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data  5918  5887  0 12:16 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5919  5887  0 12:16 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5920  5887  0 12:16 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5921  5887  0 12:16 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>As you can<a id="id129" class="indexterm"/> see, the old worker processes with process IDs <code class="literal">5888</code>, <code class="literal">5889</code>, <code class="literal">5890</code>, and <code class="literal">5891</code> are currently shutting down. The master process has re-read the configuration files and spawned a new collection of worker processes with process IDs <code class="literal">5918</code>, <code class="literal">5919</code>, <code class="literal">5920</code>, and <code class="literal">5921</code>.</p><p>After a while, old worker processes will terminate and the instance will look like it did before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root      5887  3201  0 12:14 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data  5918  5887  1 12:16 ?        00:00:01 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5919  5887  3 12:16 ?        00:00:02 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5920  5887  6 12:16 ?        00:00:03 nginx: worker process</strong></span>
<span class="strong"><strong>www-data  5921  5887  3 12:16 ?        00:00:02 nginx: worker process</strong></span>
</pre></div><p>The new worker processes have picked up the new configuration now.</p></div><div class="section" title="Reopening the log file"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Reopening the log file</h2></div></div></div><p>Reopening<a id="id130" class="indexterm"/> the log file is simple yet extremely important for the continuous operation of your server. When log file reopening is triggered with the <code class="literal">USR1</code> signal, the master process of an instance takes the list of configured log files and opens each of them. If successful, it closes the old log files and signals worker processes to reopen the log files. Worker processes can now safely repeat the same procedure, and after that the log output is redirected to the new files. After that, worker processes close all old log file descriptors that they currently hold open.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>The paths to log files do not change during this procedure. Nginx expects that the old log files are renamed before triggering this function. That's why while opening log files with same paths, Nginx effectively creates or opens new files.</p></div></div><p>The<a id="id131" class="indexterm"/> steps of the log file reopening procedure are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log files are renamed or moved to new locations via an external tool.</li><li class="listitem">You send Nginx the <code class="literal">USR1</code> signal. Nginx closes the old files and opens new ones.</li><li class="listitem">Old files <a id="id132" class="indexterm"/>are now closed and can be archived.</li><li class="listitem">New files are now active and being used.</li></ol></div><p>A typical tool for managing Nginx log files is <span class="strong"><strong>logrotate</strong></span>. The logrotate tool is a quite common <a id="id133" class="indexterm"/>tool that can be found in many Linux distributions. Here is <a id="id134" class="indexterm"/>an example configuration file for logrotate that automatically performs the log file rotation procedure:</p><div class="informalexample"><pre class="programlisting">/var/log/nginx/*.log {
        daily
        missingok
        rotate 7
        compress
        delaycompress
        notifempty
        create 640 nginx adm
        sharedscripts
        postrotate
                [ -f /var/run/nginx.pid ] &amp;&amp; kill -USR1 `cat /var/run/nginx.pid`
        endscript
}</pre></div><p>The preceding script daily rotates each log file it can find in the <code class="literal">/var/log/nginx</code> folder. The log files are kept until seven files have accumulated. The <code class="literal">delaycompress</code> options specify that the log files should not be compressed immediately after rotation to avoid a situation where Nginx keeps writing to a file being compressed.</p><p>Problems<a id="id135" class="indexterm"/> in log file rotation procedure can lead to losses of data. Here is a checklist that will help you to configure your log file rotation procedure correctly:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Make sure the <code class="literal">USR1</code> signal is delivered only after log files are moved. Failure to do so will make Nginx write to rotated files instead of new ones.</li><li class="listitem" style="list-style-type: disc">Make sure Nginx has enough rights to create files in the log folder. If Nginx is not able to open new log files, the rotation procedure will fail.</li></ul></div></div><div class="section" title="Nginx binary upgrade"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Nginx binary upgrade</h2></div></div></div><p>Nginx is <a id="id136" class="indexterm"/>capable of updating its own binary while<a id="id137" class="indexterm"/> operating. This is done by passing listening sockets to a new binary and listing to them in a special environment variable.</p><p>This function can be used to safely upgrade your binary on-the-fly to a new version or try out new features if you use a custom binary with plugins.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>With other web servers, this operation would require stopping your server completely and starting it again with a new binary. Your service would be unavailable for a brief period. The Nginx binary upgrade function is used to avoid interruption of your service and provides a fall-back option if something goes wrong with the new binary.</p></div></div><p>To upgrade<a id="id138" class="indexterm"/> you binary, first make sure it has the same source code configuration as the old binary. Refer to the <span class="emphasis"><em>Copying source code configuration from pre-built packages</em></span> section in <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Nginx">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Nginx</em></span>, to learn how to build a binary with source code configuration from another binary.</p><p>When the new binary is built, rename the old one and put the new binary into its place:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mv /usr/sbin/nginx /usr/sbin/nginx.old</strong></span>
<span class="strong"><strong># mv objs/nginx /usr/sbin/nginx</strong></span>
</pre></div><p>The preceding sequence assumes your current working directory contains a Nginx source code tree.</p><p>Next, send the <code class="literal">USR2</code> signal to the master process of the running instance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -USR2 12995</strong></span>
</pre></div><p>The master process will rename its PID file by adding an <code class="literal">.oldbin</code> suffix and start the new binary that will create a new master process. The new master process will read and parse the configuration and spawn new worker processes. The instance now looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root     12995     1  0 13:28 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 12996 12995  0 13:28 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 12997 12995  0 13:28 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 12998 12995  0 13:28 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 12999 12995  0 13:28 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>root     13119 12995  0 13:30 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 13120 13119  2 13:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13121 13119  0 13:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13122 13119  0 13:30 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13123 13119  0 13:30 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>In the preceding code, we can see two master processes: one for the old binary (<code class="literal">12995</code>) and one for the new binary (<code class="literal">13119</code>). The new master process inherits the listening sockets from the old master process, and workers of both instances accept incoming connections.</p></div><div class="section" title="Graceful worker shutdown"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Graceful worker shutdown</h2></div></div></div><p>In order to fully test-drive the new binary, we need to ask the old master process to gracefully<a id="id139" class="indexterm"/> shut down its worker processes. Once the new binary has started and the working processes of the new binary are running, send the master process of the old instance the <code class="literal">WINCH</code> signal using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -WINCH 12995</strong></span>
</pre></div><p>Then, connections will be accepted only by workers of the new instance. The worker processes of the old instance will gracefully shut down:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root     12995     1  0 13:28 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 12996 12995  2 13:28 ?        00:00:17 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data 12998 12995  1 13:28 ?        00:00:13 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>www-data 12999 12995  2 13:28 ?        00:00:18 nginx: worker process is shutting down</strong></span>
<span class="strong"><strong>root     13119 12995  0 13:30 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 13120 13119  2 13:30 ?        00:00:18 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13121 13119  2 13:30 ?        00:00:16 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13122 13119  2 13:30 ?        00:00:12 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13123 13119  2 13:30 ?        00:00:15 nginx: worker process</strong></span>
</pre></div><p>Finally, the worker processes of the old binary will quit and only the worker processes of the new binary will remain:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root     12995     1  0 13:28 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>root     13119 12995  0 13:30 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 13120 13119  3 13:30 ?        00:00:20 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13121 13119  3 13:30 ?        00:00:20 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13122 13119  2 13:30 ?        00:00:16 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13123 13119  2 13:30 ?        00:00:17 nginx: worker process</strong></span>
</pre></div><p>Now, only the worker processes of the new binary are accepting and processing incoming connections.</p></div><div class="section" title="Finalizing the upgrade procedure"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec31"/>Finalizing the upgrade procedure</h2></div></div></div><p>Once<a id="id140" class="indexterm"/> only the workers of the new binary are running, you have two choices.</p><p>If the new binary is working well, the old master process can be terminated by sending the <code class="literal">QUIT</code> signal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -QUIT 12995</strong></span>
</pre></div><p>The old master process will remove its PID file and the instance is now ready for the next upgrade. Later, if you find any issues with the new binary, you can downgrade to the old binary by repeating the whole binary upgrade procedure.</p><p>If the new binary is not working properly, you can restart the worker processes of the old master process by sending the <code class="literal">HUP</code> signal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -HUP 12995</strong></span>
</pre></div><p>The old master process will restart its working processes without re-reading the configuration files, and workers of both old and new binaries will now accept incoming connections:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># ps -C nginx -f</strong></span>

<span class="strong"><strong>UID        PID  PPID  C STIME TTY          TIME CMD</strong></span>
<span class="strong"><strong>root     12995     1  0 13:28 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>root     13119 12995  0 13:30 ?        00:00:00 nginx: master process /usr/sbin/nginx</strong></span>
<span class="strong"><strong>www-data 13120 13119  4 13:30 ?        00:01:25 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13121 13119  4 13:30 ?        00:01:29 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13122 13119  4 13:30 ?        00:01:21 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13123 13119  4 13:30 ?        00:01:27 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13397 12995  4 14:02 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13398 12995  0 14:02 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13399 12995  0 14:02 ?        00:00:00 nginx: worker process</strong></span>
<span class="strong"><strong>www-data 13400 12995  0 14:02 ?        00:00:00 nginx: worker process</strong></span>
</pre></div><p>The processes of the new binary can be gracefully shut down by sending the new master process the <code class="literal">QUIT</code> signal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -QUIT  13119</strong></span>
</pre></div><p>After that, you need to return the old binary back to its location:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mv /usr/sbin/nginx.old /usr/sbin/nginx</strong></span>
</pre></div><p>The instance is now ready for the next upgrade.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>If a worker process is taking too long to quit for some reason, you can force it to quit by directly sending it the <code class="literal">KILL</code> signal.</p></div></div><p>If the new binary is not working properly and you need an urgent solution, you can urgently shut down the new master process by sending the <code class="literal">TERM</code> signal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># kill -TERM  13119</strong></span>
</pre></div><p>The processes of the new binary will immediately quit. The old master process will be notified and it will start new worker processes. The old master process will also move its PID file<a id="id141" class="indexterm"/> back to its original location so that it replaces the PID file of the new binary. After that, you need to return the old binary back to its original location:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mv /usr/sbin/nginx.old /usr/sbin/nginx</strong></span>
</pre></div><p>The instance is now ready for further operation or the next upgrade.</p></div><div class="section" title="Handling difficult cases"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec32"/>Handling difficult cases</h2></div></div></div><p>In extremely <a id="id142" class="indexterm"/>rare cases, you might run into a difficult situation. If a worker process does not shut down when asked to in a reasonable time, there might be a problem with it. Typical signs of such problems are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A process spends too much time in the running state (R) and does not shut down</li><li class="listitem" style="list-style-type: disc">A process spends too much time in the noninterruptible sleep state (D) and does not shut down</li><li class="listitem" style="list-style-type: disc">A process is sleeping (S) and does not shut down</li></ul></div><p>In each of these cases, you can force the worker process to shut down by first sending the <code class="literal">TERM</code> signal directly to the worker process. If the worker process does not react within 30 seconds, you can force the process to quit by sending it the <code class="literal">KILL</code> signal.</p></div></div>
<div class="section" title="Distribution-specific startup scripts"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Distribution-specific startup scripts</h1></div></div></div><p>On<a id="id143" class="indexterm"/> Ubuntu, Debian, and RHEL, the startup script automates the preceding control sequences. By using the startup script, you don't need to remember the exact sequence of the commands and signal names. The following <a id="id144" class="indexterm"/>table illustrates the use of the startup script:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Command</p>
</th><th style="text-align: left" valign="bottom">
<p>Equivalent to</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx start</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">sbin</code>/<code class="literal">nginx</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx stop</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">TERM</code>, <code class="literal">wait 30 seconds</code>, then <code class="literal">KILL</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx restart</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx stop</code> and <code class="literal">service nginx start</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx configtest</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">nginx -t &lt;config file&gt;</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx reload</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx configtest</code> and <code class="literal">HUP</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx rotate</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">USR1</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx upgrade</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">USR2</code> and <code class="literal">QUIT to the old master</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">service nginx status</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">show status of the instance</code>
</p>
</td></tr></tbody></table></div><p>The binary<a id="id145" class="indexterm"/> upgrade procedure is limited <a id="id146" class="indexterm"/>to starting the new binary and signaling the old master process to gracefully shut down, so you don't have an option to test-drive the new binary in this case.</p></div>
<div class="section" title="Allocating worker processes"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Allocating worker processes</h1></div></div></div><p>We now<a id="id147" class="indexterm"/> consider recommendations on allocating worker processes. First, let's discuss a little bit about the background. Nginx is an asynchronous web server, which means actual input/output operations run asynchronously with the execution of a worker process. Each worker process runs an event loop that fetches all file descriptors that need processing using a special system call, and then services each of these file descriptors using nonblocking I/O operations. Hence, each worker process serves multiple connections.</p><p>In this situation, the time between an event occurs on a file descriptor, and this file descriptor can be serviced (that is latency) depends on how soon a full event processing cycle can be completed. Therefore, in order to achieve higher latency it makes sense to penalize the competition for CPU resources between worker processes in favor of more connections per process, because this would reduce the number of context switches between worker processes.</p><p>Therefore, <span class="emphasis"><em>on the systems that are CPU-bound</em></span>, it makes sense to allocate as many worker processes as there are CPU cores in the system. For example, consider this output of the <code class="literal">top</code> command (this output can be obtained by pressing <span class="emphasis"><em>1</em></span> on the keyboard after <code class="literal">top</code> starts):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>top - 10:52:54 up 48 min,  2 users,  load average: 0.11, 0.18, 0.27</strong></span>
<span class="strong"><strong>Tasks: 273 total,   2 running, 271 sleeping,   0 stopped,   0 zombie</strong></span>
<span class="strong"><strong>%Cpu0  :  1.7 us,  0.3 sy,  0.0 ni, 97.7 id,  0.3 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu1  :  0.7 us,  0.3 sy,  0.0 ni, 94.7 id,  4.0 wa,  0.0 hi,  0.3 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu2  :  1.7 us,  1.0 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu3  :  3.0 us,  1.0 sy,  0.0 ni, 95.0 id,  1.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu4  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu5  :  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu6  :  0.3 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
<span class="strong"><strong>%Cpu7  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</strong></span>
</pre></div><p>This system <a id="id148" class="indexterm"/>has eight independent CPU cores. The maximum number of worker processes that will not compete for CPU cores on this system is therefore eight. To configure Nginx to start a specified number of worker processes, you can use the <code class="literal">worker_processes</code> directive in the main configuration file:</p><div class="informalexample"><pre class="programlisting">worker_processes 8;</pre></div><p>The preceding command will instruct Nginx to start eight worker processes to serve the incoming connections.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>If the number of worker processes is set to a number lower than the number of CPU cores, Nginx will not be able to take advantage of all parallelism available in your system.</p></div></div><p>To extend the maximum number of connections that can be processed by a worker process, use the <code class="literal">worker_connections</code> directive:</p><div class="informalexample"><pre class="programlisting">events {
    worker_connections 10000;
}</pre></div><p>The preceding command will extend the total number of connection that can be allocated to 10,000. This includes both inbound (connections from clients) and outbound connections (connections to proxied servers and other external resources).</p><p>
<span class="emphasis"><em>On disk I/O-bound systems</em></span>, in the absence of the AIO facility, additional latency might be introduced into the event cycle due to blocking disk I/O operations. While a worker process is waiting for a blocking disk I/O operation to complete on a certain file descriptor, the other file descriptors cannot be serviced. However, other processes can use the available CPU resources. Therefore, adding worker processes past the number of available I/O channels might not lead to an improvement in performance.</p><p>On systems with mixed resource demands, a worker process allocation strategy other than the previously mentioned two might be needed to achieve better performance. Try varying the numbers of workers in order to obtain the configuration that works best. This can range <a id="id149" class="indexterm"/>from one worker to hundreds of workers.</p></div>
<div class="section" title="Setting up Nginx to serve static data"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Setting up Nginx to serve static data</h1></div></div></div><p>Now that <a id="id150" class="indexterm"/>you are more proficient in installing, configuring, and managing Nginx, we can proceed with some practical questions. Let's see how we can set up Nginx to serve static data such as images, CSS, or JavaScript files.</p><p>First, we will take the sample configuration from the previous chapter and make it support multiple virtual hosts using wild card inclusion:</p><div class="informalexample"><pre class="programlisting">error_log  logs/error.log;

worker_processes 8;

events {
    use epoll;
    worker_connections  10000;
}

http {
    include           mime.types;
    default_type      application/octet-stream;

    include /etc/nginx/site-enabled/*.conf;
}</pre></div><p>We have set up Nginx to take advantage of eight processor cores and include all configurations files located in <code class="literal">/etc/nginx/site-enabled</code>.</p><p>Next, we will configure a virtual host <code class="literal">static.example.com</code> for serving static data. The following content goes into the file <code class="literal">/etc/nginx/site-enabled/static.example.com.conf</code>:</p><div class="informalexample"><pre class="programlisting">server {
    listen       80;
    server_name  static.example.com;

    access_log  /var/log/nginx/static.example.com-access.log  main;

    sendfile on;
    sendfile_max_chunk 1M;
    tcp_nopush on;
    gzip_static on;

    root /usr/local/www/static.example.com;
}</pre></div><p>This file configures virtual host <code class="literal">static.example.com</code>. The virtual host root location is set as <code class="literal">/usr/local/www/static.example.com</code>. To enable more efficient retrieval of static files, we encourage Nginx to use the sendfile() system call (<code class="literal">sendfile on</code>) and set the maximum<a id="id151" class="indexterm"/> sendfile chunk to 1 MB. We also enable the "TCP_NOPUSH" option to improve TCP segment utilization when using sendfile() (<code class="literal">tcp_nopush on</code>).</p><p>The <code class="literal">gzip_static on</code> directive instructs Nginx to check for gzipped copies of static files, such as <code class="literal">main.js.gz</code> for <code class="literal">main.js</code> and <code class="literal">styles.css.gz</code> for <code class="literal">styles.css</code>. If they are found, Nginx will indicate the presence of the <code class="literal">.gzip</code> content encoding, and use the content of the compressed files instead of the original one.</p><p>This configuration is suitable for virtual hosts that serve small-to-medium size static files.</p></div>
<div class="section" title="Installing SSL certificates"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Installing SSL certificates</h1></div></div></div><p>Today, more than 60 percent of the HTTP traffic on the Internet is protected by SSL. In the presence<a id="id152" class="indexterm"/> of sophisticated attacks such as cache poisoning and DNS hijacking, SSL is mandatory if your web content has any value.</p><p>Nginx has high-class SSL support and makes it easy for you to configure. Let's walk over the installation procedure of an SSL virtual host.</p><p>Before we start, make sure the <code class="literal">openssl</code> package is installed on your system:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># apt-get install openssl</strong></span>
</pre></div><p>This will insure that you have the necessary tools to go over the SSL certificate issuing procedure.</p><div class="section" title="Creating a Certificate Signing Request"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec33"/>Creating a Certificate Signing Request</h2></div></div></div><p>You <a id="id153" class="indexterm"/>need an SSL certificate<a id="id154" class="indexterm"/> in order to set up an SSL virtual host. In order to obtain a real certificate, you need to contact a certification authority to issue an SSL certificate. A certification authority will usually charge you a fee for that.</p><p>To issue <a id="id155" class="indexterm"/>an SSL certificate, a certification authority needs a <span class="strong"><strong>Certificate Signing Request</strong></span> (<span class="strong"><strong>CSR</strong></span>) from you. A CSR is a message created by you and sent to a certification authority containing your identification data, such as distinguished name, address, and your public key.</p><p>To generate a CSR, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>openssl req -new -newkey rsa:2048 -nodes -keyout your_domain_name.key -out your_domain_name.csr</strong></span>
</pre></div><p>This will start the process of generating two files: a private key for the decryption of your SSL <a id="id156" class="indexterm"/>certificate (<code class="literal">your_domain_name.key</code>) and a certificate signing request (<code class="literal">your_domain_name.csr</code>) used to apply for a new SSL certificate.</p><p>This command will ask you for your identification data:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Country name</strong></span> (<span class="strong"><strong>C</strong></span>): This <a id="id157" class="indexterm"/>is a two-letter country code, for example, NL or US.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>State or province</strong></span> (<span class="strong"><strong>S</strong></span>): This is the full name of <a id="id158" class="indexterm"/>the state you or your company is in, for example, Noord-Holland.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Locality or city</strong></span> (<span class="strong"><strong>L</strong></span>): This<a id="id159" class="indexterm"/> is the city or town you or your company is in, for example, Amsterdam.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Organization</strong></span> (<span class="strong"><strong>O</strong></span>): If your company or<a id="id160" class="indexterm"/> department has <span class="emphasis"><em>&amp;</em></span>, <span class="emphasis"><em>@</em></span>, or any other symbol using the <span class="emphasis"><em>Shift</em></span> key in its name, you must spell out the symbol or omit it to enroll. For example, XY &amp; Z Corporation would be XYZ Corporation or XY and Z Corporation.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Organizational Unit</strong></span> (<span class="strong"><strong>OU</strong></span>): This field is <a id="id161" class="indexterm"/>the name of the department or organization unit making the request.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Common name</strong></span> (<span class="strong"><strong>CN</strong></span>): This is the full name<a id="id162" class="indexterm"/> of the host you are protecting.</li></ul></div><p>The last field is of particular importance here. It must match the full name of the host you are protecting. For instance, if you registered a domain <code class="literal">example.com</code> and users will connect to <code class="literal">www.example.com</code>, you must enter <code class="literal">www.example.com</code> into the common name field. If you enter <code class="literal">example.com</code> into that field, the certificate will not be valid for <code class="literal">www.example.com</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>Do not fill in optional attributes such as e-mail address, challenge password, or the optional company name when generating the CSR. They do not add much value, but just expose more personal data.</p></div></div><p>Your CSR is ready now. After you save your private key to some secure place, you can proceed with contacting a certification authority and enrolling for an SSL certificate. Present your CSR once requested.</p></div><div class="section" title="Installing an issued SSL certificate"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec34"/>Installing an issued SSL certificate</h2></div></div></div><p>Once <a id="id163" class="indexterm"/>your certificate is issued, you can proceed with setting up your SSL server. Save your certificate under a descriptive name such as <code class="literal">your_domain_name.crt</code>. Move it to a secure directory that only Nginx and superuser have access to. We will use <code class="literal">/etc/ssl</code> for simplicity as an example of such a directory.</p><p>Now, you can start adding configuration for your secure virtual host:</p><div class="informalexample"><pre class="programlisting">server     {
        listen 443;
  server_name your.domain.com;
  ssl on;
  ssl_certificate /etc/ssl/your_domain_name.crt;
  ssl_certificate_key /etc/ssl/your_domain_name.key;
  [… the rest of the configuration ...]
}</pre></div><p>The name of the domain in the <code class="literal">server_name</code> directive must match the value of the common name field in your certificate signing request.</p><p>After the configuration is saved, restart Nginx using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># service nginx restart</strong></span>
</pre></div><p>Navigate to <code class="literal">https://your.domain.com</code> to open a secure connection to your server now.</p></div><div class="section" title="Permanently redirecting from a nonsecure virtual host"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec35"/>Permanently redirecting from a nonsecure virtual host</h2></div></div></div><p>The <a id="id164" class="indexterm"/>preceding configuration handles only requests issued to the HTTPS service (port <code class="literal">443</code>) of your server. Most of the time, you will be running the plain HTTP service (port <code class="literal">80</code>) next to the secure one.</p><p>For a number of reasons, it's unwise to have different configurations for the plain HTTP and HTTPS services for the same host name. If certain resources are available over plain HTTP but not over SSL or the other way around, this might lead to bad references if a URL pointing to one of your resources is treated in a scheme-agnostic way.</p><p>Likewise, if certain resources are made available over both plain HTTP and SSL by mistake, then it is a security error because the resource can be obtained and interacted with in a nonsecure way by simply changing the <code class="literal">https://</code> scheme to <code class="literal">http://</code>.</p><p>To avoid these problems and to simplify your configuration, you can set up a simple permanent redirect from the non-SSL virtual host to the SSL virtual host:</p><div class="informalexample"><pre class="programlisting">server {
    listen       80;
    server_name  your.domain.com;

    rewrite ^/(.*)$ https://your.domain.com/$1 permanent;
}</pre></div><p>This ensures that all requests over plain HTTP to any resource on your web site will be redirected <a id="id165" class="indexterm"/>to the identical resource on the SSL virtual host.</p></div></div>
<div class="section" title="Managing temporary files"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Managing temporary files</h1></div></div></div><p>Managing<a id="id166" class="indexterm"/> temporary files is usually not a big deal, but you must be aware of it. Nginx uses temporary files to store transient data such as the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Large request bodies received from users</li><li class="listitem" style="list-style-type: disc">Large response bodies received from proxied servers or via FastCGI, SCGI, or UWCGI protocols.</li></ul></div><p>In the <span class="emphasis"><em>Installing Nginx</em></span> section of <a class="link" href="ch01.html" title="Chapter 1. Getting Started with Nginx">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Nginx</em></span>, you saw the default location of temporary folders for these files. The following table lists the configuration directives that specify temporary folders for various Nginx core modules:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Directive</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">client_body_temp_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies temporary path for <a id="id167" class="indexterm"/>client request body data</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">proxy_temp_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies temporary path for<a id="id168" class="indexterm"/> responses from proxied servers</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">fastcgi_temp_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies temporary path<a id="id169" class="indexterm"/> for responses from FastCGI servers</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">scgi_temp_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies temporary path for <a id="id170" class="indexterm"/>responses from SCGI servers</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">uwsgi_temp_path</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies temporary path for<a id="id171" class="indexterm"/> responses from UWCGI servers</p>
</td></tr></tbody></table></div><p>The arguments of the preceding directives are as follows</p><div class="informalexample"><pre class="programlisting">proxy_temp_path &lt;path&gt; [&lt;level1&gt; [&lt;level2&gt; [&lt;level3&gt;]]]</pre></div><p>In the preceding code, <code class="literal">&lt;path&gt;</code> specifies the path to the directory that contains temporary files, and the levels specify the number of characters in each level of hashed directories.</p><p>What is a hashed directory? In UNIX, a directory in the file system is essentially a file that simply contains a list of entries of that directory. So, imagine one of your temporary directories contains 100,000 entries. Each search in this directory routinely scans all of these 100,000 entries, which is not very efficient. To avoid this, you can split your temporary directory into a number of subdirectories, each of them containing a limited set of temporary files.</p><p>By specifying levels, you instruct Nginx to split your temporary directory into a set of subdirectories, each having a specified number of characters in its name, for example, a directive:</p><div class="informalexample"><pre class="programlisting">proxy_temp_path /var/lib/nginx/proxy 2;</pre></div><p>The preceding line of code instructs Nginx to store a temporary file named <code class="literal">3924510929</code> under the path <code class="literal">/var/lib/nginx/proxy/29/3924510929</code>.</p><p>Likewise, the <a id="id172" class="indexterm"/>directive <code class="literal">proxy_temp_path /var/lib/nginx/proxy 1 2</code> instructs Nginx to store a temporary file named <code class="literal">1673539942</code> under the path <code class="literal">/var/lib/nginx/proxy/2/94/1673539942</code>.</p><p>As you can see, the characters that constitute the names of the intermediary directories are extracted from the tail of the temporary file name.</p><p>Both hierarchical and nonhierarchical temporary directory structures have to be purged from time to time. This could be achieved by walking the directory tree and removing all files residing in those directories. You can use a command like the following one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>find /var/lib/nginx/proxy -type f -regex '.+/[0-9]+$' | xargs -I '{}' rm "{}"</strong></span>
</pre></div><p>You can use the command from the interactive shell. This command will find all files ending with digits located in the temporary directory and remove each of these files by running <code class="literal">rm</code>. This command will prompt the removal if it finds something strange.</p><p>For the noninteractive mode, you can use a more dangerous command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>find /var/lib/nginx/proxy -type f -regex '.+/[0-9]+$' | xargs -I '{}' rm -f "{}"</strong></span>
</pre></div><p>This command will not prompt the removal of files.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>This command is dangerous as it blindly removes a broadly-specified set of files. To avoid data loss, stick to the following principles when managing temporary directories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Never store anything but temporary files inside temporary directories</li><li class="listitem" style="list-style-type: disc">Always use absolute paths in the first argument of a <code class="literal">find</code> command</li><li class="listitem" style="list-style-type: disc">If possible, check what you are about to remove by substituting <code class="literal">rm</code> with <code class="literal">echo</code> in order to print the list of files to be supplied to <code class="literal">rm</code></li><li class="listitem" style="list-style-type: disc">Make<a id="id173" class="indexterm"/> sure Nginx stores temporary files under a specially-designated user such as <code class="literal">nobody</code> or <code class="literal">www-data</code>, and never under the superuser</li><li class="listitem" style="list-style-type: disc">Make sure the command above runs under a specially-designated user such as <code class="literal">nobody</code> or <code class="literal">www-data</code>, and never under the superuser</li></ul></div></div></div></div>
<div class="section" title="Communicating issues to developers"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Communicating issues to developers</h1></div></div></div><p>If you <a id="id174" class="indexterm"/>are running nonstable versions of Nginx for<a id="id175" class="indexterm"/> trial or using your own or third-party modules for Nginx, your instance might occasionally experience crashes. If you decide to communicate these issues to developers, here is a guide that will help you to do it most efficiently.</p><p>Developers usually don't have access to production systems, but knowing the environment your Nginx instance is running in is crucial to trace the cause of the problem.</p><p>Therefore, you need to provide detailed information about the issue. Detailed information about a crash can be found in the core file that was created after the crash.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>
<span class="strong"><strong>Warning!</strong></span>
</p><p>The core file contains a memory dump of a worker process at the moment of a crash and therefore can contain sensitive information, such as passwords, keys, or private data. Therefore, never share core files with people you don't trust.</p></div></div><p>Instead, use the<a id="id176" class="indexterm"/> following procedure to obtain detailed information about a crash:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get a copy of the Nginx binary that you run with debugging information (see following instructions)</li><li class="listitem">If a core file is available, run <code class="literal">gdb</code> on the binary with the debugging information:<div class="informalexample"><pre class="programlisting"># gdb ./nginx-binary core</pre></div></li><li class="listitem">If the run is successful, this will open the <code class="literal">gdb</code> prompt. Type <code class="literal">bt full</code> in it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(gdb) bt full</strong></span>
<span class="strong"><strong>[… produces a dump … ]</strong></span>
</pre></div></li></ol></div><p>The preceding command will produce a long dump of the stack at the moment of the crash and it's usually sufficient to debug a wide variety of problems. Make a summary of the configuration that resulted in a crash and send it over to the developer along with the full stack trace.</p><div class="section" title="Creating a binary with debugging information"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec36"/>Creating a binary with debugging information</h2></div></div></div><p>A detailed <a id="id177" class="indexterm"/>stack trace can be obtained only from <a id="id178" class="indexterm"/>a binary with debugging information. You don't necessarily need to run a binary with debugging information. It's only necessary to have a binary that is identical to the one that you run, but with extra debugging information on top of it.</p><p>It is possible to produce such a binary from the source code of the binary that you are running by configuring the source tree with an extra <code class="literal">–with-debug</code> option. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, obtain configuration script arguments from the binary your instance is running:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /usr/sbin/nginx -V</strong></span>
</pre></div></li><li class="listitem">Add the <code class="literal">–with-debug</code> option in front of the argument string and run the configuration scripts:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./configure –with-debug --with-cc-opt='-g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro' …</strong></span>
</pre></div></li></ol></div><p>Follow the remaining steps of the build procedure (refer to the previous chapter for details). Once you finish, a binary identical to the one that you are running but with debugging information appears in the <code class="literal">objs</code> directory of your source tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ file objs/nginx</strong></span>
<span class="strong"><strong>objs/nginx: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=7afba0f9be717c965a3cfaaefb6e2325bdcea676, not stripped</strong></span>
</pre></div><p>Now, you can use this binary to obtain a full stack trace from the core file produced by its twin binary.</p><p>Refer to the previous section in order to learn how to produce a stack trace.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Summary</h1></div></div></div><p>In this chapter, you learned a lot of Nginx management techniques. We covered almost the full circle of Nginx operation, except for problem-dependent details. In the next and further chapters, you will start learning about particular features of Nginx and how to apply them. This will add some more flesh to your Nginx core skills.</p></div></body></html>