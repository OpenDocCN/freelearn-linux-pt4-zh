<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Virtualization and Cloud Computing inside the Ubuntu Server"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Virtualization and Cloud Computing inside the Ubuntu Server</h1></div></div></div><p>Virtualization and Cloud computing are some of the hottest topics in system administration communities nowadays. They give system administrators the possibility to run more servers on the same hardware and use resources in a reliable manner. The Cloud concept, which is already based on virtualization, provides much more benefits, especially via security and new business models such as SaaS, PaaS, and IaaS.</p><p>Ubuntu provides a good set of virtualization and Cloud computing platforms. In this chapter, we will have a look at how the Ubuntu Server handles the most well known platforms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In the first section, we will focus on virtualization. We will discover how to manage three big virtualization programs, namely KVM, XenServer, and Docker.</li><li class="listitem" style="list-style-type: disc">In the second section, we will take a look at the Cloud capabilities provided by the Ubuntu Server.</li></ul></div><div class="section" title="Virtualization"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Virtualization</h1></div></div></div><p>There are a number <a class="indexterm" id="id276"/>of different virtualization technologies available under the Ubuntu Server. In this section, we will discover the virtualization concept in general with its different technologies and approach. Then, we will study some of the most popular virtualization programs, such as <a class="indexterm" id="id277"/>
<span class="strong"><strong>Kernel-based Virtual Machine</strong></span> (<span class="strong"><strong>KVM</strong></span>), XenServer and Docker, with one program from each technology.</p><div class="section" title="An introduction to virtualization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec54"/>An introduction to virtualization</h2></div></div></div><p>Virtualization is<a class="indexterm" id="id278"/> used to run one or more operating systems / applications as a simple software on one or more computers/servers instead of not being able to install more than one <a class="indexterm" id="id279"/>operating system per machine. These <span class="strong"><strong>virtual machines</strong></span> are called <span class="strong"><strong>VMs</strong></span>, <span class="strong"><strong>environments</strong></span>, and even <a class="indexterm" id="id280"/>
<span class="strong"><strong>VEs</strong></span>. The virtualization of <a class="indexterm" id="id281"/>
<span class="strong"><strong>operating systems</strong></span> (<span class="strong"><strong>OS</strong></span>) is a technique that allows you to run multiple OSes simultaneously on a single computer as if they were working on separate computers.</p></div><div class="section" title="The benefits of virtualization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec55"/>The benefits of virtualization</h2></div></div></div><p>Virtualization has<a class="indexterm" id="id282"/> several advantages. The following are some of its benefits:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">You can use a different OS without restarting your computer to use programs that do not natively work on Ubuntu</li><li class="listitem" style="list-style-type: disc">You can also use devices that don't work with Ubuntu but that can work with other OSs</li><li class="listitem" style="list-style-type: disc">You can perform testing under the operating systems without jeopardizing a stable environment</li><li class="listitem" style="list-style-type: disc">You can also perform software testing in controlled, isolated and secure environments</li><li class="listitem" style="list-style-type: disc">You can transport OS from one computer to another using a virtual machine running on a computer with a compatible hypervisor</li></ul></div><p>Individuals and <span class="strong"><strong>Small and Medium Enterprise</strong></span> (<span class="strong"><strong>SMEs</strong></span>) / <span class="strong"><strong>Small and Medium Industries</strong></span> (<span class="strong"><strong>SMIs</strong></span>) will generally be more interested in running two different OSes at the same time to run software that are compatible with one hypervisor but not the other. Large companies are increasingly using virtualization to save space in server rooms, simplify installations, facilitate restarts after incidents, and of course, develop secure and reliable business networks.</p></div><div class="section" title="Different techniques of virtualization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Different techniques of virtualization</h2></div></div></div><p>The main aspect <a class="indexterm" id="id283"/>of the virtualization concept is an entity called <a class="indexterm" id="id284"/>
<span class="strong"><strong>hypervisor</strong></span>. A hypervisor <a class="indexterm" id="id285"/>is a software, firmware, or hardware that creates and runs virtual machines. The machine that runs a hypervisor is called the host machine, and every virtual machine that runs on a hypervisor is called a guest machine. There are two types of hypervisors that we will see in this section. Besides a hypervisor, there is another main piece of virtualization concept that takes more than one name. Some call it <a class="indexterm" id="id286"/>an <span class="strong"><strong>isolator</strong></span>, while <a class="indexterm" id="id287"/>others call it a <span class="strong"><strong>container</strong></span>, <span class="strong"><strong>virtualization engine</strong></span>, or even an <a class="indexterm" id="id288"/>
<span class="strong"><strong>operating-system-level virtualization</strong></span>. In our case, we <a class="indexterm" id="id289"/>will call it an isolator. We will discover this, as well as the two hypervisor types, in this section.</p><div class="section" title="Type 1 hypervisor"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec15"/>Type 1 hypervisor</h3></div></div></div><p>This <a class="indexterm" id="id290"/>type of hypervisor is also called a <a class="indexterm" id="id291"/>
<span class="strong"><strong>native</strong></span> or <span class="strong"><strong>bare-metal hypervisor</strong></span>. It runs <a class="indexterm" id="id292"/>directly on a host's hardware, and it handles the hardware directly and manages the guest OS. An example <a class="indexterm" id="id293"/>of this hypervisor type is the <span class="strong"><strong>XenServer</strong></span>.</p><p>We can model it by using the following schema:</p><div class="mediaobject"><img alt="Type 1 hypervisor" src="graphics/B04800_05_01.jpg"/></div></div><div class="section" title="Type 2 hypervisor"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec16"/>Type 2 hypervisor</h3></div></div></div><p>This type <a class="indexterm" id="id294"/>of hypervisor is also called a <a class="indexterm" id="id295"/>
<span class="strong"><strong>hosted hypervisor</strong></span>. It runs on the host OS just like any other software. It provides an emulation of the hardware level to guest systems. An example of this hypervisor type is <a class="indexterm" id="id296"/>
<span class="strong"><strong>Oracle VirtualBox</strong></span>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>Some virtualization programs, such as KVM, can't be easily classified into either of the two types. <span class="strong"><strong>KVM</strong></span> is a <a class="indexterm" id="id297"/>kernel module that converts the host OS to a type 1 hypervisor, but at the same time, the host OS always works as a general-purpose OS that runs other applications that compete for VM resources. Therefore, KVM can also be categorized as a type 2 hypervisor.</p></div></div><p>We can model <a class="indexterm" id="id298"/>type 2 hypervisor by using the following schema:</p><div class="mediaobject"><img alt="Type 2 hypervisor" src="graphics/B04800_05_02.jpg"/></div></div><div class="section" title="An isolator"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec17"/>An isolator</h3></div></div></div><p>An <a class="indexterm" id="id299"/>isolator<a class="indexterm" id="id300"/> is a piece of software that is used to isolate applications' executions in what are called contexts or execution areas. An isolator allows you to run the same application in a multi-instance mode several times (multiple execution instances) even if it was not designed for it. This solution is very efficient because of the little overhead (the time spent by a system to do more than just manage itself). Note that virtualized environments are not completely isolated. The performance is always a key factor. However, we cannot really talk about OS virtualization. We can model it by using the following schema:</p><div class="mediaobject"><img alt="An isolator" src="graphics/B04800_05_03.jpg"/></div></div></div><div class="section" title="The different approaches towards virtualization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec57"/>The different approaches towards virtualization</h2></div></div></div><p>Before <a class="indexterm" id="id301"/>starting a virtualization project, it is mandatory to understand the two main approaches in this field—<span class="strong"><strong>full virtualization</strong></span> and <span class="strong"><strong>paravirtualization</strong></span>. Both XenServer and KVM offer these two approaches. Therefore, you need to know the differences between them very well, and this is what we will explain in the following sections.</p><div class="section" title="Paravirtualization"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec18"/>Paravirtualization</h3></div></div></div><p>The <a class="indexterm" id="id302"/>paravirtualization approach requires<a class="indexterm" id="id303"/> a modified version of the guest OS, that generates special instructions that can be easily handled by the hypervisor, which simply interprets and passes them to the physical hardware.</p><p>In this case, the guest OS knows that it is virtualized. As a result, it will generate instructions that are best optimized for use in a VE and don't need to be translated first.</p></div><div class="section" title="Full virtualization"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec19"/>Full virtualization</h3></div></div></div><p>The other <a class="indexterm" id="id304"/>approach is full virtualization, which allows you to use an unmodified OS as a guest. One of its disadvantages is that it requires special hardware support, which is something that is nowadays provided as a special feature in modern CPUs (both AMD and Intel processors). Thanks to this built-in support within the server's CPU, fully virtualized machines can work as efficiently as possible in spite of the fact that the instructions coming from the virtualized OS first need to be translated by the hypervisor.</p></div></div><div class="section" title="KVM (Kernel-based Virtual Machine)"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec58"/>KVM (Kernel-based Virtual Machine)</h2></div></div></div><p>In this <a class="indexterm" id="id305"/>section, we will discover the default virtualization technology that is actually supported by Ubuntu. Named <a class="indexterm" id="id306"/>
<span class="strong"><strong>KVM</strong></span> (<span class="strong"><strong>Kernel-based Virtual Machine</strong></span>), this virtualization technology is a free software with support built into the Linux kernel. This software takes advantage of the virtualization support that is built into the Intel and AMD processors and allows you to run a number of different distributions and OSes as <span class="strong"><strong>VMs</strong></span> (<span class="strong"><strong>virtual machines</strong></span>) on a single host.</p><div class="section" title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec20"/>Prerequisites</h3></div></div></div><p>Before <a class="indexterm" id="id307"/>starting the installation process, you should verify that your computer supports virtualization. To check this, you need to run the <code class="literal">kvm-ok</code> command, which is a part of the <code class="literal">cpu-checker</code> package. Therefore, you first of all need to install this package and then invoke the command by using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install cpu-checker</strong></span>
<span class="strong"><strong>sudo kvm-ok</strong></span>
</pre></div><p>Then, check the result and verify that you got the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>INFO: Your CPU does not support KVM extensions</strong></span>
<span class="strong"><strong>KVM acceleration can NOT be used</strong></span>
</pre></div><p>This means that you should look for another computer. However, you may get something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>INFO: /dev/kvm does not exist</strong></span>
<span class="strong"><strong>HINT: sudo modprobe kvm_intel</strong></span>
<span class="strong"><strong>INFO: Your CPU supports KVM extensions</strong></span>
<span class="strong"><strong>KVM acceleration can be used</strong></span>
</pre></div><p>Alternatively, you may get something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>INFO: /dev/kvm exists</strong></span>
<span class="strong"><strong>KVM acceleration can be used</strong></span>
</pre></div><p>This means that you can move on to the next section.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>Sometimes your CPU supports virtualization, but you may get a message saying that it can't. In such cases, most of the time, virtualization is disabled in the BIOS. Therefore, you will have to enable it from there. All that you need to do is restart your computer and access the BIOS by using the appropriate function key (it appears on the screen for a few seconds just after the boot; most of the time, you need to use <span class="emphasis"><em>F12</em></span>). From the BIOS screen that appears, look for something such as a CPU or performance heading and select it. Then, look for a virtualization selection, such as <span class="strong"><strong>Intel Virtualization Technology</strong></span>, and enable it. Save your changes and reboot.</p></div></div></div><div class="section" title="Configuring the KVM networking"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec21"/>Configuring the KVM networking</h3></div></div></div><p>There are two <a class="indexterm" id="id308"/>main ways that can be used to set up the network for your VMs. The default networking setup provides a private network under <span class="emphasis"><em>192.168.122.0/24</em></span>. A DHCP server will hand out the rest of the IPs. Alternatively, you can set up static IPs for your VM. The IP of the KVM host is <span class="emphasis"><em>192.168.122.1</em></span>. VMs communicate with the outside world via this gateway by using <a class="indexterm" id="id309"/>
<span class="strong"><strong>NAT</strong></span> (<span class="strong"><strong>Network Address Translation</strong></span>). This works fine, especially for VMs on a desktop, but since we are talking about servers here, my assumption is that you want machines outside the KVM host to be able to communicate with your VMs. While you can certainly set up some iptables DNAT rules and forward traffic back in, this solution doesn't scale very well. The real solution is to set up a bridged network so that your VMs appear to be on the same network as that of your host.</p><p>It is relatively simple to set up the <code class="literal">br0</code> bridge interface on Ubuntu. Essentially, you need to identify the interface over which you want to bridge traffic (probably <code class="literal">eth0</code> or possibly <code class="literal">bond0</code> if you set up bonding), transfer all of its configuration to <code class="literal">br0</code> along with a few extra bridge options, and change the original interface to the manual mode. It will make more sense when you see the examples. Consider an instance where I had a DHCP set up for <code class="literal">eth0,</code> and my old configuration in <code class="literal">/etc/network/interfaces</code> looked like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>auto eth0</strong></span>
<span class="strong"><strong>iface eth0 inet dhcp</strong></span>
</pre></div><p>Then, my new configuration will look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>auto eth0</strong></span>
<span class="strong"><strong>iface eth0 inet manual</strong></span>
<span class="strong"><strong>auto br0</strong></span>
<span class="strong"><strong>iface br0 inet dhcp</strong></span>
<span class="strong"><strong>bridge_ports eth0</strong></span>
<span class="strong"><strong>bridge_fd 9</strong></span>
<span class="strong"><strong>bridge_hello 2</strong></span>
<span class="strong"><strong>bridge_maxage 12</strong></span>
<span class="strong"><strong>bridge_stp off</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>For more information about network bridging on an Ubuntu Server, you can visit <a class="ulink" href="https://help.ubuntu.com/community/NetworkConnectionBridge">https://help.ubuntu.com/community/NetworkConnectionBridge</a>.</p></div></div><p>Note that I <a class="indexterm" id="id310"/>changed the <code class="literal">inet</code> mode for <code class="literal">eth0</code> from <code class="literal">dhcp</code> to <code class="literal">manual</code>. If <code class="literal">eth0</code> has a static IP configured, I can just transfer the configuration to <code class="literal">br0</code> instead. Let's take a look at the following configuration:</p><div class="informalexample"><pre class="programlisting">auto eth0
iface eth0 inet static
address 192.168.0.5
network 192.168.0.0
netmask 255.255.255.0
broadcast 192.168.0.255
gateway 192.168.0.1</pre></div><p>This will go to the following configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>auto eth0</strong></span>
<span class="strong"><strong>iface eth0 inet manual</strong></span>
<span class="strong"><strong>auto br0</strong></span>
<span class="strong"><strong>iface br0 inet static</strong></span>
<span class="strong"><strong>address 192.168.0.5</strong></span>
<span class="strong"><strong>network 192.168.0.0</strong></span>
<span class="strong"><strong>netmask 255.255.255.0</strong></span>
<span class="strong"><strong>broadcast 192.168.0.255</strong></span>
<span class="strong"><strong>gateway 192.168.0.1</strong></span>
<span class="strong"><strong>bridge_ports eth0</strong></span>
<span class="strong"><strong>bridge_fd 9</strong></span>
<span class="strong"><strong>bridge_hello 2</strong></span>
<span class="strong"><strong>bridge_maxage 12</strong></span>
<span class="strong"><strong>bridge_stp off</strong></span>
</pre></div><p>Once I have set up <code class="literal">/etc/network/interfaces</code> to have the bridge, I then restart the network by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo /etc/init.d/networking restart</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note35"/>Note</h3><p>To know more about advanced network configurations for KVM on the Ubuntu Server, you can take a look at the community page at <a class="ulink" href="https://help.ubuntu.com/community/KVM/Networking">https://help.ubuntu.com/community/KVM/Networking</a> and the official Ubuntu documentation at <a class="ulink" href="https://wiki.ubuntu.com/KvmWithBridge">https://wiki.ubuntu.com/KvmWithBridge</a>.</p></div></div></div><div class="section" title="The KVM installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec22"/>The KVM installation</h3></div></div></div><p>To perform <a class="indexterm" id="id311"/>the KVM virtualization, you need to install some additional software besides KVM. The following software components need to be added:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">libvirt</code>: This provides an interface to the virtualization hardware</li><li class="listitem" style="list-style-type: disc"><code class="literal">qemu</code>: This emulates the PC hardware to virtual machines</li><li class="listitem" style="list-style-type: disc"><code class="literal">bridge-utils</code>: This offers a way to bridge networking from virtual machines through the host</li></ul></div><p>To install the basic software needed for the KVM virtualization, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install libvirt-bin kvm bridge-utils qemu-common qemu-kvm qemu-utils</strong></span>
</pre></div><p>Note that to manage this set of programs, you will mainly use the <a class="indexterm" id="id312"/>
<span class="strong"><strong>command language interpreter</strong></span> (<span class="strong"><strong>CLI</strong></span>) commands. In case you would like to use a GUI to manage your VMs, there is the famous <code class="literal">virt-manager</code> graphical software that does the job. Note that you need to either have a graphical environment installed, or connect to your server by using SSH with the <code class="literal">-X</code> option. There is another solution—install <code class="literal">virt-manager</code> on another desktop/laptop that has a graphical environment and then use it to remotely manage your VMs by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>virt-manager -c qemu+ssh://root@your-server-ip-or-name/system</strong></span>
</pre></div><p>To install <code class="literal">virt-manager</code>, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install virt-manager</strong></span>
</pre></div><p>With <code class="literal">virt-manager</code> installed, you now have a choice of managing virtual machines from a graphical interface or from the command line. Next, you want to make sure that the user account from which you want to manage virtualization is configured to do so.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>After finishing the installation process, you should add the user whom you wish to manage the virtualization of the <code class="literal">libvirtd</code> group by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo adduser &lt;user_name&gt; libvirtd</strong></span>
</pre></div></div></div><p>Finally, reboot your server, log in as the user, and check whether the virtualization services are running. At this point, you can start managing your VE.</p></div><div class="section" title="Managing virtual machines"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec23"/>Managing virtual machines</h3></div></div></div><p>If you <a class="indexterm" id="id313"/>want to manage a VE by using the <code class="literal">virt-manager</code> graphic tool, simply run the <code class="literal">virt-manager</code> command, and you will get an intuitive, easy-to-use GUI. You can customize it by editing the <span class="strong"><strong>preferences</strong></span> sub-menu of the <span class="strong"><strong>edit</strong></span> menu. You also need to check the connection details via the <span class="strong"><strong>edit</strong></span> menu to customize advanced settings, such as networks and storage, as shown in the following screenshot:</p><div class="mediaobject"><img alt="Managing virtual machines" src="graphics/B04800_05_04.jpg"/></div><p>Then, you can start creating and managing your VMs; it is very easy and intuitive.</p><p>On the other hand, if you choose to work using CLI, you should master the options that can be used with the main virtualization commands. To get started, you can use the <code class="literal">virt-install</code> command to install a virtual machine. With <code class="literal">virt-clone</code>, you can clone an existing virtual image. To manage VMs, you can use the <code class="literal">virsh</code> command to list information about VMs as well as start, stop, and reboot them.</p><p>Note that before creating a VM by using <code class="literal">virt-install</code>, you need to create the storage image beforehand. One way to do that is by using the <code class="literal">qemu-img</code> command. For example, the following command will create a 20 GB storage image named <code class="literal">ubuntuserver</code> of the <code class="literal">qcow2</code> type under the <code class="literal">/media/Data</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo qemu-img create -f qcow2 /media/Data/ubuntuserver.qcow2 20G</strong></span>
</pre></div><p>After performing<a class="indexterm" id="id314"/> this step, you can create a VM. Here's an example of a <code class="literal">virt-install</code> command line that creates an Ubuntu virtual machine. This command incorporates many of the options that you would need to click on or fill in on the <code class="literal">virt-manager</code> window. Note that this command incorporates the image that we created earlier in this section by using the <code class="literal">qemu-img</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>virt-install --connect qemu:///system --name ubuntu_server15.04 \</strong></span>
<span class="strong"><strong>--ram 1024 --disk path=/media/Data/ubuntuserver.qcow2,format=qcow2 \</strong></span>
<span class="strong"><strong>--network=bridge:virbr0,model=virtio --vnc --os-type=linux \</strong></span>
<span class="strong"><strong>--cdrom /media/Data/ISOs/ ubuntu-15.04-server-amd64.iso \</strong></span>
<span class="strong"><strong>--noautoconsole --keymap=en-us</strong></span>
</pre></div><p>To see the signification of each of these options (and much more) that can be used with the <code class="literal">virt-install</code> command, you need to check the <code class="literal">virt-install man</code> page (type man <code class="literal">virt-install</code>).</p><p>Once the <code class="literal">virt-install</code> command starts, you can open an application from the desktop to see the progress of the installation. The <code class="literal">virt-manager</code> and <code class="literal">virt-viewer</code> commands are among those commands that you can use to view your VM's console. After the VM is installed, you can manage your VMs by using the <code class="literal">virsh</code> command.</p><p>The <code class="literal">virsh</code> command provides a good way to manage your VMs after they are created. You can use <code class="literal">virsh</code> to see which VMs are running. Then, you can start, stop, pause, and otherwise manage them. There are many alternatives to the <code class="literal">virsh</code> command that you can use to manage VMs. Refer to the <code class="literal">virsh man</code> page (type <code class="literal">man virsh</code>) for details.</p></div></div><div class="section" title="XenServer"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec59"/>XenServer</h2></div></div></div><p>The <a class="indexterm" id="id315"/>second virtualization platform that we will discuss is the famous <a class="indexterm" id="id316"/>
<span class="strong"><strong>XenServer</strong></span>. Since version 7.10, XenServer was supported by the Ubuntu Server, but starting from version 8.04, Canonical made the decision to go with KVM as the default solution for virtualization in the Ubuntu Server.</p><p>In this section, we will discuss how to set up the Ubuntu Server as a host for the Xen virtualization. We will also learn how to install guests in a Xen environment. Just before starting the hands-on part of this section, let's discover a bit of the Xen terminology. In Xen, there's no difference between a host and a guest OS. This is because the words "host" and "guest" suggest a hierarchical relation that doesn't exist (take a look at the type 1 hypervisor model described in the first section of this chapter). So, Xen talks about domains. There is the <span class="strong"><strong>domain 0</strong></span> OS<a class="indexterm" id="id317"/> (which can be compared to the host OS in other virtualization technologies) and the other OSes (which can be compared to guest OSs). </p><p>These other OSes are referred to as <a class="indexterm" id="id318"/>
<span class="strong"><strong>domain U</strong></span> machines. The <span class="strong"><strong>domain 0</strong></span> OS (or just <span class="strong"><strong>dom0</strong></span>) is the first OS that <a class="indexterm" id="id319"/>loads on a physical machine, and it has specific responsibilities in the Xen environment, including driver management. The <span class="strong"><strong>domain U</strong></span> (or just <span class="strong"><strong>domU</strong></span>) machines are virtualized machines that do not have a special responsibility with regard to virtualization.</p><div class="section" title="Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec24"/>Prerequisites</h3></div></div></div><p>The <a class="indexterm" id="id320"/>hardware supported by Linux is available for Xen (it should just be compatible with the kernel). By default, Xen supports all the operating systems that are modified to operate within it, which is called paravirtualization. Xen supports unmodified operating systems as well through hardware virtualization, which is called full virtualization. However, here we must use a CPU that has hardware virtualization support (Intel VT and AMD-V). There is more than one way of verifying this point. For example, we can check the flags set for the CPU in <code class="literal">/proc/cpuinfo</code>. By using the <code class="literal">egrep</code> command, we can search that file for Intel-VT support (vmx) or AMD-V support (svm) by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>egrep "(svm|vmx)" /proc/cpuinfo</strong></span>
</pre></div><p>The output of this command is shown in the following screenshot:</p><div class="mediaobject"><img alt="Prerequisites" src="graphics/B04800_05_05.jpg"/></div><p>Another way of checking this point is by using the <code class="literal">xm dmesg</code> command to see an overview of all the features that are relevant to the Xen virtualization, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo xm dmesg | grep VMX</strong></span>
<span class="strong"><strong>(XEN) HVM: VMX enabled</strong></span>
<span class="strong"><strong>(XEN) VMX: MSR intercept bitmap enabled</strong></span>
</pre></div><p>If you don't <a class="indexterm" id="id321"/>get a result, your CPU doesn't support virtualization, which means that you can't virtualize unmodified OSes.</p></div><div class="section" title="Installing XenServer"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec25"/>Installing XenServer</h3></div></div></div><p>Installing <a class="indexterm" id="id322"/>XenServer is quite easy for Ubuntu. All that you have to do is run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install xen-hypervisor</strong></span>
</pre></div><p>Then, restart your system and from <a class="indexterm" id="id323"/>
<span class="strong"><strong>GRand Unified Bootloader</strong></span> (<span class="strong"><strong>GRUB</strong></span>), choose the entry containing XenServer.</p><p>After rebooting, check whether or not you are using the good kernel. Run the <code class="literal">sudo uname -a</code> command and verify that <code class="literal">xen</code> is present in the result. Also, verify that <code class="literal">dom0</code> is launched by using the <code class="literal">sudo xm list</code> command.</p><p>Verify that the network is properly configured. The <code class="literal">ifconfig</code> command must return at least three interfaces, namely <code class="literal">lo</code> (the loopback system), <code class="literal">eth0</code> (the bridge that is no longer your network interface but is the image for <code class="literal">domU</code>), and <code class="literal">peth0</code> (the network interface). If this is not the case, edit the <code class="literal">/etc/xen/xend-config.sxp</code> file and ensure that you have the following lines uncommented:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(network-script network-bridge)</strong></span>
<span class="strong"><strong>(vif-script vif-bridge)</strong></span>
</pre></div><p>Don't forget to restart the Xen daemon after modifying this script by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>service xend restart</strong></span>
</pre></div><p>The <code class="literal">Xen</code> daemon is responsible for virtual network infrastructure management.</p></div><div class="section" title="The networking concept in a XenServer environment"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec26"/>The networking concept in a XenServer environment</h3></div></div></div><p>Xen <a class="indexterm" id="id324"/>has a specific network environment that is different from other VEs. Every domain (starting from <code class="literal">dom0</code>) has its own virtual drivers that serve as a network card; they are simply named <code class="literal">eth0</code>, <code class="literal">eth1</code> and so on. Inside the <code class="literal">dom0</code> OS, you will find a logical representation (logical interfaces) for these virtual drivers, with the name having the <code class="literal">vifx.y</code> pattern, where <code class="literal">x</code> represents the ID of the virtualized OS (the <span class="strong"><strong>U</strong></span> in <span class="strong"><strong>domU</strong></span>) and <code class="literal">y</code> represents the number of the virtualized network board (starting from 0).</p><p>For example, the first network card (<code class="literal">eth0</code>) in <code class="literal">dom0</code> is represented by <code class="literal">vif0.0</code>, the second network card (<code class="literal">eth1</code>) in <code class="literal">dom3</code> is represented by <code class="literal">vif3.1</code>, and so on.</p><p>Continuing with the exploring of this concept, in the <code class="literal">dom0</code> system, all the <code class="literal">vif</code> interfaces are attached to the virtual bridge (called <code class="literal">xenbr0</code>), that behaves like a real switch. Next, this bridge <a class="indexterm" id="id325"/>communicates with <code class="literal">peth0</code>, which is the representation of the physical network card, that finally talks directly to the network board in your server. The following figure is a graphical representation of how all of this is organized:</p><div class="mediaobject"><img alt="The networking concept in a XenServer environment" src="graphics/B04800_05_06.jpg"/></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>After creating the virtual network, you have to add the <code class="literal">max_loop=64</code> line loop in the <code class="literal">/etc/modules</code> file. This is mandatory as you need to ensure that you can create enough virtual disks for your virtual machines. A reboot is needed to confirm that this new setting works before you start creating virtual machines.</p></div></div></div><div class="section" title="Managing virtual machines"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec27"/>Managing virtual machines</h3></div></div></div><p>Like <a class="indexterm" id="id326"/>KVM, XenServer can also be managed by using either GUI or CLI. For GUI, there is a multitude of tools. The following are some of these tools:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Virt-manager</strong></span>: We had a <a class="indexterm" id="id327"/>look at this in the KVM section. This works very well if you wish to create and manage XenServer VMs.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>OpenXenManager</strong></span>: This is<a class="indexterm" id="id328"/> a dedicated GUI that can be used to manage Xen. You can install it by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install openxenmanager</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>XCP Project Kronos</strong></span>: <span class="strong"><strong>XCP</strong></span> <a class="indexterm" id="id329"/>stands <a class="indexterm" id="id330"/>for <span class="strong"><strong>Xen Cloud Platform</strong></span>. To install this, you have to add the <code class="literal">ppa:ubuntu-xen-org/xcp1</code> PPA to your source list (see <a class="link" href="ch02.html" title="Chapter 2. Configuring and Administering Ubuntu Server">Chapter 2</a>, <span class="emphasis"><em>Configuring and Administering Ubuntu Server</em></span>, to learn how to do this), update the package list, and finally run <code class="literal">sudo apt-get install xcp-storage-managers</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>xen-tools</strong></span>: You<a class="indexterm" id="id331"/> can install this by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install xen-tools</strong></span>
</pre></div></li></ul></div><p>In the following part of this section, we will concentrate on CLI tools. The native way of doing this is by creating a storage image by using a tool, such as <code class="literal">dd</code>, as discussed in the backup/restore section in <a class="link" href="ch04.html" title="Chapter 4. Security with Ubuntu">Chapter 4</a>, <span class="emphasis"><em>Security with Ubuntu</em></span>. Next, you need to create a configuration file for the guest system, which will contain all the settings used by the guest system (RAM, hard disk, and so on), and create a VM based on that <code class="literal">config</code> file by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo xm create -c &lt;path_to_the_conf_file&gt;</strong></span>
</pre></div><p>The <code class="literal">xm create</code> command can also be used without a <code class="literal">config</code> file, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo xm create /dev/null ramdisk=initrd.img \</strong></span>
<span class="strong"><strong>kernel=/boot/vmlinuz-2.6.12.6-xenU \</strong></span>
<span class="strong"><strong>name=ramdisk vif='' vcpus=1 \</strong></span>
<span class="strong"><strong>memory=64 root=/dev/ram0</strong></span>
</pre></div><p>The <code class="literal">xm</code> manual contains a lot of helpful information if you wish to use <code class="literal">xm</code>. Take a look at it before you start using <code class="literal">xm</code>.</p></div></div><div class="section" title="An introduction to Docker"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec60"/>An introduction to Docker</h2></div></div></div><p>Docker <a class="indexterm" id="id332"/>is a <a class="indexterm" id="id333"/>famous program that automates and simplifies the deployment of applications and services inside software containers. It is one of the best tools that is classified under the isolator category (see the first section of this chapter). Docker is based on an additional layer of abstraction and automation of an operating-system-level virtualization (also known as the isolator). It was supported by the Ubuntu Server from its 14.04 release. According to an industry analyst firm named 451 Research:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Docker is a tool that can package an application and its dependencies in a virtual container that can run on any Linux server. This helps enable flexibility and portability on where the application can run, whether on premises, public cloud, private cloud, bare metal, etc."</em></span></p></blockquote></div><div class="section" title="How Docker works"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec28"/>How Docker works</h3></div></div></div><p>As discussed earlier, an<a class="indexterm" id="id334"/> isolator provides an environment (containers) to run processes in isolation. This is exactly what's done by Docker in a lightweight manner by implementing a high-level API that uses resource isolation features provided by the Linux kernel (such as kernel namespaces, cgroups, and so on) to allow separate containers to run inside a single Linux instance.</p><p>Unlike a virtual machine, a Docker container does not require a separate OS. Instead, it uses resource isolation (CPU, memory, block I/O, network, and so on) to isolate an application's view of the operating system. There are two ways that can be used by Docker to access the Linux kernel's virtualization features. The first way involves directly using the <code class="literal">libcontainer</code> library, which has been available since Docker 0.9. The second way requires you to indirectly use a multitude of tools, such as <code class="literal">libvirt</code>, <code class="literal">systemd-nspawn</code>, and <a class="indexterm" id="id335"/>
<span class="strong"><strong>LXC</strong></span> (<span class="strong"><strong>Linux Containers</strong></span>).</p><p>The resource isolation and service restrictions that result from the use of containers provide an almost completely private view of the operating system. Therefore, every container has its own process ID space, file system structure, and network interfaces. By adding a few additional constraints for each container to use only a defined amount of resources, such as CPU, memory, and I/O, we can limit the disadvantages of sharing the same kernel between multiple containers.</p><p>The use of Docker for container management will simplify the setup of distributed systems as well as the deployment of new nodes, which can open a new era for the <a class="indexterm" id="id336"/>
<span class="strong"><strong>platform as a service </strong></span>(<span class="strong"><strong>PaaS</strong></span>) mode.</p></div><div class="section" title="Installing Docker"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec29"/>Installing Docker</h3></div></div></div><p>For Ubuntu <a class="indexterm" id="id337"/>Server 14.04 and later, Docker is part of the main Ubuntu repositories. Therefore, to install Docker, you just have to run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install docker.io</strong></span>
</pre></div><p>For older releases, you have to update your source list before executing the installation command. This is done by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a file named <code class="literal">/etc/apt/sources.list.d/docker.list</code> and put the following line in it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>deb http://get.docker.io/ubuntu docker main</strong></span>
</pre></div></li><li class="listitem">Then, download the GPG key and install the <code class="literal">lxc-docker</code> package by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 \ --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9</strong></span>
<span class="strong"><strong>sudo apt-get update</strong></span>
<span class="strong"><strong>sudo apt-get install lxc-docker</strong></span>
</pre></div></li></ol></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip17"/>Tip</h3><p>If you want to avoid using <code class="literal">sudo</code> with every Docker command, add the user to the Docker group by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo addgroup user docker</strong></span>
</pre></div></div></div><p>When the installation is finished, you have to start Docker just like any other ordinary service by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>service docker start</strong></span>
</pre></div></div><div class="section" title="Using Docker"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec30"/>Using Docker</h3></div></div></div><p>As an example, we <a class="indexterm" id="id338"/>will have a look at how to use Docker with a LAMP container. But first, let's explore some terminology that we will need later to understand different actions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>DockerFile</strong></span>: This<a class="indexterm" id="id339"/> is a source file that contains instructions for a configuration file</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Image</strong></span>: This <a class="indexterm" id="id340"/>is a compilation of <code class="literal">DockerFile</code> that is used to build a portable image that is ready for deployment</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Container</strong></span>: This<a class="indexterm" id="id341"/> is the execution of an image, simply a process that is used to run an image</li></ul></div><p>Now, let's move on to practical stuff.</p><p>First of all, for an image, you can either prepare yours or simply download one of the prepared images built by the community. You can search for them either via the Web by visiting <a class="ulink" href="https://hub.docker.com/explore/">https://hub.docker.com/explore/</a>, or via CLI by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker search lamp</strong></span>
</pre></div><p>You will get a<a class="indexterm" id="id342"/> long list of images. You have to choose your own image. For this example, we have chosen the <code class="literal">reinblau/lamp</code> image. To install this image, all that you need to do is run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker pull reinblau/lamp</strong></span>
</pre></div><p>This command will download and install this image. After finishing this step, you need to run it in a container. This is done by using the <code class="literal">docker run</code> command, which can take some arguments such as the port NATing that was used in the LAMP case:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d -p 80:80 -p 3306:3306 reinblau/lamp</strong></span>
</pre></div><p>Here, this means that port <code class="literal">80</code> of the host machine will be mapped to port <code class="literal">80</code> of the <code class="literal">docker</code> container. The same goes for port <code class="literal">3306</code>. We can get this information from the repository page of Docker.</p><p>At this step, we can start using the LAMP server that is working in this <code class="literal">docker</code> container.</p><p>When working on a <code class="literal">docker</code> container, you will have a prompt that looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>root@xxxxxx#</strong></span>
</pre></div><p>After you finish working, you need to save the changes on your image. To do this, you need to run the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>root@xxxxxx# exit</strong></span>
<span class="strong"><strong>docker commit xxxxxx image_name</strong></span>
</pre></div><p>To list the images installed on your machine, you need to run the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker images</strong></span>
</pre></div><p>To list containers (the running images), you need to execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker ps -a</strong></span>
</pre></div><p>To get the job ID of your container, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d container_name</strong></span>
</pre></div><p>This job ID is useful when you need to stop a container by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker stop JOB_ID</strong></span>
</pre></div><p>When you would like to remove the job ID, use the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm JOB-ID</strong></span>
<span class="strong"><strong>docker rm container_id</strong></span>
</pre></div><p>One of the best advantages of Docker is the possibility of easily importing/exporting images.</p><p>To export a <a class="indexterm" id="id343"/>container in <code class="literal">tar.gz</code>, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker export 2520aedc6bc5 &gt; zimbra-after-install.tgz</strong></span>
</pre></div><p>To import a <code class="literal">tar.gz</code> file, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat zimbra-after-install.tgz  | docker import - zimbra-after-install</strong></span>
</pre></div></div></div></div></div>
<div class="section" title="Cloud computing for the Ubuntu Server"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Cloud computing for the Ubuntu Server</h1></div></div></div><p>Cloud computing is <a class="indexterm" id="id344"/>one of the latest trends and hottest subjects in the IT world, and the Ubuntu Server is one of the leading OSes in this field, especially with its infrastructure based on OpenStack deployed by big names such as NASA, NSA, HP, AT&amp;T, Alcatel-Lucen, and so on.</p><p>In the following section, we will explore two of the best open source Clouds that we can easily deploy on the Ubuntu Server—one for file sharing and the other for PaaS—as well as the best part concerning OpenStack deployment on Ubuntu.</p><div class="section" title="The ownCloud software"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>The ownCloud software</h2></div></div></div><p>The <a class="indexterm" id="id345"/>
<span class="strong"><strong>ownCloud</strong></span> software<a class="indexterm" id="id346"/> allows you to create and use a storage server and share files online. In the next section, we will see the technologies used in ownCloud software.</p><div class="section" title="The technology used in ownCloud"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec31"/>The technology used in ownCloud</h3></div></div></div><p>The <a class="indexterm" id="id347"/>ownCloud software uses the WebDAV protocol to seamlessly access a server through a network drive on Linux, Windows, or Mac. There is also sync software for many platforms (Linux, Mac, Windows, Android, and so on) so that you can keep a local copy of your files and work offline. The ownCloud software not only provides a file sharing service, but also can be used to manage your calendar, contacts, bookmarks, and even music.</p><p>The project is developed in PHP. It is thus installed on many web servers. It doesn't require specific functionalities such as Java, or particular web server extensions.</p><p>In the following section, we will cover only the installation process of the ownCloud server. The installation and configuration of a client is beyond the scope of this book.</p></div><div class="section" title="The ownCloud server installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec32"/>The ownCloud server installation</h3></div></div></div><p>Since<a class="indexterm" id="id348"/> the release of version 5.0 of ownCloud, its installation is very simple. Packages for ownCloud are available for the supported versions of Ubuntu at <a class="ulink" href="http://download.owncloud.org/download/repositories/stable/">http://download.owncloud.org/download/repositories/stable/</a>.</p><p>For example, to install ownCloud for Ubuntu 15.04, run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo sh -c "echo 'deb http://download.opensuse.org \</strong></span>
<span class="strong"><strong>/repositories/isv:/ownCloud:/community/ \</strong></span>
<span class="strong"><strong>xUbuntu_15.04/ /' &gt;&gt; /etc/apt/sources.list.d/owncloud.list</strong></span>
<span class="strong"><strong>sudo apt-get update</strong></span>
<span class="strong"><strong>sudo apt-get install owncloud</strong></span>
</pre></div><p>You can <a class="indexterm" id="id349"/>add the repository key to <code class="literal">apt</code>. Keep in mind that the owner of the key may distribute updates, packages, and repositories that your system will trust. To add the key, run the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget –nv \ https://download.owncloud.org/download/repositories/stable/ \ xUbuntu_15.04/Release.key -O Release.key</strong></span>

<span class="strong"><strong>sudo apt-key add - &lt; Release.key</strong></span>
</pre></div><p>Your server is now available at <code class="literal">http://&lt;server_ip&gt;/owncloud/</code>. You must create an account in the first connection. In case the server complains about unmet dependencies, restart the web server by using the <code class="literal">sudo service apache2 reload</code> command.</p><p>To enable secure connections to the Apache server (HTTPS), run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo a2enmod ssl</strong></span>
<span class="strong"><strong>sudo a2ensite default-ssl</strong></span>
<span class="strong"><strong>sudo service apache2 reload</strong></span>
</pre></div><p>Now, log in to the server via <code class="literal">https://&lt;server_ip&gt;/owncloud/</code>. From the <span class="strong"><strong>Administration</strong></span> menu, which is available at <code class="literal">https://&lt;server_ip&gt;/owncloud/index.php/settings/admin</code>, check off the <span class="strong"><strong>Force HTTPS</strong></span> checkbox.</p></div></div><div class="section" title="CozyCloud"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"/>CozyCloud</h2></div></div></div><p>CozyCloud <a class="indexterm" id="id350"/>is a free personal Cloud server. It <a class="indexterm" id="id351"/>focuses on applications and collaboration applications related to personal data. CozyCloud is a personal <a class="indexterm" id="id352"/>
<span class="strong"><strong>PaaS</strong></span> (<span class="strong"><strong>Platform as a Service</strong></span>) solution that allows you to deploy personal web applications in a click. You can select the existing Cozy applications (Notes, Todos, Calendar, Contacts, Photos, and so on), adapt an existing Node.js application, or start your own web application <code class="literal">from-scratch</code> (documentation and tutorials related to this are available on the Internet).</p><div class="section" title="Installing CozyCloud on Ubuntu Server"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec33"/>Installing CozyCloud on Ubuntu Server</h3></div></div></div><p>First of all, start by<a class="indexterm" id="id353"/> installing Python and the <code class="literal">pip</code> tools on your machine by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>apt-get install python python-pip python-dev \ software-properties-common</strong></span>
</pre></div><p>Once you have the <code class="literal">pip</code> tools installed on your machine, you have to install <code class="literal">fabric</code> and <code class="literal">fabtools</code> by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo pip install fabric fabtools</strong></span>
</pre></div><p>Then, download the <code class="literal">fabric</code> file (a script that will run commands on your remote server), as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://raw.githubusercontent. \
com/cozy/cozy-setup/master/fabfile.py</strong></span>
</pre></div><p>Once your system is prepared, use the <code class="literal">fabric</code> script from your local machine to launch the Cozy installation (run it in the same directory as that of <code class="literal">fabfile</code> that you downloaded before). This is done by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>fab -H &lt;user&gt;@&lt;ip_address&gt; install</strong></span>
</pre></div><p>Once you run the preceding command, you need to be patient for a few moments. As you may know, deployments of some commands or applications can take some time depending on your network and hardware capabilities. When prompted by the installer, you have to enter your settings.</p><p>After the installation is complete, you can access <code class="literal">https://&lt;IP_address&gt;:443</code> to create the principal Cozy account. The use of HTTPS is mandatory. In case you simply use HTTP, you will just see the <code class="literal">nginx</code> welcome page.</p></div><div class="section" title="Using CozyCloud"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec34"/>Using CozyCloud</h3></div></div></div><p>Once<a class="indexterm" id="id354"/> the installation is complete, you can access your platform using HTTPS, as explained before. Since this is your first login, you need to register your account (provide an e-mail and a password).</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note38"/>Note</h3><p>The password that you choose will also become the key that enables Cozy applications to encrypt certain information in the database.</p></div></div><p>You will then arrive at the Cozy home page that displays the installed applications. The operation is similar to that of a smartphone—you go to the marketplace (<span class="strong"><strong>tab +Apps</strong></span>) and install the existing applications or the application that you built (if they are already on a GitHub repository).</p><p>An integral aspect of application development for CozyCloud that deserves a mention is that Cozy is a PaaS, which means that the development of an application does not depend on an SDK (as is the case with an Android or iPhone app). You can create a web application as you're <a class="indexterm" id="id355"/>used to and deploy it within Cozy or anywhere else!</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note39"/>Note</h3><p>A good starting point in case you wish to document about Cozy is <a class="ulink" href="https://github.com/cozy/cozy-setup/wiki">https://github.com/cozy/cozy-setup/wiki</a>. This page contains resources related to development.</p></div></div></div></div><div class="section" title="OpenStack"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"/>OpenStack</h2></div></div></div><p>OpenStack <a class="indexterm" id="id356"/>is a free software that allows the <a class="indexterm" id="id357"/>construction of private and public Clouds. OpenStack is a community and a project in addition to the software that is designed to help organizations implement their own Clouds. OpenStack consists of a series of software and open source projects that are maintained by the community, which includes OpenStack Compute (named Nova), OpenStack Object Storage (named Swift), OpenStack Image Service (named Glance), and many more components that are increasing in number with every new release.</p><p>Users are mainly deploying it as an <a class="indexterm" id="id358"/>
<span class="strong"><strong>IaaS</strong></span> (<span class="strong"><strong>Infrastructure as a Service</strong></span>). This technology consists of a set of interconnected projects that control pools of processing, storage, and networking resources over a datacenter, which is managed by users through multiple utilities such as a web-based dashboard, command-line tools, or even a RESTful API.</p><p>Canonical provide a fully integrated and optimized combination of the latest release of the Ubuntu Server and the latest release of OpenStack, allowing users to get the best user experience with Ubuntu OpenStack. According to the OpenStack User Survey that was conducted in November 2014:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>64% of production OpenStack clouds are run on Ubuntu.</em></span></p></blockquote></div><p>Canonical provides a set of useful tools. When associated with OpenStack, they give it more dimensions. In this section, we will start by discovering these tools. Then, we will move on to see how to install OpenStack in an Ubuntu Server.</p><div class="section" title="OpenStack tools"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec35"/>OpenStack tools</h3></div></div></div><p>As <a class="indexterm" id="id359"/>mentioned previously, Ubuntu is the most popular operating system for OpenStack in the world. Ubuntu offers a set of innovative tools and programs that help users build their enterprise-scale Cloud in the easiest and fastest way.</p><div class="section" title="Juju"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec03"/>Juju</h4></div></div></div><p>Juju, which<a class="indexterm" id="id360"/> also means magic, is the service orchestration management tool that was mainly <a class="indexterm" id="id361"/>developed by Canonical for Cloud computing. Juju concentrates on services. It provides a new concept of software deployment in an easy and quick manner with possibilities of integration and scalability on a large number of Cloud infrastructures. One of the primary components of Juju is called Charms, which can be written in any programming language that can be executed from the command line.</p></div><div class="section" title="MAAS"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec04"/>MAAS</h4></div></div></div><p>The<a class="indexterm" id="id362"/> concept of <span class="strong"><strong>Metal as a Service</strong></span> (<span class="strong"><strong>MAAS</strong></span>) was created by Canonical <a class="indexterm" id="id363"/>to provide a system that can simplify the task of setting up a physical hardware on which you can deploy complex scalable services in the same manner as that of Ubuntu's OpenStack Cloud infrastructure does.</p><p>MAAS takes care of preparing the new node, installing the Ubuntu image, configuring it, and making it functional. Besides, it checks hardware-specific tasks, such as burn-in tests, firmware, and the RAID upgrade.</p><p>The MAAS and Juju combination will breathe new life into old hardware by recycling it for use in other parts of your infrastructure.</p></div><div class="section" title="Landscape"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec05"/>Landscape</h4></div></div></div><p>The official<a class="indexterm" id="id364"/> management tool for Cloud computing under the <a class="indexterm" id="id365"/>Ubuntu Server is Landscape. It is one of the most powerful tools of the Ubuntu OpenStack combination. It is a rich web-based GUI that allows users to easily build its Cloud in minutes, monitor it in real time, and manage it in the most efficient way.</p></div><div class="section" title="LXD"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec06"/>LXD</h4></div></div></div><p>Also known<a class="indexterm" id="id366"/> as the Linux container hypervisor, LXD is the next-generation hypervisor provided by Canonical. It combines the density of containers with the manageability of virtual machines. LXD simplifies deployment and the running of VMs in a connected and secure environment with high-scalability possibilities and the ability of interoperability.</p></div><div class="section" title="Snappy"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec07"/>Snappy</h4></div></div></div><p>Snappy <a class="indexterm" id="id367"/>is a<a class="indexterm" id="id368"/> Cloud-based operating system provided by Canonical and based on the Ubuntu Server. It was developed to be used with <a class="indexterm" id="id369"/>
<span class="strong"><strong>Internet of Things</strong></span> (<span class="strong"><strong>IoT</strong></span>) devices. The difference between the Snappy Ubuntu core and the standard Ubuntu system is that applications are provided through a simpler, faster mechanism and most importantly, it provides a stronger security guarantee for apps, making it ideal for Docker and other Cloud deployment frameworks.</p></div></div><div class="section" title="The OpenStack setup"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec36"/>The OpenStack setup</h3></div></div></div><p>Mastering <a class="indexterm" id="id370"/>OpenStack, from its setup, configuration, and administration, to troubleshooting and maintenance, are subjects that need to be explained in books with hundreds of pages. In this section, we don't have much space to elaborate on all of this. So, I will try to make you get a taste of it. We will discover the main lines steps of setting up an OpenStack Cloud on an Ubuntu Server in two ways—the manual installation and the DevStack-based installation.</p></div><div class="section" title="Installing OpenStack using DevStack"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec37"/>Installing OpenStack using DevStack</h3></div></div></div><p>DevStack <a class="indexterm" id="id371"/>is a script that can be used to quickly create an OpenStack development environment or demonstrate OpenStack services and provide examples of using them from a CLI. It changed from a simple demonstration tool to a useful, quick sanity check for the OpenStack installation.</p><p>The mission of DevStack is to provide and maintain tools used for the installation of the central OpenStack services from the source (the <code class="literal">git</code> repository master or specific branches) suitable for development and operational testing. It also demonstrates and documents examples of configuring and running services as well as using command-line clients.</p><p>The following are the steps needed to install OpenStack using DevStack:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install the selected OS. In our case, it is Ubuntu Server 15.04.</li><li class="listitem">Download DevStack by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>git clone https://git.openstack.org/openstack-dev/devstack</strong></span>
</pre></div></li><li class="listitem">The <code class="literal">devstack</code> repository contains a script that installs OpenStack and templates for configuration files.</li><li class="listitem">Configure your environment. You can refer to <a class="ulink" href="http://docs.openstack.org/developer/devstack/configuration.html">http://docs.openstack.org/developer/devstack/configuration.html</a> for more details.</li><li class="listitem">Start the installation by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd devstack; ./stack.sh</strong></span>
</pre></div></li></ol></div><p>It takes a few minutes to run the preceding command. We recommend that you read the preceding script while it is being installed.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note40"/>Note</h3><p>You can find a lot of guides at <a class="ulink" href="http://docs.openstack.org/developer/devstack/">http://docs.openstack.org/developer/devstack/</a>.</p></div></div></div><div class="section" title="The manual installation"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec38"/>The manual installation</h3></div></div></div><p>The <a class="indexterm" id="id372"/>manual installation is actually the most suitable for you if you want to deploy a real Cloud and not just to test the power of OpenStack. We can install the Cloud on only one machine, but it is recommended by Canonical that you should use at least seven machines, each with two hard disks, and two of them must have two <a class="indexterm" id="id373"/>
<span class="strong"><strong>network interfaces</strong></span> (<span class="strong"><strong>NICs</strong></span>).</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note41"/>Note</h3><p>If you want to just set up OpenStack on a single Ubuntu Server based on the 14.04 LTS or 15.04 releases, a good tutorial is available at <a class="ulink" href="https://fosskb.wordpress.com/2015/04/18/installing-openstack-kilo-on-ubuntu-15-04-single-machine-setup/">https://fosskb.wordpress.com/2015/04/18/installing-openstack-kilo-on-ubuntu-15-04-single-machine-setup/</a>.</p></div></div><p>However, in case you want to get a real Cloud according to the Canonical recommendations, the<a class="indexterm" id="id374"/> following is a summary of the steps that are required if you wish to get your own Cloud OpenStack running based on the Ubuntu servers:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First of all, you have to install the Ubuntu Server on one of the machines that have two network interfaces. Then, you need to set up a private network with all the machines plugged in, with the network divided into the following three logical ranges:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The dynamic range maps an IP address to every NIC connected to the network</li><li class="listitem" style="list-style-type: disc">The static range maps an IP address to every machine connected to the network</li><li class="listitem" style="list-style-type: disc">The floating IP range maps an IP address to every instance that you'll have in your Cloud</li></ul></div></li><li class="listitem">Secondly, you need to add the needed repositories to your source list and update your package list by using the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo add-apt-repository ppa:juju/stable</strong></span>
<span class="strong"><strong>sudo add-apt-repository ppa:maas-maintainers/stable</strong></span>
<span class="strong"><strong>sudo add-apt-repository ppa:cloud-installer/stable</strong></span>
<span class="strong"><strong>sudo apt-get update</strong></span>
</pre></div></li><li class="listitem">Next, if you want to set up MAAS, you need to run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install maas</strong></span>
</pre></div></li><li class="listitem">Now, perform the following step-by-step instructions:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Access the MAAS UI at <code class="literal">http://&lt;maas_ip_address&gt;/MAAS/</code> and follow the instructions provided there to create a profile of an administrator. Then, log in with those credentials.</li><li class="listitem" style="list-style-type: disc">Import disk images for Ubuntu.</li><li class="listitem" style="list-style-type: disc">Add the SSH key to your user profile by visiting <code class="literal">http://&lt;maas_ip_address&gt;/MAAS/account/prefs/</code>.</li><li class="listitem" style="list-style-type: disc">Copy the MAAS key (you will need this later).</li><li class="listitem" style="list-style-type: disc">Fill in the other details, such as the gateway and DNS, in the networks that were automatically created for each NIC.</li></ul></div><p>Next, you need to configure the MAAS cluster, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Click on the <span class="strong"><strong>Clusters</strong></span> tab and select <span class="strong"><strong>Cluster master</strong></span>.</li><li class="listitem" style="list-style-type: disc">You will see a list of network interfaces on the machine. Click on the edit symbol for the interface that is connected to the private network where all the nodes are visible.</li><li class="listitem" style="list-style-type: disc">Set this interface to manage DHCP and DNS.</li><li class="listitem" style="list-style-type: disc">Set the router IP to the default gateway for this private network.</li><li class="listitem" style="list-style-type: disc">Fill in the details for the dynamic and static ranges; remember that you should leave gaps for the floating IPs.</li><li class="listitem" style="list-style-type: disc">Save the changes.</li></ul></div><p>Now, you <a class="indexterm" id="id375"/>need to enlist and commission the machines, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ensure that all the machines are set to the <a class="indexterm" id="id376"/><span class="strong"><strong>Preboot Execution Environment</strong></span> (<span class="strong"><strong>PXE</strong></span>) boot. If possible, disable all the other boot options, including local disk in the BIOS.</li><li class="listitem" style="list-style-type: disc">Enlist the machines by powering them on. This can usually be done by some sort of a virtual console. They will all appear in the node list in MAAS, and they can be powered down again.</li><li class="listitem" style="list-style-type: disc">Edit each machine in the node's list and fill in the power type and power parameters (that is, the username and password) so that MAAS can turn them on and off as needed.</li><li class="listitem" style="list-style-type: disc">Select all the machines and, by using the <span class="strong"><strong>Bulk action</strong></span> dropdown, commission them.</li><li class="listitem" style="list-style-type: disc">Wait until all the machines are commissioned (that is, in the <code class="literal">Ready</code> state).</li></ul></div></li><li class="listitem">At this point, you need to set up <code class="literal">Landscape</code> and launch the OpenStack <code class="literal">Autopilot</code> by using the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install openstack</strong></span>
<span class="strong"><strong>sudo openstack-install</strong></span>
</pre></div></li><li class="listitem">Choose the <span class="strong"><strong>Landscape OpenStack Autopilot</strong></span> option.</li><li class="listitem">Fill in the MAAS credentials using the MAAS key that you saved when you set up MAAS.</li><li class="listitem">Open the link to access the Landscape UI.</li><li class="listitem">Resolve the remaining issues on the checklist. Finally, click on the <span class="strong"><strong>Configure</strong></span> button.</li><li class="listitem">Go to the given URL to get to the landing page of the Landscape UI.</li><li class="listitem">The landing page contains a checklist at the bottom that shows the status of all your resources. Verify that all of them are green to confirm the sanity status of your infrastructure.</li><li class="listitem">Click <a class="indexterm" id="id377"/>on <span class="strong"><strong>Configure</strong></span> and enter an optional name for your region and Cloud.</li><li class="listitem">Select the following components (this is an initial list; more options will be added in the later versions as they pass the tests in the OpenStack Interoperability Lab):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The hypervisor component (KVM)</li><li class="listitem" style="list-style-type: disc">The networking component (Open vSwitch)</li><li class="listitem" style="list-style-type: disc">The storage components:<p>Object (Ceph, Swift)</p><p>Block (Ceph, iSCSI)</p></li></ul></div></li><li class="listitem">Select the hardware on which you need to deploy the Cloud and click on <span class="strong"><strong>Save selection</strong></span>.</li><li class="listitem">Click on <span class="strong"><strong>Install</strong></span> to build your Cloud.</li><li class="listitem">Finally, start using your Cloud!</li></ol></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Summary</h1></div></div></div><p>In this chapter, we focused on one of the most interesting features of the Ubuntu Server, namely virtualization and Cloud computing. At this point, you can define and easily use a good set of programs related to this subject on the Ubuntu server.</p><p>In the next chapter, we will discover some useful tips that an Ubuntu system administrator needs to make their life easier.</p></div></body></html>