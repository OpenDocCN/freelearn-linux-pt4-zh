- en: Chapter 6. CoreOS Storage Management
  prefs: []
  type: TYPE_NORMAL
- en: Storage is a critical component of distributed infrastructure. The initial focus
    of Container technology was on Stateless Containers with Storage managed by traditional
    technologies such as NAS and SAN. Stateless Containers are typically web applications
    such as NGINX and Node.js where there is no need to persist data. In recent times,
    there has been a focus on Stateful Containers and there are many new technologies
    being developed to achieve Stateful Containers. Stateful Containers are databases
    such as SQL and redis that need data to be persisted. CoreOS and Docker integrates
    well with different Storage technologies and there is active work going on to
    fill the gaps in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following three aspects of CoreOS storage will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The CoreOS base filesystem and partition table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Container filesystem, which is composed of the Union filesystem and Copy-on-write
    (CoW) storage driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Container data volumes for shared data persistence, which can be local,
    distributed, or shared external storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The CoreOS filesystem and mounting AWS EBS and NFS storage to the CoreOS filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker Container filesystem for storing Container images which includes
    both storage drivers and the Union filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker data volumes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container data persistence using Flocker, GlusterFS and Ceph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage concepts
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some storage terms along with their basic definitions that
    we will use in this chapter and beyond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Local storage: This is Storage attached to the localhost. An example is a local
    hard disk with ZFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network storage: This is a common storage accessed through a network. This
    can either be SAN or a cluster storage such as Ceph and GlusterFS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud storage: This is Storage provided by a cloud provider such as AWS EBS,
    OpenStack Cinder, and Google cloud storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Block storage: This requires low latency and is typically used for an OS-related
    filesystem. Some examples are AWS EBS and OpenStack Cinder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Object storage: This is used for immutable storage items where latency is not
    a big concern. Some examples are AWS S3 and OpenStack Swift.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NFS: This is a distributed filesystem. This can be run on top of any cluster
    storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CoreOS filesystem
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered the details of the CoreOS partition table in [Chapter 3](index_split_075.html#filepos216260),
    CoreOS Autoupdate. The following screenshot shows the default partitioning in
    the AWS CoreOS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00390.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By default, CoreOS uses root partitioning for the Container filesystem. In the
    preceding table, `/dev/xvda9` will be used to store Container images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following output shows Docker using Ext4 filesystem with Overlay storage driver
    in a CoreOS node running in AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00440.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To get extra storage, external storage can be mounted in CoreOS.
  prefs: []
  type: TYPE_NORMAL
- en: Mounting the AWS EBS volume
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Elastic Block Store (EBS) provides you with persistent block-level storage
    volumes to be used with Amazon EC2 instances in the AWS cloud. The following example
    shows you how to add an extra EBS volume to the CoreOS node running in AWS and
    use it for the Container filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the following `cloud-config` as `cloud-config-mntdocker.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config coreos:   etcd2:     name: etcdserver     initial-cluster: etcdserver=http://$private_ipv4:2380
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: format-ephemeral.service       command: start       content: |         [Unit]
            Description=Formats the ephemeral drive         After=dev-xvdf.device
            Requires=dev-xvdf.device         [Service]         Type=oneshot         RemainAfterExit=yes
            ExecStart=/usr/sbin/wipefs -f /dev/xvdf         ExecStart=/usr/sbin/mkfs.btrfs -f /dev/xvdf
        - name: var-lib-docker.mount       command: start       content: |         [Unit]
            Description=Mount ephemeral to /var/lib/docker         Requires=format-ephemeral.service
            After=format-ephemeral.service         Before=docker.service         [Mount]
            What=/dev/xvdf         Where=/var/lib/docker         Type=btrfs`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are some details on the preceding `cloud-config` unit file:'
  prefs: []
  type: TYPE_NORMAL
- en: The Format-ephemeral service takes care of formatting the filesystem as `btrfs`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Mount service takes care of mounting the new volume in `/var/lib/docker`
    before `docker.service` is started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can start the CoreOS node with the preceding `cloud-config` with extra EBS
    volume using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 1 --instance-type t2.micro --key-name "smakam-oregon" --security-groups "coreos-test" --user-data file://cloud-config-mntdocker.yaml --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"DeleteOnTermination\":false,\"VolumeSize\":8,\"VolumeType\":\"gp2\"}}]"`'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding command creates a single-node CoreOS cluster with one extra volume
    of 8 GB. The new volume is mounted as `/var/lib/docker` with the `btrfs` filesystem.
    The `/dev/sdf` directory gets mounted in the CoreOS system as `/dev/xvdf`, so
    the mount file uses `/dev/xvdf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the partition table in the node with the preceding `cloud-config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00461.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, there is a new 8 GB partition where `/var/lib/docker` is mounted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows you that the docker filesystem is using the `btrfs`
    storage driver as we requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00399.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Mounting NFS storage
  prefs: []
  type: TYPE_NORMAL
- en: We can mount a volume on a CoreOS node using NFS. NFS allows a shared storage
    mechanism where all CoreOS nodes in the cluster can see the same data. This approach
    can be used for Container data persistence when Containers are moved across nodes.
    In the following example, we run the NFS server in a Linux server and mount this
    volume in a CoreOS node running in the Vagrant environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to set up NFS mounting on the CoreOS node:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the NFS server and export directories that are to be shared.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the CoreOS `cloud-config` to start `rpc-statd.service`. Mount services
    also need to be started in the `cloud-config` to mount the necessary NFS directories
    to local directories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up NFS server
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the NFS server. I had set up my Ubuntu 14.04 machine as an NFS server.
    The following are the steps that I performed to set up the NFS server:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the NFS server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo apt-get install nfs-kernel-server`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an NFS directory with the appropriate owner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo mkdir /var/nfs``sudo chown core /var/nfs (I have created a core user)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Export the `NFS` directory to the necessary nodes. In my case, `172.17.8.[101-103]`
    are the IP addresses of the CoreOS cluster. Create `/etc/exports` with the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`/var/nfs    172.17.8.101(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.102(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.103(rw,sync,no_root_squash,no_subtree_check)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Start the NFS server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sudo exportfs -a``sudo service nfs-kernel-server start`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note: NFS is pretty sensitive to UserID (UID) and Group ID (GID) checks, and
    write access from the client machine won''t work unless this is properly set up.
    It is necessary for the UID and GID of the client user to match with the UID and
    GID of the directory setup in the server. Another option is to set the `no_root_squash`
    option (as in the preceding example) so that the root user from the client can
    make modifications as the UserID in the server.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As shown in the following command, we can see the directory exported after
    making the necessary configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up the CoreOS node as a client for the NFS
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `cloud-config` can be used to mount remote `/var/nfs in /mnt/data`
    in all the nodes of the CoreOS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config  write-files:   - path: /etc/conf.d/nfs     permissions: ''0644''
        content: |       OPTS_RPC_MOUNTD=""  coreos:   etcd2:     discovery: <yourtoken>
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: rpc-statd.service       command: start       enable: true     - name: mnt-data.mount
          command: start       content: |         [Mount]         What=172.17.8.110:/var/nfs
            Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding config, `cloud-config`, `rpc-statd.service` is necessary for
    the NFS client service and `mnt-data.mount` is necessary to mount the NFS volume
    in the `/mnt/data` local directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output is in one of the CoreOS nodes that have done the NFS mount.
    As we can see, the NFS mount is successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After this step, any CoreOS nodes in the cluster can read and write from `/mnt/data`.
  prefs: []
  type: TYPE_NORMAL
- en: The container filesystem
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers use the CoW filesystem to store Container images. The following
    are some characteristics of the CoW filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple users/processes can share the same data as if they have their own copy
    of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If data is changed by any one process or user, a new copy of the data is made
    for this process/user only at that point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple running containers share the same set of files till changes are made
    to the files. This makes starting the containers really fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These characteristics allow the Container filesystem operations to be really
    fast. Docker supports multiple storage drivers that are capable of CoW. Each OS
    chooses a default storage driver. Docker provides you with an option to change
    the storage driver. To change the storage driver, we need to specify the storage
    driver in `/etc/default/docker` and restart the Docker daemon:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DOCKER_OPTS="--storage-driver=<driver>"`'
  prefs: []
  type: TYPE_NORMAL
- en: The major supported storage drivers are `aufs`, `devicemapper`, `btrfs`, and
    `overlay`. We need to make sure that the storage driver is supported by the OS
    on which Docker is installed before changing the Storage driver.
  prefs: []
  type: TYPE_NORMAL
- en: Storage drivers
  prefs: []
  type: TYPE_NORMAL
- en: 'The storage driver is responsible for managing the filesystem. The following
    table captures the differences between major storage drivers supported by Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Property | AUFS | Device mapper | BTRTS | OverlayFS | ZFS |'
  prefs: []
  type: TYPE_TB
- en: '| File/block | File-based | Block-based | File-based | File-based | File-based
    |'
  prefs: []
  type: TYPE_TB
- en: '| Linux kernel support | Not in the main kernel | Present in the main kernel
    | Present in the main kernel | Present in the main kernel > 3.18 | Not in the
    main kernel |'
  prefs: []
  type: TYPE_TB
- en: '| OS | Ubuntu default | Red Hat |   | Red Hat | Solaris |'
  prefs: []
  type: TYPE_TB
- en: '| Performance | Not suitable to write big files; useful for PaaS scenarios
    | First write slow | Updating a lot of small files can cause low performance |
    Better than AUFS | Takes up a lot of memory |'
  prefs: []
  type: TYPE_TB
- en: A storage driver needs to be chosen based on the type of workload, the need
    for availability in the main Linux kernel, and the comfort level with a particular
    storage driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the default AUFS storage driver used by Docker running
    on the Ubuntu system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00408.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The following output shows Docker using the Overlay driver in the CoreOS node.
    CoreOS was using `btrfs` sometime back. Due to `btrfs` stability issues, they
    moved to the Overlay driver recently.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `/var/lib/docker` directory is where the container metadata and volume
    data is stored. The following important information is stored here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers: The container metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Volumes: The host volumes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage drivers such as aufs and device mapper: These will contain diffs and
    layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the directory output in the Ubuntu system running
    Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00062.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Docker and the Union filesystem
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker images make use of the Union filesystem to create an image composed
    of multiple layers. The Union filesystem makes use of the CoW techniques. Each
    layer is like a snapshot of the image with a particular change. The following
    example shows you the image layers of an Ubuntu docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00417.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each layer shows the operations done on the base layer to get this new layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the layering, let''s take this base Ubuntu image and create a
    new container image using the following Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FROM ubuntu:14.04``MAINTAINER Sreenivas Makam <smxxxx@yahoo.com>``# Install apache2``RUN apt-get install -y apache2``EXPOSE 80``ENTRYPOINT ["/usr/sbin/apache2ctl"]``CMD ["-D", "FOREGROUND"]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a new Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker build -t="smakam/apachetest"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the layers of this new screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00419.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first four layers are the ones created from the Dockerfile, and the last
    four layers are part of the Ubuntu 14.04 base image. In case you have the Ubuntu
    14.04 image in your system and try to download `smakam/apachetest`, only the first
    four layers would be downloaded as the other layers will already be present in
    the host machine and can be reused. This layer reuse mechanism allows a faster
    download of Docker images from the Docker hub as well as efficient storage of
    Docker images in the Container filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: Container data
  prefs: []
  type: TYPE_NORMAL
- en: Container data is not part of the Container filesystem and is stored in the
    host filesystem where Container runs. Container data can be used to store data
    that needs to be manipulated frequently, such as a database. Container data typically
    needs to be shared between multiple Containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker volumes
  prefs: []
  type: TYPE_NORMAL
- en: Changes made in the container are stored as part of the Union filesystem. If
    we want to save some data outside the scope of the container, volumes can be used.
    Volumes are stored as part of the host filesystem and it gets mounted in the Container.
    When container changes are committed, volumes are not committed as they reside
    outside the Container filesystem. Volumes can be used to share the source code
    with the host filesystem, maintain persistent data like a database, share data
    between containers, and function as a scratch pad for the container. Volumes give
    better performance over the Union filesystem for applications such as databases
    where we need to do frequent read and write operations. Using volumes does not
    guarantee Container data persistence. Using data-only Containers is an approach
    to maintain the persistence and share data across Containers. There are other
    approaches such as using shared and distributed storage to persist Container data
    across hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Container volume
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example starts the Redis container with the `/data` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d --name redis -v /data redis`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run Docker to inspect Redis, we can get details about the volumes mounted
    by this container, as can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `Source` directory is the directory in the host machine and `Destination`
    is the directory in the Container.
  prefs: []
  type: TYPE_NORMAL
- en: Volumes with the host mount directory
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of code sharing with the mounting host directory
    using Volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d --name nginxpersist -v /home/core/local:/usr/share/nginx/html -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we perform `docker inspect nginxpersist`, we can see both the host directory
    and the container mount directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the host machine, code development can be done in the `/home/core/local`
    location, and any code change in the host machine automatically reflects in the
    container.
  prefs: []
  type: TYPE_NORMAL
- en: As the host directory can vary across hosts, this makes Containers unportable
    and Dockerfile does not support the host mount option.
  prefs: []
  type: TYPE_NORMAL
- en: A data-only container
  prefs: []
  type: TYPE_NORMAL
- en: Docker has support for a data-only container that is pretty powerful. Multiple
    containers can inherit the volume from a data-only container. The advantage with
    a data-only container over regular host-based volume mounting is that we don't
    have to worry about host file permissions. Another advantage is a data-only container
    can be moved across hosts, and some of the recent Docker volume plugins take care
    of moving the volume data when the container moves across hosts.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows you how volumes can be persisted when containers
    die and restart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a volume container, `redisvolume`, for `redis` and use this volume
    in the `redis1` container. The `hellocounter` container counts the number of web
    hits and uses the `redis` container for counter-persistence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d --name redisvolume -v /data redis``docker run -d --name redis1 --volumes-from redisvolume redis``docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the running containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s access the hellocounter container multiple times using curl, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s stop this container and restart another container using the following
    commands. The new redis container, `redis2`, still uses the same `redisvolume`
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker stop redis1 hello1``docker rm redis1 hello1``docker run -d --name redis2 --volumes-from redisvolume redis``docker run -d --name hello2 --link redis2:redis -p 5001:5000 smakam/hellocounter python app.py`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to access the hellocounter container using port `5001`, we will see
    that the counter starts from `6` as the previous value `5` is persisted in the
    database even though we have stopped that container and restarted a new redis
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00380.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A data-only container can also be used to share data between containers. An
    example use case could be a web container writing a log file and a log processing
    container processing the log file and exporting it to a central server. Both the
    web and log containers can mount the same volume with one container writing to
    the volume and another reading from the volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'To back up the `redisvolume` container data that we created, we can use the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run --volumes-from redisvolume -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /data`'
  prefs: []
  type: TYPE_NORMAL
- en: This will take `/data` from `redisvolume` and back up the content to `backup.tar`
    in the current host directory using an Ubuntu container to do the backup.
  prefs: []
  type: TYPE_NORMAL
- en: Removing volumes
  prefs: []
  type: TYPE_NORMAL
- en: As part of removing a container, if we use the `docker rm –v` option, the volume
    will be automatically deleted. If we forget to use the `-v` option, volumes will
    be left dangling. This has the disadvantage that the space allocated in the host
    machine for the volume will be unused and not removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker until release 1.7 does not yet have a native solution to handle dangling
    volumes. There are some experimental containers available to clean up dangling
    volumes. I use this test Container, `martin/docker-cleanup-volumes`, to clean
    up my dangling volumes. First, we can determine the dangling volumes using the
    `dry-run` option. The following is an example that shows four dangling volumes
    and one volume that is in use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we remove the `dry-run` option, dangling volumes will be deleted, as shown
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Docker Volume plugin
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the Network plugin for Docker, the Volume plugin extends storage functionality
    for Docker containers. Volume plugins provide advanced storage functionality such
    as volume persistence across nodes. The following figure shows you the volume
    plugin architecture where the Volume driver exposes a standard set of APIs, which
    plugins can implement. GlusterFS, Flocker, Ceph, and a few other companies provide
    Docker volume plugins. Unlike the Docker networking plugin, Docker does not have
    a native volume plugin and relies on plugins from external vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Flocker
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker data volumes are tied to a single node where the Container is created.
    When Containers are moved across nodes, data volumes don''t get moved. Flocker
    addresses this issue of moving the data volumes along with the Container. The
    following figure shows you all the important blocks in the Flocker architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some internals of the Flocker implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: The Flocker agent runs in each node and takes care of talking to the Docker
    daemon and the Flocker control service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flocker control service takes care of managing the volumes as well as the
    Flocker cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently supported backend storage includes Amazon AWS EBS, Rackspace block
    storage, and EMC ScaleIO. Local storage using ZFS is available on an experimental
    basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the REST API and Flocker CLI are used to manage volumes as well as Docker
    containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker can manage volumes using Flocker as a data volume plugin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Flocker plugin will take care of managing data volumes, which includes migrating
    the volume associated with the Container when the Container moves across hosts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flocker will use the Container networking technology to talk across hosts—this
    can be native Docker networking or Docker networking plugins such as Weave.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next three examples, we will illustrate how Flocker achieves Container
    data persistence in different environments.
  prefs: []
  type: TYPE_NORMAL
- en: Flocker volume migration using AWS EBS as a backend
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will illustrate data persistence using AWS EBS as a storage backend.
    In this example, we will create three Linux nodes in the AWS cloud. One node will
    serve as the Flocker master running the control service and the other two nodes
    will run Flocker agents running the containers and mounting the EBS storage. Using
    these nodes, we will create a stateful Container and demonstrate Container data
    persistence on Container migration. The example will use a hellocounter container
    with the redis container backend and illustrates data persistence when the redis
    counter is moved across hosts. The following figure shows you how the master and
    agents are tied to the EBS backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00141.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I followed the procedure mentioned on the Flocker web page—[https://docs.clusterhq.com/en/1.4.0/labs/installer.html](https://docs.clusterhq.com/en/1.4.0/labs/installer.html)—for
    this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the summary of steps to setup Flocker volume migration using
    AWS EBS:'
  prefs: []
  type: TYPE_NORMAL
- en: It's necessary to have an AWS account to create VMs running Docker containers
    and Flocker services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the execution of frontend Flocker commands, we need a Linux host. In my
    case, it's a Ubuntu 14.04 VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Flocker frontend tools on the Linux host using Flocker scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the Flocker control service on the control node and Flocker agents on
    the slave nodes using Flocker scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we can create containers on slave nodes with a data volume and
    migrate containers keeping the volume persistent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following are the relevant outputs after installing the Flocker frontend
    tools and Flocker control service and agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the version of the Flocker frontend tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Flocker node list shows the two AWS nodes that will run Flocker agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00147.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows you the Flocker volume list. Initially, there are
    no volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00148.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the main processes in the master node. We can see the control
    service running in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00150.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the main processes in the slave node. Here, we can see the Flocker
    agents and the Flocker docker plugin running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00153.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's create a hellocounter container with the redis container backend on a
    particular slave node, update the counter in the database, and then move the container
    to demonstrate that the data volume gets persisted as the container is moved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first set up some shortcuts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NODE1="52.10.201.177" (this public ip address corresponds to the private address shown in flocker list-nodes output)``NODE2="52.25.14.152"``KEY="keylocation "`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start the `hellocontainer` and `redis` containers on `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ssh -i $KEY root@$NODE1 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest``ssh -i $KEY root@$NODE1 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the volumes created and attached at this point. 100 GB EBS volume
    is attached to slave `node1` at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00156.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the following output, we can see the two containers running in `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00158.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s create some entries in the database now. The counter value is currently
    at `6`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00160.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s remove the containers in `NODE1` and create the `hellocounter`
    and `redis` containers in `NODE2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ssh -i $KEY root@$NODE1 docker stop hellocounter``ssh -i $KEY root@$NODE1 docker stop redis``ssh -i $KEY root@$NODE1 docker rm -f hellocounter``ssh -i $KEY root@$NODE1 docker rm -f redis``ssh -i $KEY root@$NODE2 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest``ssh -i $KEY root@$NODE2 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the volume has migrated to the second slave node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the containers in `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s check whether the data is persistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00441.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the preceding output, the counter value starts from the previous
    count of `6` and is incremented to `7`, which shows that the redis database is
    persistent when the redis container is moved across the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Flocker volume migration using the ZFS backend
  prefs: []
  type: TYPE_NORMAL
- en: This example will illustrate data persistence using ZFS as a storage backend
    and Vagrant Ubuntu cluster. ZFS is an open source filesystem that focuses on data
    integrity, replication, and performance. I followed the procedure at [https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html)
    to set up a two-node Vagrant Ubuntu Flocker cluster and at [https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html)
    to try out the sample application that allows container migration with the associated
    volume migration. The sample application uses the MongoDB container for data storage
    and illustrates data persistence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the summary of steps to setup Flocker volume migration using
    ZFS backend:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Flocker client tools and the `mongodb` client in the client machine.
    In my case, this is a Ubuntu 14.04 VM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a two-node Vagrant Ubuntu cluster. As part of the cluster setup, Flocker
    services are started in each of the nodes and this includes control and agent
    services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the flocker-deploy script starting the `mongodb` container on `node1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the `mongodb` client and write some entries in `node1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the flocker-deploy script moving the `mongodb` container from `node1`
    to `node2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the `mongbdb` client to `node2` and check whether the data is retained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After starting the two-node Vagrant cluster, let's check the relevant Flocker
    services.
  prefs: []
  type: TYPE_NORMAL
- en: '`Node1` has both the Flocker control and agent services running, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`Node2` has only the Flocker agent service running and is being managed by
    `Node1`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the Flocker node list; this shows the two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s deploy the `mongodb` container on `node1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00221.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the volume list. As we can see, the volume is attached to `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows you the container in `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add some data to `mongodb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s redeploy the container to `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the volume output. As we can see, the volume is moved to `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see from the following output, the `mongodb` content, `the data`,
    is preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Flocker on CoreOS with an AWS EBS backend
  prefs: []
  type: TYPE_NORMAL
- en: Flocker has recently integrated with CoreOS on an experimental basis with the
    AWS EBS backend storage. I followed the procedures at https://github.com/ clusterhq/flocker-coreos
    and [https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/](https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/)
    for this example. I had some issues with getting version 1.4.0 of the Flocker
    tools to work with CoreOS nodes. The 1.3.0 version of tools ([https://docs.clusterhq.com/en/1.3.0/labs/installer.html](https://docs.clusterhq.com/en/1.3.0/labs/installer.html))
    worked fine.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have illustrated Container data persistence on the CoreOS
    cluster with Docker using the Flocker plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the summary of steps to setup Flocker volume migration on
    CoreOS cluster running on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a CoreOS cluster using AWS Cloudformation with the template specified
    by Flocker along with a newly created discovery token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create `cluster.yml` with the node IP and access details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Flocker script to configure the CoreOS nodes with the Flocker control
    service as well as Flocker agents. Flocker scripts also take care of replacing
    the default Docker binary in the CoreOS node with the Docker binary that supports
    the volume plugin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check that Container migration is working fine with data persistence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I used the following Cloudformation script to create a CoreOS cluster using
    the template file from Flocker:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aws cloudformation create-stack     --stack-name coreos-test1     --template-body file://coreos-stable-flocker-hvm.template     --capabilities CAPABILITY_IAM     --tags Key=Name,Value=CoreOS     --parameters      ParameterKey=DiscoveryURL,ParameterValue="your token"         ParameterKey=KeyPair,ParameterValue="your keypair"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the details of the CoreOS cluster that has three nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00145.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the old and new Docker versions installed. Docker version
    1.8.3 supports the Volume plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00149.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the CoreOS version in the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00154.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the Flocker node list with three CoreOS nodes running
    Flocker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00159.jpg)'
  prefs: []
  type: TYPE_IMG
- en: I tried the same `hellocounter` example as mentioned in the previous section,
    and the volume moved automatically across the nodes. The following output shows
    the volume initially attached to `node1` and later moved to `node2` as part of
    the Container move.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the volume attached to `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the volume attached to `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.jpg)'
  prefs: []
  type: TYPE_IMG
- en: According to the Flocker documentation, they have a plan to support the ZFS
    backend on CoreOS at some point, to allow us to use local storage instead of AWS
    EBS. It's still not certain if CoreOS will support ZFS natively.
  prefs: []
  type: TYPE_NORMAL
- en: Flocker recent additions
  prefs: []
  type: TYPE_NORMAL
- en: 'Flocker added the following functionality recently, as of November 2015:'
  prefs: []
  type: TYPE_NORMAL
- en: The Flocker volume hub ([https://clusterhq.com/volumehub/](https://clusterhq.com/volumehub/))
    manages all Flocker volumes from a central location.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flocker dvol ([https://clusterhq.com/dvol/](https://clusterhq.com/dvol/)) provides
    you with a Git-like functionality for data volumes. This can help manage databases
    such as a codebase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS is a distributed filesystem where the storage is distributed across
    multiple nodes and presented as a single unit. GlusterFS is an open source project
    and works on any kind of storage hardware. Red Hat has acquired Gluster, which
    started GlusterFS. The following are some properties of GlusterFS:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple servers with their associated storage are joined to a GlusterFS cluster
    using the peering relationship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS can work on top of the commodity storage as well as SAN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By avoiding a central metadata server and using a distributed hashing algorithm,
    GlusterFS clusters are scalable and can expand into very large clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bricks are the smallest component of storage from the GlusterFS perspective.
    A brick consists of mount points created from a storage disk with a base filesystem.
    Bricks are tied to a single server. A single server can have multiple bricks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volumes are composed of multiple bricks. Volumes are mounted to the client device
    as a mount directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Major volume types are distributed, replicated, and striped. A distributed volume
    type allows the distributing of files across multiple bricks. A replicated volume
    type allows multiple replicas of the file, which is useful from a redundancy perspective.
    A striped volume type allows the splitting of a large file into multiple smaller
    files and distributing them across the bricks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GlusterFS supports multiple access methods to access the GlusterFS volume, and
    this includes native FUSE-based access, SMB, NFS, and REST.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows you the different layers of GlusterFS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a GlusterFS cluster
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, I have set up a two-node GlusterFS 3.5 cluster with
    each server running a Ubuntu 14.04 VM. I have used the GlusterFS server node as
    the GlusterFS client as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of steps to setup a GlusterFS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the GlusterFS server on both the nodes and the client software on one
    of the nodes in the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GlusterFS nodes must be able to talk to each other. We can either set up DNS
    or use a static `/etc/hosts` approach for the nodes to talk to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turn off firewalls, if needed, for the servers to be able to talk to each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up GlusterFS server peering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create bricks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create volumes on top of the created bricks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the client machine, mount the volumes to mountpoint and start using GlusterFS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following commands need to be executed in each server. This will install
    the GlusterFS server component. This needs to be executed on both the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo apt-get install software-properties-common``sudo add-apt-repository ppa:gluster/glusterfs-3.5``sudo apt-get update``sudo apt-get install glusterfs-server`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will install the GlusterFS client. This is necessary
    only in `node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo apt-get install glusterfs-client`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up `/etc/hosts` to allow nodes to talk to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '`192.168.56.102  gluster1``192.168.56.101  gluster2`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Disable the firewall:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo iptables -I INPUT -p all -s 192.168.56.102 -j ACCEPT``sudo iptables -I INPUT -p all -s 192.168.56.101 -j ACCEPT`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the replicated volume and start it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo gluster volume create volume1 replica 2 transport tcp gluster1:/gluster-storage gluster2:/gluster-storage force (/gluster-storage is the brick in each node)``sudo gluster volume start volume1`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a server probe in each node. The following command is for `Node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo gluster peer probe gluster2`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command is for `Node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo gluster peer probe gluster1`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do a client mount of the GlusterFS volume. This is needed in `Node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo mkdir /storage-pool``sudo mount -t glusterfs gluster2:/volume1 /storage-pool`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the status of the GlusterFS cluster and created volume
    in `Node1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00178.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at `Node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the volume detail. As we can see, `volume1` is set up as the
    replicated volume type with two bricks on `gluster1` and `gluster2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the client mount point in the `df –k` output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we can write and read contents from the client mount point, `/storage-pool`.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up GlusterFS for a CoreOS cluster
  prefs: []
  type: TYPE_NORMAL
- en: By setting up CoreOS nodes to use the GlusterFS filesystem, Container volumes
    can use GlusterFS to store volume-related data. This allows Containers to move
    across nodes and keep the volume persistent. CoreOS does not support a local GlusterFS
    client at this point. One way to use GlusterFS in CoreOS is to export the GlusterFS
    volume through NFS and do NFS mounting from the CoreOS node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing to use the GlusterFS cluster created in the previous section, we
    can enable NFS in the GlusterFS cluster as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo gluster volume set volume1 nfs.disable off`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cloud-config` for CoreOS that was used in the Mounting NFS Storage section
    can be used here as well. The following is the mount-specific section where we
    have mounted the GlusterFS volume, `172.17.8.111:/volume1`, in `/mnt/data` of
    the CoreOS node:'
  prefs: []
  type: TYPE_NORMAL
- en: '`    - name: mnt-data.mount       command: start       content: |         [Mount]
            What=172.17.8.111:/volume1         Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a bunch of files in the GlusterFS volume, `/volume1`, and I was able
    to read and write from the CoreOS node. The following output shows you the `/mnt/data`
    content in the CoreOS node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Accessing GlusterFS using the Docker Volume plugin
  prefs: []
  type: TYPE_NORMAL
- en: Using the GlusterFS volume plugin ([https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs))
    for Docker, we can create and manage volumes using a regular Docker volume CLI.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will install the GlusterFS Docker volume plugin
    and create a persistent `hellocounter` application. I used the same Ubuntu 14.04
    VM that is running GlusterFS volumes to run Docker as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps needed to set up the Docker volume plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker experimental release supports the GlusterFS volume plugin, so the
    experimental Docker release needs to be downloaded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GlusterFS Docker volume plugin needs to be downloaded and started. GO ([https://golang.org/doc/install](https://golang.org/doc/install))
    needs to be installed to get the volume plugin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Docker with the GlusterFS Docker volume plugin. For this, the Docker service
    needs to be stopped and restarted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the Docker experimental release version running in both the
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Download and start the GlusterFS volume plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '`go get github.com/calavera/docker-volume-glusterfs``sudo docker-volume-glusterfs -servers gluster1:gluster2 &`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `redis` container with the GlusterFS volume driver as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d -v volume1:/data --volume-driver=glusterfs --name=redis redis:latest`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the `hellocounter` container and link it to the `redis` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the counter by accessing it a few times, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, stop the containers in `node1` and start them in `node2`. Let''s see the
    running containers in `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we access the `hellocounter` container now, we can see that the counter
    starts from `3` as the previous count is persisted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ceph
  prefs: []
  type: TYPE_NORMAL
- en: 'Ceph provides you with distributed storage like GlusterFS and is an open source
    project. Ceph was originally developed by Inktank and later acquired by Red Hat.
    The following are some properties of Ceph:'
  prefs: []
  type: TYPE_NORMAL
- en: Ceph uses Reliable Autonomic Distributed Object Store (RADOS) as the storage
    mechanism. Other storage access mechanisms such as file and block are implemented
    on top of RADOS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both Ceph and GlusterFS seem to have similar properties. According to Red Hat,
    Ceph is positioned more for OpenStack integration and GlusterFS is for Big data
    analytics, and there will be some overlap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two key components in Ceph. They are Monitor and OSD. Monitor stores
    the cluster map and Object Storage Daemons (OSD) are the individual storage nodes
    that form the storage cluster. Both storage clients and OSDs use the CRUSH algorithm
    to distribute the data across the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to GlusterFS, setting up Ceph seemed a little complex and there is
    active work going on to run Ceph components as Docker containers as well as integrate
    Ceph with CoreOS. There is also work going on for the Ceph Docker volume plugin.
  prefs: []
  type: TYPE_NORMAL
- en: NFS
  prefs: []
  type: TYPE_NORMAL
- en: NFS is a distributed filesystem that allows client computers to access network
    storage as if the storage is attached locally. We can achieve Container data persistence
    using shared NFS storage.
  prefs: []
  type: TYPE_NORMAL
- en: Container data persistence using NFS
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will cover a web application example that uses NFS for
    data persistence. The following are some details of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: The `hellocounter.service` unit starts a container that keeps track of the number
    of web accesses to the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Hellocounter.service` uses the `redis.service` container to keep track of
    the access count'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Redis container uses NFS storage to store the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the database container dies for some reason, Fleet restarts the container
    in another node in the cluster, and as the service uses NFS storage, the count
    is persisted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows you the example used in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the prerequisites and the required steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the NFS server and a three-node CoreOS cluster mounting the NFS data as
    specified in the Mounting NFS storage section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start `hellocounter.service` and `redis.service` using fleet with the X-fleet
    property to control the scheduling of the Containers. The `hellocounter.service`
    is started on all the nodes; `redis.service` is started on one of the nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code for `Hellocounter@.service` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=hello counter with redis backend  [Service] Restart=always
    RestartSec=15 ExecStartPre=-/usr/bin/docker kill %p%i ExecStartPre=-/usr/bin/docker rm %p%i
    ExecStartPre=/usr/bin/docker pull smakam/hellocounter  ExecStart=/usr/bin/docker run --name %p%i -e SERVICE_NAME=redis -p 5000:5000 smakam/hellocounter python
    app.py  ExecStop=/usr/bin/docker stop %p%i  [X-Fleet] X-Conflicts=%p@*.service`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for `Redis.service` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Unit] Description=app-redis  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill %p
    ExecStartPre=-/usr/bin/docker rm %p ExecStartPre=/usr/bin/docker pull redis ExecStart=/usr/bin/docker run --name redis -v /mnt/data/hellodata:/data redis  ExecStop=/usr/bin/docker stop %p  [X-Fleet]
    Conflicts=redis.service`'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start three instances of `hellocounter@.service` and one instance of `redis.service`.
    The following screenshot shows three instances of `hellocounter` service and 1
    instance of `redis` service running in the CoreOS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding screenshot, `hellocounter@2.service` and `redis.service`
    are in the same node node3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try accessing the web service from `node3` a few times to check the
    count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The counter value is currently at `6` and stored in NFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s reboot `node3`. As shown in the following output, we can see only
    two machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the services running. As can be seen from the following output,
    `redis.service` has moved from `node3` to `node2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s check the web access count in `node2`. As we can see from the following
    output, the count started at `7` as the previous count was set to `6` on `node3`.
    This proves that container data is persisted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: This example is not practical as there are multiple instances of a web
    server operating independently. In a more practical example, a load balancer would
    be the frontend. This example''s purpose is just to illustrate container data
    persistence using NFS.'
  prefs: []
  type: TYPE_NORMAL
- en: The Docker 1.9 update
  prefs: []
  type: TYPE_NORMAL
- en: Docker 1.9 added named volumes, and this makes volumes as a first-class citizen
    in Docker. Docker volumes can be managed using `docker volume`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows you the options in `docker volume`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A named volume deprecates a data-only container that was used earlier to share
    volumes across Containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands shows the same example used earlier with a named
    volume instead of a data-only container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker volume create --name redisvolume``docker run -d --name redis1 -v redisvolume:/data redis``docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py`'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we create a named volume, `redisvolume`, which is
    used in the `redis1` container. The `hellocounter` application links to the `redis1`
    container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows you information about the `redis1` volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another advantage with named volumes is that we don't need to worry about the
    dangling volume problem that was present before.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered different storage options available for the storing
    of Container images and Container data in a CoreOS system. Technologies such as
    Flocker, GlusterFS, NFS, and Docker volumes and their integration with Containers
    and CoreOS were illustrated with practical examples. Container storage technologies
    are still evolving and will take some time to mature. There is a general industry
    trend to move away from expensive SAN technologies toward local and distributed
    storage. In the next chapter, we will discuss Container runtime Docker and Rkt
    and how they integrate with CoreOS.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs: []
  type: TYPE_NORMAL
- en: 'GlusterFS: [http://gluster.readthedocs.org/](http://gluster.readthedocs.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GlusterFS Docker volume plugin: [https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flocker: [https://docs.clusterhq.com](https://docs.clusterhq.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker volume plugins: [https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md](https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Docker Storage driver: [https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker volumes: [https://docs.docker.com/userguide/dockervolumes/](https://docs.docker.com/userguide/dockervolumes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mounting storage on CoreOS: [https://coreos.com/os/docs/latest/mounting-storage.html](https://coreos.com/os/docs/latest/mounting-storage.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Container filesystem: [http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1](http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceph: [http://docs.ceph.com/](http://docs.ceph.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceph Docker: [https://github.com/ceph/ceph-docker](https://github.com/ceph/ceph-docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  prefs: []
  type: TYPE_NORMAL
- en: 'Persistent data storage in the CoreOS cluster: [https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1](https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating a GlusterFS cluster: [https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers](https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GlusterFS Overview: [https://www.youtube.com/watch?v=kvr6p9gSOX0](https://www.youtube.com/watch?v=kvr6p9gSOX0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stateful Containers using Flocker on CoreOS: [http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047](http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Storage webinar: [https://blog.docker.com/2015/12/persistent-storage-docker/](https://blog.docker.com/2015/12/persistent-storage-docker/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Contiv volume plugin: [https://github.com/contiv/volplugin](https://github.com/contiv/volplugin)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceph RADOS: [http://ceph.com/papers/weil-rados-pdsw07.pdf](http://ceph.com/papers/weil-rados-pdsw07.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
