- en: Chapter 6. CoreOS Storage Management
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章 CoreOS存储管理
- en: Storage is a critical component of distributed infrastructure. The initial focus
    of Container technology was on Stateless Containers with Storage managed by traditional
    technologies such as NAS and SAN. Stateless Containers are typically web applications
    such as NGINX and Node.js where there is no need to persist data. In recent times,
    there has been a focus on Stateful Containers and there are many new technologies
    being developed to achieve Stateful Containers. Stateful Containers are databases
    such as SQL and redis that need data to be persisted. CoreOS and Docker integrates
    well with different Storage technologies and there is active work going on to
    fill the gaps in this area.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 存储是分布式基础设施中的关键组件。容器技术的初步关注点是无状态容器，存储由传统技术如NAS和SAN管理。无状态容器通常是像NGINX和Node.js这样的Web应用程序，不需要持久化数据。近年来，越来越多的关注点转向有状态容器，并且许多新技术正在开发中以实现有状态容器。有状态容器是如SQL和Redis这样的数据库，数据需要持久化。CoreOS和Docker与不同的存储技术集成得很好，并且在这个领域有很多积极的工作正在进行中。
- en: 'Following three aspects of CoreOS storage will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖CoreOS存储的以下三个方面：
- en: The CoreOS base filesystem and partition table
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS基础文件系统和分区表
- en: The Container filesystem, which is composed of the Union filesystem and Copy-on-write
    (CoW) storage driver
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器文件系统，由Union文件系统和写时复制（CoW）存储驱动组成
- en: The Container data volumes for shared data persistence, which can be local,
    distributed, or shared external storage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于共享数据持久化的容器数据卷，可以是本地的、分布式的或共享的外部存储
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The CoreOS filesystem and mounting AWS EBS and NFS storage to the CoreOS filesystem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS文件系统及将AWS EBS和NFS存储挂载到CoreOS文件系统
- en: The Docker Container filesystem for storing Container images which includes
    both storage drivers and the Union filesystem.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储容器镜像的Docker容器文件系统，包含存储驱动程序和Union文件系统。
- en: Docker data volumes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker数据卷
- en: Container data persistence using Flocker, GlusterFS and Ceph
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Flocker、GlusterFS和Ceph实现容器数据持久化
- en: Storage concepts
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 存储概念
- en: 'The following are some storage terms along with their basic definitions that
    we will use in this chapter and beyond:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在本章及以后将使用的一些存储术语及其基本定义：
- en: 'Local storage: This is Storage attached to the localhost. An example is a local
    hard disk with ZFS.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地存储：这是附加到本地主机的存储。一个例子是带有ZFS的本地硬盘。
- en: 'Network storage: This is a common storage accessed through a network. This
    can either be SAN or a cluster storage such as Ceph and GlusterFS.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络存储：这是通过网络访问的常见存储。可以是SAN或像Ceph和GlusterFS这样的集群存储。
- en: 'Cloud storage: This is Storage provided by a cloud provider such as AWS EBS,
    OpenStack Cinder, and Google cloud storage.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云存储：由云提供商提供的存储，例如AWS EBS、OpenStack Cinder和Google云存储。
- en: 'Block storage: This requires low latency and is typically used for an OS-related
    filesystem. Some examples are AWS EBS and OpenStack Cinder.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块存储：需要低延迟，通常用于操作系统相关的文件系统。一些例子包括AWS EBS和OpenStack Cinder。
- en: 'Object storage: This is used for immutable storage items where latency is not
    a big concern. Some examples are AWS S3 and OpenStack Swift.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象存储：用于不可变存储项，其中延迟不是大问题。一些例子包括AWS S3和OpenStack Swift。
- en: 'NFS: This is a distributed filesystem. This can be run on top of any cluster
    storage.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS：这是一种分布式文件系统。可以在任何集群存储之上运行。
- en: The CoreOS filesystem
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS文件系统
- en: 'We covered the details of the CoreOS partition table in [Chapter 3](index_split_075.html#filepos216260),
    CoreOS Autoupdate. The following screenshot shows the default partitioning in
    the AWS CoreOS cluster:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](index_split_075.html#filepos216260)中详细介绍了CoreOS的分区表，CoreOS自动更新。以下截图展示了AWS
    CoreOS集群中的默认分区：
- en: '![](img/00390.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00390.jpg)'
- en: By default, CoreOS uses root partitioning for the Container filesystem. In the
    preceding table, `/dev/xvda9` will be used to store Container images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CoreOS使用根分区作为容器文件系统。在前面的表格中，`/dev/xvda9`将用于存储容器镜像。
- en: 'Following output shows Docker using Ext4 filesystem with Overlay storage driver
    in a CoreOS node running in AWS:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示Docker在AWS上运行的CoreOS节点中使用Ext4文件系统和Overlay存储驱动：
- en: '![](img/00440.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00440.jpg)'
- en: To get extra storage, external storage can be mounted in CoreOS.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得额外的存储，可以在CoreOS中挂载外部存储。
- en: Mounting the AWS EBS volume
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载AWS EBS卷
- en: Amazon Elastic Block Store (EBS) provides you with persistent block-level storage
    volumes to be used with Amazon EC2 instances in the AWS cloud. The following example
    shows you how to add an extra EBS volume to the CoreOS node running in AWS and
    use it for the Container filesystem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Elastic Block Store (EBS) 为您提供持久的块级存储卷，可以与 AWS 云中的 Amazon EC2 实例一起使用。以下示例展示了如何向运行在
    AWS 中的 CoreOS 节点添加一个额外的 EBS 卷，并将其用于容器文件系统。
- en: 'Rename the following `cloud-config` as `cloud-config-mntdocker.yml`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下 `cloud-config` 文件重命名为 `cloud-config-mntdocker.yml`：
- en: '`#cloud-config coreos:   etcd2:     name: etcdserver     initial-cluster: etcdserver=http://$private_ipv4:2380
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: format-ephemeral.service       command: start       content: |         [Unit]
            Description=Formats the ephemeral drive         After=dev-xvdf.device
            Requires=dev-xvdf.device         [Service]         Type=oneshot         RemainAfterExit=yes
            ExecStart=/usr/sbin/wipefs -f /dev/xvdf         ExecStart=/usr/sbin/mkfs.btrfs -f /dev/xvdf
        - name: var-lib-docker.mount       command: start       content: |         [Unit]
            Description=Mount ephemeral to /var/lib/docker         Requires=format-ephemeral.service
            After=format-ephemeral.service         Before=docker.service         [Mount]
            What=/dev/xvdf         Where=/var/lib/docker         Type=btrfs`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: etcdserver     initial-cluster: etcdserver=http://$private_ipv4:2380
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: format-ephemeral.service       command: start       content: |         [Unit]
            Description=格式化临时驱动器         After=dev-xvdf.device         Requires=dev-xvdf.device
            [Service]         Type=oneshot         RemainAfterExit=yes         ExecStart=/usr/sbin/wipefs -f /dev/xvdf
            ExecStart=/usr/sbin/mkfs.btrfs -f /dev/xvdf     - name: var-lib-docker.mount
          command: start       content: |         [Unit]         Description=将临时存储挂载到/var/lib/docker
            Requires=format-ephemeral.service         After=format-ephemeral.service
            Before=docker.service         [Mount]         What=/dev/xvdf         Where=/var/lib/docker
            Type=btrfs`'
- en: 'Following are some details on the preceding `cloud-config` unit file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有关前面提到的 `cloud-config` 单元文件的一些详细信息：
- en: The Format-ephemeral service takes care of formatting the filesystem as `btrfs`
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Format-ephemeral 服务负责将文件系统格式化为 `btrfs`
- en: The Mount service takes care of mounting the new volume in `/var/lib/docker`
    before `docker.service` is started
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mount 服务负责在 `docker.service` 启动之前将新卷挂载到 `/var/lib/docker`
- en: 'We can start the CoreOS node with the preceding `cloud-config` with extra EBS
    volume using the following commands:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令启动带有额外 EBS 卷的 CoreOS 节点，并使用前述 `cloud-config`：
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 1 --instance-type t2.micro --key-name "smakam-oregon" --security-groups "coreos-test" --user-data file://cloud-config-mntdocker.yaml --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"DeleteOnTermination\":false,\"VolumeSize\":8,\"VolumeType\":\"gp2\"}}]"`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 1 --instance-type t2.micro --key-name "smakam-oregon" --security-groups "coreos-test" --user-data file://cloud-config-mntdocker.yaml --block-device-mappings "[{\"DeviceName\":\"/dev/sdf\",\"Ebs\":{\"DeleteOnTermination\":false,\"VolumeSize\":8,\"VolumeType\":\"gp2\"}}]"`'
- en: The preceding command creates a single-node CoreOS cluster with one extra volume
    of 8 GB. The new volume is mounted as `/var/lib/docker` with the `btrfs` filesystem.
    The `/dev/sdf` directory gets mounted in the CoreOS system as `/dev/xvdf`, so
    the mount file uses `/dev/xvdf`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令创建了一个单节点 CoreOS 集群，且附加了一个 8 GB 的额外卷。该新卷以 `btrfs` 文件系统挂载为 `/var/lib/docker`。`/dev/sdf`
    目录被挂载到 CoreOS 系统中的 `/dev/xvdf`，因此挂载文件使用的是 `/dev/xvdf`。
- en: 'The following is the partition table in the node with the preceding `cloud-config`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是节点中带有前述 `cloud-config` 配置的分区表：
- en: '![](img/00461.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00461.jpg)'
- en: As we can see, there is a new 8 GB partition where `/var/lib/docker` is mounted.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`/var/lib/docker` 被挂载到一个新的 8 GB 分区上。
- en: 'The following output shows you that the docker filesystem is using the `btrfs`
    storage driver as we requested:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了 docker 文件系统正在使用我们请求的 `btrfs` 存储驱动：
- en: '![](img/00399.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00399.jpg)'
- en: Mounting NFS storage
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 挂载 NFS 存储
- en: We can mount a volume on a CoreOS node using NFS. NFS allows a shared storage
    mechanism where all CoreOS nodes in the cluster can see the same data. This approach
    can be used for Container data persistence when Containers are moved across nodes.
    In the following example, we run the NFS server in a Linux server and mount this
    volume in a CoreOS node running in the Vagrant environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NFS在CoreOS节点上挂载一个卷。NFS提供了一种共享存储机制，集群中的所有CoreOS节点都可以看到相同的数据。这种方法可以用于容器数据持久化，当容器在节点之间移动时。在以下示例中，我们在Linux服务器上运行NFS服务器，并在Vagrant环境中运行的CoreOS节点上挂载该卷。
- en: 'The following are the steps to set up NFS mounting on the CoreOS node:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置CoreOS节点上NFS挂载的步骤：
- en: Start the NFS server and export directories that are to be shared.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动NFS服务器并导出要共享的目录。
- en: Set up the CoreOS `cloud-config` to start `rpc-statd.service`. Mount services
    also need to be started in the `cloud-config` to mount the necessary NFS directories
    to local directories.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置CoreOS的`cloud-config`以启动`rpc-statd.service`。挂载服务还需要在`cloud-config`中启动，以便将必要的NFS目录挂载到本地目录。
- en: Setting up NFS server
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设置NFS服务器
- en: 'Start the NFS server. I had set up my Ubuntu 14.04 machine as an NFS server.
    The following are the steps that I performed to set up the NFS server:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 启动NFS服务器。我已将我的Ubuntu 14.04机器设置为NFS服务器。以下是我为设置NFS服务器所执行的步骤：
- en: 'Install the NFS server:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装NFS服务器：
- en: '`sudo apt-get install nfs-kernel-server`'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo apt-get install nfs-kernel-server`'
- en: 'Create an NFS directory with the appropriate owner:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建具有适当所有者的NFS目录：
- en: '`sudo mkdir /var/nfs``sudo chown core /var/nfs (I have created a core user)`'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo mkdir /var/nfs``sudo chown core /var/nfs （我已经创建了一个core用户）`'
- en: 'Export the `NFS` directory to the necessary nodes. In my case, `172.17.8.[101-103]`
    are the IP addresses of the CoreOS cluster. Create `/etc/exports` with the following
    commands:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`NFS`目录导出到必要的节点。在我的例子中，`172.17.8.[101-103]`是CoreOS集群的IP地址。使用以下命令创建`/etc/exports`：
- en: '`/var/nfs    172.17.8.101(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.102(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.103(rw,sync,no_root_squash,no_subtree_check)`'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`/var/nfs    172.17.8.101(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.102(rw,sync,no_root_squash,no_subtree_check)``/var/nfs    172.17.8.103(rw,sync,no_root_squash,no_subtree_check)`'
- en: 'Start the NFS server:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动NFS服务器：
- en: '`sudo exportfs -a``sudo service nfs-kernel-server start`'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sudo exportfs -a``sudo service nfs-kernel-server start`'
- en: Note
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: NFS is pretty sensitive to UserID (UID) and Group ID (GID) checks, and
    write access from the client machine won''t work unless this is properly set up.
    It is necessary for the UID and GID of the client user to match with the UID and
    GID of the directory setup in the server. Another option is to set the `no_root_squash`
    option (as in the preceding example) so that the root user from the client can
    make modifications as the UserID in the server.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：NFS对用户ID（UID）和组ID（GID）的检查非常敏感，除非正确设置，否则客户端机器无法进行写入访问。客户端用户的UID和GID必须与服务器中目录的UID和GID匹配。另一个选择是设置`no_root_squash`选项（如前面示例所示），以便客户端的root用户可以像服务器中的UserID一样进行修改。
- en: 'As shown in the following command, we can see the directory exported after
    making the necessary configuration:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如下命令所示，我们可以看到在完成必要的配置后导出的目录：
- en: '![](img/00005.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00005.jpg)'
- en: Setting up the CoreOS node as a client for the NFS
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将CoreOS节点设置为NFS客户端
- en: 'The following `cloud-config` can be used to mount remote `/var/nfs in /mnt/data`
    in all the nodes of the CoreOS cluster:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`cloud-config`可以用于在CoreOS集群的所有节点中挂载远程的`/var/nfs`到`/mnt/data`：
- en: '`#cloud-config  write-files:   - path: /etc/conf.d/nfs     permissions: ''0644''
        content: |       OPTS_RPC_MOUNTD=""  coreos:   etcd2:     discovery: <yourtoken>
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: rpc-statd.service       command: start       enable: true     - name: mnt-data.mount
          command: start       content: |         [Mount]         What=172.17.8.110:/var/nfs
            Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config  write-files:   - path: /etc/conf.d/nfs     permissions: ''0644''
        content: |       OPTS_RPC_MOUNTD=""  coreos:   etcd2:     discovery: <yourtoken>
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: rpc-statd.service       command: start       enable: true     - name: mnt-data.mount
          command: start       content: |         [Mount]         What=172.17.8.110:/var/nfs
            Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
- en: In the preceding config, `cloud-config`, `rpc-statd.service` is necessary for
    the NFS client service and `mnt-data.mount` is necessary to mount the NFS volume
    in the `/mnt/data` local directory.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配置中，`cloud-config`中，`rpc-statd.service` 是 NFS 客户端服务所必需的，`mnt-data.mount`
    是将 NFS 卷挂载到`/mnt/data`本地目录所必需的。
- en: 'The following output is in one of the CoreOS nodes that have done the NFS mount.
    As we can see, the NFS mount is successful:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出来自已经执行 NFS 挂载的 CoreOS 节点之一。正如我们所看到的，NFS 挂载成功：
- en: '![](img/00023.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpg)'
- en: After this step, any CoreOS nodes in the cluster can read and write from `/mnt/data`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤之后，集群中的任何 CoreOS 节点都可以从`/mnt/data`进行读写。
- en: The container filesystem
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 容器文件系统
- en: 'Containers use the CoW filesystem to store Container images. The following
    are some characteristics of the CoW filesystem:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 容器使用 CoW 文件系统存储容器镜像。以下是 CoW 文件系统的一些特性：
- en: Multiple users/processes can share the same data as if they have their own copy
    of the data.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个用户/进程可以共享相同的数据，就像它们拥有自己的数据副本一样。
- en: If data is changed by any one process or user, a new copy of the data is made
    for this process/user only at that point.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据由某个进程或用户更改，该进程/用户将仅在此时创建数据的新副本。
- en: Multiple running containers share the same set of files till changes are made
    to the files. This makes starting the containers really fast.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个运行中的容器共享同一组文件，直到对文件进行更改。这使得容器启动非常快速。
- en: 'These characteristics allow the Container filesystem operations to be really
    fast. Docker supports multiple storage drivers that are capable of CoW. Each OS
    chooses a default storage driver. Docker provides you with an option to change
    the storage driver. To change the storage driver, we need to specify the storage
    driver in `/etc/default/docker` and restart the Docker daemon:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性使得容器文件系统操作非常快速。Docker 支持多种能够进行 CoW 的存储驱动程序。每个操作系统选择一个默认的存储驱动程序。Docker 提供了一个选项，可以更改存储驱动程序。要更改存储驱动程序，我们需要在`/etc/default/docker`中指定存储驱动程序，并重新启动
    Docker 守护进程：
- en: '`DOCKER_OPTS="--storage-driver=<driver>"`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`DOCKER_OPTS="--storage-driver=<driver>"`'
- en: The major supported storage drivers are `aufs`, `devicemapper`, `btrfs`, and
    `overlay`. We need to make sure that the storage driver is supported by the OS
    on which Docker is installed before changing the Storage driver.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 主要支持的存储驱动程序包括`aufs`、`devicemapper`、`btrfs`和`overlay`。在更改存储驱动程序之前，我们需要确保存储驱动程序被
    Docker 所安装的操作系统支持。
- en: Storage drivers
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 存储驱动程序
- en: 'The storage driver is responsible for managing the filesystem. The following
    table captures the differences between major storage drivers supported by Docker:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 存储驱动程序负责管理文件系统。以下表格列出了 Docker 支持的主要存储驱动程序之间的差异：
- en: '| Property | AUFS | Device mapper | BTRTS | OverlayFS | ZFS |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | AUFS | 设备映射器 | BTRTS | OverlayFS | ZFS |'
- en: '| File/block | File-based | Block-based | File-based | File-based | File-based
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 文件/块 | 基于文件 | 基于块 | 基于文件 | 基于文件 | 基于文件 |'
- en: '| Linux kernel support | Not in the main kernel | Present in the main kernel
    | Present in the main kernel | Present in the main kernel > 3.18 | Not in the
    main kernel |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Linux 内核支持 | 不在主内核中 | 主内核中存在 | 主内核中存在 | 主内核中存在 > 3.18 | 不在主内核中 |'
- en: '| OS | Ubuntu default | Red Hat |   | Red Hat | Solaris |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 操作系统 | Ubuntu 默认 | Red Hat |   | Red Hat | Solaris |'
- en: '| Performance | Not suitable to write big files; useful for PaaS scenarios
    | First write slow | Updating a lot of small files can cause low performance |
    Better than AUFS | Takes up a lot of memory |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 性能 | 不适合写入大文件；适用于 PaaS 场景 | 首次写入较慢 | 更新大量小文件可能导致低性能 | 比 AUFS 更好 | 占用大量内存
    |'
- en: A storage driver needs to be chosen based on the type of workload, the need
    for availability in the main Linux kernel, and the comfort level with a particular
    storage driver.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 存储驱动程序需要根据工作负载类型、主 Linux 内核的可用性需求以及对特定存储驱动程序的熟悉程度来选择。
- en: 'The following output shows the default AUFS storage driver used by Docker running
    on the Ubuntu system:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了在 Ubuntu 系统上运行的 Docker 使用的默认 AUFS 存储驱动程序：
- en: '![](img/00408.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00408.jpg)'
- en: The following output shows Docker using the Overlay driver in the CoreOS node.
    CoreOS was using `btrfs` sometime back. Due to `btrfs` stability issues, they
    moved to the Overlay driver recently.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 Docker 在 CoreOS 节点上使用 Overlay 驱动程序。CoreOS 曾经使用 `btrfs`，但由于 `btrfs` 的稳定性问题，它们最近转向了
    Overlay 驱动程序。
- en: '![](img/00046.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00046.jpg)'
- en: 'The `/var/lib/docker` directory is where the container metadata and volume
    data is stored. The following important information is stored here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`/var/lib/docker` 目录是容器元数据和卷数据存储的位置。以下重要信息存储在这里：'
- en: 'Containers: The container metadata'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器：容器元数据
- en: 'Volumes: The host volumes'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷：主机卷
- en: 'Storage drivers such as aufs and device mapper: These will contain diffs and
    layers'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储驱动程序，如 aufs 和 device mapper：这些将包含差异和层
- en: 'The following screenshot shows the directory output in the Ubuntu system running
    Docker:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了在运行 Docker 的 Ubuntu 系统中的目录输出：
- en: '![](img/00062.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00062.jpg)'
- en: Docker and the Union filesystem
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 和 Union 文件系统
- en: 'Docker images make use of the Union filesystem to create an image composed
    of multiple layers. The Union filesystem makes use of the CoW techniques. Each
    layer is like a snapshot of the image with a particular change. The following
    example shows you the image layers of an Ubuntu docker image:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 镜像利用 Union 文件系统创建一个由多个层组成的镜像。Union 文件系统使用了写时复制（CoW）技术。每一层就像是镜像的快照，记录了特定的变化。以下示例展示了一个
    Ubuntu Docker 镜像的镜像层：
- en: '![](img/00417.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00417.jpg)'
- en: Each layer shows the operations done on the base layer to get this new layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层显示了在基础层上执行的操作，以获得这层新内容。
- en: 'To illustrate the layering, let''s take this base Ubuntu image and create a
    new container image using the following Dockerfile:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明层级结构，我们以这个基础的 Ubuntu 镜像为例，并使用以下 Dockerfile 创建一个新的容器镜像：
- en: '`FROM ubuntu:14.04``MAINTAINER Sreenivas Makam <smxxxx@yahoo.com>``# Install apache2``RUN apt-get install -y apache2``EXPOSE 80``ENTRYPOINT ["/usr/sbin/apache2ctl"]``CMD ["-D", "FOREGROUND"]`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`FROM ubuntu:14.04` `MAINTAINER Sreenivas Makam <smxxxx@yahoo.com>` `# 安装 apache2`
    `RUN apt-get install -y apache2` `EXPOSE 80` `ENTRYPOINT ["/usr/sbin/apache2ctl"]`
    `CMD ["-D", "FOREGROUND"]`'
- en: 'Build a new Docker image:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个新的 Docker 镜像：
- en: '`docker build -t="smakam/apachetest"`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker build -t="smakam/apachetest"`'
- en: 'Let''s look at the layers of this new screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个新截图的层：
- en: '![](img/00419.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00419.jpg)'
- en: The first four layers are the ones created from the Dockerfile, and the last
    four layers are part of the Ubuntu 14.04 base image. In case you have the Ubuntu
    14.04 image in your system and try to download `smakam/apachetest`, only the first
    four layers would be downloaded as the other layers will already be present in
    the host machine and can be reused. This layer reuse mechanism allows a faster
    download of Docker images from the Docker hub as well as efficient storage of
    Docker images in the Container filesystem.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个层是由 Dockerfile 创建的，而最后四个层是 Ubuntu 14.04 基础镜像的一部分。如果系统中已经有了 Ubuntu 14.04 镜像并尝试下载
    `smakam/apachetest`，那么只会下载前四个层，因为其他层已经存在于主机中并且可以重用。这个层重用机制使得从 Docker Hub 下载 Docker
    镜像更快，并且能够有效地存储 Docker 镜像在容器文件系统中。
- en: Container data
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 容器数据
- en: Container data is not part of the Container filesystem and is stored in the
    host filesystem where Container runs. Container data can be used to store data
    that needs to be manipulated frequently, such as a database. Container data typically
    needs to be shared between multiple Containers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 容器数据不是容器文件系统的一部分，而是存储在容器运行的主机文件系统中。容器数据可以用来存储需要频繁操作的数据，例如数据库。容器数据通常需要在多个容器之间共享。
- en: Docker volumes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 卷
- en: Changes made in the container are stored as part of the Union filesystem. If
    we want to save some data outside the scope of the container, volumes can be used.
    Volumes are stored as part of the host filesystem and it gets mounted in the Container.
    When container changes are committed, volumes are not committed as they reside
    outside the Container filesystem. Volumes can be used to share the source code
    with the host filesystem, maintain persistent data like a database, share data
    between containers, and function as a scratch pad for the container. Volumes give
    better performance over the Union filesystem for applications such as databases
    where we need to do frequent read and write operations. Using volumes does not
    guarantee Container data persistence. Using data-only Containers is an approach
    to maintain the persistence and share data across Containers. There are other
    approaches such as using shared and distributed storage to persist Container data
    across hosts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器中所做的更改会作为联合文件系统的一部分存储。如果我们希望将一些数据保存在容器范围之外，可以使用卷。卷存储在主机文件系统中，并在容器中挂载。当容器的更改被提交时，卷不会被提交，因为它们位于容器文件系统之外。卷可以用来与主机文件系统共享源代码、保持数据库等持久化数据、在容器之间共享数据，并充当容器的草稿板。对于需要频繁读写操作的应用（如数据库），卷比联合文件系统提供更好的性能。使用卷并不能保证容器数据的持久性。使用仅数据容器是一种保持持久性并在容器之间共享数据的方法。还有其他方法，比如使用共享和分布式存储来跨主机持久化容器数据。
- en: Container volume
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 容器卷
- en: 'The following example starts the Redis container with the `/data` volume:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例启动带有`/data`卷的Redis容器：
- en: '`docker run -d --name redis -v /data redis`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d --name redis -v /data redis`'
- en: 'If we run Docker to inspect Redis, we can get details about the volumes mounted
    by this container, as can be seen in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行Docker以检查Redis容器，我们可以获得该容器挂载的卷的详细信息，如下截图所示：
- en: '![](img/00122.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.jpg)'
- en: The `Source` directory is the directory in the host machine and `Destination`
    is the directory in the Container.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`Source`目录是主机机器上的目录，`Destination`是容器中的目录。'
- en: Volumes with the host mount directory
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 带有主机挂载目录的卷
- en: 'The following is an example of code sharing with the mounting host directory
    using Volume:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过挂载主机目录使用卷进行代码共享的示例：
- en: '`docker run -d --name nginxpersist -v /home/core/local:/usr/share/nginx/html -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d --name nginxpersist -v /home/core/local:/usr/share/nginx/html
    -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx`'
- en: 'If we perform `docker inspect nginxpersist`, we can see both the host directory
    and the container mount directory:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们执行`docker inspect nginxpersist`，我们可以看到主机目录和容器挂载目录：
- en: '![](img/00123.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpg)'
- en: In the host machine, code development can be done in the `/home/core/local`
    location, and any code change in the host machine automatically reflects in the
    container.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机机器中，可以在`/home/core/local`位置进行代码开发，主机中的任何代码更改会自动反映在容器中。
- en: As the host directory can vary across hosts, this makes Containers unportable
    and Dockerfile does not support the host mount option.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于主机目录在不同主机上可能不同，这使得容器无法移植，并且Dockerfile不支持主机挂载选项。
- en: A data-only container
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 仅数据容器
- en: Docker has support for a data-only container that is pretty powerful. Multiple
    containers can inherit the volume from a data-only container. The advantage with
    a data-only container over regular host-based volume mounting is that we don't
    have to worry about host file permissions. Another advantage is a data-only container
    can be moved across hosts, and some of the recent Docker volume plugins take care
    of moving the volume data when the container moves across hosts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Docker支持功能强大的仅数据容器。多个容器可以继承来自仅数据容器的卷。与常规基于主机的卷挂载相比，使用仅数据容器的优势在于我们无需担心主机文件权限。另一个优势是仅数据容器可以在主机之间移动，并且一些最近的Docker卷插件能够在容器跨主机移动时处理卷数据的迁移。
- en: The following example shows you how volumes can be persisted when containers
    die and restart.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了容器死亡并重启时，如何持久化卷。
- en: 'Let''s create a volume container, `redisvolume`, for `redis` and use this volume
    in the `redis1` container. The `hellocounter` container counts the number of web
    hits and uses the `redis` container for counter-persistence:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个名为`redisvolume`的卷容器，用于`redis`，并在`redis1`容器中使用这个卷。`hellocounter`容器统计网页点击次数，并使用`redis`容器来保持计数的持久性：
- en: '`docker run -d --name redisvolume -v /data redis``docker run -d --name redis1 --volumes-from redisvolume redis``docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d --name redisvolume -v /data redis``docker run -d --name redis1
    --volumes-from redisvolume redis``docker run -d --name hello1 --link redis1:redis
    -p 5000:5000 smakam/hellocounter python app.py`'
- en: 'Let''s see the running containers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看正在运行的容器：
- en: '![](img/00126.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpg)'
- en: 'Let''s access the hellocounter container multiple times using curl, as shown
    in the following image:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 curl 多次访问 hellocounter 容器，如下图所示：
- en: '![](img/00129.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpg)'
- en: 'Now, let''s stop this container and restart another container using the following
    commands. The new redis container, `redis2`, still uses the same `redisvolume`
    container:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们停止这个容器并使用以下命令重启另一个容器。新的 redis 容器 `redis2` 仍然使用相同的 `redisvolume` 容器：
- en: '`docker stop redis1 hello1``docker rm redis1 hello1``docker run -d --name redis2 --volumes-from redisvolume redis``docker run -d --name hello2 --link redis2:redis -p 5001:5000 smakam/hellocounter python app.py`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker stop redis1 hello1``docker rm redis1 hello1``docker run -d --name redis2
    --volumes-from redisvolume redis``docker run -d --name hello2 --link redis2:redis
    -p 5001:5000 smakam/hellocounter python app.py`'
- en: 'If we try to access the hellocounter container using port `5001`, we will see
    that the counter starts from `6` as the previous value `5` is persisted in the
    database even though we have stopped that container and restarted a new redis
    container:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试通过端口 `5001` 访问 hellocounter 容器，我们会看到计数器从 `6` 开始，因为即使我们已经停止了该容器并重新启动了一个新的
    redis 容器，之前的值 `5` 仍然保存在数据库中：
- en: '![](img/00380.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00380.jpg)'
- en: A data-only container can also be used to share data between containers. An
    example use case could be a web container writing a log file and a log processing
    container processing the log file and exporting it to a central server. Both the
    web and log containers can mount the same volume with one container writing to
    the volume and another reading from the volume.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据容器也可以用于在容器之间共享数据。一个示例用例是，网页容器写入日志文件，而日志处理容器处理该日志文件并将其导出到中央服务器。网页和日志容器都可以挂载同一个卷，一个容器写入该卷，另一个容器从该卷读取数据。
- en: 'To back up the `redisvolume` container data that we created, we can use the
    following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要备份我们创建的 `redisvolume` 容器数据，可以使用以下命令：
- en: '`docker run --volumes-from redisvolume -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /data`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run --volumes-from redisvolume -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar
    /data`'
- en: This will take `/data` from `redisvolume` and back up the content to `backup.tar`
    in the current host directory using an Ubuntu container to do the backup.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 `redisvolume` 中获取 `/data`，并使用 Ubuntu 容器将内容备份到当前主机目录中的 `backup.tar`。
- en: Removing volumes
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 删除卷
- en: As part of removing a container, if we use the `docker rm –v` option, the volume
    will be automatically deleted. If we forget to use the `-v` option, volumes will
    be left dangling. This has the disadvantage that the space allocated in the host
    machine for the volume will be unused and not removed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作为删除容器的一部分，如果我们使用 `docker rm –v` 选项，卷将被自动删除。如果我们忘记使用 `-v` 选项，卷将会悬挂。这有一个缺点，即为卷分配的主机空间将被浪费且无法删除。
- en: 'Docker until release 1.7 does not yet have a native solution to handle dangling
    volumes. There are some experimental containers available to clean up dangling
    volumes. I use this test Container, `martin/docker-cleanup-volumes`, to clean
    up my dangling volumes. First, we can determine the dangling volumes using the
    `dry-run` option. The following is an example that shows four dangling volumes
    and one volume that is in use:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.7 之前版本还没有原生的解决方案来处理悬挂的卷。有一些实验性的容器可用来清理悬挂的卷。我使用这个测试容器 `martin/docker-cleanup-volumes`
    来清理悬挂的卷。首先，我们可以使用 `dry-run` 选项来确定悬挂的卷。以下是一个示例，显示了四个悬挂的卷和一个正在使用的卷：
- en: '![](img/00131.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00131.jpg)'
- en: 'If we remove the `dry-run` option, dangling volumes will be deleted, as shown
    in the following image:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去掉 `dry-run` 选项，悬挂的卷将被删除，如下图所示：
- en: '![](img/00134.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00134.jpg)'
- en: The Docker Volume plugin
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 卷插件
- en: 'Like the Network plugin for Docker, the Volume plugin extends storage functionality
    for Docker containers. Volume plugins provide advanced storage functionality such
    as volume persistence across nodes. The following figure shows you the volume
    plugin architecture where the Volume driver exposes a standard set of APIs, which
    plugins can implement. GlusterFS, Flocker, Ceph, and a few other companies provide
    Docker volume plugins. Unlike the Docker networking plugin, Docker does not have
    a native volume plugin and relies on plugins from external vendors:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Docker 的网络插件，卷插件扩展了 Docker 容器的存储功能。卷插件提供了高级存储功能，例如跨节点的卷持久化。下图展示了卷插件架构，其中卷驱动程序暴露了一组标准的
    API，插件可以实现这些 API。GlusterFS、Flocker、Ceph 和其他几家公司提供 Docker 卷插件。与 Docker 网络插件不同，Docker
    没有原生的卷插件，而是依赖外部供应商的插件：
- en: '![](img/00137.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.jpg)'
- en: Flocker
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker
- en: 'Docker data volumes are tied to a single node where the Container is created.
    When Containers are moved across nodes, data volumes don''t get moved. Flocker
    addresses this issue of moving the data volumes along with the Container. The
    following figure shows you all the important blocks in the Flocker architecture:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 数据卷绑定到创建容器的单一节点。当容器在节点之间迁移时，数据卷不会随之迁移。Flocker 解决了将数据卷与容器一起迁移的问题。下图展示了
    Flocker 架构中的所有重要组件：
- en: '![](img/00139.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00139.jpg)'
- en: 'The following are some internals of the Flocker implementation:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Flocker 实现的一些内部细节：
- en: The Flocker agent runs in each node and takes care of talking to the Docker
    daemon and the Flocker control service.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker 代理在每个节点上运行，负责与 Docker 守护进程和 Flocker 控制服务进行通信。
- en: The Flocker control service takes care of managing the volumes as well as the
    Flocker cluster.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker 控制服务负责管理卷以及 Flocker 集群。
- en: Currently supported backend storage includes Amazon AWS EBS, Rackspace block
    storage, and EMC ScaleIO. Local storage using ZFS is available on an experimental
    basis.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前支持的后端存储包括 Amazon AWS EBS、Rackspace 块存储和 EMC ScaleIO。使用 ZFS 的本地存储目前处于实验性阶段。
- en: Both the REST API and Flocker CLI are used to manage volumes as well as Docker
    containers.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REST API 和 Flocker CLI 都用于管理卷和 Docker 容器。
- en: Docker can manage volumes using Flocker as a data volume plugin.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 可以使用 Flocker 作为数据卷插件来管理卷。
- en: The Flocker plugin will take care of managing data volumes, which includes migrating
    the volume associated with the Container when the Container moves across hosts.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker 插件将负责管理数据卷，包括在容器跨主机迁移时迁移与容器关联的卷。
- en: Flocker will use the Container networking technology to talk across hosts—this
    can be native Docker networking or Docker networking plugins such as Weave.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker 将使用容器网络技术在主机之间进行通信——这可以是原生 Docker 网络，或者是 Docker 网络插件，如 Weave。
- en: In the next three examples, we will illustrate how Flocker achieves Container
    data persistence in different environments.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个示例中，我们将演示 Flocker 如何在不同环境下实现容器数据持久化。
- en: Flocker volume migration using AWS EBS as a backend
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS EBS 作为后端的 Flocker 卷迁移
- en: 'This example will illustrate data persistence using AWS EBS as a storage backend.
    In this example, we will create three Linux nodes in the AWS cloud. One node will
    serve as the Flocker master running the control service and the other two nodes
    will run Flocker agents running the containers and mounting the EBS storage. Using
    these nodes, we will create a stateful Container and demonstrate Container data
    persistence on Container migration. The example will use a hellocounter container
    with the redis container backend and illustrates data persistence when the redis
    counter is moved across hosts. The following figure shows you how the master and
    agents are tied to the EBS backend:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例将演示使用 AWS EBS 作为存储后端的数据持久化。在本示例中，我们将在 AWS 云中创建三个 Linux 节点。一个节点将作为 Flocker
    主节点，运行控制服务，另外两个节点将运行 Flocker 代理，运行容器并挂载 EBS 存储。通过这些节点，我们将创建一个有状态容器，并演示容器迁移时的容器数据持久化。该示例将使用一个
    hellocounter 容器，后端为 redis 容器，并演示当 redis 计数器跨主机迁移时的数据持久化。下图展示了主节点和代理节点如何与 EBS 后端关联：
- en: '![](img/00141.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00141.jpg)'
- en: I followed the procedure mentioned on the Flocker web page—[https://docs.clusterhq.com/en/1.4.0/labs/installer.html](https://docs.clusterhq.com/en/1.4.0/labs/installer.html)—for
    this example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我按照 Flocker 网页上提到的过程操作，参考了[https://docs.clusterhq.com/en/1.4.0/labs/installer.html](https://docs.clusterhq.com/en/1.4.0/labs/installer.html)。
- en: 'The following are the summary of steps to setup Flocker volume migration using
    AWS EBS:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 AWS EBS 设置 Flocker 卷迁移的步骤总结：
- en: It's necessary to have an AWS account to create VMs running Docker containers
    and Flocker services.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建运行 Docker 容器和 Flocker 服务的虚拟机需要拥有 AWS 账户。
- en: For the execution of frontend Flocker commands, we need a Linux host. In my
    case, it's a Ubuntu 14.04 VM.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于执行前端 Flocker 命令，我们需要一台 Linux 主机。在我的例子中，是一台 Ubuntu 14.04 虚拟机。
- en: Install Flocker frontend tools on the Linux host using Flocker scripts.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Flocker 脚本在 Linux 主机上安装 Flocker 前端工具。
- en: Install the Flocker control service on the control node and Flocker agents on
    the slave nodes using Flocker scripts.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Flocker 脚本在控制节点上安装 Flocker 控制服务，并在从节点上安装 Flocker 代理。
- en: At this point, we can create containers on slave nodes with a data volume and
    migrate containers keeping the volume persistent.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，我们可以在从节点上创建带有数据卷的容器，并迁移容器，同时保持数据卷持久化。
- en: The following are the relevant outputs after installing the Flocker frontend
    tools and Flocker control service and agents.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是安装 Flocker 前端工具和 Flocker 控制服务与代理后的相关输出。
- en: 'This is the version of the Flocker frontend tools:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Flocker 前端工具的版本：
- en: '![](img/00144.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.jpg)'
- en: 'The Flocker node list shows the two AWS nodes that will run Flocker agents:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker 节点列表显示了将运行 Flocker 代理的两个 AWS 节点：
- en: '![](img/00147.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00147.jpg)'
- en: 'The following output shows you the Flocker volume list. Initially, there are
    no volumes:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 Flocker 卷列表。最初没有任何卷：
- en: '![](img/00148.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpg)'
- en: 'Let''s look at the main processes in the master node. We can see the control
    service running in the following screenshot:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看主节点中的主要进程。以下截图展示了控制服务正在运行：
- en: '![](img/00150.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00150.jpg)'
- en: 'Let''s look at the main processes in the slave node. Here, we can see the Flocker
    agents and the Flocker docker plugin running:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看从节点中的主要进程。在这里，我们可以看到 Flocker 代理和 Flocker Docker 插件正在运行：
- en: '![](img/00153.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00153.jpg)'
- en: Let's create a hellocounter container with the redis container backend on a
    particular slave node, update the counter in the database, and then move the container
    to demonstrate that the data volume gets persisted as the container is moved.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在特定的从节点上创建一个 hellocounter 容器，后端是 redis 容器，更新数据库中的计数器，然后移动容器，演示数据卷在容器移动时如何保持持久化。
- en: 'Let''s first set up some shortcuts:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先设置一些快捷方式：
- en: '`NODE1="52.10.201.177" (this public ip address corresponds to the private address shown in flocker list-nodes output)``NODE2="52.25.14.152"``KEY="keylocation "`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`NODE1="52.10.201.177" (此公共 IP 地址对应 flocker list-nodes 输出中显示的私有地址)``NODE2="52.25.14.152"``KEY="keylocation"`'
- en: 'Let''s start the `hellocontainer` and `redis` containers on `node1`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 `node1` 上启动 `hellocontainer` 和 `redis` 容器：
- en: '`ssh -i $KEY root@$NODE1 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest``ssh -i $KEY root@$NODE1 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh -i $KEY root@$NODE1 docker run -d -v demo:/data --volume-driver=flocker
    --name=redis redis:latest``ssh -i $KEY root@$NODE1 docker run -d -e USE_REDIS_HOST=redis
    --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
- en: 'Let''s look at the volumes created and attached at this point. 100 GB EBS volume
    is attached to slave `node1` at this point:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看此时创建并附加的卷。100 GB 的 EBS 卷已附加到从节点 `node1`：
- en: '![](img/00156.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00156.jpg)'
- en: 'From the following output, we can see the two containers running in `node1`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们可以看到两个容器正在 `node1` 上运行：
- en: '![](img/00158.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00158.jpg)'
- en: 'Let''s create some entries in the database now. The counter value is currently
    at `6`, as shown in the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来创建一些数据库条目。当前计数器的值为 `6`，如下图所示：
- en: '![](img/00160.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00160.jpg)'
- en: 'Now, let''s remove the containers in `NODE1` and create the `hellocounter`
    and `redis` containers in `NODE2`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们移除 `NODE1` 上的容器，并在 `NODE2` 上创建 `hellocounter` 和 `redis` 容器：
- en: '`ssh -i $KEY root@$NODE1 docker stop hellocounter``ssh -i $KEY root@$NODE1 docker stop redis``ssh -i $KEY root@$NODE1 docker rm -f hellocounter``ssh -i $KEY root@$NODE1 docker rm -f redis``ssh -i $KEY root@$NODE2 docker run -d -v demo:/data --volume-driver=flocker --name=redis redis:latest``ssh -i $KEY root@$NODE2 docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh -i $KEY root@$NODE1 docker stop hellocounter``ssh -i $KEY root@$NODE1
    docker stop redis``ssh -i $KEY root@$NODE1 docker rm -f hellocounter``ssh -i $KEY
    root@$NODE1 docker rm -f redis``ssh -i $KEY root@$NODE2 docker run -d -v demo:/data
    --volume-driver=flocker --name=redis redis:latest``ssh -i $KEY root@$NODE2 docker
    run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter
    smakam/hellocounter`'
- en: 'As we can see, the volume has migrated to the second slave node:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，卷已迁移到第二个从节点：
- en: '![](img/00163.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00163.jpg)'
- en: 'Let''s look at the containers in `node2`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 `node2` 中的容器：
- en: '![](img/00166.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00166.jpg)'
- en: 'Now, let''s check whether the data is persistent:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查数据是否持久化：
- en: '![](img/00441.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00441.jpg)'
- en: As we can see from the preceding output, the counter value starts from the previous
    count of `6` and is incremented to `7`, which shows that the redis database is
    persistent when the redis container is moved across the nodes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看到，计数器值从先前的`6`开始，并递增到`7`，这表明当redis容器跨节点移动时，redis数据库是持久化的。
- en: Flocker volume migration using the ZFS backend
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ZFS后端的Flocker卷迁移
- en: This example will illustrate data persistence using ZFS as a storage backend
    and Vagrant Ubuntu cluster. ZFS is an open source filesystem that focuses on data
    integrity, replication, and performance. I followed the procedure at [https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html)
    to set up a two-node Vagrant Ubuntu Flocker cluster and at [https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html)
    to try out the sample application that allows container migration with the associated
    volume migration. The sample application uses the MongoDB container for data storage
    and illustrates data persistence.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例将展示使用ZFS作为存储后端和Vagrant Ubuntu集群的数据显示持久化。ZFS是一个开源文件系统，重点关注数据完整性、复制和性能。我按照[https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/vagrant-setup.html)的流程，设置了一个两节点的Vagrant
    Ubuntu Flocker集群，并参照[https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html](https://docs.clusterhq.com/en/1.4.0/using/tutorial/volumes.html)尝试了一个示例应用程序，该应用程序支持容器迁移及相关的卷迁移。示例应用程序使用MongoDB容器进行数据存储，并演示了数据持久化。
- en: 'The following are the summary of steps to setup Flocker volume migration using
    ZFS backend:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用ZFS后端设置Flocker卷迁移的步骤总结：
- en: Install Flocker client tools and the `mongodb` client in the client machine.
    In my case, this is a Ubuntu 14.04 VM.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端机器中安装Flocker客户端工具和`mongodb`客户端。我的情况下，这是一个Ubuntu 14.04虚拟机。
- en: Create a two-node Vagrant Ubuntu cluster. As part of the cluster setup, Flocker
    services are started in each of the nodes and this includes control and agent
    services.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个两节点Vagrant Ubuntu集群。在集群设置过程中，Flocker服务将在每个节点上启动，包括控制服务和代理服务。
- en: Start the flocker-deploy script starting the `mongodb` container on `node1`.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动flocker-deploy脚本，启动`mongodb`容器在`node1`上运行。
- en: Start the `mongodb` client and write some entries in `node1`.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`mongodb`客户端并在`node1`中写入一些条目。
- en: Start the flocker-deploy script moving the `mongodb` container from `node1`
    to `node2`.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动flocker-deploy脚本，将`mongodb`容器从`node1`迁移到`node2`。
- en: Start the `mongbdb` client to `node2` and check whether the data is retained.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`mongodb`客户端到`node2`，检查数据是否被保留。
- en: After starting the two-node Vagrant cluster, let's check the relevant Flocker
    services.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 启动两节点Vagrant集群后，让我们检查相关的Flocker服务。
- en: '`Node1` has both the Flocker control and agent services running, as shown in
    the following screenshot:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`Node1`上运行着Flocker控制服务和代理服务，如下图所示：'
- en: '![](img/00170.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00170.jpg)'
- en: '`Node2` has only the Flocker agent service running and is being managed by
    `Node1`, as shown in the following screenshot:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`Node2`仅运行Flocker代理服务，并由`Node1`管理，如下图所示：'
- en: '![](img/00173.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00173.jpg)'
- en: 'Let''s look at the Flocker node list; this shows the two nodes:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看Flocker节点列表；这显示了两个节点：
- en: '![](img/00176.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00176.jpg)'
- en: 'Let''s deploy the `mongodb` container on `node1` as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在`node1`上部署`mongodb`容器：
- en: '![](img/00221.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00221.jpg)'
- en: 'Let''s look at the volume list. As we can see, the volume is attached to `node1`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看卷列表。如我们所见，卷附加到`node1`：
- en: '![](img/00179.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00179.jpg)'
- en: 'The following output shows you the container in `node1`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了`node1`中的容器：
- en: '![](img/00182.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00182.jpg)'
- en: 'Let''s add some data to `mongodb`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们往`mongodb`中添加一些数据：
- en: '![](img/00185.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00185.jpg)'
- en: 'Now, let''s redeploy the container to `node2`:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将容器重新部署到`node2`：
- en: '![](img/00127.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00127.jpg)'
- en: 'Let''s look at the volume output. As we can see, the volume is moved to `node2`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看卷的输出。如我们所见，卷已移动到`node2`：
- en: '![](img/00135.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00135.jpg)'
- en: 'As we can see from the following output, the `mongodb` content, `the data`,
    is preserved:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出可以看到，`mongodb`内容，`数据`，得到了保留：
- en: '![](img/00140.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpg)'
- en: Flocker on CoreOS with an AWS EBS backend
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS EBS后端的CoreOS上的Flocker
- en: Flocker has recently integrated with CoreOS on an experimental basis with the
    AWS EBS backend storage. I followed the procedures at https://github.com/ clusterhq/flocker-coreos
    and [https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/](https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/)
    for this example. I had some issues with getting version 1.4.0 of the Flocker
    tools to work with CoreOS nodes. The 1.3.0 version of tools ([https://docs.clusterhq.com/en/1.3.0/labs/installer.html](https://docs.clusterhq.com/en/1.3.0/labs/installer.html))
    worked fine.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker最近已与CoreOS进行实验性集成，使用AWS EBS后端存储。我按照https://github.com/clusterhq/flocker-coreos和[https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/](https://clusterhq.com/2015/09/01/flocker-runs-on-coreos/)上的程序进行此示例。过程中，我遇到了一些问题，无法使Flocker工具的版本1.4.0与CoreOS节点配合使用。工具的1.3.0版本（[https://docs.clusterhq.com/en/1.3.0/labs/installer.html](https://docs.clusterhq.com/en/1.3.0/labs/installer.html)）运行正常。
- en: In this example, we have illustrated Container data persistence on the CoreOS
    cluster with Docker using the Flocker plugin.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们演示了如何使用Flocker插件在CoreOS集群中通过Docker实现容器数据持久化。
- en: 'The following are the summary of steps to setup Flocker volume migration on
    CoreOS cluster running on AWS:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置AWS上运行的CoreOS集群中的Flocker卷迁移的步骤总结：
- en: Create a CoreOS cluster using AWS Cloudformation with the template specified
    by Flocker along with a newly created discovery token.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Flocker指定的模板和新创建的发现令牌，通过AWS Cloudformation创建CoreOS集群。
- en: Create `cluster.yml` with the node IP and access details.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含节点IP和访问详情的`cluster.yml`文件。
- en: Start the Flocker script to configure the CoreOS nodes with the Flocker control
    service as well as Flocker agents. Flocker scripts also take care of replacing
    the default Docker binary in the CoreOS node with the Docker binary that supports
    the volume plugin.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Flocker脚本来配置CoreOS节点，包含Flocker控制服务和Flocker代理。Flocker脚本还会处理将CoreOS节点中默认的Docker二进制文件替换为支持Volume插件的Docker二进制文件。
- en: Check that Container migration is working fine with data persistence.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查容器迁移是否能正常工作并保持数据持久化。
- en: 'I used the following Cloudformation script to create a CoreOS cluster using
    the template file from Flocker:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了以下Cloudformation脚本，通过Flocker的模板文件创建CoreOS集群：
- en: '`aws cloudformation create-stack     --stack-name coreos-test1     --template-body file://coreos-stable-flocker-hvm.template     --capabilities CAPABILITY_IAM     --tags Key=Name,Value=CoreOS     --parameters      ParameterKey=DiscoveryURL,ParameterValue="your token"         ParameterKey=KeyPair,ParameterValue="your keypair"`'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws cloudformation create-stack --stack-name coreos-test1 --template-body
    file://coreos-stable-flocker-hvm.template --capabilities CAPABILITY_IAM --tags
    Key=Name,Value=CoreOS --parameters ParameterKey=DiscoveryURL,ParameterValue="your
    token" ParameterKey=KeyPair,ParameterValue="your keypair"`'
- en: 'The following are the details of the CoreOS cluster that has three nodes:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是包含三台节点的CoreOS集群的详细信息：
- en: '![](img/00145.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.jpg)'
- en: 'The following are the old and new Docker versions installed. Docker version
    1.8.3 supports the Volume plugin:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是已安装的旧版本和新版本Docker。Docker版本1.8.3支持Volume插件：
- en: '![](img/00149.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00149.jpg)'
- en: 'The following is the CoreOS version in the node:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是节点中的CoreOS版本：
- en: '![](img/00154.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00154.jpg)'
- en: 'The following output shows the Flocker node list with three CoreOS nodes running
    Flocker:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了Flocker节点列表，列出了三台运行Flocker的CoreOS节点：
- en: '![](img/00159.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00159.jpg)'
- en: I tried the same `hellocounter` example as mentioned in the previous section,
    and the volume moved automatically across the nodes. The following output shows
    the volume initially attached to `node1` and later moved to `node2` as part of
    the Container move.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试了与上一部分中提到的相同的`hellocounter`示例，卷会在节点之间自动移动。以下输出显示了最初附加到`node1`的卷，后来作为容器迁移的一部分，移动到`node2`。
- en: 'This is the volume attached to `node1`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这是附加到`node1`的卷：
- en: '![](img/00164.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00164.jpg)'
- en: 'This is the volume attached to `node2`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这是附加到`node2`的卷：
- en: '![](img/00169.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00169.jpg)'
- en: According to the Flocker documentation, they have a plan to support the ZFS
    backend on CoreOS at some point, to allow us to use local storage instead of AWS
    EBS. It's still not certain if CoreOS will support ZFS natively.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Flocker文档，他们计划在某个时刻支持CoreOS上的ZFS后端，以允许我们使用本地存储而不是AWS EBS。目前还不确定CoreOS是否会原生支持ZFS。
- en: Flocker recent additions
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker最近的新增功能
- en: 'Flocker added the following functionality recently, as of November 2015:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Flocker在2015年11月新增了以下功能：
- en: The Flocker volume hub ([https://clusterhq.com/volumehub/](https://clusterhq.com/volumehub/))
    manages all Flocker volumes from a central location.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker卷中心（[https://clusterhq.com/volumehub/](https://clusterhq.com/volumehub/)）管理来自中心位置的所有Flocker卷。
- en: Flocker dvol ([https://clusterhq.com/dvol/](https://clusterhq.com/dvol/)) provides
    you with a Git-like functionality for data volumes. This can help manage databases
    such as a codebase.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flocker dvol ([https://clusterhq.com/dvol/](https://clusterhq.com/dvol/)) 为你提供类似
    Git 的数据卷功能。这可以帮助管理数据库，如代码库。
- en: GlusterFS
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS
- en: 'GlusterFS is a distributed filesystem where the storage is distributed across
    multiple nodes and presented as a single unit. GlusterFS is an open source project
    and works on any kind of storage hardware. Red Hat has acquired Gluster, which
    started GlusterFS. The following are some properties of GlusterFS:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS 是一个分布式文件系统，存储分布在多个节点上，并作为一个单一的单元呈现。GlusterFS 是一个开源项目，可以在任何类型的存储硬件上运行。Red
    Hat 已收购 Gluster，而 Gluster 是 GlusterFS 的发源地。以下是 GlusterFS 的一些特性：
- en: Multiple servers with their associated storage are joined to a GlusterFS cluster
    using the peering relationship.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多台服务器及其关联的存储通过对等关系（peering）加入 GlusterFS 集群。
- en: GlusterFS can work on top of the commodity storage as well as SAN.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS 可以在普通存储以及 SAN 上运行。
- en: By avoiding a central metadata server and using a distributed hashing algorithm,
    GlusterFS clusters are scalable and can expand into very large clusters.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过避免使用中央元数据服务器并采用分布式哈希算法，GlusterFS 集群具备可扩展性，能够扩展为非常大的集群。
- en: Bricks are the smallest component of storage from the GlusterFS perspective.
    A brick consists of mount points created from a storage disk with a base filesystem.
    Bricks are tied to a single server. A single server can have multiple bricks.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 GlusterFS 的角度来看，砖块（Bricks）是存储的最小组件。一个砖块由带有基础文件系统的存储磁盘上的挂载点组成。砖块绑定到单个服务器上。一个服务器可以拥有多个砖块。
- en: Volumes are composed of multiple bricks. Volumes are mounted to the client device
    as a mount directory.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷（Volumes）由多个砖块组成。卷作为挂载目录挂载到客户端设备上。
- en: Major volume types are distributed, replicated, and striped. A distributed volume
    type allows the distributing of files across multiple bricks. A replicated volume
    type allows multiple replicas of the file, which is useful from a redundancy perspective.
    A striped volume type allows the splitting of a large file into multiple smaller
    files and distributing them across the bricks.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的卷类型有分布式（distributed）、复制（replicated）和条带化（striped）。分布式卷类型允许将文件分布到多个砖块上。复制卷类型允许文件有多个副本，从冗余角度来看非常有用。条带化卷类型允许将一个大文件分割成多个较小的文件并将其分布到各个砖块上。
- en: GlusterFS supports multiple access methods to access the GlusterFS volume, and
    this includes native FUSE-based access, SMB, NFS, and REST.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS 支持多种访问方法来访问 GlusterFS 卷，包括基于 FUSE 的本地访问、SMB、NFS 和 REST。
- en: 'The following figure shows you the different layers of GlusterFS:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 GlusterFS 的不同层次结构：
- en: '![](img/00174.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00174.jpg)'
- en: Setting up a GlusterFS cluster
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 GlusterFS 集群
- en: In the following example, I have set up a two-node GlusterFS 3.5 cluster with
    each server running a Ubuntu 14.04 VM. I have used the GlusterFS server node as
    the GlusterFS client as well.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我已经设置了一个由两台节点组成的 GlusterFS 3.5 集群，每台服务器都运行 Ubuntu 14.04 虚拟机。我还将 GlusterFS
    服务器节点作为 GlusterFS 客户端使用。
- en: 'The following is a summary of steps to setup a GlusterFS cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置 GlusterFS 集群的步骤概览：
- en: Install the GlusterFS server on both the nodes and the client software on one
    of the nodes in the cluster.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两台节点上安装 GlusterFS 服务器，并在集群中的一个节点上安装客户端软件。
- en: GlusterFS nodes must be able to talk to each other. We can either set up DNS
    or use a static `/etc/hosts` approach for the nodes to talk to each other.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GlusterFS 节点必须能够相互通信。我们可以设置 DNS 或者使用静态的 `/etc/hosts` 方法让节点之间进行通信。
- en: Turn off firewalls, if needed, for the servers to be able to talk to each other.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有需要，关闭防火墙，以便服务器能够相互通信。
- en: Set up GlusterFS server peering.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 GlusterFS 服务器对等。
- en: Create bricks.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建砖块。
- en: Create volumes on top of the created bricks.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建的砖块基础上创建卷。
- en: In the client machine, mount the volumes to mountpoint and start using GlusterFS.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户端机器上，将卷挂载到挂载点，并开始使用 GlusterFS。
- en: 'The following commands need to be executed in each server. This will install
    the GlusterFS server component. This needs to be executed on both the nodes:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令需要在每台服务器上执行。这将安装 GlusterFS 服务器组件。此操作需要在两台节点上执行：
- en: '`sudo apt-get install software-properties-common``sudo add-apt-repository ppa:gluster/glusterfs-3.5``sudo apt-get update``sudo apt-get install glusterfs-server`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo apt-get install software-properties-common``sudo add-apt-repository ppa:gluster/glusterfs-3.5``sudo
    apt-get update``sudo apt-get install glusterfs-server`'
- en: 'The following command will install the GlusterFS client. This is necessary
    only in `node1`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将安装 GlusterFS 客户端。仅在 `node1` 中需要此操作：
- en: '`sudo apt-get install glusterfs-client`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo apt-get install glusterfs-client`'
- en: 'Set up `/etc/hosts` to allow nodes to talk to each other:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `/etc/hosts` 以允许节点相互通信：
- en: '`192.168.56.102  gluster1``192.168.56.101  gluster2`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`192.168.56.102  gluster1``192.168.56.101  gluster2`'
- en: 'Disable the firewall:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用防火墙：
- en: '`sudo iptables -I INPUT -p all -s 192.168.56.102 -j ACCEPT``sudo iptables -I INPUT -p all -s 192.168.56.101 -j ACCEPT`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo iptables -I INPUT -p all -s 192.168.56.102 -j ACCEPT``sudo iptables -I
    INPUT -p all -s 192.168.56.101 -j ACCEPT`'
- en: 'Create the replicated volume and start it:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 创建复制卷并启动它：
- en: '`sudo gluster volume create volume1 replica 2 transport tcp gluster1:/gluster-storage gluster2:/gluster-storage force (/gluster-storage is the brick in each node)``sudo gluster volume start volume1`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo gluster volume create volume1 replica 2 transport tcp gluster1:/gluster-storage
    gluster2:/gluster-storage force (/gluster-storage 是每个节点中的砖块)``sudo gluster volume
    start volume1`'
- en: 'Set up a server probe in each node. The following command is for `Node1`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点中设置服务器探针。以下命令适用于 `Node1`：
- en: '`sudo gluster peer probe gluster2`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo gluster peer probe gluster2`'
- en: 'The following command is for `Node2`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令适用于 `Node2`：
- en: '`sudo gluster peer probe gluster1`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo gluster peer probe gluster1`'
- en: 'Do a client mount of the GlusterFS volume. This is needed in `Node1`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对 GlusterFS 卷进行客户端挂载。这是在 `Node1` 中需要的操作：
- en: '`sudo mkdir /storage-pool``sudo mount -t glusterfs gluster2:/volume1 /storage-pool`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo mkdir /storage-pool``sudo mount -t glusterfs gluster2:/volume1 /storage-pool`'
- en: 'Now, let''s look at the status of the GlusterFS cluster and created volume
    in `Node1`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看 `Node1` 中的 GlusterFS 集群状态和创建的卷：
- en: '![](img/00178.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00178.jpg)'
- en: 'Now, let''s look at `Node2`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 `Node2`：
- en: '![](img/00183.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00183.jpg)'
- en: 'Let''s look at the volume detail. As we can see, `volume1` is set up as the
    replicated volume type with two bricks on `gluster1` and `gluster2`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下卷的详细信息。正如我们所见，`volume1` 被设置为复制卷类型，且在 `gluster1` 和 `gluster2` 上有两个砖块：
- en: '![](img/00074.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00074.jpg)'
- en: 'The following output shows the client mount point in the `df –k` output:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 `df -k` 输出中的客户端挂载点：
- en: '![](img/00078.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00078.jpg)'
- en: At this point, we can write and read contents from the client mount point, `/storage-pool`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以从客户端挂载点 `/storage-pool` 读写内容。
- en: Setting up GlusterFS for a CoreOS cluster
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为 CoreOS 集群设置 GlusterFS
- en: By setting up CoreOS nodes to use the GlusterFS filesystem, Container volumes
    can use GlusterFS to store volume-related data. This allows Containers to move
    across nodes and keep the volume persistent. CoreOS does not support a local GlusterFS
    client at this point. One way to use GlusterFS in CoreOS is to export the GlusterFS
    volume through NFS and do NFS mounting from the CoreOS node.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 CoreOS 节点使用 GlusterFS 文件系统，容器卷可以使用 GlusterFS 存储卷相关数据。这使得容器能够在节点之间迁移并保持卷的持久性。此时，CoreOS
    不支持本地 GlusterFS 客户端。使用 GlusterFS 的一种方法是通过 NFS 导出 GlusterFS 卷，并从 CoreOS 节点进行 NFS
    挂载。
- en: 'Continuing to use the GlusterFS cluster created in the previous section, we
    can enable NFS in the GlusterFS cluster as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用上一节中创建的 GlusterFS 集群，我们可以按照以下步骤在 GlusterFS 集群中启用 NFS：
- en: '`sudo gluster volume set volume1 nfs.disable off`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo gluster volume set volume1 nfs.disable off`'
- en: 'The `cloud-config` for CoreOS that was used in the Mounting NFS Storage section
    can be used here as well. The following is the mount-specific section where we
    have mounted the GlusterFS volume, `172.17.8.111:/volume1`, in `/mnt/data` of
    the CoreOS node:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 用于挂载 NFS 存储部分的 CoreOS `cloud-config` 也可以在这里使用。以下是挂载特定部分，我们在 CoreOS 节点的 `/mnt/data`
    中挂载了 GlusterFS 卷 `172.17.8.111:/volume1`：
- en: '`    - name: mnt-data.mount       command: start       content: |         [Mount]
            What=172.17.8.111:/volume1         Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`    - name: mnt-data.mount       command: start       content: |         [Mount]         What=172.17.8.111:/volume1         Where=/mnt/data         Type=nfs         Options=vers=3,sec=sys,noauto`'
- en: 'I created a bunch of files in the GlusterFS volume, `/volume1`, and I was able
    to read and write from the CoreOS node. The following output shows you the `/mnt/data`
    content in the CoreOS node:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 GlusterFS 卷 `/volume1` 中创建了一些文件，并能够从 CoreOS 节点进行读写。以下输出显示了 CoreOS 节点中的 `/mnt/data`
    内容：
- en: '![](img/00082.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.jpg)'
- en: Accessing GlusterFS using the Docker Volume plugin
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 卷插件访问 GlusterFS
- en: Using the GlusterFS volume plugin ([https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs))
    for Docker, we can create and manage volumes using a regular Docker volume CLI.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GlusterFS 卷插件（[https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs)）为
    Docker，我们可以使用常规的 Docker 卷 CLI 创建和管理卷。
- en: In the following example, we will install the GlusterFS Docker volume plugin
    and create a persistent `hellocounter` application. I used the same Ubuntu 14.04
    VM that is running GlusterFS volumes to run Docker as well.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将安装GlusterFS Docker卷插件，并创建一个持久化的`hellocounter`应用程序。我使用的是运行GlusterFS卷的相同Ubuntu
    14.04虚拟机来运行Docker。
- en: 'The following are the steps needed to set up the Docker volume plugin:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是设置Docker卷插件所需的步骤：
- en: The Docker experimental release supports the GlusterFS volume plugin, so the
    experimental Docker release needs to be downloaded.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker实验性版本支持GlusterFS卷插件，因此需要下载实验性Docker版本。
- en: The GlusterFS Docker volume plugin needs to be downloaded and started. GO ([https://golang.org/doc/install](https://golang.org/doc/install))
    needs to be installed to get the volume plugin.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS Docker卷插件需要下载并启动。需要安装GO（[https://golang.org/doc/install](https://golang.org/doc/install)）以获取卷插件。
- en: Use Docker with the GlusterFS Docker volume plugin. For this, the Docker service
    needs to be stopped and restarted.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker和GlusterFS Docker卷插件。为此，必须停止并重新启动Docker服务。
- en: 'The following is the Docker experimental release version running in both the
    nodes:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在两个节点上运行的Docker实验性发布版本：
- en: '![](img/00087.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpg)'
- en: 'Download and start the GlusterFS volume plugin:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并启动GlusterFS卷插件：
- en: '`go get github.com/calavera/docker-volume-glusterfs``sudo docker-volume-glusterfs -servers gluster1:gluster2 &`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`go get github.com/calavera/docker-volume-glusterfs``sudo docker-volume-glusterfs
    -servers gluster1:gluster2 &`'
- en: 'Start the `redis` container with the GlusterFS volume driver as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GlusterFS卷驱动启动`redis`容器，如下所示：
- en: '`docker run -d -v volume1:/data --volume-driver=glusterfs --name=redis redis:latest`'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d -v volume1:/data --volume-driver=glusterfs --name=redis redis:latest`'
- en: 'Start the `hellocounter` container and link it to the `redis` container:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 启动`hellocounter`容器并将其链接到`redis`容器：
- en: '`docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter smakam/hellocounter`'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d -e USE_REDIS_HOST=redis --link redis:redis -p 80:5000 --name=hellocounter
    smakam/hellocounter`'
- en: 'Update the counter by accessing it a few times, as shown in the following screenshot:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问计数器几次来更新它，如下图所示：
- en: '![](img/00089.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00089.jpg)'
- en: 'Now, stop the containers in `node1` and start them in `node2`. Let''s see the
    running containers in `node2`:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，停止`node1`中的容器，并在`node2`中启动它们。我们来看看`node2`中正在运行的容器：
- en: '![](img/00092.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00092.jpg)'
- en: 'If we access the `hellocounter` container now, we can see that the counter
    starts from `3` as the previous count is persisted:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在访问`hellocounter`容器，我们可以看到计数器从`3`开始，因为先前的计数值已经被持久化：
- en: '![](img/00094.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00094.jpg)'
- en: Ceph
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph
- en: 'Ceph provides you with distributed storage like GlusterFS and is an open source
    project. Ceph was originally developed by Inktank and later acquired by Red Hat.
    The following are some properties of Ceph:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph为你提供类似于GlusterFS的分布式存储，并且是一个开源项目。Ceph最初由Inktank开发，后来被Red Hat收购。以下是Ceph的一些特性：
- en: Ceph uses Reliable Autonomic Distributed Object Store (RADOS) as the storage
    mechanism. Other storage access mechanisms such as file and block are implemented
    on top of RADOS.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph使用可靠的自适应分布式对象存储（RADOS）作为存储机制。其他存储访问机制如文件和块存储是基于RADOS实现的。
- en: Both Ceph and GlusterFS seem to have similar properties. According to Red Hat,
    Ceph is positioned more for OpenStack integration and GlusterFS is for Big data
    analytics, and there will be some overlap.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph和GlusterFS似乎具有类似的特性。根据Red Hat的说法，Ceph更适合OpenStack集成，而GlusterFS则用于大数据分析，二者会有一些重叠。
- en: There are two key components in Ceph. They are Monitor and OSD. Monitor stores
    the cluster map and Object Storage Daemons (OSD) are the individual storage nodes
    that form the storage cluster. Both storage clients and OSDs use the CRUSH algorithm
    to distribute the data across the cluster.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceph有两个关键组件。它们分别是Monitor和OSD。Monitor存储集群地图，而对象存储守护进程（OSD）是形成存储集群的单个存储节点。存储客户端和OSD都使用CRUSH算法将数据分布到集群中。
- en: Compared to GlusterFS, setting up Ceph seemed a little complex and there is
    active work going on to run Ceph components as Docker containers as well as integrate
    Ceph with CoreOS. There is also work going on for the Ceph Docker volume plugin.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 与GlusterFS相比，设置Ceph似乎稍微复杂一些，并且目前有活跃的工作正在进行，以使Ceph组件能够作为Docker容器运行，并且将Ceph与CoreOS集成。此外，还有工作在进行中，涉及Ceph
    Docker卷插件。
- en: NFS
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: NFS
- en: NFS is a distributed filesystem that allows client computers to access network
    storage as if the storage is attached locally. We can achieve Container data persistence
    using shared NFS storage.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: NFS是一种分布式文件系统，它允许客户端计算机像本地存储一样访问网络存储。我们可以通过共享的NFS存储实现容器数据持久化。
- en: Container data persistence using NFS
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NFS实现容器数据持久化。
- en: 'In this section, we will cover a web application example that uses NFS for
    data persistence. The following are some details of the application:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一个使用NFS进行数据持久化的Web应用示例。以下是该应用的一些细节：
- en: The `hellocounter.service` unit starts a container that keeps track of the number
    of web accesses to the application
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hellocounter.service`单元启动一个容器，用来跟踪应用程序的网页访问次数。'
- en: '`Hellocounter.service` uses the `redis.service` container to keep track of
    the access count'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Hellocounter.service`使用`redis.service`容器来跟踪访问计数。'
- en: The Redis container uses NFS storage to store the data
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis容器使用NFS存储来保存数据。
- en: When the database container dies for some reason, Fleet restarts the container
    in another node in the cluster, and as the service uses NFS storage, the count
    is persisted
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据库容器因某种原因停止时，Fleet会在集群中的另一个节点重启该容器，由于该服务使用NFS存储，计数值会被持久化。
- en: 'The following figure shows you the example used in this section:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了本节中使用的示例：
- en: '![](img/00097.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00097.jpg)'
- en: 'The following are the prerequisites and the required steps:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前提条件和所需的步骤：
- en: Start the NFS server and a three-node CoreOS cluster mounting the NFS data as
    specified in the Mounting NFS storage section.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动NFS服务器和一个三节点的CoreOS集群，将NFS数据挂载到如“挂载NFS存储”部分所述。
- en: Start `hellocounter.service` and `redis.service` using fleet with the X-fleet
    property to control the scheduling of the Containers. The `hellocounter.service`
    is started on all the nodes; `redis.service` is started on one of the nodes.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Fleet启动`hellocounter.service`和`redis.service`，通过X-fleet属性控制容器的调度。`hellocounter.service`在所有节点上启动；`redis.service`在其中一个节点上启动。
- en: 'The code for `Hellocounter@.service` is as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hellocounter@.service`的代码如下：'
- en: '`[Unit] Description=hello counter with redis backend  [Service] Restart=always
    RestartSec=15 ExecStartPre=-/usr/bin/docker kill %p%i ExecStartPre=-/usr/bin/docker rm %p%i
    ExecStartPre=/usr/bin/docker pull smakam/hellocounter  ExecStart=/usr/bin/docker run --name %p%i -e SERVICE_NAME=redis -p 5000:5000 smakam/hellocounter python
    app.py  ExecStop=/usr/bin/docker stop %p%i  [X-Fleet] X-Conflicts=%p@*.service`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=hello counter with redis backend  [Service] Restart=always
    RestartSec=15 ExecStartPre=-/usr/bin/docker kill %p%i ExecStartPre=-/usr/bin/docker rm %p%i
    ExecStartPre=/usr/bin/docker pull smakam/hellocounter  ExecStart=/usr/bin/docker run --name %p%i -e SERVICE_NAME=redis -p 5000:5000 smakam/hellocounter python
    app.py  ExecStop=/usr/bin/docker stop %p%i  [X-Fleet] X-Conflicts=%p@*.service`'
- en: 'The code for `Redis.service` is as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '`Redis.service`的代码如下：'
- en: '`[Unit] Description=app-redis  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill %p
    ExecStartPre=-/usr/bin/docker rm %p ExecStartPre=/usr/bin/docker pull redis ExecStart=/usr/bin/docker run --name redis -v /mnt/data/hellodata:/data redis  ExecStop=/usr/bin/docker stop %p  [X-Fleet]
    Conflicts=redis.service`'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`[Unit] Description=app-redis  [Service] Restart=always RestartSec=5 ExecStartPre=-/usr/bin/docker kill %p
    ExecStartPre=-/usr/bin/docker rm %p ExecStartPre=/usr/bin/docker pull redis ExecStart=/usr/bin/docker run --name redis -v /mnt/data/hellodata:/data redis  ExecStop=/usr/bin/docker stop %p  [X-Fleet]
    Conflicts=redis.service`'
- en: Let's start three instances of `hellocounter@.service` and one instance of `redis.service`.
    The following screenshot shows three instances of `hellocounter` service and 1
    instance of `redis` service running in the CoreOS cluster.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动三个`hellocounter@.service`实例和一个`redis.service`实例。下图显示了在CoreOS集群中运行的三个`hellocounter`服务实例和一个`redis`服务实例。
- en: '![](img/00099.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00099.jpg)'
- en: As we can see in the preceding screenshot, `hellocounter@2.service` and `redis.service`
    are in the same node node3.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图可以看到，`hellocounter@2.service`和`redis.service`都在同一个节点node3上。
- en: 'Let''s try accessing the web service from `node3` a few times to check the
    count:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试从`node3`访问网页服务几次，来检查计数值：
- en: '![](img/00102.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.jpg)'
- en: The counter value is currently at `6` and stored in NFS.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 当前计数器的值为`6`，并存储在NFS中。
- en: 'Now, let''s reboot `node3`. As shown in the following output, we can see only
    two machines:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重启`node3`。从以下输出中可以看到，只剩下两台机器：
- en: '![](img/00104.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00104.jpg)'
- en: 'Let''s look at the services running. As can be seen from the following output,
    `redis.service` has moved from `node3` to `node2`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看正在运行的服务。从以下输出可以看出，`redis.service`已从`node3`移至`node2`：
- en: '![](img/00108.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.jpg)'
- en: 'Now, let''s check the web access count in `node2`. As we can see from the following
    output, the count started at `7` as the previous count was set to `6` on `node3`.
    This proves that container data is persisted:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下 `node2` 上的 Web 访问计数。正如我们从以下输出中看到的，计数从 `7` 开始，因为之前的计数在 `node3` 上设置为
    `6`。这证明了容器数据已被持久化：
- en: '![](img/00110.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpg)'
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: This example is not practical as there are multiple instances of a web
    server operating independently. In a more practical example, a load balancer would
    be the frontend. This example''s purpose is just to illustrate container data
    persistence using NFS.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此示例不具备实际应用性，因为有多个独立运行的 Web 服务器实例。在一个更实际的示例中，负载均衡器将作为前端。此示例的目的是仅为了说明如何使用 NFS
    实现容器数据持久化。
- en: The Docker 1.9 update
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.9 更新
- en: Docker 1.9 added named volumes, and this makes volumes as a first-class citizen
    in Docker. Docker volumes can be managed using `docker volume`.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 1.9 添加了命名卷，这使得卷成为 Docker 中的一级公民。Docker 卷可以使用 `docker volume` 进行管理。
- en: 'The following screenshot shows you the options in `docker volume`:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 `docker volume` 的选项：
- en: '![](img/00114.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00114.jpg)'
- en: A named volume deprecates a data-only container that was used earlier to share
    volumes across Containers.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 命名卷取代了之前用于跨容器共享卷的数据-only 容器。
- en: 'The following set of commands shows the same example used earlier with a named
    volume instead of a data-only container:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一组命令展示了使用命名卷替代数据-only 容器的相同示例：
- en: '`docker volume create --name redisvolume``docker run -d --name redis1 -v redisvolume:/data redis``docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter python app.py`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker volume create --name redisvolume` `docker run -d --name redis1 -v redisvolume:/data
    redis` `docker run -d --name hello1 --link redis1:redis -p 5000:5000 smakam/hellocounter
    python app.py`'
- en: In the preceding example, we create a named volume, `redisvolume`, which is
    used in the `redis1` container. The `hellocounter` application links to the `redis1`
    container.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个名为 `redisvolume` 的命名卷，该卷用于 `redis1` 容器。`hellocounter` 应用程序链接到
    `redis1` 容器。
- en: 'The following screenshot shows you information about the `redis1` volume:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 `redis1` 卷的信息：
- en: '![](img/00116.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00116.jpg)'
- en: Another advantage with named volumes is that we don't need to worry about the
    dangling volume problem that was present before.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命名卷的另一个优势是，我们不再需要担心之前存在的悬空卷问题。
- en: Summary
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered different storage options available for the storing
    of Container images and Container data in a CoreOS system. Technologies such as
    Flocker, GlusterFS, NFS, and Docker volumes and their integration with Containers
    and CoreOS were illustrated with practical examples. Container storage technologies
    are still evolving and will take some time to mature. There is a general industry
    trend to move away from expensive SAN technologies toward local and distributed
    storage. In the next chapter, we will discuss Container runtime Docker and Rkt
    and how they integrate with CoreOS.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 CoreOS 系统中存储容器镜像和容器数据的不同存储选项。通过实际示例展示了 Flocker、GlusterFS、NFS 和 Docker
    卷等技术以及它们与容器和 CoreOS 的集成。容器存储技术仍在发展中，并且需要一些时间才能成熟。业界普遍趋势是从昂贵的 SAN 技术转向本地和分布式存储。在下一章中，我们将讨论容器运行时
    Docker 和 Rkt，以及它们如何与 CoreOS 集成。
- en: References
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'GlusterFS: [http://gluster.readthedocs.org/](http://gluster.readthedocs.org/)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GlusterFS: [http://gluster.readthedocs.org/](http://gluster.readthedocs.org/)'
- en: 'The GlusterFS Docker volume plugin: [https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GlusterFS Docker 卷插件: [https://github.com/calavera/docker-volume-glusterfs](https://github.com/calavera/docker-volume-glusterfs)'
- en: 'Flocker: [https://docs.clusterhq.com](https://docs.clusterhq.com)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Flocker: [https://docs.clusterhq.com](https://docs.clusterhq.com)'
- en: 'Docker volume plugins: [https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md](https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker 卷插件: [https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md](https://github.com/docker/docker/blob/master/docs/extend/plugins_volume.md)'
- en: 'The Docker Storage driver: [https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker 存储驱动: [https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/](https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/)'
- en: 'Docker volumes: [https://docs.docker.com/userguide/dockervolumes/](https://docs.docker.com/userguide/dockervolumes/)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker 卷: [https://docs.docker.com/userguide/dockervolumes/](https://docs.docker.com/userguide/dockervolumes/)'
- en: 'Mounting storage on CoreOS: [https://coreos.com/os/docs/latest/mounting-storage.html](https://coreos.com/os/docs/latest/mounting-storage.html)'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在 CoreOS 上挂载存储: [https://coreos.com/os/docs/latest/mounting-storage.html](https://coreos.com/os/docs/latest/mounting-storage.html)'
- en: 'The Container filesystem: [http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1](http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1)'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '容器文件系统: [http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1](http://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html#1)'
- en: 'Ceph: [http://docs.ceph.com/](http://docs.ceph.com/)'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ceph: [http://docs.ceph.com/](http://docs.ceph.com/)'
- en: 'Ceph Docker: [https://github.com/ceph/ceph-docker](https://github.com/ceph/ceph-docker)'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ceph Docker: [https://github.com/ceph/ceph-docker](https://github.com/ceph/ceph-docker)'
- en: Further reading and tutorials
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读和教程
- en: 'Persistent data storage in the CoreOS cluster: [https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1](https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CoreOS 集群中的持久数据存储: [https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1](https://gist.github.com/Luzifer/c184b6b04d83e6d6fbe1)'
- en: 'Creating a GlusterFS cluster: [https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers](https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '创建一个 GlusterFS 集群: [https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers](https://www.digitalocean.com/community/tutorials/how-to-create-a-redundant-storage-pool-using-glusterfs-on-ubuntu-servers)'
- en: 'GlusterFS Overview: [https://www.youtube.com/watch?v=kvr6p9gSOX0](https://www.youtube.com/watch?v=kvr6p9gSOX0)'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GlusterFS 概述: [https://www.youtube.com/watch?v=kvr6p9gSOX0](https://www.youtube.com/watch?v=kvr6p9gSOX0)'
- en: 'Stateful Containers using Flocker on CoreOS: [http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047](http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047)'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在 CoreOS 上使用 Flocker 的有状态容器: [http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047](http://www.slideshare.net/ClusterHQ/stateful-containers-flocker-on-coreos-54492047)'
- en: 'Docker Storage webinar: [https://blog.docker.com/2015/12/persistent-storage-docker/](https://blog.docker.com/2015/12/persistent-storage-docker/)'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker 存储网络研讨会: [https://blog.docker.com/2015/12/persistent-storage-docker/](https://blog.docker.com/2015/12/persistent-storage-docker/)'
- en: 'The Contiv volume plugin: [https://github.com/contiv/volplugin](https://github.com/contiv/volplugin)'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Contiv 卷插件: [https://github.com/contiv/volplugin](https://github.com/contiv/volplugin)'
- en: 'Ceph RADOS: [http://ceph.com/papers/weil-rados-pdsw07.pdf](http://ceph.com/papers/weil-rados-pdsw07.pdf)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ceph RADOS: [http://ceph.com/papers/weil-rados-pdsw07.pdf](http://ceph.com/papers/weil-rados-pdsw07.pdf)'
