<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Proxying and Caching"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Proxying and Caching</h1></div></div></div><p>Designed as a web accelerator and a frontend server, Nginx has powerful tools to delegate complex tasks to upstream servers while focusing on heavy lifting. Reverse proxy is one such tool that turns Nginx into an essential component of any high-performance web service.</p><p>By abstracting away complexities of HTTP and handling them in a scalable and efficient manner, Nginx allows web applications to focus on solving the problem they are designed to solve without stumbling upon low-level details.</p><p>In this chapter, you will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to set up Nginx as a reverse proxy</li><li class="listitem" style="list-style-type: disc">How to make proxying transparent for the upstream server and the end user</li><li class="listitem" style="list-style-type: disc">How to handle upstream errors</li><li class="listitem" style="list-style-type: disc">How to use Nginx cache</li></ul></div><p>You will find out how to use all features of Nginx reverse proxy and turn it into a powerful tool for accelerating and scaling your web service.</p><div class="section" title="Nginx as a reverse proxy"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Nginx as a reverse proxy</h1></div></div></div><p>HTTP is a complex<a id="id179" class="indexterm"/> protocol that deals with data of different modality and has numerous optimizations that—if implemented properly—can lead to a significant increase in web service performance.</p><p>At the same time, web application developers have less time to deal with low-level issues and optimizations. The mere idea of decoupling a web application server from a frontend server shifts the focus on managing incoming traffic to the frontend, while shifting the focus on functionality, application logic, and features to the web application server. This is where Nginx comes into play as a decoupling point.</p><p>An example of a decoupling point is SSL termination: Nginx receives and processes inbound SSL connections, it forwards the request over plain HTTP to an application server, and wraps the received response back into SSL. The application server no longer needs to take care of storing certificates, SSL sessions, handling encrypted and unencrypted<a id="id180" class="indexterm"/> transmission, and so on.</p><p>Other examples of decoupling are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Efficient handling of static files and delegating the dynamic part to the upstream</li><li class="listitem" style="list-style-type: disc">Rate, request, and connection limiting</li><li class="listitem" style="list-style-type: disc">Compressing responses from the upstream</li><li class="listitem" style="list-style-type: disc">Caching responses from the upstream</li><li class="listitem" style="list-style-type: disc">Accelerating uploads and downloads</li></ul></div><p>By shifting these functions to a Nginx-powered frontend, you are essentially investing in the reliability of your website.</p></div></div>
<div class="section" title="Setting up Nginx as a reverse proxy"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Setting up Nginx as a reverse proxy</h1></div></div></div><p>Nginx can <a id="id181" class="indexterm"/>be easily configured to work as a reverse proxy:</p><div class="informalexample"><pre class="programlisting">location /example {
    proxy_pass http://upstream_server_name;
}</pre></div><p>In the preceding code, <code class="literal">upstream_server_name</code> is the host name of the upstream server. When a request for location is received, it will be passed to the upstream server with a specified host name.</p><p>If the upstream server does not have a host name, an IP address can be used instead:</p><div class="informalexample"><pre class="programlisting">location /example {
    proxy_pass http://192.168.0.1;
}</pre></div><p>If the upstream server is listening on a nonstandard port, the port can be added to the destination URL:</p><div class="informalexample"><pre class="programlisting">location /example {
    proxy_pass http://192.168.0.1:8080;
}</pre></div><p>The destination URL in the preceding examples does not have a path. This makes Nginx pass the request as is, without rewriting the path in the original request.</p><p>If a path is specified in the destination URL, it will replace a part of the path from the original request that corresponds to the matching part of the location. For example, consider the following configuration:</p><div class="informalexample"><pre class="programlisting">location /download {
    proxy_pass http://192.168.0.1/media;
}</pre></div><p>If a request <a id="id182" class="indexterm"/>for <code class="literal">/download/BigFile.zip</code> is received, the path in the destination URL is <code class="literal">/media</code> and it corresponds to the matching <code class="literal">/download</code> part of the original request URI. This part will be replaced with <code class="literal">/media</code> before passing to the upstream server, so the passed request path will look like <code class="literal">/media/BigFile.zip</code>.</p><p>If <code class="literal">proxy_pass</code> directive is used inside a regex location, the matching part cannot be computed. In this case, a destination URI without a path must be used:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass http://192.168.0.1;
}</pre></div><p>The same applies to cases where the request path was changed with the rewrite directive and is used by a <code class="literal">proxy_pass</code> directive.</p><p>Variables can be a part of the destination URL as well:</p><div class="informalexample"><pre class="programlisting">location ~* ^/(index|content|sitemap)\.html$ {
    proxy_pass http://192.168.0.1/html/$1;
}</pre></div><p>In fact, any part or even the whole destination URL can be specified by a variable:</p><div class="informalexample"><pre class="programlisting">location /example {
    proxy_pass $destination;
}</pre></div><p>This gives enough flexibility in specifying the destination URL for the upstream server. In <a class="link" href="ch05.html" title="Chapter 5. Managing Inbound and Outbound Traffic">Chapter 5</a>, <span class="emphasis"><em>Managing Inbound and Outbound Traffic</em></span>, we will find out how to specify multiple servers as an upstream and distribute connections among them.</p><div class="section" title="Setting the backend the right way"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>Setting the backend the right way</h2></div></div></div><p>The right <a id="id183" class="indexterm"/>way to configure a backend is to avoid passing everything to it. Nginx has powerful configuration directives that help you ensure that only specific requests are delegated to the backend.</p><p>Consider the following configuration:</p><div class="informalexample"><pre class="programlisting">location ~* \.php$ {
    proxy_pass http://backend;
    [...]
}</pre></div><p>This passes <a id="id184" class="indexterm"/>every request with a URI that ends with <code class="literal">.php</code> to the PHP interpreter. This is not only inefficient due to the intensive use of regular expressions, but also a serious security issue on most PHP setups because it may allow arbitrary code execution by an attacker.</p><p>Nginx has an elegant solution for this problem in the form of the <code class="literal">try_files</code> directive. The <code class="literal">try_files</code> directive takes a list of files and a location as the last argument. Nginx tries specified files in consecutive order and if none of them exists, it makes an internal redirect to the specified location. Consider the following example:</p><div class="informalexample"><pre class="programlisting">location / {
    try_files $uri $uri/ @proxy;
}

location @proxy {
    proxy_pass http://backend;
}</pre></div><p>The preceding configuration first looks up a file corresponding to the request URI, looks for a directory corresponding to the request URI in the hope of returning an index of that directory, and finally makes an internal redirect to the named location <code class="literal">@proxy</code> if none of these files or directories exist.</p><p>This configuration makes sure that whenever a request URI points to an object in the filesystem it is handled by Nginx itself using efficient file operations, and only if there is no match in the filesystem for the given request URI is it delegated to the backend.</p></div><div class="section" title="Adding transparency"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Adding transparency</h2></div></div></div><p>Once <a id="id185" class="indexterm"/>forwarded to an upstream server, a request loses certain properties of the original request. For example, the virtual host in a forwarded request is replaced by the host/port combination of the destination URL. The forwarded request is received from an IP address of the Nginx proxy, and the upstream server's functionality based on the client's IP address might not function properly.</p><p>The forwarded request needs to be adjusted so that the upstream server can obtain the missing information of the original request. This can be easily done with the <code class="literal">proxy_set_header</code> directive:</p><div class="informalexample"><pre class="programlisting">proxy_set_header &lt;header&gt; &lt;value&gt;;</pre></div><p>The <code class="literal">proxy_set_header</code> directive takes two arguments, the first of which is the name of the header <a id="id186" class="indexterm"/>that you want to set in the proxied request, and the second is the value for this header. Again, both arguments can contain variables.</p><p>Here is how you can pass the virtual host name from the original request:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1;
    proxy_set_header Host $host;
}</pre></div><p>The variable <code class="literal">$host</code> has a smart functionality. It does not simply pass the virtual host name from the original request, but uses the name of the server the request is processed by if the host header of the original request is empty or missing. If you insist on using the bare virtual host name from the original request, you can use the <code class="literal">$http_host</code> variable instead of <code class="literal">$host</code>.</p><p>Now that you know how to manipulate the proxied request, we can let the upstream server know the IP address of the original client. This can be done by setting <code class="literal">X-Real-IP</code> and/or the <code class="literal">X-Forwarded-For</code> headers:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
}</pre></div><p>This will make the upstream server aware of the original client's IP address via <code class="literal">X-Real-IP</code> or the <code class="literal">X-Forwarded-For</code> header. Most application servers support this header and take appropriate actions to properly reflect the original IP address in their API.</p></div><div class="section" title="Handling redirects"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/>Handling redirects</h2></div></div></div><p>The <a id="id187" class="indexterm"/>next challenge is rewriting redirects. When the upstream server issues a temporary or permanent redirect (HTTP status codes <code class="literal">301</code> or <code class="literal">302</code>), the absolute URI in the location or refresh headers needs to be rewritten so that it contains a proper host name (the host name of the server the original request came to).</p><p>This can be done using the <code class="literal">proxy_redirect</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_redirect http://localhost:8080/app http://www.example.com;
}</pre></div><p>Consider a <a id="id188" class="indexterm"/>web application that is running at <code class="literal">http://localhost:8080/app</code>, while the original server has the address <code class="literal">http://www.example.com</code>. Assume the web application issues a temporary redirect (HTTP 302) to <code class="literal">http://localhost:8080/app/login</code>. With the preceding configuration, Nginx will rewrite the URI in the location header to <code class="literal">http://www.example.com/login</code>.</p><p>If the redirect URI was not rewritten, the client would be redirected to <code class="literal">http://localhost:8080/app/login</code>, which is valid only within a local domain, so the web application would not be able to work properly. With the <code class="literal">proxy_redirect</code> directive, the redirect URI will be properly rewritten by Nginx, and the web application will be able to perform the redirect properly.</p><p>The host name in the second argument of the <code class="literal">proxy_redirect</code> directive can be omitted:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_redirect http://localhost:8080/app /;
}</pre></div><p>The preceding code can be further reduced to the following configuration using variables:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_redirect http://$proxy_host/app /;
}</pre></div><p>The same transparency option can be applied to cookies. In the preceding example, consider cookies are set to the domain <code class="literal">localhost:8080</code>, since the application server replies at <code class="literal">http://localhost:8080</code>. The cookies will not be returned by the browser, because the cookie domain does not match the request domain.</p></div><div class="section" title="Handling cookies"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/>Handling cookies</h2></div></div></div><p>To make<a id="id189" class="indexterm"/> cookies work properly, the domain name in cookies needs to be rewritten by the Nginx proxy. To do this, you can use the <code class="literal">proxy_cookie_domain</code> directive as shown here:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_cookie_domain localhost:8080 www.example.com;
}</pre></div><p>In the preceding example, Nginx replaces the cookie domain <code class="literal">localhost:8080</code> in the upstream response with <code class="literal">www.example.com</code>. The cookies set by the upstream server will refer to the domain <code class="literal">www.example.com</code> and the browser will return cookies in subsequent requests.</p><p>If cookie path needs to be rewritten as well due to application server being rooted at a different path, you can use the <code class="literal">proxy_cookie_path</code> directive as shown in the following code:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_cookie_path /my_webapp/ /;
}</pre></div><p>In this example, whenever Nginx detects a cookie with a prefix specified in the first argument of the <code class="literal">proxy_cookie_path </code>directive (<code class="literal">/my_webapp/</code>), it replaces this prefix with the value in the second argument of the <code class="literal">proxy_cookie_path </code>directive (<code class="literal">/</code>).</p><p>Putting everything together for the <code class="literal">www.example.com</code> domain and the web application running at <code class="literal">localhost:8080</code>, we get the following configuration:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://localhost:8080;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_redirect http://$proxy_host/app /;
    proxy_cookie_domain $proxy_host www.example.com;
    proxy_cookie_path /my_webapp/ /;
}</pre></div><p>The preceding configuration ensures transparency for a web application server so that it doesn't even need to know which virtual host it is running on.</p></div><div class="section" title="Using SSL"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/>Using SSL</h2></div></div></div><p>If the<a id="id190" class="indexterm"/> upstream server supports SSL, connections to the upstream server can be secured by simply changing the destination URL scheme to <code class="literal">https</code>:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass https://192.168.0.1;
}</pre></div><p>If the authenticity of the upstream server needs to be verified, this can be enabled using the <code class="literal">proxy_ssl_verify</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass https://192.168.0.1;
    proxy_ssl_verify on;
}</pre></div><p>The certificate of the upstream server will be verified against certificates of well-known certification authorities. In Unix-like operating systems, they are usually stored in <code class="literal">/etc/ssl/certs</code>.</p><p>If an upstream uses a trusted certificate that cannot be verified by well-known certification authorities or a self-signed certificate, it can be specified and declared as trusted using the <code class="literal">proxy_ssl_trusted_certificate</code> directive. This directive specifies the path to the certificate of the upstream server or a certificate chain required to authenticate the upstream server in PEM format. Consider the following example:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass https://192.168.0.1;
    proxy_ssl_verify on;
    proxy_ssl_trusted_certificate /etc/nginx/upstream.pem;
}</pre></div><p>If Nginx needs to authenticate itself to the upstream server, the client certificate and the key can be specified using the <code class="literal">proxy_ssl_certificate</code> and <code class="literal">proxy_ssl_certificate_key</code> directives. The directive <code class="literal">proxy_ssl_certificate</code> specifies the path to the client certificate in PEM format, while <code class="literal">proxy_ssl_certificate_key</code> specifies the path to the private key from the client certificate in PEM format. Consider the following example:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass https://192.168.0.1;
    proxy_ssl_certificate /etc/nginx/client.pem;
    proxy_ssl_certificate_key /etc/nginx/client.key;
}</pre></div><p>The specified certificate will be presented while setting up the secure connection to the upstream server, <a id="id191" class="indexterm"/>and its authenticity will be verified by specified private key.</p></div><div class="section" title="Handling errors"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/>Handling errors</h2></div></div></div><p>If Nginx experiences a<a id="id192" class="indexterm"/> problem contacting the upstream server or the upstream server returns an error, there is an option to take certain actions.</p><p>The upstream server connectivity errors can be handled using the <code class="literal">error_page</code> directive:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass http://192.168.0.1;
    error_page 500 502 503 504 /50x.html;
}</pre></div><p>This will make Nginx return the document from the file <code class="literal">50x.html</code> once an upstream connectivity error has occurred.</p><p>This will not change the HTTP status code in the response. To change the HTTP status code to successful, you can use the following syntax:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass http://192.168.0.1;
    error_page 500 502 503 504 =200 /50x.html;
}</pre></div><p>A more sophisticated action can be taken upon failure of an upstream server using an <code class="literal">error_page</code> directive that points to a named location:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass http://upstreamA;
    error_page 500 502 503 504 @retry;
}

location @retry {
    proxy_pass http://upstreamB;
    error_page 500 502 503 504 =200 /50x.html;
}</pre></div><p>In the preceding configuration, Nginx first tries to fulfill the request by forwarding it to the <code class="literal">upstreamA</code> server. If this results in an error, Nginx switches to a named location <code class="literal">@retry</code> in an attempt to try with the <code class="literal">upstreamB</code> server. Request an URI while switching so that the <code class="literal">upstreamB</code> server will receive an identical request. If this doesn't help either, Nginx returns a static file <code class="literal">50x.html</code> pretending no error occurred.</p><p>If an upstream has replied but returned an error, it can be intercepted rather than passed to the client using the <code class="literal">proxy_intercept_errors</code> directive:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass http://upstreamA;
    proxy_intercept_errors on;
    error_page 500 502 503 504 403 404 @retry;
}

location @retry {
    proxy_pass http://upstreamB;
    error_page 500 502 503 504 =200 /50x.html;
}</pre></div><p>In the <a id="id193" class="indexterm"/>preceding configuration, the <code class="literal">upstreamB</code> server will be called even when the <code class="literal">upstreamA</code> server replies but returns erroneous HTTP status code, such as <code class="literal">403</code> or <code class="literal">404</code>. This gives <code class="literal">upstreamB</code> an opportunity to fix the soft errors of <code class="literal">upstreamA</code>, if necessary.</p><p>However, this configuration pattern must not proliferate too much. In <a class="link" href="ch05.html" title="Chapter 5. Managing Inbound and Outbound Traffic">Chapter 5</a>, <span class="emphasis"><em>Managing Inbound and Outbound Traffic</em></span>, we will find out how to handle such situations in a more elegant way, without sophisticated configuration structures.</p></div><div class="section" title="Choosing an outbound IP address"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/>Choosing an outbound IP address</h2></div></div></div><p>Sometimes, when<a id="id194" class="indexterm"/> your proxy server has multiple network interfaces, it becomes necessary to choose which IP address should be used as outbound address for upstream connections. By default, the system will choose the address of the interface that adjoins the network containing the host used as destination in the default route.</p><p>To choose a particular IP address for outbound connections, you can use the <code class="literal">proxy_bind</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass https://192.168.0.1;
    proxy_bind 192.168.0.2;
}</pre></div><p>This will make Nginx bind outbound sockets to the IP address <code class="literal">192.168.0.2</code> before making a connection. The <a id="id195" class="indexterm"/>upstream server will then see connections coming from IP address <code class="literal">192.168.0.2</code>.</p></div><div class="section" title="Accelerating downloads"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/>Accelerating downloads</h2></div></div></div><p>Nginx is very<a id="id196" class="indexterm"/> efficient at heavy operations, such as handling large uploads and downloads. These operations can be delegated to Nginx using built-in functionality and third-party modules.</p><p>To accelerate download, the upstream server must be able to issue the <code class="literal">X-Accel-Redirect</code> header that points to the location of a resource which needs to be returned, instead of the response obtained from the upstream. Consider the following configuration:</p><div class="informalexample"><pre class="programlisting">location ~* (script1|script2|script3)\.php$ {
    proxy_pass https://192.168.0.1;
}

location /internal-media/ {
    internal;
    alias /var/www/media/;
}</pre></div><p>With the preceding configuration, once Nginx detects the <code class="literal">X-Accel-Redirect</code> header in the upstream response, it performs an internal redirect to the location specified in this header. Assume the upstream server instructs Nginx to perform an internal redirect to <code class="literal">/internal-media/BigFile.zip</code>. This path will be matched against the location <code class="literal">/internal-media</code>. This location specifies the document root at <code class="literal">/var/www/media</code>. So if a file <code class="literal">/var/www/media/BigFile.zip</code> exists, it will be returned to the client using efficient file operations.</p><p>For many web application servers, this feature provides an enormous speed up—both because they might not handle large downloads efficiently and because proxying reduces efficiency of large downloads.</p></div></div>
<div class="section" title="Caching"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Caching</h1></div></div></div><p>Once Nginx is set<a id="id197" class="indexterm"/> up as a reverse proxy, it's logical to turn it into a caching proxy. Fortunately, this can be achieved very easily with Nginx.</p><div class="section" title="Configuring caches"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec45"/>Configuring caches</h2></div></div></div><p>Before you<a id="id198" class="indexterm"/> can enable caching for a certain location, you need to configure a cache. A cache is a filesystem directory containing files with cached items and a shared memory segment where information about cached items is stored.</p><p>A cache can be declared using the <code class="literal">proxy_cache_path</code> directive:</p><div class="informalexample"><pre class="programlisting">proxy_cache_path &lt;path&gt; keys_zone=&lt;name&gt;:&lt;size&gt; [other parameters...];</pre></div><p>The preceding command declares a cache rooted at the path <code class="literal">&lt;path&gt;</code> with a shared memory segment named <code class="literal">&lt;name&gt;</code> of the size <code class="literal">&lt;size&gt;</code>.</p><p>This directive has to be specified in the <code class="literal">http</code> section of the configuration. Each instance of the directive declares a new cache and must specify a unique name for a shared memory segment. Consider the following example:</p><div class="informalexample"><pre class="programlisting">http {
    proxy_cache_path /var/www/cache keys_zone=my_cache:8m;
    [...]
}</pre></div><p>The preceding configuration declares a cache rooted at <code class="literal">/var/www/cache</code> with a shared memory segment named <code class="literal">my_cache</code>, which is 8 MB in size. Each cache item takes around 128 bytes in memory, thus the preceding configuration allocates space for around 64,000 items.</p><p>The following table lists other parameters of <code class="literal">proxy_cache_path</code> and their meaning:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">levels</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies hierarchy<a id="id199" class="indexterm"/> levels of the cache directory</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">inactive</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the time <a id="id200" class="indexterm"/>after which a cache item will be removed from the cache if it was not used, regardless of freshness</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">max_size</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies maximum<a id="id201" class="indexterm"/> size (total size) of all cache items</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">loader_files</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the <a id="id202" class="indexterm"/>number of files a <span class="strong"><strong>cache loader</strong></span> process<a id="id203" class="indexterm"/> loads in each iteration</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">loader_sleep</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the <a id="id204" class="indexterm"/>time interval a cache loader process sleeps between each iteration</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">loader_threshold</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Specifies the <a id="id205" class="indexterm"/>time limit for each iteration of a cache loader process</p>
</td></tr></tbody></table></div><p>Once Nginx starts, it processes all configured caches and allocates shared memory segments for each of the caches.</p><p>After that, a special process called cache loader takes care of loading cached items into memory. Cache loader loads items in iterations. The parameters <code class="literal">loader_files</code>, <code class="literal">loader_sleep</code>, and <code class="literal">loader_threshold</code> define the behavior of the cache loader process.</p><p>When running, a <a id="id206" class="indexterm"/>special process called <span class="strong"><strong>cache manager</strong></span><a id="id207" class="indexterm"/> monitors the total disk space taken by all cache items and evicts less requested items if the total consumed space is larger than specified in the <code class="literal">max_size</code> parameter.</p></div><div class="section" title="Enabling caching"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec46"/>Enabling caching</h2></div></div></div><p>To enable <a id="id208" class="indexterm"/>caching for a location, you need to specify the cache using the <code class="literal">proxy_cache</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
}</pre></div><p>The argument of the <code class="literal">proxy_cache</code> directive is the name of a shared memory segment that points to one of the caches configured using the <code class="literal">proxy_cache_path</code> directive. The same cache can be used in multiple locations. The upstream response will be cached if it is possible to determine the expiration interval for it. The primary source for the expiration interval for Nginx is the upstream itself. The following table explains which upstream response header influences <a id="id209" class="indexterm"/>caching and how:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Upstream response header</p>
</th><th style="text-align: left" valign="bottom">
<p>How it influences caching</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">X-Accel-Expires</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the cache item expiration interval in seconds. If the value starts from <code class="literal">@</code>, then the number following it is UNIX timestamp when the item is due to expire. This header has the higher priority.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">Expires</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This specifies the cache item expiration time stamp.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">Cache-Control</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This enables or disables caching</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">Set-Cookie</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This disables caching</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">Vary</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The special value <code class="literal">*</code> disables caching.</p>
</td></tr></tbody></table></div><p>It is also possible to explicitly<a id="id210" class="indexterm"/> specify an expiration interval for various response codes using the <code class="literal">proxy_cache_valid</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_valid 200 301 302 1h;
}</pre></div><p>This sets <a id="id211" class="indexterm"/>the expiration interval for responses with codes <code class="literal">200</code>, <code class="literal">301</code>, <code class="literal">302</code> to <code class="literal">1h</code> (1 hour). Note that the default status code list for the <code class="literal">proxy_cache_valid</code> directive is <code class="literal">200</code>, <code class="literal">301</code>, and <code class="literal">302</code>, so the preceding configuration can be simplified as follows:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_valid 10m;
}</pre></div><p>To enable caching for negative responses, such as <code class="literal">404</code>, you can extend the status code list in the <code class="literal">proxy_cache_valid</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_valid 200 301 302 1h;
    proxy_cache_valid 404 1m;
}</pre></div><p>The preceding configuration will cache <code class="literal">404</code> responses for <code class="literal">1m</code> (1 minute). The expiration interval for negative responses is deliberately set to much lower values than that of the positive responses. Such an optimistic approach ensures higher availability by expecting negative responses to improve, considering them as transient and assuming a shorter expected lifetime.</p></div><div class="section" title="Choosing a cache key"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec47"/>Choosing a cache key</h2></div></div></div><p>Choosing the right <a id="id212" class="indexterm"/>cache key is important for the best operation of the cache. The cache key must be selected such that it maximizes the expected efficiency of the cache, provided that each cached item has valid content for all subsequent requests that evaluate to the same key. This requires some explanation.</p><p>First, let's consider efficiency. When Nginx refers to the upstream server in order to revalidate a cache item, it obviously stresses the upstream server. With each subsequent cache hit, Nginx reduces the stress on the upstream server in comparison to the situation when requests were forwarded to the upstream without caching. Thus, the efficiency of the cache can be represented as <span class="emphasis"><em>Efficiency = (Number hits + Number misses) / Number misses</em></span>.</p><p>Thus, when nothing can be cached, each request leads to a cache miss and the efficiency is 1. But when we get 99 subsequent cache hits for each cache miss, the efficiency evaluates to <span class="emphasis"><em>(99 + 1) / 1 = 100</em></span>, which is 100 times larger!</p><p>Second, if a document is cached but it is not valid for all requests that evaluate to the same key, clients might see content that is not valid for their requests.</p><p>For example, the upstream analyses the <code class="literal">Accept-Language</code> header and returns the version of the document in the most suitable language. If the cache key does not include the language, the first user to request the document will obtain it in their language and trigger the caching in that language. All users that subsequently request this document will see the cached version of the document, and thus they might see it in the wrong language.</p><p>If the cache key includes the language of the document, the cache will contain multiple separate items for the same document in each requested language, and all users will see it in the proper language.</p><p>The default cache key is <code class="literal">$scheme$proxy_host$request_uri</code>.</p><p>This might not be optimal because of the following reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The web application server at <code class="literal">$proxy_host</code> can be responsible for multiple domains</li><li class="listitem" style="list-style-type: disc">The HTTP and HTTPS versions of the website can be identical (<code class="literal">$scheme</code> variable is redundant, thus duplicating items in the cache)</li><li class="listitem" style="list-style-type: disc">Content can vary depending on query arguments</li></ul></div><p>Thus, considering everything described previously and given that HTTP and HTTPS versions of the website are identical and content varies depending on query arguments, we can set the cache key to a more optimal value <code class="literal">$host$request_uri$is_args$args</code>. To change the default cache item key, you can use the <code class="literal">proxy_cache_key</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_key "$host$uri$is_args$args";
}</pre></div><p>This directive<a id="id213" class="indexterm"/> takes a script as its argument which is evaluated into a value of a cache key at runtime.</p></div><div class="section" title="Improving cache efficiency and availability"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec48"/>Improving cache efficiency and availability</h2></div></div></div><p>The efficiency and availability<a id="id214" class="indexterm"/> of the cache can be improved. You can prevent an<a id="id215" class="indexterm"/> item from being cached until it gets a certain minimum number of requests. This could be achieved using the <code class="literal">proxy_cache_min_uses</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_min_uses 5;
}</pre></div><p>In the preceding example, the response will be cached once the item gets no less than five requests. This prevents the cache from being populated by infrequently used items, thus reducing the disk space used for caching.</p><p>Once the item has expired, it can be revalidated without being evicted. To enable revalidation, use the <code class="literal">proxy_cache_revalidate</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://192.168.0.1:8080;
    proxy_cache my_cache;
    proxy_cache_revalidate on;
}</pre></div><p>In the preceding example, once a cache item expires, Nginx will revalidate it by making a conditional request to the upstream server. This request will include the <code class="literal">If-Modified-Since</code> and/or <code class="literal">If-None-Match</code> headers as a reference to the cached version. If the upstream server responds with a <code class="literal">304 Not Modified</code> response, the cache item remains in the cache and the expiration time stamp is reset.</p><p>Multiple simultaneous requests can <a id="id216" class="indexterm"/>be prohibited from filling the cache at the same time. Depending on the upstream reaction time, this might speed up cache population while reducing the load on the upstream server at the same time. To enable this behavior, you can use the <code class="literal">proxy_cache_lock</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_cache my_cache;
    proxy_cache_lock on;
}</pre></div><p>Once the behavior<a id="id217" class="indexterm"/> is enabled, only one request will be allowed to populate a cache item it is related to. The other requests related to this cache item will wait until either the cache item is populated or the lock timeout expires. The lock timeout can be specified using the <code class="literal">proxy_cache_lock_directive</code> directive.</p><p>If higher availability of the cache is required, you can configure Nginx to reply with stale data when a request refers to a cached item. This is very useful when Nginx acts as an edge server in a distribution network. The users and search engine crawlers will see your web site available, even though the main site experiences connectivity problems. To enable replying with stale data, use the <code class="literal">proxy_cache_use_stale</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_cache my_cache;
    proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;
}</pre></div><p>The preceding configuration enables replying with stale data in case of connectivity error, upstream error (<code class="literal">502</code>, <code class="literal">503</code>, or <code class="literal">504</code>), and connection timeout. The following table lists all possible<a id="id218" class="indexterm"/> values for arguments of the <code class="literal">proxy_cache_use_stale</code> directive:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Value</p>
</th><th style="text-align: left" valign="bottom">
<p>Meaning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">error</code>
</p>
</td><td style="text-align: left" valign="top">
<p>A connection error has occurred or an error during sending a request or receiving a reply has occurred</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">timeout</code>
</p>
</td><td style="text-align: left" valign="top">
<p>A connection timed out during setup, sending a request or receiving a reply</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">invalid_header</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server has returned an empty or invalid reply</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">updating</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Enables<a id="id219" class="indexterm"/> stale replies while the cache item is being updated</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_500</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">500</code> (Internal Server Error)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_502</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">502</code> (Bad Gateway)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_503</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">503</code> (Service Unavailable)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_504</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">504</code> (Gateway Timeout)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_403</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">403</code> (Forbidden)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">http_404</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The upstream server returned a reply with HTTP status code <code class="literal">404</code> (Not Found)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">off</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Disables use of stale replies</p>
</td></tr></tbody></table></div></div><div class="section" title="Handling exceptions and borderline cases"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec49"/>Handling exceptions and borderline cases</h2></div></div></div><p>When <a id="id220" class="indexterm"/>caching is <a id="id221" class="indexterm"/>not desirable or not efficient, it can be bypassed or disabled. This can happen in the following instances:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A resource is dynamic and varies depending on external factors</li><li class="listitem" style="list-style-type: disc">A resource is user-specific and varies depending on cookies</li><li class="listitem" style="list-style-type: disc">Caching does not add much value</li><li class="listitem" style="list-style-type: disc">A resource is not static, for example a video stream</li></ul></div><p>When bypass is forced, Nginx forwards the request to the backend without looking up an item in the cache. The bypass can be configured using the <code class="literal">proxy_cache_bypass</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_cache my_cache;
    proxy_cache_bypass $do_not_cache $arg_nocache;
}</pre></div><p>This directive <a id="id222" class="indexterm"/>can take one or more arguments. When any of them evaluate to true (nonempty value and not 0), Nginx does not look up an item in the cache for a given request. Instead, it directly forwards the request to the upstream server. The<a id="id223" class="indexterm"/> item can still be stored in the cache.</p><p>To prevent an item from being stored in the cache, you can use the <code class="literal">proxy_no_cache</code> directive:</p><div class="informalexample"><pre class="programlisting">location @proxy {
    proxy_pass http://backend;
    proxy_cache my_cache;
    proxy_no_cache $do_not_cache $arg_nocache;
}</pre></div><p>This directive works exactly like the <code class="literal">proxy_cache_bypass</code> directive, but prevents items from being stored in the cache. When only the <code class="literal">proxy_no_cache</code> directive is specified, the items can still be returned from the cache. The combination of both <code class="literal">proxy_cache_bypass </code>and <code class="literal">proxy_no_cache </code>disables caching completely.</p><p>Now, let's consider a real-world example when caching needs to be disabled for all user-specific pages. Assume that you have a website powered by WordPress and you want to enable caching for all pages but disable caching for all customized or user-specific pages. To implement this, you can use a configuration similar to the following:</p><div class="informalexample"><pre class="programlisting">location ~* wp\-.*\.php|wp\-admin {
    proxy_pass http://backend;

    proxy_set_header Host $http_host;
    proxy_set_header X-Real-IP $remote_addr;
}

location / {
    if ($http_cookie ~* "comment_author_|wordpress_|wp-postpass_" ) {
        set $do_not_cache 1;
    }

    proxy_pass http://backend;

    proxy_set_header Host $http_host;
    proxy_set_header X-Real-IP $remote_addr;

    proxy_cache my_cache;
    proxy_cache_bypass $do_not_cache;
    proxy_no_cache $do_not_cache;
}</pre></div><p>In the preceding <a id="id224" class="indexterm"/>configuration, we first delegate all requests pertaining to the WordPress administrative area to the upstream server. We then use the <code class="literal">if</code> directive to look up WordPress login cookies and set the <code class="literal">$do_not_cache</code> variable to <code class="literal">1</code> if they are present. Then, we<a id="id225" class="indexterm"/> enable caching for all other locations but disable caching whenever the <code class="literal">$do_not_cache</code> variable is set to <code class="literal">1</code> using the <code class="literal">proxy_cache_bypass </code>and <code class="literal">proxy_no_cache </code>directives. This disables caching for all requests with WordPress login cookies.</p><p>The preceding configuration can be extended to extract no-cache flags from arguments or HTTP headers, to further tune your caching.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Summary</h1></div></div></div><p>In this chapter, you learned how to work with proxying and caching—some of the most important Nginx features. These features practically define Nginx as a web accelerator and being proficient in them is essential to get the most out of Nginx.</p><p>In the next chapter, we'll look into how to rewrite engine works in Nginx and the basics of access control.</p></div></body></html>