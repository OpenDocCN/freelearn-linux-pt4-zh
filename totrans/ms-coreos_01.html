<html><head></head><body>
<p id="filepos77735" class="calibre_"><span class="calibre1"><span class="bold">Chapter 1. CoreOS Overview</span></span></p><p class="calibre_8">CoreOS is a <a/>Container-optimized Linux-based operating system to deploy a distributed application across a cluster of nodes. Along with providing a secure operating system, CoreOS provides services such as <tt class="calibre2">etcd</tt> and <tt class="calibre2">fleet</tt> that simplify the Container-based distributed application deployment. This chapter will provide you with an overview of Microservices and distributed application development concepts along with the basics of CoreOS, Containers, and Docker. Microservices is a software application development style where applications are composed of small, independent services talking to each other with APIs. After going through the chapter, you will be able to appreciate the role of CoreOS and Containers in the Microservices architecture.</p><p class="calibre_8">The following topics will be covered in this chapter:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Distributed application development—an overview and components</li><li value="2" class="calibre_13">Comparison of currently available minimalist Container-optimized OSes</li><li value="3" class="calibre_13">Containers—technology and advantages</li><li value="4" class="calibre_13">Docker—architecture and advantages</li><li value="5" class="calibre_13">CoreOS—architecture and components </li><li value="6" class="calibre_13">An overview of CoreOS components—<tt class="calibre2">systemd</tt>, <tt class="calibre2">etcd</tt>, <tt class="calibre2">fleet</tt>, <tt class="calibre2">flannel</tt>, and <tt class="calibre2">rkt</tt></li><li value="7" class="calibre_13">Docker versus Rkt</li><li value="8" class="calibre_13">A workflow for distributed application development with Docker, Rkt, and CoreOS</li></ul><div class="mbp_pagebreak" id="calibre_pb_23"/>


<p id="filepos79754" class="calibre_14"><span class="calibre1"><span class="bold">Distributed application development</span></span></p><p class="calibre_8">Distributed application <a/>development involves designing and coding a microservice-based application rather than creating a monolithic application. Each standalone service in the microservice-based application can be created as a Container. Distributed applications existed even before Containers were available. Containers provide the additional benefit of isolation and portability to each individual service in the distributed application. The following diagram shows you an example of a microservice-based <a/>application spanning multiple hosts:</p><p class="calibre_9"><img src="images/00186.jpg" class="calibre_15"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_24"/>


<p id="filepos80658" class="calibre_9"><span class="calibre3"><span class="bold">Components of distributed application development</span></span></p><p class="calibre_8">The following are the primary<a/> components of distributed application development. This assumes that individual services of the distributed application are created as Containers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Applications or microservices.</li><li value="2" class="calibre_13">Cloud infrastructure—public (AWS, GCE, and Digital Ocean) or private.</li><li value="3" class="calibre_13">Base OS—CoreOS, Atomic, Rancher OS, and others.</li><li value="4" class="calibre_13">Distributed data store and service discovery—<tt class="calibre2">etcd</tt>, <tt class="calibre2">consul</tt>, and <tt class="calibre2">Zookeeper</tt>.</li><li value="5" class="calibre_13">Load balancer—NGINX and HAProxy.</li><li value="6" class="calibre_13">Container runtime—Docker, Rkt, and LXC.</li><li value="7" class="calibre_13">Container orchestration—Fleet, Kubernetes, Mesos, and Docker Swarm.</li><li value="8" class="calibre_13">Storage—local or distributed storage. Some examples are GlusterFS and Ceph for cluster storage and AWS EBS for cloud storage. Flocker's upcoming storage driver plugin promises to work across different storage mechanisms.</li><li value="9" class="calibre_13">Networking—using cloud-based networking such as AWS VPC, CoreOS Flannel, or Docker networking.</li><li value="10" class="calibre_13">Miscellaneous—Container monitoring (cadvisor, Sysdig, and Newrelic) and Logging (Spout and Logentries).</li><li value="11" class="calibre_13">An update strategy to update microservices, such as a rolling upgrade.</li></ul><div class="mbp_pagebreak" id="calibre_pb_25"/>


<p id="filepos82616" class="calibre_14"><span class="calibre3"><span class="bold">Advantages and disadvantages</span></span></p><p class="calibre_8">The following are<a/> some advantages of distributed application development:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Application developers of each microservice can work independently. If necessary, different microservices can even have their own programming language.</li><li value="2" class="calibre_13">Application component reuse becomes high. Different unrelated projects can use the same microservice.</li><li value="3" class="calibre_13">Each individual service can be horizontally scaled. CPU and memory usage for each microservice can be tuned appropriately.</li><li value="4" class="calibre_13">Infrastructure can be treated like cattle rather than a pet, and it is not necessary to differentiate between each individual infrastructure component.</li><li value="5" class="calibre_13">Applications can be deployed in-house or on a public, private, or hybrid cloud.</li></ul><p class="calibre_8">The following are some problems <a/>associated with the microservices approach:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The number of microservices to manage can become huge and this makes it complex to manage the application.</li><li value="2" class="calibre_13">Debugging can become difficult.</li><li value="3" class="calibre_13">Maintaining integrity and consistency is difficult so services must be designed to handle failures.</li><li value="4" class="calibre_13">Tools are constantly changing, so there is a need to stay updated with current technologies.</li></ul><div class="mbp_pagebreak" id="calibre_pb_26"/>


<p id="filepos84547" class="calibre_"><span class="calibre1"><span class="bold">A minimalist Container-optimized OS</span></span></p><p class="calibre_8">This is a new OS category for developing distributed applications that has become popular in recent years. Traditional<a/> Linux-based OSes were bulky for Container deployment and did not natively provide the services that Containers need. The<a/> following are some common characteristics of a Container-optimized OS:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The OS needs to be bare-minimal and fast to bootup</li><li value="2" class="calibre_13">It should have an automated update strategy</li><li value="3" class="calibre_13">Application development should be done using Containers</li><li value="4" class="calibre_13">Redundancy and clustering should be built-in</li></ul><p class="calibre_8">The following table captures the comparison of features of four common Container-optimized OSes. Other OSes such as VMWare Photon and Mesos DCOS have not been included.</p><table border="1" valign="top" class="calibre_16"><tr valign="top" class="calibre_17"><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Feature</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">CoreOS</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Rancher OS</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Atomic</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Ubuntu snappy</span></p></th></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Company</p></td><td valign="top" class="calibre_19"><p class="calibre_8">CoreOS</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Rancher Labs</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Red Hat </p></td><td valign="top" class="calibre_19"><p class="calibre_8">Canonical</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Containers</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker and Rkt</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Snappy packages and Docker</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Maturity</p></td><td valign="top" class="calibre_19"><p class="calibre_8">First release in 2013, relatively mature</p></td><td valign="top" class="calibre_19"><p class="calibre_8">First release in early 2015, pretty new</p></td><td valign="top" class="calibre_19"><p class="calibre_8">First release in early 2015, pretty new</p></td><td valign="top" class="calibre_19"><p class="calibre_8">First release in early 2015, pretty new</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Service management</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Systemd and Fleet</p></td><td valign="top" class="calibre_19"><p class="calibre_8">System docker manages system services and user docker manages user containers</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Systemd</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Systemd and Upstart</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Tools</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Etcd, fleet, and flannel</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Rancher has tools for service discovery, load balancing, dns, storage, and networking</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Flannel and other RedHat tools</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Ubuntu tools</p></td></tr><a/><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Orchestration</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Kubernetes and Tectonic</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Rancher's own orchestration and Kubernetes</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Kubernetes. Atomic app, and Nulecule also used</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Kubernetes and any other orchestration tool</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Update</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Automatic, uses A and B partitions</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Automatic</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Automatic, uses rpm-os-tree</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Automatic</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Registry</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker hub and Quay</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker hub</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker hub</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Docker hub</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Debugging</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Toolbox</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Rancher's own tools and external tools</p></td><td valign="top" class="calibre_19"><p class="calibre_8">RedHat tools</p></td><td valign="top" class="calibre_19"><p class="calibre_8">Ubuntu debug tools</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">Security</p></td><td valign="top" class="calibre_19"><p class="calibre_8">SELinux can be turned on</p></td><td valign="top" class="calibre_19"><p class="calibre_8">There is a plan to add SELinux and AppArmor support</p></td><td valign="top" class="calibre_19"><p class="calibre_8">SELinux enabled by default, additional security</p></td><td valign="top" class="calibre_19"><p class="calibre_8">AppArmor security profile can be used</p></td></tr></table><div class="mbp_pagebreak" id="calibre_pb_27"/>


<p id="filepos91776" class="calibre_"><span class="calibre1"><span class="bold">Containers</span></span></p><p class="calibre_8">Containers do virtualization <a/>at the OS level while VMs do virtualization at the hardware level. Containers in a single host share the same kernel. As Containers are lightweight, hundreds of containers can run on a single host. In a microservices-based design, the approach taken is to split a single application into multiple small independent components and run each component as a Container. LXC, Docker, and Rkt are examples of Container runtime implementations.</p><div class="mbp_pagebreak" id="calibre_pb_28"/>


<p id="filepos92408" class="calibre_9"><span class="calibre3"><span class="bold">Technology</span></span></p><p class="calibre_8">The following are the<a/> two critical Linux kernel technologies that are used in Containers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Namespaces</span>: They virtualize processes, networks, filesystems, users, and so on</li><a/><li value="2" class="calibre_13"><span class="bold">cgroups</span>: They limit the usage of the CPU, memory, and I/O per group of processes</li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_29"/>


<p id="filepos93022" class="calibre_14"><span class="calibre3"><span class="bold">Advantages</span></span></p><p class="calibre_8">The following are <a/>some significant advantages of Containers:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Each container is isolated from other Containers. There is no issue of shared package management, shared libraries, and so on.</li><li value="2" class="calibre_13">Compared to a VM, Containers have smaller footprints and are faster to load and run.</li><li value="3" class="calibre_13">They provide an efficient usage of computing power.</li><li value="4" class="calibre_13">They can work seamlessly across dev, test, and production. This makes Containers DevOps-friendly.</li></ul><div class="mbp_pagebreak" id="calibre_pb_30"/>


<p id="filepos93893" class="calibre_14"><span class="calibre3"><span class="bold">An overview of Docker architecture</span></span></p><p class="calibre_8">Docker is a Container<a/> runtime implementation. Even though Containers were available for quite a long time, Docker revolutionized Container technology <a/>by making it easier to use. The following image shows you the main components of Docker (the <a/>
<span class="bold">Docker engine</span>, <span class="bold">Docker </span>
<a/>
<span class="bold">CLI</span>, <span class="bold">Docker</span>
<a/>
<span class="bold"> REST</span>, and <span class="bold">Docker hub</span>) and how they<a/> interact with each other:</p><p class="calibre_9"><img src="images/00192.jpg" class="calibre_20"/></p><p class="calibre_8">
</p><p class="calibre_8">Following are some details on the Docker architecture:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The Docker daemon runs in every host where Docker is installed and started.</li><li value="2" class="calibre_13">Docker uses Linux kernel container facilities such as namespaces and cgroups through the libcontainer library.</li><a/><li value="3" class="calibre_13">The Docker client can run in the host machine or externally and it communicates with the Docker daemon using the REST interface. There is also a CLI interface that the Docker client provides.</li><a/><li value="4" class="calibre_13">The Docker hub is the repository for Docker images. Both private and public images can be hosted in the Docker hub repository.</li><li value="5" class="calibre_13">Dockerfile is used to create container images. The following is a sample Dockerfile that is used to create a Container that starts the Apache web service exposing port <tt class="calibre2">80</tt> to the outside world:<p class="calibre_"><img src="images/00194.jpg" class="calibre_21"/></p><p class="calibre_8">
</p></li><a/><li value="6" class="calibre_13">The Docker platform as of release 1.9 includes orchestration tools such as Swarm, Compose, Kitematic, and Machine as well as native networking and storage solutions. Docker follows a batteries-included pluggable approach for orchestration, storage, and networking where a native Docker solution can be swapped with vendor plugins. For example, Weave can be used as an external networking plugin, Flocker can be used as an external storage plugin, and Kubernetes can be used as an external orchestration plugin. These external plugins can replace the native Docker solutions.</li></ul><div class="mbp_pagebreak" id="calibre_pb_31"/>


<p id="filepos96622" class="calibre_14"><span class="calibre3"><span class="bold">Advantages of Docker</span></span></p><p class="calibre_8">The following are<a/> some significant advantages of Docker:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Docker has revolutionized Container packaging and tools around Containers and this has helped both application developers and infrastructure administrators</li><li value="2" class="calibre_13">It is easier to deploy and upgrade individual containers</li><li value="3" class="calibre_13">It is more suitable for the microservices architecture</li><li value="4" class="calibre_13">It works great across all Linux distributions as long as the kernel version is greater than or equal to 3.10</li><li value="5" class="calibre_13">The Union filesystem makes it faster to download and keep different versions of container images</li><a/><li value="6" class="calibre_13">Container management tools such as Dockerfile, Docker engine CLI, Machine, Compose, and Swarm make it easy to manage containers</li><li value="7" class="calibre_13">Docker provides an easy way to share Container images using public and private registry services</li></ul><div class="mbp_pagebreak" id="calibre_pb_32"/>


<p id="filepos97996" class="calibre_"><span class="calibre1"><span class="bold">CoreOS</span></span></p><p class="calibre_8">CoreOS belongs to the minimalist Container-optimized OS category. CoreOS is the first OS in this category<a/> and many new OSes have appeared recently in the same category. CoreOS's mission is to improve the security and reliability of the Internet. CoreOS is a pioneer in this space and its first alpha release was in July 2013. A lot of developments have happened in the past two years in the area of networking, distributed storage, container runtime, authentication, and security. CoreOS is used by PaaS providers (such as Dokku and Deis), Web application development companies, and many enterprise and service providers developing distributed applications.</p><div class="mbp_pagebreak" id="calibre_pb_33"/>


<p id="filepos98816" class="calibre_9"><span class="calibre3"><span class="bold">Properties</span></span></p><p class="calibre_8">The following are<a/> some of the key CoreOS properties:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The kernel is very small and fast to bootup.</li><li value="2" class="calibre_13">The base OS and all services are open sourced. Services can also be used standalone in non-CoreOS systems.</li><li value="3" class="calibre_13">No package management is provided by the OS. Libraries and packages are part of the application developed using Containers.</li><li value="4" class="calibre_13">It enables secure, large server clusters that can be used for distributed application development.</li><li value="5" class="calibre_13">It is based on principles from the Google Chrome OS.</li><li value="6" class="calibre_13">Container runtime, SSH, and kernel are the primary components.</li><li value="7" class="calibre_13">Every process is managed by systemd.</li><li value="8" class="calibre_13">Etcd, fleet, and flannel are all controller units running on top of the kernel.</li><li value="9" class="calibre_13">It supports both Docker and Rkt Container runtime.</li><li value="10" class="calibre_13">Automatic updates are provided with A and B partitions.</li><li value="11" class="calibre_13">The Quay registry service can be used to store public and private Container images.</li><li value="12" class="calibre_13">CoreOS release channels (stable, beta, and alpha) are used to control the release cycle.</li><li value="13" class="calibre_13">Commercial products include the Coreupdate service (part of the commercially managed and enterprise CoreOS), Quay enterprise, and Tectonic (CoreOS + Kubernetes).</li><a/><li value="14" class="calibre_13">It currently runs on x86 processors.</li></ul><div class="mbp_pagebreak" id="calibre_pb_34"/>


<p id="filepos100976" class="calibre_14"><span class="calibre3"><span class="bold">Advantages</span></span></p><p class="calibre_8">The following are some<a/> significant advantages of CoreOS:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">The kernel auto-update feature protects the kernel from security vulnerabilities.</li><li value="2" class="calibre_13">The CoreOS memory footprint is very small.</li><li value="3" class="calibre_13">The management of CoreOS machines is done at the cluster level rather than at an individual machine level.</li><li value="4" class="calibre_13">It provides service-level (using systemd) and node-level (using fleet) redundancy.</li><li value="5" class="calibre_13">Quay provides you with a private and public Container repository. The repository can be used for both Docker and Rkt containers.</li><li value="6" class="calibre_13">Fleet is used for basic service orchestration and Kubernetes is used for application service orchestration.</li><li value="7" class="calibre_13">It is supported by all major cloud providers such as AWS, GCE, Azure, and DigitalOcean.</li><li value="8" class="calibre_13">Majority of CoreOS components are open sourced and the customer can choose the combination of tools that is necessary for their specific application.</li></ul><div class="mbp_pagebreak" id="calibre_pb_35"/>


<p id="filepos102495" class="calibre_14"><span class="calibre3"><span class="bold">Supported platforms</span></span></p><p class="calibre_8">The following<a/> are the official and community-supported CoreOS platforms. This is not an exhaustive list.</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">For exhaustive list of CoreOS supported platforms, please refer to this link (<a href="https://coreos.com/os/docs/latest/">https://coreos.com/os/docs/latest/</a>).</p><p class="calibre_8">The platforms that are officially supported are as follows:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Cloud platforms such as AWS, GCE, Microsoft Azure, DigitalOcean, and OpenStack</li><li value="2" class="calibre_13">Bare metal with PXE</li><li value="3" class="calibre_13">Vagrant</li></ul><p class="calibre_8">The platforms that are community-supported are as follows:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">CloudStack</li><li value="2" class="calibre_13">VMware</li></ul><div class="mbp_pagebreak" id="calibre_pb_36"/>


<p id="filepos103789" class="calibre_14"><span class="calibre3"><span class="bold">CoreOS components</span></span></p><p class="calibre_8">The following are<a/> the CoreOS core components and CoreOS ecosystem. The ecosystem can become pretty large if automation, management, and monitoring tools are<a/> included. These have not been included here.</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Core components: Kernel, systemd, etcd, fleet, flannel, and rkt</li><li value="2" class="calibre_13">CoreOS ecosystem: Docker and Kubernetes</li></ul><p class="calibre_8">The following image shows you the different layers in the CoreOS architecture:</p><p class="calibre_9"><img src="images/00197.jpg" class="calibre_22"/></p><p class="calibre_8">
</p><p id="filepos104698" class="calibre_9"><span class="calibre3"><span class="bold">Kernel</span></span></p><p class="calibre_8">CoreOS uses the<a/> latest Linux kernel in its distribution. The following screenshot shows the Linux kernel version running in the CoreOS stable release 766.3.0:</p><p class="calibre_9"><img src="images/00199.jpg" class="calibre_23"/></p><p class="calibre_8">
</p><p id="filepos105149" class="calibre_9"><span class="calibre3"><span class="bold">Systemd</span></span></p><p class="calibre_8">Systemd is an init system <a/>used by CoreOS to start, stop, and <a/>manage processes. SysVinit is one of the oldest init systems. The following are some <a/>of the common init systems used in the Unix world:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Systemd: CoreOS and RedHat</li><li value="2" class="calibre_13">Upstart: Ubuntu</li><li value="3" class="calibre_13">Supervisord: The Python world</li></ul><p class="calibre_8">The following are<a/> some of the common functionality performed by an<a/> init system:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">It is the first process to start</li><li value="2" class="calibre_13">It controls the ordering and execution of all the user processes</li><li value="3" class="calibre_13">It takes care of restarting processes if they die or hang</li><li value="4" class="calibre_13">It takes care of process ownership and resources</li></ul><p class="calibre_8">The following are <a/>some specifics of systemd:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Every process in systemd runs in one cgroup and this includes forked processes. If the systemd service is killed, all the processes associated with the service, including forked processes, are killed. This also provides you with a nice way to control resource usage. If we run a Container in systemd, we can control the resource usage even if the container contains multiple processes. Additionally, systemd takes care of restarting containers that die if we specify the <tt class="calibre2">restart</tt> option in systemd.</li><li value="2" class="calibre_13">Systemd units are run and controlled on a single machine.</li><li value="3" class="calibre_13">These are some systemd unit types—service, socket, device, and mount.</li><li value="4" class="calibre_13">The <tt class="calibre2">Service</tt> type is the most common type and is used to define a service with its dependencies. The <tt class="calibre2">Socket</tt> type is used to expose services to the external world. For example, <tt class="calibre2">docker.service</tt> exposes external connectivity to the Docker engine through <tt class="calibre2">docker.socket</tt>. Sockets can also be used to export logs to external machines.</li><li value="5" class="calibre_13">The <tt class="calibre2">systemctl</tt> CLI can be used to control Systemd units.</li></ul><p id="filepos107914" class="calibre_14"><span class="bold">Systemd units</span></p><p class="calibre_8">The following are some important systemd <a/>units in a CoreOS system.</p><p id="filepos108097" class="calibre_9"><span class="calibre5"><span class="bold">Etcd2.service</span></span></p><p class="calibre_8">The following is an<a/> example <tt class="calibre2">etcd2.service</tt> unit file:</p><p class="calibre_9"><img src="images/00201.jpg" class="calibre_24"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some details about the etcd2 service unit file:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">All units have the <tt class="calibre2">[Unit]</tt> and <tt class="calibre2">[Install]</tt> sections. There is a type-specific section such as <tt class="calibre2">[Service]</tt> for service units.</li><li value="2" class="calibre_13">The <tt class="calibre2">Conflicts</tt> option notifies that either <tt class="calibre2">etcd</tt> or <tt class="calibre2">etcd2</tt> can run, but not both.</li><li value="3" class="calibre_13">The <tt class="calibre2">Environment</tt> option specifies the environment variables to be used by <tt class="calibre2">etcd2</tt>. The <tt class="calibre2">%m</tt> unit specifier allows the machine ID to be taken automatically based on where the service is running.</li><li value="4" class="calibre_13">The <tt class="calibre2">ExecStart</tt> option specifies the executable to be run.</li><li value="5" class="calibre_13">The <tt class="calibre2">Restart</tt> option specifies whether the service can be restarted. The <tt class="calibre2">Restartsec</tt> option specifies the time interval after which the service should be restarted.</li><a/><li value="6" class="calibre_13"><tt class="calibre2">LimitNoFILE</tt> specifies the file count limit.</li><li value="7" class="calibre_13">The <tt class="calibre2">WantedBy</tt> option in the <tt class="calibre2">Install</tt> section specifies the group to which this service belongs. The grouping mechanism allows systemd to start up groups of processes at the same time.</li></ul><p id="filepos109993" class="calibre_14"><span class="calibre5"><span class="bold">Fleet.service</span></span></p><p class="calibre_8">The following is an<a/> example of the <tt class="calibre2">fleet.service</tt> unit file:</p><p class="calibre_9"><img src="images/00203.jpg" class="calibre_25"/></p><p class="calibre_8">
</p><p class="calibre_8">In the preceding unit file, we can see two dependencies for <tt class="calibre2">fleet.service.</tt>
<tt class="calibre2">etcd.Service</tt> and <tt class="calibre2">etcd2.service</tt> are specified as dependencies as Fleet depends on them to communicate between fleet <a/>agents in different nodes. The <tt class="calibre2">fleet.socket</tt> socket unit is also specified as a dependency as it is used by external clients to talk to Fleet.</p><p id="filepos110780" class="calibre_9"><span class="calibre5"><span class="bold">Docker.service</span></span></p><p class="calibre_8">The Docker service <a/>consists of the following components:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><tt class="calibre2">Docker.service</tt>: This starts the Docker daemon</li><li value="2" class="calibre_13"><tt class="calibre2">Docker.socket</tt>: This allows communication with the Docker daemon from the CoreOS node</li><li value="3" class="calibre_13"><tt class="calibre2">Docker-tcp.socket</tt>: This allows communication with the Docker daemon from external hosts with port <tt class="calibre2">2375</tt> as the listening port</li></ul><p class="calibre_8">The following <tt class="calibre2">docker.service</tt> unit file starts the Docker daemon:</p><p class="calibre_9"><img src="images/00205.jpg" class="calibre_26"/></p><p class="calibre_8">
</p><p class="calibre_8">The following<a/>
<tt class="calibre2">docker.socket</tt> unit file starts the local socket stream:</p><p class="calibre_9"><img src="images/00208.jpg" class="calibre_27"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Tip</span></span></p><p class="calibre_8"><span class="bold">Downloading the example code</span>
</p><p class="calibre_8">You can download the example code files from your account at <a href="http://www.packtpub.com">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this b</p><p class="calibre_8">The following <tt class="calibre2">docker-tcp.socket</tt> unit file sets up a listening socket for remote client communication:</p><p class="calibre_9"><img src="images/00212.jpg" class="calibre_28"/></p><p class="calibre_8">
</p><p class="calibre_8">The <tt class="calibre2">docker ps</tt> command uses <tt class="calibre2">docker.socket</tt> and <tt class="calibre2">docker -H tcp://127.0.0.1:2375 ps</tt> uses <tt class="calibre2">docker-tcp.socket</tt> unit to communicate with<a/> the Docker daemon running in the local system.</p><p id="filepos113046" class="calibre_9"><span class="bold">The procedure to start a simple systemd service</span></p><p class="calibre_8">Let's start a<a/> simple <tt class="calibre2">hello1.service</tt> unit that runs a Docker busybox container, as shown in the following image:</p><p class="calibre_9"><img src="images/00215.jpg" class="calibre_29"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are the steps to start <tt class="calibre2">hello1.service</tt>:</p><div class="calibre_11"> </div><ol class="calibre_30"><li value="1" class="calibre_13">Copy <tt class="calibre2">hello1.service</tt> as sudo to <tt class="calibre2">/etc/systemd/system</tt>.</li><li value="2" class="calibre_13">Enable the service:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo systemctl enable /etc/systemd/system/hello1.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li><li value="3" class="calibre_31">Start <tt class="calibre2">hello1.service</tt>:<p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">sudo systemctl start hello1.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p></li></ol><p class="calibre_8">This creates the<a/> following link:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">core@core-01 /etc/systemd/system/multi-user.target.wants $ ls -la</span></tt><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">lrwxrwxrwx 1 root root   34 Aug 12 13:25 hello1.service -&gt; /etc/systemd/system/hello1.service</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">Now, we can see the status of <tt class="calibre2">hello1.service</tt>:</p><p class="calibre_9"><img src="images/00219.jpg" class="calibre_32"/></p><p class="calibre_8">
</p><p class="calibre_8">In the preceding output, we can see that the service is in the active state. At the end, we can also see <tt class="calibre2">stdout</tt> where the echo output is logged.</p><p class="calibre_8">Let's look at the running Docker containers:</p><p class="calibre_9"><img src="images/00223.jpg" class="calibre_33"/></p><p class="calibre_8">
</p><p class="calibre_9"><span class="calibre3"><span class="bold">Note</span></span></p><p class="calibre_8">When starting Docker Containers with systemd, it is necessary to avoid using the <tt class="calibre2">-d</tt> option as it prevents the Container process to be monitored by systemd. More <a/>details can be found at <a href="https://coreos.com/os/docs/latest/getting-started-with-docker.html">https://coreos.com/os/docs/latest/getting-started-with-docker.html</a>.</p><p id="filepos115796" class="calibre_9"><span class="bold">Demonstrating systemd HA</span></p><p class="calibre_8">In the <tt class="calibre2">hello1.service</tt>
<a/> created, we specified two options:</p><p class="calibre_8"><tt class="calibre2">Restart=always<br class="calibre4"/>RestartSec=30s</tt></p><p class="calibre_8">This means that the service should be restarted after 30 seconds in case the service exits for some reason.</p><p class="calibre_8">Let's stop the Docker <tt class="calibre2">hello1</tt> container:</p><p class="calibre_9"><img src="images/00226.jpg" class="calibre_34"/></p><p class="calibre_8">
</p><p class="calibre_8">Service gets restarted automatically after 30 seconds, as shown in the following screenshot:</p><p class="calibre_9"><img src="images/00229.jpg" class="calibre_35"/></p><p class="calibre_8">
</p><p class="calibre_8">The following screenshot shows you that the <tt class="calibre2">hello1</tt> container is running again. From the Container status output, we can see that the container is up only for a minute:</p><p class="calibre_9"><img src="images/00233.jpg" class="calibre_36"/></p><p class="calibre_8">
</p><p class="calibre_8">We can also confirm the service restarted from the systemd logs associated with that service. In the<a/> following output, we can see that the service exited and restarted after 30 seconds:</p><p class="calibre_9"><img src="images/00236.jpg" class="calibre_37"/></p><p class="calibre_8">
</p><p id="filepos117567" class="calibre_9"><span class="calibre3"><span class="bold">Etcd</span></span></p><p class="calibre_8">Etcd is a distributed<a/> key-value store used by all machines in the CoreOS cluster to read/write and exchange data. Etcd uses the Raft consensus algorithm (<a href="https://raft.github.io/">https://raft.github.io/</a>) to maintain a<a/> highly available cluster. Etcd is used to share configuration and monitoring data across CoreOS machines and for doing service discovery. All other CoreOS services such as Fleet and Flannel use etcd as a distributed database. Etcd can also be used as a standalone outside CoreOS. In fact, many complex distributed <a/>application projects such as Kubernetes and Cloudfoundry use etcd for their distributed key-value store. The <tt class="calibre2">etcdctl</tt> utility is the CLI frontend for etcd.</p><p class="calibre_8">The following are two sample use cases of etcd.</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Service discovery</span>: Service discovery can be used to communicate service connectivity details across containers. Let's take an example WordPress application with a WordPress application container and MySQL database container. If one of the machines has a database container and wants to communicate its service IP address and port number, it can use etcd to write the relevant key and data; the WordPress container in another host can use the key value to write to the appropriate database.</li><a/><li value="2" class="calibre_13"><span class="bold">Configuration sharing</span>: The Fleet master talks to Fleet agents using etcd to decide which node in the cluster will execute the Fleet service unit.</li><a/></ul><p id="filepos119345" class="calibre_14"><span class="bold">Etcd discovery</span></p><p class="calibre_8">The members in<a/> the cluster discover themselves using either a static approach or dynamic approach. In the static approach, we need to mention the IP addresses of all the neighbors statically in every node of the cluster. In the dynamic approach, we use the discovery token approach where we get a distributed token from a central etcd server and use this in all members of the cluster so that the members can discover <a/>each other.</p><p class="calibre_8">Get a distributed token as follows:</p><p class="calibre_8"><tt class="calibre2"><br class="calibre4"/></tt><tt class="calibre2"><span class="bold">curl https://discovery.etcd.io/new?size=&lt;size&gt;</span></tt><tt class="calibre2"><br class="calibre4"/></tt></p><p class="calibre_8">The following is an example of getting a discovery token for a cluster size of three:</p><p class="calibre_9"><img src="images/00239.jpg" class="calibre_38"/></p><p class="calibre_8">
</p><p class="calibre_8">The discovery token feature is hosted by CoreOS and is implemented as an etcd cluster as well.</p><p id="filepos120558" class="calibre_9"><span class="bold">Cluster size</span></p><p class="calibre_8">It is preferable to have an odd-sized etcd cluster as it gives a better failure tolerance. The following table <a/>shows the majority count and failure tolerance for common cluster sizes up to five. With a cluster size of two, we cannot determine majority.</p><table border="1" valign="top" class="calibre_16"><tr valign="top" class="calibre_17"><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Cluster size</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Majority</span></p></th><th valign="top" class="calibre_18"><p class="calibre_8"><span class="bold">Failure tolerance</span></p></th></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">1</p></td><td valign="top" class="calibre_19"><p class="calibre_8">1</p></td><td valign="top" class="calibre_19"><p class="calibre_8">0</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">3</p></td><td valign="top" class="calibre_19"><p class="calibre_8">2</p></td><td valign="top" class="calibre_19"><p class="calibre_8">1</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">4</p></td><td valign="top" class="calibre_19"><p class="calibre_8">3</p></td><td valign="top" class="calibre_19"><p class="calibre_8">1</p></td></tr><tr valign="top" class="calibre_17"><td valign="top" class="calibre_19"><p class="calibre_8">5</p></td><td valign="top" class="calibre_19"><p class="calibre_8">3</p></td><td valign="top" class="calibre_19"><p class="calibre_8">2</p></td></tr></table><p class="calibre_8">The Majority count tells us the number of nodes that is necessary to have a working cluster, and failure tolerance tells us the number of nodes that can fail and still keep the cluster operational.</p><p id="filepos122665" class="calibre_9"><span class="bold">Etcd cluster details</span></p><p class="calibre_8">The following screenshot shows the Etcd member <a/>list in a 3 node CoreOS cluster:</p><p class="calibre_9"><img src="images/00242.jpg" class="calibre_39"/></p><p class="calibre_8">
</p><p class="calibre_8">We can see that there are three members that are part of the etcd cluster with their machine ID, machine name, IP address, and port numbers used for etcd server-to-server and client-to-server <a/>communication.</p><p class="calibre_8">The following output shows you the etcd cluster health:</p><p class="calibre_9"><img src="images/00245.jpg" class="calibre_40"/></p><p class="calibre_8">
</p><p class="calibre_8">Here, we can see that all three members of the etcd cluster are healthy.</p><p class="calibre_8">The following output shows you etcd statistics with the cluster leader:</p><p class="calibre_9"><img src="images/00248.jpg" class="calibre_41"/></p><p class="calibre_8">
</p><p class="calibre_8">We can see that the member ID matches with the leader ID, <tt class="calibre2">41419684c778c117</tt>.</p><p class="calibre_8">The following output shows you etcd statistics with the cluster member:</p><p class="calibre_9"><img src="images/00251.jpg" class="calibre_42"/></p><p class="calibre_8">
</p><p id="filepos124348" class="calibre_9"><span class="bold">Simple set and get operations using etcd</span></p><p class="calibre_8">In the following <a/>example, we will set the <tt class="calibre2">/message1</tt> key to the <tt class="calibre2">Book1</tt> value<a/> and then later retrieve the value of the <tt class="calibre2">/message1</tt> key:</p><p class="calibre_9"><img src="images/00255.jpg" class="calibre_43"/></p><p class="calibre_8">
</p><p id="filepos124817" class="calibre_9"><span class="calibre3"><span class="bold">Fleet</span></span></p><p class="calibre_8">Fleet is a cluster <a/>manager/scheduler that controls service creation at the cluster level. Like systemd being the init system for a node, Fleet serves as the init system for a cluster. Fleet uses etcd for internode communication.</p><p id="filepos125176" class="calibre_9"><span class="bold">The Fleet architecture</span></p><p class="calibre_8">The<a/> following image<a/> shows you the components of the Fleet architecture:</p><p class="calibre_9"><img src="images/00051.jpg" class="calibre_44"/></p><p class="calibre_8">
</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Fleet uses master, slave model with Fleet Engine playing master role and Fleet agent playing slave role. Fleet engine is responsible for scheduling Fleet units and Fleet agent is responsible for executing the units as well as reporting the status back to the Fleet engine.</li><li value="2" class="calibre_13">One master engine is elected among the CoreOS cluster using etcd.</li><li value="3" class="calibre_13">When the user starts a Fleet service, each agent bids for that service. Fleet uses a very simple <tt class="calibre2">least-loaded</tt> scheduling algorithm to schedule the unit to the appropriate node. Fleet units also consist of metadata that is useful to control where the unit runs with respect to the node property as well as based on other services running on that particular node.</li><a/><li value="4" class="calibre_13">The Fleet agent processes the unit and gives it to systemd for execution.</li><li value="5" class="calibre_13">If any node dies, a new Fleet engine is elected and the scheduled units in that node are rescheduled to a new node. Systemd provides HA at the node level; Fleet provides HA at the cluster level.</li></ul><p class="calibre_8">Considering that CoreOS and Google are working closely on the Kubernetes project, a common question that comes up is the role of Fleet if Kubernetes is going to do container orchestration. Fleet is typically used for the orchestration of critical system services using systemd while Kubernetes is used for application container orchestration. Kubernetes is composed of multiple services such as the kubelet server, API server, scheduler, and replication controller and they all run as Fleet units. For smaller deployments, Fleet can also be used for application orchestration.</p><p id="filepos127486" class="calibre_9"><span class="bold">A Fleet scheduling example</span></p><p class="calibre_8">The following is a<a/> three-node CoreOS cluster with some metadata<a/> present for each node:</p><p class="calibre_9"><img src="images/00054.jpg" class="calibre_45"/></p><p class="calibre_8">
</p><p id="filepos127870" class="calibre_9"><span class="calibre5"><span class="bold">A global unit example</span></span></p><p class="calibre_8">A global unit executes<a/> the same service unit on all the nodes in the cluster.</p><p class="calibre_8">The following is a sample <tt class="calibre2">helloglobal.service</tt>:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=My Service<br class="calibre4"/>After=docker.service<br class="calibre4"/>[Service]<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill hello<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm hello<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull busybox<br class="calibre4"/>ExecStart=/usr/bin/docker run --name hello busybox /bin/sh -c "while true; do echo Hello World; sleep 1; done"<br class="calibre4"/>ExecStop=/usr/bin/docker stop hello<br class="calibre4"/>[X-Fleet]<br class="calibre4"/>Global=true</tt></p><p class="calibre_8">Let's execute the<a/> unit as follows:</p><p class="calibre_9"><img src="images/00057.jpg" class="calibre_46"/></p><p class="calibre_8">
</p><p class="calibre_8">We can see that the same service is started on all three nodes:</p><p class="calibre_9"><img src="images/00059.jpg" class="calibre_47"/></p><p class="calibre_8">
</p><p id="filepos129245" class="calibre_9"><span class="calibre5"><span class="bold">Scheduling based on metadata</span></span></p><p class="calibre_8">Let's say that we have<a/> a three-node CoreOS cluster with the following metadata:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Node1 (compute=web, rack=rack1)</li><li value="2" class="calibre_13">Node2 (compute=web, rack=rack2)</li><li value="3" class="calibre_13">Node3 (compute=db, rack=rack3)</li></ul><p class="calibre_8">We have used the <tt class="calibre2">compute</tt> metadata to identity the type of machine as <tt class="calibre2">web</tt> or <tt class="calibre2">db</tt>. We have used the <tt class="calibre2">rack</tt> metadata to identify the rack number. Fleet metadata for a node can be specified in the Fleet section of the <tt class="calibre2">cloud-config</tt>.</p><p class="calibre_8">Let's start a web <a/>service and database service with each having its corresponding metadata and see where they get scheduled.</p><p class="calibre_8">This is the web service:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=Apache web server service<br class="calibre4"/>After=etcd.service<br class="calibre4"/>After=docker.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/>KillMode=none<br class="calibre4"/>EnvironmentFile=/etc/environment<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill nginx<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm nginx<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull nginx<br class="calibre4"/>ExecStart=/usr/bin/docker run --name nginx -p ${COREOS_PUBLIC_IPV4}:8080:80 nginx<br class="calibre4"/>ExecStop=/usr/bin/docker stop nginx<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>MachineMetadata=compute=web</tt></p><p class="calibre_8">This is the database service:</p><p class="calibre_8"><tt class="calibre2">[Unit]<br class="calibre4"/>Description=Redis DB service<br class="calibre4"/>After=etcd.service<br class="calibre4"/>After=docker.service<br class="calibre4"/><br class="calibre4"/>[Service]<br class="calibre4"/>TimeoutStartSec=0<br class="calibre4"/>KillMode=none<br class="calibre4"/>EnvironmentFile=/etc/environment<br class="calibre4"/>ExecStartPre=-/usr/bin/docker kill redis<br class="calibre4"/>ExecStartPre=-/usr/bin/docker rm redis<br class="calibre4"/>ExecStartPre=/usr/bin/docker pull redis<br class="calibre4"/>ExecStart=/usr/bin/docker run --name redis redis<br class="calibre4"/>ExecStop=/usr/bin/docker stop redis<br class="calibre4"/><br class="calibre4"/>[X-Fleet]<br class="calibre4"/>MachineMetadata=compute=db</tt></p><p class="calibre_8">Let's start the services using Fleet:</p><p class="calibre_9"><img src="images/00061.jpg" class="calibre_35"/></p><p class="calibre_8">
</p><p class="calibre_8">As we can<a/> see, <tt class="calibre2">nginxweb.service</tt> got started on <tt class="calibre2">Node1</tt> and <tt class="calibre2">nginxdb.service</tt> got started on <tt class="calibre2">Node3</tt>. This is because <tt class="calibre2">Node1</tt> and <tt class="calibre2">Node2</tt> were of the <tt class="calibre2">web</tt> type and <tt class="calibre2">Node3</tt> was of the <tt class="calibre2">db</tt> type.</p><p id="filepos132212" class="calibre_9"><span class="bold">Fleet HA</span></p><p class="calibre_8">When any of the nodes has<a/> an issue and does not respond, Fleet automatically takes care of scheduling the service units to the next appropriate machine.</p><p class="calibre_8">From the preceding example, let's reboot <tt class="calibre2">Node1</tt>, which has <tt class="calibre2">nginxweb.service</tt>. The service gets scheduled to <tt class="calibre2">Node2</tt> and not to <tt class="calibre2">Node3</tt> because Node2 has the <tt class="calibre2">web</tt> metadata:</p><p class="calibre_9"><img src="images/00064.jpg" class="calibre_48"/></p><p class="calibre_8">
</p><p class="calibre_8">In the preceding output, we can see that <tt class="calibre2">nginxweb.service</tt> is rescheduled to <tt class="calibre2">Node2</tt> and that <tt class="calibre2">Node1</tt> is not visible in the Fleet cluster.</p><p id="filepos133096" class="calibre_9"><span class="calibre3"><span class="bold">Flannel</span></span></p><p class="calibre_8">Flannel uses an Overlay <a/>network to allow Containers across different hosts to talk to each other. Flannel is not part of the base CoreOS image. This is<a/> done to keep the CoreOS image size minimal. When Flannel is started, the flannel container image is retrieved from the Container image repository. The Docker daemon is typically started after the Flannel service so that containers can get the IP address assigned by Flannel. This represents a chicken-and-egg problem as Docker is necessary to download<a/> the Flannel image. The CoreOS team has solved this problem by running a master Docker service whose only purpose is to download the Flannel container. </p><p class="calibre_8">The following image shows you how Flannel agents in each node communicate using <a/>
<span class="bold">etcd</span>:</p><p class="calibre_9"><img src="images/00069.jpg" class="calibre_49"/></p><p class="calibre_8">
</p><p class="calibre_8">The following are some <a/>Flannel internals:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Flannel runs without a central server and uses etcd for communication between the nodes.</li><li value="2" class="calibre_13">As part of starting Flannel, we need to supply a configuration file that contains the IP subnet to be used for the cluster as well as the backend protocol method (such as UDP and VXLAN). The following is a sample configuration that specifies the subnet range and backend protocol as UDP:<p class="calibre_"><img src="images/00071.jpg" class="calibre_50"/></p><p class="calibre_8">
</p></li><li value="3" class="calibre_13">Each node in the cluster requests an IP address range for containers created in that host and registers this IP range with etcd.</li><a/><li value="4" class="calibre_13">As every node in the cluster knows the IP address range allocated for every other node, it knows how to reach containers created on any node in the cluster.</li><li value="5" class="calibre_13">When containers are created, containers get an IP address in the range allocated to the node.</li><li value="6" class="calibre_13">When Containers need to talk across hosts, Flannel does the encapsulation based on the backend encapsulation protocol chosen. Flannel, in the destination node, de-encapsulates the packet and hands it over to the Container.</li><li value="7" class="calibre_13">By not using port-based mapping to talk across containers, Flannel simplifies Container-to-Container communication.</li></ul><p class="calibre_8">The following image shows the data path for Container-to-Container communication using Flannel:</p><p class="calibre_9"><img src="images/00075.jpg" class="calibre_51"/></p><p class="calibre_8">
</p><p id="filepos136312" class="calibre_9"><span class="bold">A Flannel service unit</span></p><p class="calibre_8">The following is an<a/> example of a flannel service unit where we set the IP range for the flannel network as <tt class="calibre2">10.1.0.0/16</tt>:</p><p class="calibre_9"><img src="images/00079.jpg" class="calibre_52"/></p><p class="calibre_8">
</p><p class="calibre_8">In a three-node etcd cluster, the following is a sample output that shows the Container IP address range picked by each node. Each node requests an IP range with a 24-bit mask. <tt class="calibre2">10.1.19.0/24</tt> is picked by node A, <tt class="calibre2">10.1.3.0/24</tt> is picked by node B, and <tt class="calibre2">10.1.62.0/24</tt> is picked by node C:</p><p class="calibre_9"><img src="images/00083.jpg" class="calibre_53"/></p><p class="calibre_8">
</p><p id="filepos137242" class="calibre_9"><span class="calibre3"><span class="bold">Rkt</span></span></p><p class="calibre_8">Rkt is the Container runtime<a/> developed by CoreOS. Rkt does not have a daemon and is managed by systemd. Rkt uses the <span class="bold">Application Container image</span> (<span class="bold">ACI</span>) image format, which is according to the APPC <a/>specification (<a href="https://github.com/appc/spec">https://github.com/appc/spec</a>). Rkt's execution is split into three stages. This approach was taken so that some of the stages can be replaced by a different implementation if needed. Following are details on the three stages of Rkt execution:</p><p class="calibre_8">Stage 0:</p><p class="calibre_8">This is the first stage of Container execution. This stage does image discovery, retrieval and sets up filesystem for stages 1 and 2.</p><p class="calibre_8">Stage 1:</p><p class="calibre_8">This stage sets up the execution environment for containers like Container namespace, cgroups using the filesystem setup by stage 0.</p><p class="calibre_8">Stage 2:</p><p class="calibre_8">This stage executes the Container using execution environment setup by stage 1 and filesystem setup by stage 0.</p><p class="calibre_8">As of release 0.10.0, Rkt is still under active development and is not ready for production.</p><div class="mbp_pagebreak" id="calibre_pb_37"/>


<p id="filepos138726" class="calibre_9"><span class="calibre3"><span class="bold">The CoreOS cluster architecture</span></span></p><p class="calibre_8">Nodes in the CoreOS <a/>cluster are used to run critical CoreOS services <a/>such as etcd, fleet, Docker, systemd, flannel, and journald as well as application containers. It is important to avoid using the same host to run critical services as well as application containers so that there is no resource contention for critical services. This kind of scheduling can be achieved using the Fleet metadata to separate the core machines and worker machines. The following are two cluster approaches.</p><p id="filepos139378" class="calibre_9"><span class="calibre3"><span class="bold">The development cluster</span></span></p><p class="calibre_8">The following<a/> image shows a development cluster with three CoreOS nodes:</p><p class="calibre_9"><img src="images/00088.jpg" class="calibre_54"/></p><p class="calibre_8">
</p><p class="calibre_8">To try out CoreOS and etcd, we can start with a single-node cluster. With this approach, there is no need to have dynamic discovery of cluster members. Once this works fine, we can expand the cluster size to three or five to achieve redundancy. The static or dynamic discovery approach can be used to discover CoreOS members. As CoreOS critical services and application containers run in the same cluster, there could be resource contention in this approach.</p><p id="filepos140263" class="calibre_9"><span class="calibre3"><span class="bold">The production cluster</span></span></p><p class="calibre_8">The following image<a/> shows a production cluster with a three-node master cluster and five-node worker cluster:</p><p class="calibre_9"><img src="images/00090.jpg" class="calibre_55"/></p><p class="calibre_8">
</p><p class="calibre_8">We can have a three or five-node master cluster to run critical CoreOS services and then have a dynamic worker cluster to run application Containers. The master cluster will run etcd, fleet, and <a/>other critical services. In worker nodes, etcd will be set up to proxy to master nodes so that worker nodes can use master nodes for etcd communication. Fleet, in worker nodes, will also be set up to use etcd in master nodes.</p><div class="mbp_pagebreak" id="calibre_pb_38"/>


<p id="filepos141169" class="calibre_"><span class="calibre1"><span class="bold">Docker versus Rkt</span></span></p><p class="calibre_8">As this is a <a/>controversial topic, I will try to give a neutral stand here.</p><div class="mbp_pagebreak" id="calibre_pb_39"/>


<p id="filepos141417" class="calibre_9"><span class="calibre3"><span class="bold">History</span></span></p><p class="calibre_8">CoreOS team started the Rkt project because of the following reasons:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Container interoperability issue needed to be addressed since Docker runtime was not fully following the Container manifest specification</li><li value="2" class="calibre_13">Getting Docker to run under systemd had some issues because of Docker running as the daemon</li><li value="3" class="calibre_13">Container image discovery and image signing required improvements</li><li value="4" class="calibre_13">Security model for Containers needed to be improved</li></ul><div class="mbp_pagebreak" id="calibre_pb_40"/>


<p id="filepos142273" class="calibre_14"><span class="calibre3"><span class="bold">APPC versus OCI</span></span></p><p class="calibre_8">APPC (<a href="https://github.com/appc/spec">https://github.com/appc/spec</a>) and <a/>OCI (<a href="https://github.com/opencontainers/specs">https://github.com/opencontainers/specs</a>) define Container standards.</p><p class="calibre_8">The APPC specification is <a/>primarily driven by CoreOS along with a few other community members. The APPC specification defines the following:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13"><span class="bold">Image format</span>: Packaging and signing</li><a/><li value="2" class="calibre_13"><span class="bold">Runtime</span>: How to execute the Container</li><li value="3" class="calibre_13"><span class="bold">Naming and Sharing</span>: Automatic discovery</li></ul><p class="calibre_8">APPC is implemented by Rkt, Kurma, Jetpack, and others.</p><p class="calibre_8">OCI (<a href="https://www.opencontainers.org/">https://www.opencontainers.org/</a>) is<a/> an open container initiative project started in April 2015 and has members from all major companies including Docker and CoreOS. Runc is an implementation of OCI. The following image shows you how APPC, OCI, Docker, and Rkt are related:</p><p class="calibre_9"><img src="images/00111.jpg" class="calibre_56"/></p><p class="calibre_8">
</p><div class="mbp_pagebreak" id="calibre_pb_41"/>


<p id="filepos143851" class="calibre_9"><span class="calibre3"><span class="bold">The current status</span></span></p><p class="calibre_8">Based on the latest developments, there is consensus among the community to having a common container specification called <a/>the Open Container Specification. Anyone can develop a Container runtime based on this specification. This will allow Container images to be interoperable. Docker, Rkt, and Odin are examples of Container runtime.</p><p class="calibre_8">The original APPC container specification proposed by CoreOS covers four different elements of <a/>container management—packaging, signing, naming (sharing the container with others), and runtime. As per<a/> the latest CoreOS blog update (<a href="https://coreos.com/blog/making-sense-of-standards.html">https://coreos.com/blog/making-sense-of-standards.html</a>), APPC and OCI will intersect only on runtime and APPC will continue to focus on image format, signing, and distribution. Runc is an implementation of OCI and Docker uses Runc.</p><div class="mbp_pagebreak" id="calibre_pb_42"/>


<p id="filepos144953" class="calibre_9"><span class="calibre3"><span class="bold">Differences between Docker and Rkt</span></span></p><p class="calibre_8">Following are some differences between Docker and Rkt Container runtimes:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Docker uses LibContainer APIs to access the Linux kernel Container functionality while Rkt uses the Systemd-nspawn API to access the Linux kernel Container functionality. The following image illustrates this:<p class="calibre_"><img src="images/00098.jpg" class="calibre_57"/></p><p class="calibre_8">
</p></li><a/><li value="2" class="calibre_13">Docker requires a daemon to manage Container images, remote APIs, and Container processes. Rkt is daemonless and Container resources are managed by systemd. This makes Rkt integrate better with init systems such as systemd and upstart.</li><li value="3" class="calibre_13">Docker has a complete platform to manage containers such as Machine, Compose, and Swarm. CoreOS will use some of its own tools such as Flannel for the Networking and combines it with tools such as Kubernetes for Orchestration.</li><li value="4" class="calibre_13">Docker is pretty mature and production-ready as compared to Rkt. As of the Rkt release 0.10.0, Rkt is not yet ready for production.</li><li value="5" class="calibre_13">For the Container image registry, Docker has the Docker hub and Rkt has Quay. Quay also has Docker images.</li></ul><p class="calibre_8">CoreOS is planning to support both Docker and Rkt and users will have a choice to use the corresponding Container runtime for their applications.</p><div class="mbp_pagebreak" id="calibre_pb_43"/>


<p id="filepos146802" class="calibre_"><span class="calibre1"><span class="bold">A workflow for distributed application development with Docker and CoreOS</span></span></p><p class="calibre_8">The following is a<a/> typical workflow to develop microservices using Docker and CoreOS:</p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">Select applications that need to be containerized. This could be greenfield or legacy applications. For legacy applications, reverse engineering might be required to split the monolithic application and containerize the individual components.</li><a/><li value="2" class="calibre_13">Create a Dockerfile for each microservice. The Dockerfile defines how to create the Container image from the base image. Dockerfile itself could be source-controlled.</li><li value="3" class="calibre_13">Split the stateless and stateful pieces of the application. For stateful applications, a storage strategy needs to be decided.</li><li value="4" class="calibre_13">Microservices need to talk to each other and some of the services should be reachable externally. Assuming that basic network connectivity between services is available, services can talk to each other either statically by defining a service name to IP address and port number mapping or by using service discovery where services can dynamically discover and talk to each other.</li><li value="5" class="calibre_13">Docker container images need to be stored in a private or public repository so that they can be shared among development, QA, and production teams.</li><li value="6" class="calibre_13">The application can be deployed in a private or public cloud. An appropriate infrastructure has to be selected based on the business need.</li><li value="7" class="calibre_13">Select the CoreOS cluster size and cluster architecture. It's better to make infrastructure dynamically scalable.</li><li value="8" class="calibre_13">Write CoreOS unit files for basic services such as etcd, fleet, and flannel.</li><li value="9" class="calibre_13">Finalize a storage strategy—local versus distributed versus cloud.</li><li value="10" class="calibre_13">For orchestration of smaller applications, fleet can be used. For complex applications, the Kubernetes kind of Orchestration solution will be necessary.</li><li value="11" class="calibre_13">For production clusters, appropriate monitoring, logging, and upgrading strategies also need to be worked out.</li></ul><div class="mbp_pagebreak" id="calibre_pb_44"/>


<p id="filepos149512" class="calibre_"><span class="calibre1"><span class="bold">Summary</span></span></p><p class="calibre_8">In this chapter, we covered the basics of CoreOS, Containers, and Docker and how they help in distributed application development and deployment. These technologies are under active development and will revolutionize and create a new software development and distribution model. We will explore each individual topic in detail in the following chapters. In the next chapter, we will cover how to set up the CoreOS development environment in Vagrant as well as in a public cloud.</p><div class="mbp_pagebreak" id="calibre_pb_45"/>


<p id="filepos150132" class="calibre_"><span class="calibre1"><span class="bold">References</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">APPC specification: <a href="https://github.com/appc/spec/blob/master/SPEC.md">https://github.com/appc/spec/blob/master/SPEC.md</a></li><a/><li value="2" class="calibre_13">OCI specification: <a href="https://github.com/opencontainers/specs">https://github.com/opencontainers/specs</a></li><a/><li value="3" class="calibre_13">CoreOS documentation: <a href="https://coreos.com/docs/">https://coreos.com/docs/</a></li><a/><li value="4" class="calibre_13">Docker documentation: <a href="https://docs.docker.com/">https://docs.docker.com/</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_46"/>


<p id="filepos150959" class="calibre_"><span class="calibre1"><span class="bold">Further reading and tutorials</span></span></p><div class="calibre_11"> </div><ul class="calibre_12"><li value="1" class="calibre_13">A blog on the minimalist operating system: <a href="https://blog.docker.com/2015/02/the-new-minimalist-operating-systems/">https://blog.docker.com/2015/02/the-new-minimalist-operating-systems/</a> and <a href="https://blog.codeship.com/container-os-comparison/">https://blog.codeship.com/container-os-comparison/</a></li><a/><li value="2" class="calibre_13">Container basics: <a href="http://www.slideshare.net/jpetazzo/anatomy-of-a-container-namespaces-cgroups-some-filesystem-magic-linuxcon">http://www.slideshare.net/jpetazzo/anatomy-of-a-container-namespaces-cgroups-some-filesystem-magic-linuxcon</a></li><a/><li value="3" class="calibre_13">An introduction to Docker: <a href="https://www.youtube.com/watch?v=Q5POuMHxW-0">https://www.youtube.com/watch?v=Q5POuMHxW-0</a></li><a/><li value="4" class="calibre_13">Mesos overview: <a href="https://www.youtube.com/watch?v=gVGZHzRjvo0">https://www.youtube.com/watch?v=gVGZHzRjvo0</a></li><a/><li value="5" class="calibre_13">The CoreOS presentation: <a href="http://www.slideshare.net/RichardLister/core-os">http://www.slideshare.net/RichardLister/core-os</a></li><a/><li value="6" class="calibre_13">DigitalOcean CoreOS tutorials: <a href="https://www.digitalocean.com/community/tags/coreos?type=tutorials">https://www.digitalocean.com/community/tags/coreos?type=tutorials</a></li><a/><li value="7" class="calibre_13">Microservices' characteristics: <a href="http://martinfowler.com/articles/microservices.html">http://martinfowler.com/articles/microservices.html</a></li><a/><li value="8" class="calibre_13">The Docker daemon issue: <a href="http://www.ibuildthecloud.com/blog/2014/12/03/is-docker-fundamentally-flawed/">http://www.ibuildthecloud.com/blog/2014/12/03/is-docker-fundamentally-flawed/</a> and <a href="https://github.com/ibuildthecloud/systemd-docker">https://github.com/ibuildthecloud/systemd-docker</a></li><a/></ul><div class="mbp_pagebreak" id="calibre_pb_47"/>
</body></html>