<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Mesos Big Data Frameworks 2"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Mesos Big Data Frameworks 2</h1></div></div></div><p>This chapter is a guide to deploying important big data storage frameworks, such as Cassandra, the Elasticsearch-Logstash-Kibana (ELK) stack, and Kafka, on Mesos.</p><div class="section" title="Cassandra on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Cassandra on Mesos</h1></div></div></div><p>This section <a class="indexterm" id="id882"/>will introduce Cassandra and explain how to set up Cassandra on Mesos while also discussing the problems commonly encountered during the setup process.</p><div class="section" title="Introduction to Cassandra"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec139"/>Introduction to Cassandra</h2></div></div></div><p>
<span class="strong"><strong>Cassandra</strong></span> is<a class="indexterm" id="id883"/> an open source, scalable NoSQL database that is fully distributed with no single point of failure and is highly performant for most standard use cases. It is both <a class="indexterm" id="id884"/>horizontally as well as vertically scalable. <span class="strong"><strong>Horizontal scalability</strong></span> or <span class="strong"><strong>scale-out solution</strong></span><a class="indexterm" id="id885"/> involves adding more<a class="indexterm" id="id886"/> nodes with commodity hardware to the existing cluster while <span class="strong"><strong>vertical scalability</strong></span> or <span class="strong"><strong>scale-up solution</strong></span> <a class="indexterm" id="id887"/>means adding more CPU and memory resources to a node with specialized hardware.</p><p>Cassandra was developed by Facebook engineers to address the inbox search use case and was inspired by Google Bigtable, which served as the foundation for its storage model, and Amazon DynamoDB, which was the foundation of its distribution model. It was open sourced in 2008 and became an Apache top-level project in early 2010. It provides a query language called <a class="indexterm" id="id888"/>
<span class="strong"><strong>Cassandra Query Language</strong></span> or <span class="strong"><strong>CQL</strong></span>, which has a SQL-like syntax, to communicate with the database.</p><p>Cassandra provides various <a class="indexterm" id="id889"/>capabilities, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">High performance</li><li class="listitem" style="list-style-type: disc">Continuous uptime (no single point of failure)</li><li class="listitem" style="list-style-type: disc">Ease of use</li><li class="listitem" style="list-style-type: disc">Data replication and distribution across datacenters</li></ul></div><p>Instead of using a traditional master-slave or sharded design, Cassandra's architecture uses an elegant and simple <span class="strong"><strong>ring design</strong></span><a class="indexterm" id="id890"/> without any masters. This allows it to provide all the features and benefits listed earlier.</p><p>The Cassandra Ring Design diagram<a class="indexterm" id="id891"/> is shown as follows (source: <a class="ulink" href="http://www.planetcassandra.org">www.planetcassandra.org</a>):</p><div class="mediaobject"><img alt="Introduction to Cassandra" src="graphics/B05186_09_00.jpg"/></div><p>A large number of companies use Cassandra in production, including Apple, Instagram, eBay, Spotify, Comcast, and Netflix among others.</p><p>Cassandra is best used when you need:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">No single failure point</li><li class="listitem" style="list-style-type: disc">Real-time writes</li><li class="listitem" style="list-style-type: disc">Flexibility</li><li class="listitem" style="list-style-type: disc">Horizontal scaling</li><li class="listitem" style="list-style-type: disc">Reliability</li><li class="listitem" style="list-style-type: disc">A clearly defined table schema in a NoSQL environment</li></ul></div><p>Some of the common use cases<a class="indexterm" id="id892"/> are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storing, managing, and performing analysis on data generated by messaging applications (Instagram and Comcast, among others, use Cassandra for this purpose)</li><li class="listitem" style="list-style-type: disc">Storing data patterns for the detection of fraudulent activity</li><li class="listitem" style="list-style-type: disc">Storing<a class="indexterm" id="id893"/> user-selected and curated items (shopping cart, playlists, and so on)</li><li class="listitem" style="list-style-type: disc">Recommendation and personalization</li></ul></div><p>
<span class="strong"><strong>Performance benchmark</strong></span>
</p><p>The<a class="indexterm" id="id894"/> following performance benchmark conducted by an independent database firm showed that for mixed operational and analytical workloads, Cassandra was far superior to other open source NoSQL technologies (source: <a class="ulink" href="http://www.datastax.com">www.datastax.com</a>):</p><div class="mediaobject"><img alt="Introduction to Cassandra" src="graphics/B05186_09_01.jpg"/></div></div><div class="section" title="Setting up Cassandra on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec140"/>Setting up Cassandra on Mesos</h2></div></div></div><p>This section<a class="indexterm" id="id895"/> covers the process of deploying Cassandra on top of Mesos. The recommended way of deploying Cassandra on Mesos is through Marathon. At the time of writing this book, Cassandra on Mesos is in an experimental stage, and the configuration described here might change in future releases.</p><p>The Mesosphere team has already packaged the necessary JAR files and the Cassandra executor in a<a class="indexterm" id="id896"/> tarball that can be directly submitted to Mesos through Marathon with the following JSON code:</p><div class="informalexample"><pre class="programlisting">  {
    "healthChecks": [
      {
        "timeoutSeconds": 5,
        "protocol": "HTTP",
        "portIndex": 0,
        "path": "/health/cluster",
        "maxConsecutiveFailures": 0,
        "intervalSeconds": 30,
        "gracePeriodSeconds": 120
      },
      {
        "timeoutSeconds": 5,
        "protocol": "HTTP",
        "portIndex": 0,
        "path": "/health/process",
        "maxConsecutiveFailures": 3,
        "intervalSeconds": 30,
        "gracePeriodSeconds": 120
      }
    ],
    "id": "/cassandra/dev-test",
    "instances": 1,
    "cpus": 0.5,
    "mem": 512,
    "ports": [0],
    "uris": [
      "https://downloads.mesosphere.io/cassandra-mesos/artifacts/0.2.1-SNAPSHOT-608-master-d1c2cf30c8/cassandra-mesos-0.2.1-SNAPSHOT-608-master-d1c2cf30c8.tar.gz",
      "https://downloads.mesosphere.io/java/jre-7u76-linux-x64.tar.gz"
    ],
    "env": {
      "CASSANDRA_ZK_TIMEOUT_MS": "10000",
      "CASSANDRA_HEALTH_CHECK_INTERVAL_SECONDS": "60",
      "MESOS_ZK": "zk://localhost:2181/mesos",
      "JAVA_OPTS": "-Xms256m -Xmx256m",
      "CASSANDRA_CLUSTER_NAME": "dev-test",
      "CASSANDRA_ZK": "zk://localhost:2181/cassandra-mesos",
      "CASSANDRA_NODE_COUNT": "3",
      "CASSANDRA_RESOURCE_CPU_CORES": "2.0",
      "CASSANDRA_RESOURCE_MEM_MB": "2048",
      "CASSANDRA_RESOURCE_DISK_MB": "2048"
    },
    "cmd": "$(pwd)/jre*/bin/java $JAVA_OPTS -classpath cassandra-mesos-framework.jar io.mesosphere.mesos.frameworks.cassandra.framework.Main"
  }</pre></div><p>Edit the<a class="indexterm" id="id897"/> JSON code by pointing <code class="literal">MESOS_ZK</code> and any other parameters that you need to change accordingly, save this JSON code in <code class="literal">cassandra-mesos.json</code>, and then submit it to Marathon with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -X POST -H "Content-Type: application/json" -d cassandra-mesos.json http://marathon-machine:8080/v2/apps</strong></span>
</pre></div><p>Once submitted, the framework will bootstrap itself. We also need to expand the port ranges managed by each Mesos node to include the standard Cassandra ports. We can pass the port ranges as resources when starting the process.</p><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">--resources='ports:[31000-32000,7000-7001,7199-7199,9042-9042,9160-9160]'</pre></div><p>Cassandra on Mesos provides a REST endpoint to tune the setup. We can access this endpoint on port <code class="literal">18080</code> by default (unless changed).</p></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec141"/>An advanced configuration guide</h2></div></div></div><p>As <a class="indexterm" id="id898"/>mentioned previously, Cassandra on Mesos takes in runtime configuration through environment variables. We can use the following environment variables to bootstrap the configuration of the framework. After the initial run, the configurations are read from the framework state stored in ZooKeeper:</p><div class="informalexample"><pre class="programlisting"># name of the cassandra cluster, this will be part of the framework name in Mesos
CASSANDRA_CLUSTER_NAME=dev-cluster

# Mesos ZooKeeper URL to locate leading master
MESOS_ZK=zk://localhost:2181/mesos

# ZooKeeper URL to be used to store framework state
CASSANDRA_ZK=zk://localhost:2181/cassandra-mesos

# The number of nodes in the cluster (default 3)
CASSANDRA_NODE_COUNT=3

# The number of seed nodes in the cluster (default 2)
# set this to 1, if you only want to spawn one node
CASSANDRA_SEED_COUNT=2

# The number of CPU Cores for each Cassandra Node (default 2.0)
CASSANDRA_RESOURCE_CPU_CORES=2.0

# The number of Megabytes of RAM for each Cassandra Node (default 2048)
CASSANDRA_RESOURCE_MEM_MB=2048

# The number of Megabytes of Disk for each Cassandra Node (default 2048)
CASSANDRA_RESOURCE_DISK_MB=2048

# The number of seconds between each health check of the Cassandra node (default 60)
CASSANDRA_HEALTH_CHECK_INTERVAL_SECONDS=60

# The default bootstrap grace time - the minimum interval between two node starts
# You may set this to a lower value in pure local development environments.
CASSANDRA_BOOTSTRAP_GRACE_TIME_SECONDS=120

# The number of seconds that should be used as the mesos framework timeout (default 604800 seconds / 7 days)
CASSANDRA_FAILOVER_TIMEOUT_SECONDS=604800

# The mesos role to used to reserve resources (default *). If this is set, the framework accepts offers that have resources for that role or the default role *
CASSANDRA_FRAMEWORK_MESOS_ROLE=*

# A pre-defined data directory specifying where Cassandra should write its data. 
# Ensure that this directory can be created by the user the framework is running as (default. [mesos sandbox]).
# NOTE:
# This field is slated to be removed and the framework will be able to allocate the data volume itself.
CASSANDRA_DATA_DIRECTORY=.</pre></div><p>Here are some references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/mesosphere/cassandra-mesos">https://github.com/mesosphere/cassandra-mesos</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://mesosphere.github.io/cassandra-mesos/">http://mesosphere.github.io/cassandra-mesos/</a></li></ul></div></div></div></div>
<div class="section" title="The Elasticsearch-Logstash-Kibana (ELK) stack on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec77"/>The Elasticsearch-Logstash-Kibana (ELK) stack on Mesos</h1></div></div></div><p>This section<a class="indexterm" id="id899"/> will introduce the <a class="indexterm" id="id900"/>
<span class="strong"><strong>Elasticsearch-Logstash-Kibana</strong></span> (<span class="strong"><strong>ELK</strong></span>) stack and explain how to set it up on Mesos while also discussing the problems commonly encountered during the setup process.</p><div class="section" title="Introduction to Elasticsearch, Logstash, and Kibana"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec142"/>Introduction to Elasticsearch, Logstash, and Kibana</h2></div></div></div><p>The ELK stack, a combination of <span class="strong"><strong>Elasticsearch</strong></span>, <span class="strong"><strong>Logstash</strong></span>, and <span class="strong"><strong>Kibana</strong></span>, is an end-to-end solution for <a class="indexterm" id="id901"/>
<span class="strong"><strong>log analytics</strong></span>. Elasticsearch provides search capabilities, Logstash is a log management software, while Kibana serves as the visualization layer. The stack is commercially backed by a company <a class="indexterm" id="id902"/>called <span class="strong"><strong>Elastic</strong></span>.</p><div class="section" title="Elasticsearch"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec94"/>Elasticsearch</h3></div></div></div><p>Elasticsearch<a class="indexterm" id="id903"/> is a Lucene-based <a class="indexterm" id="id904"/>open source distributed search engine designed for high scalability and fast search query response time. It simplifies the usage of Lucene, a highly performant search engine library, by providing a powerful REST API on top. Some of the important concepts in Elasticsearch are highlighted as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Document</strong></span>: This <a class="indexterm" id="id905"/>is a JSON object stored in an index</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Index</strong></span>: This<a class="indexterm" id="id906"/> is a document collection</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Type</strong></span>: This<a class="indexterm" id="id907"/> is a logical partition of an index representing a category of documents</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Field</strong></span>: This <a class="indexterm" id="id908"/>is a key-value pair within a document</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mapping</strong></span>: This is <a class="indexterm" id="id909"/>used to map every field with its datatype</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Shard</strong></span>: This is <a class="indexterm" id="id910"/>the physical location where an index's data is stored (the data is stored on one primary shard and copied on a set of replica shards)</li></ul></div></div><div class="section" title="Logstash"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec95"/>Logstash</h3></div></div></div><p>This is a tool <a class="indexterm" id="id911"/>to collect and process <a class="indexterm" id="id912"/>the log events generated by a wide variety of systems. It includes a rich set of input and output connectors to ingest the logs and make them available for analysis. Some of its important features<a class="indexterm" id="id913"/> are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The ability to convert logs to a common format for the ease of use</li><li class="listitem" style="list-style-type: disc">The ability to process multiple log formats, including custom ones</li><li class="listitem" style="list-style-type: disc">A<a class="indexterm" id="id914"/> rich set of input and output connectors</li></ul></div></div><div class="section" title="Kibana"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec96"/>Kibana</h3></div></div></div><p>This is <a class="indexterm" id="id915"/>an Elasticsearch-based data <a class="indexterm" id="id916"/>visualization tool with a wide variety of charting and dashboarding capabilities. It is powered by the data stored in the Elasticsearch indexes and is entirely developed using HTML and JavaScript. Some of <a class="indexterm" id="id917"/>its most important features are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A graphical user interface for dashboard construction</li><li class="listitem" style="list-style-type: disc">A rich set of charts (map, pie charts, histograms, and so on)</li><li class="listitem" style="list-style-type: disc">The ability to embed charts in user applications</li></ul></div></div><div class="section" title="The ELK stack data pipeline"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec97"/>The ELK stack data pipeline</h3></div></div></div><p>Take<a class="indexterm" id="id918"/> a look at the following diagram (source: <span class="emphasis"><em>Learning ELK Stack</em></span> by Packt Publishing):</p><div class="mediaobject"><img alt="The ELK stack data pipeline" src="graphics/B05186_09_02.jpg"/></div><p>In a standard ELK stack pipeline, logs from various application servers are transported through Logstash to a central indexer module. This indexer then transmits the output to an Elasticsearch <a class="indexterm" id="id919"/>cluster, where it can be queried directly or visualized in a dashboard by leveraging Kibana.</p></div></div><div class="section" title="Setting up Elasticsearch-Logstash-Kibana on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec143"/>Setting up Elasticsearch-Logstash-Kibana on Mesos</h2></div></div></div><p>This <a class="indexterm" id="id920"/>section explains how to set up Elasticsearch, Logstash, and Kibana on top of Mesos. We will first take a look at how to set up Elasticsearch on top of Mesos followed by Logstash and Kibana.</p><div class="section" title="Elasticsearch on Mesos"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec98"/>Elasticsearch on Mesos</h3></div></div></div><p>We will use <a class="indexterm" id="id921"/>Marathon to deploy Elasticsearch, and this can be done in two ways: through the Docker image, which is highly recommended, and through <code class="literal">elasticsearch-mesos jar</code>. Both are explained in the following section.</p><p>We can use the following Marathon file to deploy Elasticsearch on top of Mesos. It uses the Docker image:</p><div class="informalexample"><pre class="programlisting">{
  "id": "elasticsearch-mesos-scheduler",
  "container": {
    "docker": {
      "image": "mesos/elasticsearch-scheduler",
      "network": "HOST"
    }
},
"args": ["--zookeeperMesosUrl", "zk://zookeeper-node:2181/mesos"],
  "cpus": 0.2,
  "mem": 512.0,
  "env": {
    "JAVA_OPTS": "-Xms128m -Xmx256m"
  },
  "instances": 1
}</pre></div><p>Ensure that <code class="literal">zookeeper-node</code> is changed to the address of the ZooKeeper node that you have on the cluster. We can save this to an <code class="literal">elasticsearch.json</code> file and then deploy it on Marathon with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -k -XPOST -d @elasticsearch.json -H "Content-Type: application/json" http://marathon-machine:8080/v2/apps</strong></span>
</pre></div><p>As mentioned before, we can also use the JAR file to deploy Elasticsearch on top of Mesos with the following Marathon file:</p><div class="informalexample"><pre class="programlisting">{
  "id": "elasticsearch",
  "cpus": 0.2,
  "mem": 512,
  "instances": 1,
  "cmd": "java -jar scheduler-0.7.0.jar --frameworkUseDocker false --zookeeperMesosUrl zk://10.0.0.254:2181 --frameworkName elasticsearch --elasticsearchClusterName mesos-elasticsearch --elasticsearchCpu 1 --elasticsearchRam 1024 --elasticsearchDisk 1024 --elasticsearchNodes 3 --elasticsearchSettingsLocation /home/ubuntu/elasticsearch.yml",
  "uris": ["https://github.com/mesos/elasticsearch/releases/download/0.7.0/scheduler-0.7.0.jar"],
  "env": {
    "JAVA_OPTS": "-Xms256m -Xmx512m"
  },
  "ports": [31100],
  "requirePorts": true,
  "healthChecks": [
    {
     "gracePeriodSeconds": 120,
      "intervalSeconds": 10,
      "maxConsecutiveFailures": 6,
      "path": "/",
      "portIndex": 0,
      "protocol": "HTTP",
      "timeoutSeconds": 5
    }
  ]
}</pre></div><p>In both<a class="indexterm" id="id922"/> cases, the <code class="literal">JAVA_OPTS</code> environment variable is required, and if it's not set, it will cause problems with the Java heap space. We can save this as <code class="literal">elasticsearch.json</code> and submit it to Marathon with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -k -XPOST -d @elasticsearch.json -H "Content-Type: application/json" http://MARATHON_IP_ADDRESS:8080/v2/apps</strong></span>
</pre></div><p>Both Docker image and the JAR file take in the following command-line arguments, similar to the <code class="literal">--zookeeperMesosUrl</code> argument:</p><div class="informalexample"><pre class="programlisting">    --dataDir
         The host data directory used by Docker volumes in the executors. [DOCKER MODE ONLY]
         Default: /var/lib/mesos/slave/elasticsearch

    --elasticsearchClusterName
         Name of the Elasticsearch cluster
         Default: mesos-ha

    --elasticsearchCpu
         The amount of CPU resource to allocate to the Elasticsearch instance.
         Default: 1.0

    --elasticsearchDisk
         The amount of Disk resource to allocate to the Elasticsearch instance
         (MB).
         Default: 1024.0

    --elasticsearchExecutorCpu
         The amount of CPU resource to allocate to the Elasticsearch executor.
         Default: 0.1
    
    --elasticsearchExecutorRam
         The amount of ram resource to allocate to the Elasticsearch executor
         (MB).
         Default: 32.0

    --elasticsearchNodes
         Number of Elasticsearch instances.
         Default: 3

    --elasticsearchPorts
         User specified Elasticsearch HTTP and transport ports. [NOT RECOMMENDED]
         Default: &lt;empty string&gt;

    --elasticsearchRam
         The amount of ram resource to allocate to the Elasticsearch instance
         (MB).
         Default: 256.0

    --elasticsearchSettingsLocation
         Path or URL to Elasticsearch yml settings file. [In docker mode file must be in /tmp/config] E.g. '/tmp/config/elasticsearch.yml' or 'https://gist.githubusercontent.com/mmaloney/5e1da5daa58b70a3a671/raw/elasticsearch.yml'
         Default: &lt;empty string&gt;

    --executorForcePullImage
         Option to force pull the executor image. [DOCKER MODE ONLY]
         Default: false

    --executorImage
         The docker executor image to use. E.g. 'elasticsearch:latest' [DOCKER
         MODE ONLY]
         Default: elasticsearch:latest

    --executorName
         The name given to the executor task.
         Default: elasticsearch-executor

    --frameworkFailoverTimeout
         The time before Mesos kills a scheduler and tasks if it has not recovered
         (ms).
         Default: 2592000.0

    --frameworkName
         The name given to the framework.
         Default: elasticsearch

    --frameworkPrincipal
         The principal to use when registering the framework (username).
         Default: &lt;empty string&gt;

    --frameworkRole
         Used to group frameworks for allocation decisions, depending on the
         allocation policy being used.
         Default: *

    --frameworkSecretPath
         The path to the file which contains the secret for the principal
         (password). Password in file must not have a newline.
         Default: &lt;empty string&gt;

    --frameworkUseDocker
         The framework will use docker if true, or jar files if false. If false, the user must ensure that the scheduler jar is available to all slaves.
         Default: true

    --javaHome
         When starting in jar mode, if java is not on the path, you can specify
         the path here. [JAR MODE ONLY]
         Default: &lt;empty string&gt;

    --useIpAddress
         If true, the framework will resolve the local ip address. If false, it
         uses the hostname.
         Default: false

    --webUiPort
         TCP port for web ui interface.
         Default: 31100

    --zookeeperMesosTimeout
         The timeout for connecting to zookeeper for Mesos (ms).
         Default: 20000

    * --zookeeperMesosUrl
         Zookeeper urls for Mesos in the format zk://IP:PORT,IP:PORT,...)
         Default: zk://mesos.master:2181</pre></div></div><div class="section" title="Logstash on Mesos"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec99"/>Logstash on Mesos</h3></div></div></div><p>This section<a class="indexterm" id="id923"/> explains how to run Logstash on top of Mesos. Once Logstash is deployed on the cluster, any program that runs on Mesos can log an event that is then passed by Logstash and sent to a central log location.</p><p>We can run Logstash as a Marathon application and deploy it on top of Mesos with the following Marathon file:</p><div class="informalexample"><pre class="programlisting">{
  "id": "/logstash",
  "cpus": 1,
  "mem": 1024.0,
  "instances": 1,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "mesos/logstash-scheduler:0.0.6",
      "network": "HOST"
    }
  },
  "env": {
    "ZK_URL": "zk://123.0.0.12:5181/logstash",
    "ZK_TIMEOUT": "20000",
    "FRAMEWORK_NAME": "logstash",
    "FAILOVER_TIMEOUT": "60",
    "MESOS_ROLE": "logstash",
    "MESOS_USER": "root",
    "LOGSTASH_HEAP_SIZE": "64",
    "LOGSTASH_ELASTICSEARCH_URL": "http://elasticsearch.service.consul:1234",
    "EXECUTOR_CPUS": "0.5",
    "EXECUTOR_HEAP_SIZE": "128",
    "ENABLE_FAILOVER": "false",
    "ENABLE_COLLECTD": "true",
    "ENABLE_SYSLOG": "true",
    "ENABLE_FILE": "true",
    "ENABLE_DOCKER": "true",
    "EXECUTOR_FILE_PATH": "/var/log/*,/home/jhf/example.log"
  }
}</pre></div><p>Here, we <a class="indexterm" id="id924"/>used the Docker image for deployment, the configurations of which can be changed according to your cluster specification. Save the preceding file as <code class="literal">logstash.json</code> and submit it to Marathon with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -k -XPOST -d @logstash.json -H "Content-Type: application/json" http://MARATHON_IP_ADDRESS:8080/v2/apps</strong></span>
</pre></div><div class="section" title="Logstash on Mesos configurations"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec14"/>Logstash on Mesos configurations</h4></div></div></div><p>Logstash <a class="indexterm" id="id925"/>and Elasticsearch are tested with the Mesos version 0.25.0 and later. We need to add Logstash to the list of roles on every Mesos master machine. This can be done with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo echo logstash &gt; /etc/mesos-master/roles</strong></span>
</pre></div><p>If the purpose of Logstash is to monitor <code class="literal">syslog</code> (a message logging standard), then we need to add the TCP and UDP port <code class="literal">514</code> to the resources list in every Mesos node in the cluster. This can be done by adding the following entry in the <code class="literal">/etc/mesos-slave/resources</code> file:</p><div class="informalexample"><pre class="programlisting">ports(logstash):[514-514]</pre></div><p>To<a class="indexterm" id="id926"/> monitor <code class="literal">collectd</code>, we need to add the TCP and UDP port <code class="literal">25826</code> to the resources for the Logstash role by adding the following line to the <code class="literal">/etc/mesos-slave/resources</code> file:</p><div class="informalexample"><pre class="programlisting">ports(logstash):[25826-25826]</pre></div></div></div><div class="section" title="Kibana on Mesos"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec100"/>Kibana on Mesos</h3></div></div></div><p>If we run<a class="indexterm" id="id927"/> Kibana on Mesos, then each instance of Kibana will run as a Docker image in the Mesos cluster. For each instance of Elasticsearch, one or more instances of Kibana can be deployed to serve the users.</p><p>We can clone Kibana on the Mesos project from the following repository:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/mesos/kibana</strong></span>
</pre></div><p>Build the project with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd kibana</strong></span>
<span class="strong"><strong>$ gradlew jar</strong></span>
</pre></div><p>This will generate the Kibana JAR file (<code class="literal">kibana.jar</code>).</p><p>Once <code class="literal">kibana.jar</code> is generated, we can deploy it with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ java -jar /path/to/kibana.jar -zk zk://zookeeper:2181/mesos -v 4.3.1 -es http://es-host:9200</strong></span>
</pre></div><p>Here, <code class="literal">-zk</code> represents the ZooKeeper URI and the <code class="literal">-es</code> points to the Elasticsearch endpoint, which we deployed in the previous section. Set them accordingly.</p><p>The following command-line options are also supported by the <code class="literal">kibana.jar</code> file:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Short keyword</p>
</th><th style="text-align: left" valign="bottom">
<p>Keyword</p>
</th><th style="text-align: left" valign="bottom">
<p>Definition</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-zk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>-<code class="literal">zookeeper</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the Mesos ZooKeeper URL (Required)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-di</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-dockerimage</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the name of the Docker image to be used (The default is <code class="literal">kibana</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-v</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-version</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the version of the Kibana Docker image to be used (The default is <code class="literal">latest</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-mem</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-requiredMem</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the amount of memory (in MB) to be allocated to a single Kibana instance (The default is <code class="literal">128</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-cpu</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-requiredCpu</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the amount of CPUs to allocate to a single Kibana instance (The default is <code class="literal">0.1</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-disk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-requiredDisk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the amount of disk space (in MB) to be allocated to a single Kibana instance (The default is <code class="literal">25</code>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">-es</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">-elasticsearch</code>
</p>
</td><td style="text-align: left" valign="top">
<p>These are the URLs of Elasticsearch to start a Kibana for at startup</p>
</td></tr></tbody></table></div><p>Here are some references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://mesos-elasticsearch.readthedocs.org/en/latest/#elasticsearch-mesos-framework">http://mesos-elasticsearch.readthedocs.org/en/latest/#elasticsearch-mesos-framework</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/mesos/logstash">https://github.com/mesos/logstash</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/mesos/kibana">https://github.com/mesos/kibana</a></li></ul></div></div></div></div>
<div class="section" title="Kafka on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec78"/>Kafka on Mesos</h1></div></div></div><p>This section <a class="indexterm" id="id928"/>will introduce Kafka and explain how to set it up on Mesos while also discussing the problems commonly encountered during the setup process.</p><div class="section" title="Introduction to Kafka"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec144"/>Introduction to Kafka</h2></div></div></div><p>Kafka is a<a class="indexterm" id="id929"/> distributed publish-subscribe messaging system designed for speed, scalability, reliability, and durability. Some of the key terms<a class="indexterm" id="id930"/> used in Kafka are given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Topics</strong></span>: These<a class="indexterm" id="id931"/> are the categories where message feeds are maintained by Kafka</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Producers</strong></span>: These<a class="indexterm" id="id932"/> are the upstream processes that send messages to a particular Kafka topic</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Consumers</strong></span>: These<a class="indexterm" id="id933"/> are the downstream processes that listen to the incoming messages in a topic and process them as per requirements</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Broker</strong></span>: Each<a class="indexterm" id="id934"/> node in a Kafka cluster is called a broker</li></ul></div><p>Take a look at the following high-level diagram of Kafka (source: <a class="ulink" href="http://kafka.apache.org/documentation.html#introduction">http://kafka.apache.org/documentation.html#introduction</a>):</p><div class="mediaobject"><img alt="Introduction to Kafka" src="graphics/B05186_09_03.jpg"/></div><p>A partitioned log is maintained by the Kafka cluster for every topic, which looks similar to the following (source: <a class="ulink" href="http://kafka.apache.org/documentation.html#intro_topics">http://kafka.apache.org/documentation.html#intro_topics</a>):</p><div class="mediaobject"><img alt="Introduction to Kafka" src="graphics/B05186_09_04.jpg"/></div></div><div class="section" title="Use cases of Kafka"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec145"/>Use cases of Kafka</h2></div></div></div><p>Some<a class="indexterm" id="id935"/> important uses of Kafka are described here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Website activity tracking</strong></span>: Site<a class="indexterm" id="id936"/> activity events, such as page views and user searches, can be sent by the web application to Kafka topics. Downstream processing systems can then subscribe to these topics and consume the messages for batch analytics, monitoring, real-time dashboarding, and other such use cases.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Log aggregation</strong></span>: Kafka is <a class="indexterm" id="id937"/>used as an alternative to traditional log aggregation systems. Physical log files can be collected from various services and pushed to different Kafka topics, where different consumers can read and process them. File details are abstracted by Kafka, which enables faster processing and support for a variety of data sources.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Stream processing</strong></span>: Frameworks, for<a class="indexterm" id="id938"/> instance Spark Streaming, can consume data from a Kafka topic, process it as per requirements, and then publish the processed output to a different Kafka topic, where this output can, in turn, be consumed by other applications.</li></ul></div></div><div class="section" title="Setting up Kafka"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec146"/>Setting up Kafka</h2></div></div></div><p>Before<a class="indexterm" id="id939"/> installing Kafka on Mesos, make sure the following applications are available<a class="indexterm" id="id940"/> on the machine:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Java version 7 or later (<a class="ulink" href="http://openjdk.java.net/install/">http://openjdk.java.net/install/</a>)</li><li class="listitem" style="list-style-type: disc">Gradle (<a class="ulink" href="http://gradle.org/installation">http://gradle.org/installation</a>)</li></ul></div><p>We can<a class="indexterm" id="id941"/> clone and build the Kafka on Mesos project from the following repository:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/mesos/kafka</strong></span>
<span class="strong"><strong>$ cd kafka</strong></span>
<span class="strong"><strong>$ ./gradlew jar</strong></span>
</pre></div><p>We will also require the Kafka executor, which can be downloaded with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget https://archive.apache.org/dist/kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz</strong></span>
</pre></div><p>We will also need to set the following environment variable to point to the <code class="literal">libmesos.so</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libmesos.so</strong></span>
</pre></div><p>Once these are set, we can use the <code class="literal">kafka-mesos.sh</code> script to launch and configure Kafka on top of Mesos. Before doing so, we need to create the <code class="literal">kafka-mesos.properties</code> file with the following contents:</p><div class="informalexample"><pre class="programlisting">storage=file:kafka-mesos.json
master=zk://master:2181/mesos
zk=master:2181
api=http://master:7000</pre></div><p>This file can be used to configure the scheduler (<code class="literal">kafka-mesos.sh</code>) if we don't need to pass the arguments to the scheduler all the time. The scheduler supports the following command-line arguments:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Option</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--api</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the API URL—for example, <code class="literal">http://master:7000</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--bind-address </code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the scheduler bind address (such as master, <code class="literal">0.0.0.0</code>, <code class="literal">192.168.50.*</code>, and <code class="literal">if:eth1</code>). The default is <code class="literal">all</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--debug &lt;Boolean&gt; </code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the debug mode. The default is <code class="literal">false</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--framework-name</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the framework name. The default is <code class="literal">kafka</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--framework-role</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the framework role. The default is <code class="literal">*</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--framework-timeout</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the framework timeout (30s, 1m, or 1h). The default is <code class="literal">30d</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--jre</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the JRE zip file (<code class="literal">jre-7-openjdk.zip</code>). The default is <code class="literal">none</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--log</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the log file to use. The default is <code class="literal">stdout</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--master</code>
</p>
</td><td style="text-align: left" valign="top">
<p>These are the master connection settings. Some examples are:</p>
<p>
<code class="literal">- master:5050</code>
</p>
<p>
<code class="literal">- master:5050,master2:5050</code>
</p>
<p>
<code class="literal">- zk://master:2181/mesos</code>
</p>
<p>
<code class="literal">- zk://username:password@master:2181</code>
</p>
<p>
<code class="literal">- zk://master:2181,master2:2181/mesos</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--principal</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the principal (username) used to register the framework. The default is <code class="literal">none</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--secret</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the secret (password) used to register the framework. The default is <code class="literal">none</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--storage</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the storage for the cluster state. Some examples are:</p>
<p>
<code class="literal">- file:kafka-mesos.json</code>
</p>
<p>
<code class="literal">- zk:/kafka-mesos</code>
</p>
<p>The default is <code class="literal">file:kafka-mesos.json</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--user</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the Mesos user to run tasks. The default is <code class="literal">none</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">--zk</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is Kafka <code class="literal">zookeeper.connect</code>. Some examples are:</p>
<p>
<code class="literal">- master:2181</code>
</p>
<p>
<code class="literal">- master:2181,master2:2181</code>
</p>
</td></tr></tbody></table></div><p>Now, we can use the scheduler to run a Kafka scheduler via the following commands listed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#Start the kafka scheduler</strong></span>
<span class="strong"><strong>$ ./kafka-mesos.sh scheduler</strong></span>
</pre></div><p>The next<a class="indexterm" id="id942"/> thing we need to do is to start up one Kafka broker with the default settings. This can be done via the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker add 0</strong></span>
<span class="strong"><strong>  </strong></span>
<span class="strong"><strong>broker added:   id: 0</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m</strong></span>
</pre></div><p>At this point, our Kafka cluster will have one broker that is not yet started. We can verify this with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker list</strong></span>

<span class="strong"><strong>broker:</strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m</strong></span>
</pre></div><p>We can now start this broker with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker start 0</strong></span>

<span class="strong"><strong>broker started:</strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: true</strong></span>
<span class="strong"><strong>  state: running</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m, hostname:slave0</strong></span>
<span class="strong"><strong>  task:</strong></span>
<span class="strong"><strong>    id: broker-0-d2d94520-2f3e-4779-b276-771b4843043c</strong></span>
<span class="strong"><strong>    running: true</strong></span>
<span class="strong"><strong>    endpoint: 192.168.25.62:31000</strong></span>
<span class="strong"><strong>    attributes: rack=r1</strong></span>
</pre></div><p>If the preceding output is shown, then our broker is ready to produce and consume messages. We can now test this setup with <code class="literal">kafkacat</code>.</p><p>The <code class="literal">kafkacat</code> can be installed on the system with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get install kafkacat</strong></span>

<span class="strong"><strong>$ echo "test" |kafkacat -P -b "192.168.25.62:31000" -t testTopic -p 0</strong></span>
</pre></div><p>Now that <a class="indexterm" id="id943"/>we have pushed the test to the broker, we can read it back with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ kafkacat -C -b "192.168.25.62:31000" -t testTopic -p 0 -e</strong></span>
<span class="strong"><strong>test</strong></span>
</pre></div><p>Now, let's take a look at how we can add more brokers to the cluster at once. Run the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker add 0..2 --heap 1024 --mem 2048</strong></span>
</pre></div><p>The preceding command adds three <code class="literal">kafka</code> brokers to the cluster with the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>brokers added:</strong></span>

<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m</strong></span>

<span class="strong"><strong>  id: 1</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m</strong></span>

<span class="strong"><strong>  id: 2</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m</strong></span>
</pre></div><p>We can start all the three brokers at once with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$   ./kafka-mesos.sh broker start 0..2</strong></span>

<span class="strong"><strong>brokers started:</strong></span>
<span class="strong"><strong>  </strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: true</strong></span>
<span class="strong"><strong>  state: running</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m, hostname:slave0</strong></span>
<span class="strong"><strong>  task:</strong></span>
<span class="strong"><strong>    id: broker-0-d2d94520-2f3e-4779-b276-771b4843043c</strong></span>
<span class="strong"><strong>    running: true</strong></span>
<span class="strong"><strong>    endpoint: 192.168.25.62:31000</strong></span>
<span class="strong"><strong>    attributes: rack=r1</strong></span>

<span class="strong"><strong>  id: 1</strong></span>
<span class="strong"><strong>  active: true</strong></span>
<span class="strong"><strong>  state: running </strong></span>

<span class="strong"><strong>  id: 2</strong></span>
<span class="strong"><strong>  active: true</strong></span>
<span class="strong"><strong>  state: running </strong></span>
</pre></div><p>If we need to<a class="indexterm" id="id944"/> change the location of the Kafka logs where the data is stored, we need to first stop the particular broker and then update the location with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker stop 0</strong></span>

<span class="strong"><strong>broker stopped:</strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m, hostname:slave0, expires:2015-07-10 15:51:43+03</strong></span>

<span class="strong"><strong>$ ./kafka-mesos.sh broker update 0 --options log.dirs=/mnt/kafka/broker0</strong></span>

<span class="strong"><strong>broker updated:</strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: false</strong></span>
<span class="strong"><strong>  state: stopped</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  options: log.dirs=/mnt/kafka/broker0</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m, hostname:slave0, expires:2015-07-10 15:51:43+03</strong></span>
</pre></div><p>Once done, we<a class="indexterm" id="id945"/> can start the broker back up with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker start 0</strong></span>

<span class="strong"><strong>broker started:</strong></span>
<span class="strong"><strong>  id: 0</strong></span>
<span class="strong"><strong>  active: true</strong></span>
<span class="strong"><strong>  state: running</strong></span>
<span class="strong"><strong>  resources: cpus:1.00, mem:2048, heap:1024, port:auto</strong></span>
<span class="strong"><strong>  failover: delay:1m, max-delay:10m</strong></span>
<span class="strong"><strong>  stickiness: period:10m, hostname:slave0</strong></span>
<span class="strong"><strong>  task:</strong></span>
<span class="strong"><strong>  id: broker-0-d2d94520-2f3e-4779-b276-771b4843043c</strong></span>
<span class="strong"><strong>  running: true</strong></span>
<span class="strong"><strong>  endpoint: 192.168.25.62:31000</strong></span>
<span class="strong"><strong>  attributes: rack=r1</strong></span>
</pre></div><div class="section" title="Kafka logs management"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec101"/>Kafka logs management</h3></div></div></div><p>We<a class="indexterm" id="id946"/> can get the last 100 lines of the logs (<code class="literal">stdout</code> -default or <code class="literal">stderr</code>) of any broker in the cluster with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker log 0</strong></span>
</pre></div><p>If we need to read from the <code class="literal">stderr</code> file, then we will use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker log 0 --name stderr</strong></span>
</pre></div><p>We can read any file in the <code class="literal">kafka-*/log/</code> directory by passing on the filename to the <code class="literal">--name</code> option. For example, if we need to read <code class="literal">server.log</code>, then it can be read with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker log 0 --name server.log</strong></span>
</pre></div><p>Also, if we need to read more numbers of lines from the log, it can be read using the <code class="literal">--lines</code> option, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh broker log 0 --name server.log --lines 200</strong></span>
</pre></div></div></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec147"/>An advanced configuration guide</h2></div></div></div><p>The following <a class="indexterm" id="id947"/>are the configuration options available while <span class="emphasis"><em>adding</em></span> broker(s) to the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh help broker add</strong></span>
<span class="strong"><strong>Add broker</strong></span>
<span class="strong"><strong>Usage: broker add &lt;broker-expr&gt; [options]</strong></span>
<span class="strong"><strong>Option        Description</strong></span>
<span class="strong"><strong>--bind-address        broker bind address (broker0, 192.168.50.*, if:eth1). Default - auto</strong></span>
<span class="strong"><strong>--constraints         constraints (hostname=like:master,rack=like:1.*). See below.</strong></span>
<span class="strong"><strong>--cpus &lt;Double&gt;       cpu amount (0.5, 1, 2)</strong></span>
<span class="strong"><strong>--failover-delay      failover delay (10s, 5m, 3h)</strong></span>
<span class="strong"><strong>--failover-max-delay  max failover delay. See failoverDelay.</strong></span>
<span class="strong"><strong>--failover-max-tries  max failover tries. Default - none</strong></span>
<span class="strong"><strong>--heap &lt;Long&gt;         heap amount in Mb</strong></span>
<span class="strong"><strong>--jvm-options         jvm options string (-Xms128m -XX:PermSize=48m)</strong></span>
<span class="strong"><strong>--log4j-options       log4j options or file. Examples</strong></span>
<span class="strong"><strong>                      log4j.logger.kafka=DEBUG\, kafkaAppender</strong></span>
<span class="strong"><strong>                      file:log4j.properties</strong></span>
<span class="strong"><strong>--mem &lt;Long&gt;          mem amount in Mb</strong></span>
<span class="strong"><strong>--options             options or file. Examples:</strong></span>
<span class="strong"><strong>                      log.dirs=/tmp/kafka/$id,num.io.threads=16</strong></span>
<span class="strong"><strong>                      file:server.properties</strong></span>
<span class="strong"><strong>--port                port or range (31092, 31090..31100). Default - auto</strong></span>
<span class="strong"><strong>--stickiness-period   stickiness period to preserve same node for broker (5m, 10m, 1h)</strong></span>
<span class="strong"><strong>--volume              pre-reserved persistent volume id</strong></span>

<span class="strong"><strong>Generic        Options</strong></span>
<span class="strong"><strong>Option         Description</strong></span>
<span class="strong"><strong>------  -----------</strong></span>
<span class="strong"><strong>--api      Api url. Example: http://master:7000broker-expr examples:</strong></span>

<span class="strong"><strong>  0      - broker 0</strong></span>
<span class="strong"><strong>  0,1    - brokers 0,1</strong></span>
<span class="strong"><strong>  0..2   - brokers 0,1,2</strong></span>
<span class="strong"><strong>  0,1..2 - brokers 0,1,2</strong></span>
<span class="strong"><strong>  *      - any broker</strong></span>

<span class="strong"><strong>attribute filtering:</strong></span>
<span class="strong"><strong>  *[rack=r1]            - any broker having rack=r1</strong></span>
<span class="strong"><strong>  *[hostname=slave*]    - any broker on host with name starting with 'slave'</strong></span>
<span class="strong"><strong>  0..4[rack=r1,dc=dc1]  - any broker having rack=r1 and dc=dc1</strong></span>

<span class="strong"><strong>constraint examples:</strong></span>
<span class="strong"><strong>  like:master     - value equals 'master'</strong></span>
<span class="strong"><strong>  unlike:master   - value not equals 'master'</strong></span>
<span class="strong"><strong>  like:slave.*    - value starts with 'slave'</strong></span>
<span class="strong"><strong>  unique          - all values are unique</strong></span>
<span class="strong"><strong>  cluster         - all values are the same</strong></span>
<span class="strong"><strong>  cluster:master  - value equals 'master'</strong></span>
<span class="strong"><strong>  groupBy         - all values are the same</strong></span>
<span class="strong"><strong>  groupBy:3       - all values are within 3 different groups</strong></span>
</pre></div><p>We will <a class="indexterm" id="id948"/>now take a look at the options that are available when starting the broker(s):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh help broker start</strong></span>
<span class="strong"><strong>Start broker</strong></span>
<span class="strong"><strong>Usage: broker start &lt;broker-expr&gt; [options]</strong></span>
<span class="strong"><strong>Option     Description</strong></span>
<span class="strong"><strong>------     -----------</strong></span>
<span class="strong"><strong>--timeout  timeout (30s, 1m, 1h). 0s - no timeout</strong></span>

<span class="strong"><strong>Generic  Options</strong></span>
<span class="strong"><strong>Option   Description</strong></span>
<span class="strong"><strong>------   -----------</strong></span>
<span class="strong"><strong>--api    Api url. Example: http://master:7000</strong></span>

<span class="strong"><strong>broker    - expr examples:</strong></span>
<span class="strong"><strong>  0       - broker 0</strong></span>
<span class="strong"><strong>  0,1     - brokers 0,1</strong></span>
<span class="strong"><strong>  0..2    - brokers 0,1,2</strong></span>
<span class="strong"><strong>  0,1..2  - brokers 0,1,2</strong></span>
<span class="strong"><strong>  *       - any broker</strong></span>

<span class="strong"><strong>attribute filtering:</strong></span>
<span class="strong"><strong>  *[rack=r1]           - any broker having rack=r1</strong></span>
<span class="strong"><strong>  *[hostname=slave*]   - any broker on host with name starting with 'slave'</strong></span>
<span class="strong"><strong>  0..4[rack=r1,dc=dc1] - any broker having rack=r1 and dc=dc1</strong></span>
</pre></div><p>The following are the configuration options available while <span class="emphasis"><em>updating</em></span> broker(s) in the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh help broker update</strong></span>

<span class="strong"><strong>Update broker</strong></span>
<span class="strong"><strong>Usage: broker update &lt;broker-expr&gt; [options]</strong></span>

<span class="strong"><strong>Option                Description</strong></span>
<span class="strong"><strong>------                -----------</strong></span>
<span class="strong"><strong>--bind-address        broker bind address (broker0, 192.168.50.*, if:eth1). Default - auto</strong></span>
<span class="strong"><strong>--constraints         constraints (hostname=like:master,rack=like:1.*). See below.</strong></span>
<span class="strong"><strong>--cpus &lt;Double&gt;       cpu amount (0.5, 1, 2)</strong></span>
<span class="strong"><strong>--failover-delay      failover delay (10s, 5m, 3h)</strong></span>
<span class="strong"><strong>--failover-max-delay  max failover delay. See failoverDelay.</strong></span>
<span class="strong"><strong>--failover-max-tries  max failover tries. Default - none</strong></span>
<span class="strong"><strong>--heap &lt;Long&gt;         heap amount in Mb</strong></span>
<span class="strong"><strong>--jvm-options         jvm options string (-Xms128m -XX:PermSize=48m)</strong></span>
<span class="strong"><strong>--log4j-options       log4j options or file. Examples:</strong></span>
<span class="strong"><strong>                        log4j.logger.kafka=DEBUG\, kafkaAppender</strong></span>
<span class="strong"><strong>                        file:log4j.properties</strong></span>
<span class="strong"><strong>--mem &lt;Long&gt;          mem amount in Mb</strong></span>
<span class="strong"><strong>--options             options or file. Examples:</strong></span>
<span class="strong"><strong>                        log.dirs=/tmp/kafka/$id,num.io.threads=16</strong></span>
<span class="strong"><strong>                        file:server.properties</strong></span>
<span class="strong"><strong>--port                port or range (31092, 31090..31100). Default - auto</strong></span>
<span class="strong"><strong>--stickiness-period   stickiness period to preserve same node for broker (5m, 10m, 1h)</strong></span>
<span class="strong"><strong>--volume              pre-reserved persistent volume id</strong></span>

<span class="strong"><strong>Generic Options</strong></span>
<span class="strong"><strong>Option  Description</strong></span>
<span class="strong"><strong>------  -----------</strong></span>
<span class="strong"><strong>--api   Api url. Example: http://master:7000</strong></span>

<span class="strong"><strong>broker-expr examples:</strong></span>
<span class="strong"><strong>  0       - broker 0</strong></span>
<span class="strong"><strong>  0,1     - brokers 0,1</strong></span>
<span class="strong"><strong>  0..2    - brokers 0,1,2</strong></span>
<span class="strong"><strong>  0,1..2  - brokers 0,1,2</strong></span>
<span class="strong"><strong>  *       - any broker</strong></span>

<span class="strong"><strong>attribute filtering:</strong></span>
<span class="strong"><strong>  *[rack=r1]           - any broker having rack=r1</strong></span>
<span class="strong"><strong>  *[hostname=slave*]   - any broker on host with name starting with 'slave'</strong></span>
<span class="strong"><strong>  0..4[rack=r1,dc=dc1] - any broker having rack=r1 and dc=dc1</strong></span>

<span class="strong"><strong>constraint examples:</strong></span>
<span class="strong"><strong>  like:master     - value equals 'master'</strong></span>
<span class="strong"><strong>  unlike:master   - value not equals 'master'</strong></span>
<span class="strong"><strong>  like:slave.*    - value starts with 'slave'</strong></span>
<span class="strong"><strong>  unique          - all values are unique</strong></span>
<span class="strong"><strong>  cluster         - all values are the same</strong></span>
<span class="strong"><strong>  cluster:master  - value equals 'master'</strong></span>
<span class="strong"><strong>  groupBy         - all values are the same</strong></span>
<span class="strong"><strong>  groupBy:3       - all values are within 3 different groups</strong></span>

<span class="strong"><strong>Note: use "" arg to unset an option</strong></span>
</pre></div><p>The following<a class="indexterm" id="id949"/> are the configuration options available while stopping broker(s) in the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh help broker stop</strong></span>

<span class="strong"><strong>Stop broker</strong></span>
<span class="strong"><strong>Usage: broker stop &lt;broker-expr&gt; [options]</strong></span>

<span class="strong"><strong>Option     Description</strong></span>
<span class="strong"><strong>------     -----------</strong></span>
<span class="strong"><strong>--force    forcibly stop</strong></span>
<span class="strong"><strong>--timeout  timeout (30s, 1m, 1h). 0s - no timeout</strong></span>

<span class="strong"><strong>Generic  Options</strong></span>
<span class="strong"><strong>Option   Description</strong></span>
<span class="strong"><strong>------   -----------</strong></span>
<span class="strong"><strong>--api    Api url. Example: http://master:7000</strong></span>

<span class="strong"><strong>broker-expr examples:</strong></span>
<span class="strong"><strong>  0      - broker 0</strong></span>
<span class="strong"><strong>  0,1    - brokers 0,1</strong></span>
<span class="strong"><strong>  0..2   - brokers 0,1,2</strong></span>
<span class="strong"><strong>  0,1..2 - brokers 0,1,2</strong></span>
<span class="strong"><strong>  *      - any broker</strong></span>
<span class="strong"><strong>attribute filtering:</strong></span>
<span class="strong"><strong>  *[rack=r1]           - any broker having rack=r1</strong></span>
<span class="strong"><strong>  *[hostname=slave*]   - any broker on host with name starting with 'slave'</strong></span>
<span class="strong"><strong>  0..4[rack=r1,dc=dc1] - any broker having rack=r1 and dc=dc1</strong></span>
</pre></div><p>The following are the configuration options available while adding a topic to the broker(s) in the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-mesos.sh help topic add</strong></span>
<span class="strong"><strong>Add topic</strong></span>
<span class="strong"><strong>Usage: topic add &lt;topic-expr&gt; [options]</strong></span>

<span class="strong"><strong>Option                  Description</strong></span>
<span class="strong"><strong>------                  -----------</strong></span>
<span class="strong"><strong>--broker                &lt;broker-expr&gt;. Default - *. See below.</strong></span>
<span class="strong"><strong>--options               topic options. Example: flush.ms=60000,retention.ms=6000000</strong></span>
<span class="strong"><strong>--partitions &lt;Integer&gt;  partitions count. Default - 1</strong></span>
<span class="strong"><strong>--replicas &lt;Integer&gt;    replicas count. Default - 1</strong></span>

<span class="strong"><strong>topic-expr examples:</strong></span>
<span class="strong"><strong>  t0        - topic t0</strong></span>
<span class="strong"><strong>  t0,t1     - topics t0, t1</strong></span>
<span class="strong"><strong>  *         - any topic</strong></span>
<span class="strong"><strong>  t*        - topics starting with 't'</strong></span>

<span class="strong"><strong>broker-expr examples:</strong></span>
<span class="strong"><strong>  0      - broker 0</strong></span>
<span class="strong"><strong>  0,1    - brokers 0,1</strong></span>
<span class="strong"><strong>  0..2   - brokers 0,1,2</strong></span>
<span class="strong"><strong>  0,1..2 - brokers 0,1,2</strong></span>
<span class="strong"><strong>  *      - any broker</strong></span>
</pre></div><p>The reference<a class="indexterm" id="id950"/> for this is <a class="ulink" href="https://github.com/mesos/kafka">https://github.com/mesos/kafka</a>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec79"/>Summary</h1></div></div></div><p>This chapter introduced the reader to some important big data storage frameworks such as Cassandra, the ELK stack, and Kafka and covered topics such as the setup, configuration, and management of these frameworks on a distributed infrastructure using Mesos.</p><p>I hope that this book has armed you with all the resources that you require to effectively manage the complexities of today's modern datacenter requirements. By following the detailed step-by-step guides to deploy a Mesos cluster using the DevOps tool of your choice, you should now be in a position to handle the system administration requirements of your organization smoothly.</p></div></body></html>