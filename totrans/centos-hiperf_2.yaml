- en: Chapter 2. Installing Cluster Services and Configuring Network Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to set up and configure the basic required
    network infrastructure and also the clustering components that we installed in
    the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, we will review the basic and important concepts of firewalling
    and Internet protocols, and we will explain how to add the firewall rules that
    will allow communication between the nodes and the proper operation of the clustering
    services on each node.
  prefs: []
  type: TYPE_NORMAL
- en: If your native language is any other than English, you must have taken an English
    class or taught yourself (as I did) before being able to read this book. The same
    thing happens when two people who do not speak the same language want to communicate
    with each other. At least one of them needs to know the language of the other,
    or the two of them need to agree on a common idiom in order to be able to understand
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: In networking, the equivalent of languages in the above analogy is called **protocols**.
    In order to enable data transmission between two machines, there must be a logical
    way for them to be able to speak to each other. This is at the very heart of the
    Internet protocol suite, also known as the **Internet model**, which provides
    a set of communication protocols or rules.
  prefs: []
  type: TYPE_NORMAL
- en: It is precisely this set of protocols that make data transmission possible in
    networks such as the Internet. Later in this chapter, we will explain the protocols
    and network ports that participate in the communication inside a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and starting clustering services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having reviewed the key networking concepts that were outlined earlier, we are
    now ready to start describing the cluster services.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and enabling clustering services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will recall from the previous chapter that we installed the following clustering
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pacemaker**: This is the cluster resource manager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Corosync**: This is the messaging service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PCS**: This is the synchronization and configuration tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can probably guess from the preceding list, these components should run
    as daemons, a special type of process that runs in the background without the
    need of direct intervention or control of an administrator. Although we installed
    the necessary packages in [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 1. Cluster Basics and Installation on CentOS 7"), *Cluster Basics and
    Installation on CentOS 7*, we did not start the cluster resource manager or the
    messaging services. So, we now need to start them manually for the first time
    and enable them to run automatically on startup during the next system boot.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by configuring `pacemaker` and `corosync` first and save PCS for
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, these daemons (also known as units in
    systemd-based systems) are inactive when you first boot both nodes (and are not
    automatically started on reboot) after performing all the tasks outlined in [Chapter
    1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0 "Chapter 1. Cluster
    Basics and Installation on CentOS 7"), *Cluster Basics and Installation on CentOS
    7*. You can check their current running status using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Starting and enabling clustering services](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to start corosync and pacemaker on each node and enable both services
    to start automatically during system boot, first create the corosync configuration
    file by making a copy of the example file, which came with the installation package.
    As opposed to the pacemaker and PCS, corosync does not create the configuration
    file automatically for you:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the corosync configuration file, do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And then restart and enable the services by running the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding commands, note that we are not starting corosync manually,
    as it will launch on its own when pacemaker is started. It is important to note
    that on systemd-based systems, enabling a service is not the same as starting
    it. A unit may be enabled but not started, or the other way around. As shown in
    the following code, enabling a service involves creating a symlink to the unit's
    configuration file, which among other things specifies the actions to be taken
    on system boot and shutdown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following operations on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, before we can configure the cluster at a later stage, we need to perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start and enable the PCS daemon (`pcsd`), which will be in charge of keeping
    the corosync configuration synced on `node01` and `node02`. In order for the `pcsd`
    daemon to work as expected, corosync and pacemaker must have been started previously.
    Note that when you use the `systemctl` tool to manage services in a systemd-based
    system, you can omit the trailing `.service` after the daemon name (or use it
    if you want, as indicated in [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 1. Cluster Basics and Installation on CentOS 7"), *Cluster Basics and
    Installation on CentOS 7*). Start and enable the PCS daemon with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now set the password for the hacluster Linux account, which was created automatically
    when PCS was installed. This account is used by the PCS daemon to set up communication
    between nodes, and is best managed when the password is identical on both nodes.
    To set the password for hacluster, type the following command and assign the same
    password on both nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Under normal circumstances, starting pacemaker should start corosync automatically.
    You can check corosync''s status with the `systemctl status corosync` command.
    If for some reason that is not the case, you can still run the following command
    to manually start the messaging service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Should any of the preceding commands return an error, running `systemctl -l
    status unit`, where `unit` is either corosync or pacemaker, will return a detailed
    status about the respective service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another useful troubleshooting command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will query the systemd journal (systemd's own log) and return verbose messages
    about the last events.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these commands will provide helpful information as to what went wrong,
    and point you in the right direction to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can read more about the systemd journal in its man page, *man journalctl*,
    or in the online version, which is available at [http://www.freedesktop.org/software/systemd/man/journalctl.html](http://www.freedesktop.org/software/systemd/man/journalctl.html).
  prefs: []
  type: TYPE_NORMAL
- en: Security fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we are ready to discuss network security to only allow the proper
    network traffic between the nodes. During the initial setup and while performing
    your first tests, you may want to disable the firewall and SELinux (which is described
    later in this chapter) and then go through both of them at a later stage—it is
    up to you depending on your grade of familiarity with them at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Letting in and letting out
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After having started and enabled the services mentioned earlier, we are ready
    to take a closer look at the network processes involved in the cluster configuration
    and maintenance. With the help of the `netstat` command, a tool included in the
    `net-tools` package for CentOS 7, we will print the current listening network
    ports and verify that corosync is running and listening for connections. Before
    doing so, you will need to install the net-tools package, as it is not included
    in the minimal CentOS 7 setup, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the following screenshot, Corosync is listening on the **UDP**
    ports `5404` and `5405` of the loopback interface (`127.0.0.1`) and on the port
    `5405` of the multicast address (which is set to `239.255.1.1` by default and
    provides a logical way to identify this group of nodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Letting in and letting out](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**User Datagram Protocol** (UDP) is one of the core members of the Internet
    model. This protocol allows applications to send messages (also known as **datagrams**)
    to hosts in a network in order to set up paths for data transmission without performing
    full handshakes (or a successful connection between two hosts in a network). Additionally,
    UDP does not include error checking and correction in a network communication
    (these checks are performed at the destination application itself).'
  prefs: []
  type: TYPE_NORMAL
- en: The **Transmission Control Protocol** (**TCP**) is another core protocol of
    the Internet model. As opposed to UDP, it provides error, delivery, ordering,
    and duplicates checking of data streams between computers in a network. Several
    well-known application layer protocols (such as HTTP, SMTP, and SSH, to name a
    few) are encapsulated in TCP.
  prefs: []
  type: TYPE_NORMAL
- en: '**Internet Group Management Protocol** (**IGMP**) is the communication protocol
    used by network devices (whether they can be either hosts or routers) to establish
    multicast data transmissions, which allows one host on the network to send datagrams
    to several other systems that are interested in receiving the source content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed further, we will need to allow traffic through the firewall
    on each node. By default, the ports named in the following list are the default
    ports where these services will listen after being started, as we previously did.
    Specifically, in both nodes, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the network ports needed by `corosync` (**UDP** ports **5404** and **5405**)
    and PCS (usually **TCP** **2224**) using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the use of `-m` multiport allows you to combine a number of different
    ports in one rule instead of having to write several rules that are almost identical.
    This results in fewer rules and easier maintenance of `iptables`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Allow IGMP and multicast traffic using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the default `iptables` policy for the `INPUT` chain to `DROP`. Thus,
    any packet that does not comply with the rules that we just added will be dropped.
    Note that, as opposed to the `REJECT` policy, `DROP` does not send any response
    whatsoever to the calling client, just "radio silence" while actively dropping
    the packets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After adding the necessary rules, our firewall configuration looks as shown
    in the following code, where we can clearly see that besides the rules that we
    added in the two previous steps, there are others that were initialized by default
    when we started and enabled `iptables`, as explained in [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 1. Cluster Basics and Installation on CentOS 7"), *Cluster Basics and
    Installation on CentOS 7*. Run the following command to list the firewall rules
    along with their corresponding numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the last default rule in the `INPUT` chain implements a `REJECT` procedure
    for non-compliant packets, we will delete it because we already took care of that
    need by changing the default policy for the chain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must save the firewall rules for persistency across boots. As shown
    in the following screenshot, this consists of saving the changes to `/etc/sysconfig/iptables`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Letting in and letting out](img/00020.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'If we inspect the `/etc/sysconfig/iptables` file with our preferred text editor
    or pager, we will realize that it presents the same firewall rules in a format
    that is somewhat easier to read, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you will also need to edit the `/etc/sysconfig/iptables-config` file
    to indicate that firewall rules should be persistent on system shutdown and reboot.
    Note that these lines already exist in the file and need to be changed. As a precaution,
    you may want to back up the existing file before making the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open `/etc/sysconfig/iptables-config` with your preferred text editor
    and ensure that the indicated lines read as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As usual, do not forget to restart `iptables` (`systemctl restart iptables`)
    in order to apply changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'CentOS 7, just like the previous versions of the distribution, comes with built-in
    **SELinux** (**Security Enhanced Linux**) support. This provides native, flexible
    access control functionality for the operating system based on the kernel itself.
    You may well be wondering what to do with SELinux policies at this stage. The
    current settings, which can be displayed with the `sestatus` and `getenforce`
    commands, and are shown in the following screenshot, will do for the time being:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Letting in and letting out](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In simple terms, we will leave the default mode set to `enforcing` for security
    purposes. This should not cause any issues further down the road, but if it does,
    feel free to set the mode to `permissive` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will enable warnings and log errors to help you troubleshoot
    issues while the server is still running. In case you need to troubleshoot issues
    and you suspect that SELinux may be causing them, you should look in `/var/log/audit/audit.log`.
    SELinux log messages, which are labeled with the AVC keyword, are written to that
    file via `auditd`, the Linux auditing system, which is started by default. Otherwise,
    these messages are written to `/var/log/messages`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before you tackle the next heading, don't forget to repeat the same operations
    and save the changes on the other node as well!
  prefs: []
  type: TYPE_NORMAL
- en: Getting acquainted with PCS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are getting closer to actually setting up the cluster. Before diving into
    that task, we need to become familiar with PCS—the core component of our cluster—so
    to speak, which will be used to control and configure pacemaker and corosync.
    To begin doing that, we can just run PCS without arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the output shown in the following screenshot, which provides a
    short explanation of each option and command available in PCS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting acquainted with PCS](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We are interested in the **Commands** section, where the actual categories
    of clustering that can be managed through this tool are listed, along with a brief
    description of their usage. Each of them supports several capabilities, which
    can be shown by appending the word **help** to `pcs [category]`. For example,
    let''s'' see the functionality that is provided by the `cluster` command (which
    by the way, we will use shortly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that the output is truncated for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: You will often find yourself examining the documentation, so you should consider
    seriously becoming acquainted with the help.
  prefs: []
  type: TYPE_NORMAL
- en: Managing authentication and creating the cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to authenticate PCS to the `pcsd` service on the nodes specified
    in the command line. By default, all nodes are authenticated to each other and
    thus PCS can talk to itself from one cluster member to the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is precisely where the hacluster user (of which we changed the password
    earlier) comes in handy, as it is the account that is used for this purpose. The
    generic syntax for PCS to perform this step in a cluster with `N` nodes is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In our current setup with two nodes, setting up authentication means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be prompted to enter the username and password that will be used for
    authentication, as discussed earlier, and fortunately for us, this process does
    not need to be repeated as we can now control the cluster from any of the nodes.
    This procedure is exemplified in the following screenshot (where we set up the
    authentication for `pcs` from `node01`), and later when we create the cluster
    itself issuing the command in `node02`, from where the `/etc/corosync/corosync.conf`
    file is synchronized to the other node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Managing authentication and creating the cluster](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To create the cluster using the specified nodes, type (on one node only, after
    successfully trying the password as illustrated in the preceding screenshot) the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, `MyCluster` is the name we have chosen for our cluster (and you may want
    to change it according to your liking). Next, press *Enter* and verify the output.
    Note that it is this command that creates the cluster configuration file in `/etc/corosync/corosync.conf`
    on both nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you created the `corosync.conf` file using the sample configuration file
    as instructed earlier in this chapter (in order to start pacemaker and corosync),
    you will have to use the `--force` option to overwrite that file with the current
    settings of the newly created cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you get the following error message while trying to set up the `pcs` authentication.
    Ensure that `pcsd` is running (and enabled) on `nodeXX`, and try again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: (Here, `XX` is the node number)
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the `/etc/corosync/corosync.conf` file in `node02` should be
    identical to the same file in `node01`, as can be seen in the output of the following
    `diff` command, when run from either node. An empty output indicates that the
    corosync configuration file has been correctly synced from one node to the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step consists of actually starting the cluster by issuing the command
    (again, on one node only):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The command that is used to start the cluster (`pcs cluster start`) deserves
    further clarification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There will be times when you want to start the cluster on a specific node. In
    that case, you will name such a node instead of using the `--all` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output to the preceding command should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the cluster has been started, you can check its status from any of the
    nodes (remember that PCS makes it possible for you to manage the cluster from
    any node):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `pcs status` command provides more detailed information, including the
    status of services and resources. It is possible that you notice that one of the
    nodes is `OFFLINE`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this case, ensure that both pacemaker and corosync are enabled (as indicated
    after the `Daemon status: line`) and started on the node that''s marked as `OFFLINE`,
    and then perform `pcs status` again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another issue you may encounter is having one or more of the nodes in an unclean
    state. While that is not common, resyncing the nodes by stopping and restarting
    the cluster on both nodes will fix it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The node that is marked as **DC**, that is, **Designated Controller**, is the
    node where the cluster was originally started and from where the cluster-related
    commands will be typically issued. If for some reason, the current DC fails, a
    new designated controller is chosen automatically from the remaining nodes. You
    can see which node is the current DC with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the current DC in your cluster, do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also want to check on each node individually:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pcs status nodes` command allows you to view all information about the
    cluster and its configured resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `corosync-cmapctl` command is another tool for accessing the cluster''s
    object database, where you will be able to view the properties and configuration
    of each node. Since the output of `corosync-cmapctl` command is rather lengthy,
    you may want to filter by a chosen keyword, such as `members` or `cluster_name`,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the preceding output allows you to see the name of your cluster,
    the IP address, and the status of each member.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a virtual IP for the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 1. Cluster Basics and Installation on CentOS 7"), *Cluster Basics and
    Installation on CentOS 7*, since a cluster is by definition a group of computers
    (which we have been referring to as nodes or members) that work together so that
    the set is seen as a single system from the outside, we need to ensure that end
    users and clients see it that way.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the last thing that we will do in this chapter is configure
    a virtual IP, which is the address that external clients will use to connect to
    our cluster. Note that in an ordinary, non-cluster environment, you can use tools,
    such as `ifconfig` to configure a virtual IP for your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in our case, we will use nothing more and nothing less than PCS and
    perform two operations at once:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the IPv4 address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning it to the cluster as a whole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a virtual IP as a cluster resource
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since a virtual IP is what is called a **cluster resource**, we will use `pcs
    resource help` to look for information on to how to create it. You will need,
    in advance, to pick an IP address that is not being used in your LAN to assign
    to the virtual IP resource. After the virtual IP is initialized, you can ping
    it as usual to confirm its availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the virtual IP named `virtual_ip` with the address `192.168.0.4/24`,
    monitored everything 30 seconds on `enp0s3`, run the following command on either
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Up to this point, the virtual IP resource will show as stopped in the output
    of `pcs cluster status` or `pcs status` until a later stage when we will disable
    STONITH (which is a cluster feature that is explained in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the status of the virtual IP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To view the current status of cluster resources use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In case the newly created virtual IP is not started automatically, you will
    want to perform a more thorough check, including a verbose output of the configuration
    used by the running cluster as provided by `crm_verify`, a tool that is part of
    the pacemaker cluster resource manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**STONITH**, an acronym for **Shoot The Other Node In The Head**, represents
    a cluster feature that prevents nodes in a high-availability cluster from becoming
    active at the same time, and thus serving the same content.'
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding error message indicates, clusters with shared data need STONITH
    to ensure data integrity. However, we will defer the appropriate discussion for
    this feature for the next chapter, and we will disable it for the time being in
    order to be able to show how the virtual IP is started and becomes accessible.
    On the other hand, when `crm_verify –L –V` does not return any output, it means
    that the configuration is valid and free from errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and disable STONITH but keep in mind that we will return to this in
    the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Next, check the cluster status again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resource should now show as started when you query the cluster status.
    You can check the resource availability by pinging it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: If the ping operation returns a warning that some packets were not delivered
    to destination, refer to `/var/log/pacemaker.log` or `/var/log/cluster/corosync.log`
    for information on what could have failed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to set up and configure the basic required
    network infrastructure and also the clustering components that we installed in
    [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0 "Chapter 1. Cluster
    Basics and Installation on CentOS 7"), *Cluster Basics and Installation on CentOS
    7*. Having reviewed the concepts associated with security, firewalling, and Internet
    protocols, we were able to add the firewall rules that will allow the communication
    of each node with each other and the proper operation of the clustering services
    on each box.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the tools discussed in this article throughout the rest of this
    book, not only to check on the status of the cluster or the individual nodes,
    but also as a troubleshooting technique in case things don't go as expected.
  prefs: []
  type: TYPE_NORMAL
