<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Mesos Cluster Deployment"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Mesos Cluster Deployment</h1></div></div></div><p>This chapter explains how a Mesos cluster can be easily set up and monitored using standard deployment and configuration management tools used by system administrators and DevOps engineers. We will explain the steps that are required to set up a Mesos cluster through Ansible, Puppet, SaltStack, Chef, Terraform, or Cloudformation, and how a test environment can be set up using Playa Mesos. We will also talk about some standard monitoring tools, such as Nagios and Satellite that are can be used to monitor the cluster. We will also discuss some of the common problems faced while deploying a Mesos cluster along with their corresponding resolutions.</p><p>The topics that will be covered are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Deploying and configuring a Mesos cluster using the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ansible</li><li class="listitem" style="list-style-type: disc">Puppet</li><li class="listitem" style="list-style-type: disc">SaltStack</li><li class="listitem" style="list-style-type: disc">Chef</li><li class="listitem" style="list-style-type: disc">Terraform</li><li class="listitem" style="list-style-type: disc">Cloudformation</li></ul></div></li><li class="listitem" style="list-style-type: disc">Creating test environments using Playa Mesos</li><li class="listitem" style="list-style-type: disc">Common deployment issues and solutions</li><li class="listitem" style="list-style-type: disc">Monitoring the Mesos cluster using the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Nagios</li><li class="listitem" style="list-style-type: disc">Satellite</li></ul></div></li></ul></div><div class="section" title="Deploying and configuring a Mesos cluster using Ansible"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec47"/>Deploying and configuring a Mesos cluster using Ansible</h1></div></div></div><p>Ansible is <a class="indexterm" id="id455"/>one of the popular infrastructure <a class="indexterm" id="id456"/>automation tools commonly used by system administrators <a class="indexterm" id="id457"/>today and recently acquired by Red Hat. Nodes are managed through <a class="indexterm" id="id458"/>
<span class="strong"><strong>Secure Shell</strong></span> (<span class="strong"><strong>SSH</strong></span>) and require only Python support. Ansible has open sourced a lot of playbooks, including an <code class="literal">ansible-mesos</code> one that we will discuss in this section.</p><p>The <a class="indexterm" id="id459"/>
<code class="literal">ansible-mesos</code> playbook can be used to<a class="indexterm" id="id460"/> install and configure a Mesos cluster with customized master as well as slave setup options. Currently, it supports Ubuntu and CentOS/Red Hat operating system-powered machines. The <code class="literal">ansible-mesos</code> playbook also supports setting specific slave executors and hence can be run with native Docker support.</p><div class="section" title="Installing Ansible"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec84"/>Installing Ansible</h2></div></div></div><p>Ansible installation<a class="indexterm" id="id461"/> is only required on a single machine. It does not require a database, nor does it need to keep a daemon running all the time. It uses SSH to manage the cluster and requires Python (versions 2.6 or 2.7) to be installed on the machine. You can even install Ansible on your laptop or personal computer and have it manage the machines running remotely. The machine where Ansible is installed is called the <span class="strong"><strong>control machine</strong></span>. At the time of writing this book, there is no support for Windows machines. The machines that are controlled by the control machine are known as <span class="strong"><strong>managed nodes</strong></span> and require SSH access ability from the control machine as well as Python (version 2.4 or later) installed on them.</p></div><div class="section" title="Installing the control machine"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec85"/>Installing the control machine</h2></div></div></div><p>We can run <a class="indexterm" id="id462"/>Ansible without root access as it doesn't require you to install any additional software or database servers. Execute the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Install python pip</strong></span>
<span class="strong"><strong>$ sudo easy_install pip</strong></span>

<span class="strong"><strong># Install the following python modules for ansible</strong></span>
<span class="strong"><strong>$ sudo pip install paramiko PyYAML Jinja2 httplib2 six</strong></span>

<span class="strong"><strong># Clone the repository</strong></span>
<span class="strong"><strong>$ git clone git://github.com/ansible/ansible.git --recursive</strong></span>

<span class="strong"><strong># Change the directory</strong></span>
<span class="strong"><strong>$ cd ansible</strong></span>

<span class="strong"><strong># Installation</strong></span>
<span class="strong"><strong>$ source ./hacking/env-setup</strong></span>
</pre></div><p>If <a class="indexterm" id="id463"/>everything goes well, then we can see the following output in the Terminal, indicating that the installation was successful. After this, we will be able to use the Ansible command from the Terminal.</p><div class="mediaobject"><img alt="Installing the control machine" src="graphics/B05186_05_01.jpg"/></div><p>By default, Ansible uses the inventory file in <code class="literal">/etc/ansible/hosts</code>, which is an INI-like format and could look similar to the following, for example:</p><div class="informalexample"><pre class="programlisting">mail.xyz.com

[webservers]
foo.xyz.com
bar.xyz.com

[dbservers]
one.xyz.com
two.xyz.com
three.xyz.com</pre></div><p>Here, the group names are indicated in brackets, which can be used to classify the systems that will be controlled.</p><p>We can also use the <a class="indexterm" id="id464"/>command-line option <code class="literal">-i</code> to point it to a different file rather than the one found in <code class="literal">/etc/ansible/hosts</code>.</p></div><div class="section" title="Creating an ansible-mesos setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec86"/>Creating an ansible-mesos setup</h2></div></div></div><p>Ansible executes <a class="indexterm" id="id465"/>a playbook composed of roles against a set of hosts, as described before in the hosts file, which are organized into groups. For more details, visit <a class="ulink" href="http://frankhinek.com/create-ansible-playbook-on-github-to-build-mesos-clusters/">http://frankhinek.com/create-ansible-playbook-on-github-to-build-mesos-clusters/</a>.</p><p>First, let's create a hosts file by pointing to our Mesos master and slave nodes via the following:</p><div class="informalexample"><pre class="programlisting">$ cat hosts
[mesos_masters]
ec2-….compute-1.amazonaws.com zoo_id=1 ec2-….compute-1.amazonaws.com zoo_id=2
ec2-….compute-1.amazonaws.com zoo_id=3

[mesos_workers]
ec2-….compute-1.amazonaws.com
ec2-….compute-1.amazonaws.com </pre></div><p>Here, we created two groups named <code class="literal">mesos_masters</code> and <code class="literal">mesos_workers</code> and listed the master and slave IP addresses, respectively. For the <code class="literal">mesos_masters</code> group, we will also need to specify the ZooKeeper ID as the cluster will run in high availability.</p><p>In the following steps, we will take a look at how to deploy Mesos on the machines listed in the hosts file using Ansible:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a <code class="literal">site.yml</code> file with following content:<div class="informalexample"><pre class="programlisting">--
# This playbook deploys the entire Mesos cluster infrastructure.
# RUN: ansible-playbook --ask-sudo-pass -i hosts site.yml
 
- name: deploy and configure the mesos masters
  hosts: mesos_masters
  sudo: True
 
  roles:
    - {role: mesos, mesos_install_mode: "master", tags: ["mesos-master"]}
 
- name: deploy and configure the mesos slaves
  hosts: mesos_workers
  sudo: True
 
  roles:
    - {role: mesos, mesos_install_mode: "slave", tags: ["mesos-slave"]}</pre></div></li><li class="listitem">Now, we <a class="indexterm" id="id466"/>can create a group variable file that will be applicable to all the hosts belonging to the cluster, as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir group_vars</strong></span>
<span class="strong"><strong>$ vim all</strong></span>
</pre></div></li><li class="listitem">Next, we will put the following contents in all the files:<div class="informalexample"><pre class="programlisting">---
# Variables here are applicable to all host groups
 
mesos_version: 0.20.0-1.0.ubuntu1404
mesos_local_address: "{{ansible_eth0.ipv4.address}}"
mesos_cluster_name: "XYZ"
mesos_quorum_count: "2"
zookeeper_client_port: "2181"
zookeeper_leader_port: "2888"
zookeeper_election_port: "3888"
zookeeper_url: "zk://{{ groups.mesos_masters | join(':' + zookeeper_client_port + ',') }}:{{ zookeeper_client_port }}/mesos"</pre></div></li><li class="listitem">Now, we can create the roles for the Mesos cluster. First, create a roles directory via the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir roles; cd roles</strong></span>
</pre></div></li><li class="listitem">We can now use the <code class="literal">ansible-galaxy</code> command to initialize the directory structure for this role, as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ansible-galaxy init mesos</strong></span>
</pre></div></li><li class="listitem">This will create the directory structure as follows:<div class="mediaobject"><img alt="Creating an ansible-mesos setup" src="graphics/B05186_05_02.jpg"/></div></li><li class="listitem">Now, modify<a class="indexterm" id="id467"/> the <code class="literal">mesos/handlers/main.yml</code> file with following content:<div class="informalexample"><pre class="programlisting">---
# handlers file for mesos
- name: Start mesos-master
  shell: start mesos-master
  sudo: yes
 
- name: Stop mesos-master
  shell: stop mesos-master
  sudo: yes
 
- name: Start mesos-slave
  shell: start mesos-slave
  sudo: yes
 
- name: Stop mesos-slave
  shell: stop mesos-slave
  sudo: yes
 
- name: Restart zookeeper
  shell: restart zookeeper
  sudo: yes
 
- name: Stop zookeeper
  shell: stop zookeeper
  sudo: yes</pre></div></li><li class="listitem">Next, modify <a class="indexterm" id="id468"/>the tasks from the <code class="literal">mesos/tasks/main.yml</code> file, as follows:<div class="informalexample"><pre class="programlisting">---
# tasks file for mesos
 
<span class="strong"><strong># Common tasks for all Mesos nodes</strong></span>
- name: Add key for Mesosphere repository
  apt_key: url=http://keyserver.ubuntu.com/pks/lookup?op=get&amp;fingerprint=on&amp;search=0xE56151BF state=present
  sudo: yes
 
- name: Determine Linux distribution distributor
  shell: lsb_release -is | tr '[:upper:]' '[:lower:]'
  register: release_distributor
 
- name: Determine Linux distribution codename
  command: lsb_release -cs
  register: release_codename
 
- name: Add Mesosphere repository to sources list
  copy:
    content: "deb http://repos.mesosphere.io/{{release_distributor.stdout}} {{release_codename.stdout}} main"
    dest: /etc/apt/sources.list.d/mesosphere.list
    mode: 0644
  sudo: yes
 
<span class="strong"><strong>    # Tasks for Master, Slave, and ZooKeeper nodes</strong></span>

- name: Install mesos package
  apt: pkg={{item}} state=present update_cache=yes
  with_items:
    - mesos={{ mesos_pkg_version }}
  sudo: yes
  when: mesos_install_mode == "master" or mesos_install_mode == "slave"
 
- name: Set ZooKeeper URL # used for leader election amongst masters
  copy:
    content: "{{zookeeper_url}}"
    dest: /etc/mesos/zk
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master" or mesos_install_mode == "slave"
 
<span class="strong"><strong># Tasks for Master nodes</strong></span>
- name: Disable the Mesos Slave service
  copy:
    content: "manual"
    dest: /etc/init/mesos-slave.override
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master"
 
- name: Set Mesos Master hostname
  copy:
    content: "{{mesos_local_address}}"
    dest: /etc/mesos-master/hostname
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master"
 
- name: Set Mesos Master ip
  copy:
    content: "{{mesos_local_address}}"
    dest: /etc/mesos-master/ip
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master"
 
- name: Set Mesos Master Cluster name
  copy:
    content: "{{mesos_cluster_name}}"
    dest: /etc/mesos-master/cluster
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master"
 
- name: Set Mesos Master quorum count
  copy:
    content: "{{mesos_quorum_count}}"
    dest: /etc/mesos-master/quorum
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "master"
 
<span class="strong"><strong># Tasks for Slave nodes</strong></span>
- name: Disable the Mesos Master service
  copy:
    content: "manual"
    dest: /etc/init/mesos-master.override
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "slave"
 
- name: Disable the ZooKeeper service
  copy:
    content: "manual"
    dest: /etc/init/zookeeper.override
    mode: 0644
  sudo: yes
  notify:
    - Stop zookeeper
  when: mesos_install_mode == "slave"
 
- name: Set Mesos Slave hostname
  copy:
    content: "{{mesos_local_address}}"
    dest: /etc/mesos-slave/hostname
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "slave"
 
- name: Set Mesos Slave ip
  copy:
    content: "{{mesos_local_address}}"
    dest: /etc/mesos-slave/ip
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "slave"
 
- name: Set Mesos Slave ip
  copy:
    content: "{{mesos_local_address}}"
    dest: /etc/mesos-slave/ip
    mode: 0644
  sudo: yes
  when: mesos_install_mode == "slave"
 
- name: Set Mesos Slave isolation
  copy:
    content: "cgroups/cpu,cgroups/mem"
    dest: /etc/mesos-slave/isolation
    mode: 0644
  sudo: yes
  notify:
    - Start mesos-slave
  when: mesos_install_mode == "slave"
 
<span class="strong"><strong># Tasks for ZooKeeper nodes only</strong></span>
- name: Create zookeeper config file
  template: src=zoo.cfg.j2 dest=/etc/zookeeper/conf/zoo.cfg
  sudo: yes
  when: mesos_install_mode == "master"
 
- name: Create zookeeper myid file
  template: src=zoo_id.j2 dest=/etc/zookeeper/conf/myid
  sudo: yes
  notify:
    - Restart zookeeper
    - Start mesos-master
  when: mesos_install_mode == "master"</pre></div><p>This is a <a class="indexterm" id="id469"/>standard template to configure the Mesos master slave machines in the cluster. This file also specifies the various configurations that are necessary to install components such as ZooKeeper. The steps are listed as follows:</p></li><li class="listitem">Create the ZooKeeper configuration template as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vim mesos/templates/zoo.cfg.j2</strong></span>
</pre></div></li><li class="listitem">Then, add the following content:<div class="informalexample"><pre class="programlisting">  tickTime=2000
  dataDir=/var/lib/zookeeper/
  clientPort={{ zookeeper_client_port }}
  initLimit=5
  syncLimit=2
  {% for host in groups['mesos_masters'] %}
  server.{{ hostvars[host].zoo_id }}={{ host }}:{{ zookeeper_leader_port }}:{{ zookeeper_election_port }}
  {% endfor %}</pre></div></li><li class="listitem">Next, enter the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vim mesos/templates/zoo_id.j2</strong></span>
</pre></div></li><li class="listitem">Finally, add the following content:<div class="informalexample"><pre class="programlisting">{{ zoo_id }}</pre></div></li></ol></div><p>We can <a class="indexterm" id="id470"/>now run this playbook to deploy Mesos on the machines listed in the hosts file. We only need to change the IP addresses from the hosts file to deploy on other machines.</p></div></div></div>
<div class="section" title="Deploying and configuring Mesos cluster using Puppet"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec48"/>Deploying and configuring Mesos cluster using Puppet</h1></div></div></div><p>This <a class="indexterm" id="id471"/>portion will primarily cover how one can <a class="indexterm" id="id472"/>deploy a Mesos cluster with the Puppet configuration management tool using the ZooKeeper and Mesos modules located at the following repositories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/deric/puppet-mesos">https://github.com/deric/puppet-mesos</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://github.com/deric/puppet-zookeeper">https://github.com/deric/puppet-zookeeper</a></li></ul></div><p>Puppet<a class="indexterm" id="id473"/> is <a class="indexterm" id="id474"/>an open source<a class="indexterm" id="id475"/> configuration management tool that runs on Windows, Linux, and Mac OS. Puppet Labs was founded by Luke Kanies, who produced the Puppet, in 2005. It is written in Ruby and released as free software under the GNU General Public License (GPL) until version 2.7.0 and Apache License 2.0 after that. Using this, system administrators can automate the standard tasks that they need to run regularly. More information about Puppet can be found at the following location:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://puppetlabs.com/puppet/puppet-open-source">https://puppetlabs.com/puppet/puppet-open-source</a></li></ul></div><p>The code will be organized with the profiles and roles pattern, and the node data will be stored using Hiera. Hiera is a Puppet tool to perform a key/value lookup of the configuration data. It allows a hierarchical configuration of data in Puppet, which is difficult to achieve with native Puppet code. Also, it acts as a separator of configuration data and code.</p><p>At the end of this module, you will have a highly available Mesos cluster with three masters and three slaves. Along with this, Marathon and Chronos will also be deployed in the same fashion.</p><p>We can combine several Puppet modules to manage Mesos and ZooKeeper. Let's perform <a class="indexterm" id="id476"/>the following <a class="indexterm" id="id477"/>steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, create a <code class="literal">Puppetfile</code> with the following content:<div class="informalexample"><pre class="programlisting">forge 'http://forge.puppetlabs.com'
mod 'apt',
  :git =&gt; 'git://github.com/puppetlabs/puppetlabs-apt.git', :ref =&gt; '1.7.0'

mod 'concat',
  :git =&gt; 'https://github.com/puppetlabs/puppetlabs-concat', :ref =&gt; '1.1.2'

mod 'datacat',
  :git =&gt; 'https://github.com/richardc/puppet-datacat', :ref =&gt; '0.6.1'

mod 'java',
  :git =&gt; 'https://github.com/puppetlabs/puppetlabs-java', :ref =&gt; '1.2.0'

mod 'mesos',
  :git =&gt; 'https://github.com/deric/puppet-mesos', :ref =&gt; 'v0.5.2'

mod 'stdlib',
  :git =&gt; 'https://github.com/puppetlabs/puppetlabs-stdlib', :ref =&gt; '4.5.1'

mod 'zookeeper',
  :git =&gt; 'https://github.com/deric/puppet-zookeeper', :ref =&gt; 'v0.3.5'</pre></div><p>Now, we can write the profiles and roles pattern for both Mesos masters <a class="indexterm" id="id478"/>and slaves. On the <a class="indexterm" id="id479"/>master machines, it will also include managing ZooKeeper, Marathon, and Chronos.</p></li><li class="listitem">Create the following role for the masters:<div class="informalexample"><pre class="programlisting">class role::mesos::master {
  include profile::zookeeper
  include profile::mesos::master

  # Mesos frameworks
  include profile::mesos::master::chronos
  include profile::mesos::master::marathon
}</pre></div></li><li class="listitem">Next, create the following role for the slaves:<div class="informalexample"><pre class="programlisting">class role::mesos::slave {
  include profile::mesos::slave
}</pre></div><p>Now, we<a class="indexterm" id="id480"/> can go ahead <a class="indexterm" id="id481"/>and create the reusable profiles that match the include statements listed before in the roles. The <a class="indexterm" id="id482"/>profiles will contain the calls to the Mesos and ZooKeeper modules and any other <a class="indexterm" id="id483"/>resources that we need to manage. One can consider the roles as the business logic and the profiles as the actual implementation.</p></li><li class="listitem">Create the following profile for ZooKeeper:<div class="informalexample"><pre class="programlisting">class profile::zookeeper {
  include ::java
  class { '::zookeeper':
    require =&gt; Class['java'],
  }
}</pre></div></li><li class="listitem">Create the following profile for Mesos masters:<div class="informalexample"><pre class="programlisting">class profile::mesos::master {
  class { '::mesos':
    repo =&gt; 'mesosphere',
  }

  class { '::mesos::master':
    env_var =&gt; {
      'MESOS_LOG_DIR' =&gt; '/var/log/mesos',
    },
    require =&gt; Class['profile::zookeeper'],
  }
}</pre></div></li><li class="listitem">Next, create the following profile for Mesos slaves:<div class="informalexample"><pre class="programlisting">class profile::mesos::slave {
  class { '::mesos':
    repo =&gt; 'mesosphere',
  }
  class { '::mesos::slave':
    env_var =&gt; {
      'MESOS_LOG_DIR' =&gt; '/var/log/mesos',
    },
  }
}</pre></div><p>These <a class="indexterm" id="id484"/>are the basic things we <a class="indexterm" id="id485"/>need to launch a Mesos cluster. To manage Chronos and Marathon, the following profiles will also need to be included.</p></li><li class="listitem">Create a <a class="indexterm" id="id486"/>profile for Chronos, as follows:<div class="informalexample"><pre class="programlisting">class profile::mesos::master::chronos {
  package { 'chronos':
    ensure  =&gt; '2.3.2-0.1.20150207000917.debian77',
    require =&gt; Class['profile::mesos::master'],
  }

  service { 'chronos':
    ensure  =&gt; running,
    enable  =&gt; true,
    require =&gt; Package['chronos'],
  }
}</pre></div></li><li class="listitem">Now, create <a class="indexterm" id="id487"/>a profile for Marathon via the following code:<div class="informalexample"><pre class="programlisting">class profile::mesos::master::marathon {
  package { 'marathon':
    ensure  =&gt; '0.7.6-1.0',
    require =&gt; Class['profile::mesos::master'],
  }

  service { 'marathon':
    ensure  =&gt; running,
    enable  =&gt; true,
    require =&gt; Package['marathon'],
  }
}</pre></div><p>So far, the roles and profiles don't contain any information about the machines that <a class="indexterm" id="id488"/>we will use to set up the cluster. This information will come <a class="indexterm" id="id489"/>using Hiera. The Hiera data for master would look something similar to this:</p><div class="informalexample"><pre class="programlisting">---
classes:
  - role::mesos::master

mesos::master::options:
  quorum: '2'
mesos::zookeeper: 'zk://master1:2181,master2:2181,master3:2181/mesos'
zookeeper::id: 1
zookeeper::servers: ['master1:2888:3888', 'master2:2888:3888', 'master3:2888:3888']</pre></div><p>As we <a class="indexterm" id="id490"/>are setting up a highly <a class="indexterm" id="id491"/>available cluster, the master machines are named master 1, master 2, and master 3, respectively.</p></li><li class="listitem">Hiera data for slave would look something similar to the following:<div class="informalexample"><pre class="programlisting">---
classes:
  - role::mesos::slave

mesos::slave::checkpoint: true
mesos::zookeeper: 'zk://master1:2181,master2:2181,master3:2181/mesos'</pre></div></li></ol></div><p>Now, we can initiate a Puppet run on each of the machines to install and configure Mesos, ZooKeeper, Chronos, and Marathon.</p><p>The installation of the module is the same as for any Puppet module, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ puppet module install deric-mesos</strong></span>
</pre></div><p>Once executed successfully, we can expect that the Mesos package will be installed and the <code class="literal">mesos-master</code> service will be configured in the cluster.</p></div>
<div class="section" title="Deploying and configuring a Mesos cluster using SaltStack"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Deploying and configuring a Mesos cluster using SaltStack</h1></div></div></div><p>The <a class="indexterm" id="id492"/>SaltStack platform, or Salt, is a Python-based open source configuration management software and a remote execution engine. This <a class="indexterm" id="id493"/>module explains how we can use SaltStack to install a Mesos cluster with Marathon and a <a class="indexterm" id="id494"/>few other tools in production. SaltStack is <a class="indexterm" id="id495"/>an alternative to Puppet, Ansible, Chef, and others. As with the others, it is used to automate the deployment and configuration of <a class="indexterm" id="id496"/>software on multiple servers. The SaltStack architecture consists of one node as the SaltStack master and other nodes as the minions (slaves). There are also two different roles: one master role to perform cluster actions and a slave role to run the Docker containers.</p><p>The following packages will be installed for the role master:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">ZooKeeper</li><li class="listitem" style="list-style-type: disc">The Mesos master</li><li class="listitem" style="list-style-type: disc">Marathon</li><li class="listitem" style="list-style-type: disc">Consul</li></ul></div><p>The slave role will have the following packages installed:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Mesos slave</li><li class="listitem" style="list-style-type: disc">Docker</li><li class="listitem" style="list-style-type: disc">cAdvisor (used to export metrics to prometheus)</li><li class="listitem" style="list-style-type: disc">Registrator (used to register services with Consul)</li><li class="listitem" style="list-style-type: disc">Weave (provides an overlay network between containers)</li></ul></div><p>Now, let's take a look at how these components look in the cluster. The following figure shows all these components connected together in the cluster (source: <a class="ulink" href="https://github.com/Marmelatze/saltstack-mesos-test">https://github.com/Marmelatze/saltstack-mesos-test</a>):</p><div class="mediaobject"><img alt="Deploying and configuring a Mesos cluster using SaltStack" src="graphics/B05186_05_03.jpg"/></div><div class="section" title="SaltStack installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec87"/>SaltStack installation</h2></div></div></div><p>We need to <a class="indexterm" id="id497"/>install the Salt-Master to coordinate all the Salt-Minions. SaltStack <a class="indexterm" id="id498"/>requires the masters to be an odd number. One of these <a class="indexterm" id="id499"/>masters can be used as the Salt-Master, and the others will then become<a class="indexterm" id="id500"/> the minions. Let's follow the steps mentioned here to install SaltStack:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Execute the following command to set up the master and minion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -L https://bootstrap.saltstack.com -o install_salt.sh</strong></span>
<span class="strong"><strong>$ sudo sh install_salt.sh -U -M -P -A localhost</strong></span>
<span class="strong"><strong>#Clone the repository to /srv/salt this is where the configurations are kept.</strong></span>
<span class="strong"><strong>$ sudo git clone https://github.com/Marmelatze/saltstack-mesos-test /srv/salt</strong></span>
</pre></div></li><li class="listitem">Edit <a class="indexterm" id="id501"/>the <code class="literal">/etc/salt/master</code> file <a class="indexterm" id="id502"/>and change the configurations, as follows:<div class="informalexample"><pre class="programlisting">file_roots:
  base:
    - /srv/salt/salt

# ...
pillar_roots:
  base:
    - /srv/salt/pillar</pre></div><p>Now restart the master:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service salt-master restart</strong></span>
</pre></div></li><li class="listitem">Edit the <a class="indexterm" id="id503"/>minion configuration file located at <code class="literal">/etc/salt/minion </code>via the following code:<div class="informalexample"><pre class="programlisting"># ...
mine_interval: 5
mine_functions:
  network.ip_addrs:
    interface: eth0
  zookeeper:
    - mine_function: pillar.get
    - zookeeper</pre></div></li><li class="listitem">Now, edit <a class="indexterm" id="id504"/>the <code class="literal">salt-grains</code> file located at <code class="literal">/etc/salt/grains</code> by executing the following code:<div class="informalexample"><pre class="programlisting"># /etc/salt/grains
# Customer-Id this host is assigned to (numeric)-
customer_id: 0
# ID of this host.
host_id: ID

 # ID for zookeeper, only needed for masters.
zk_id: ID

# Available roles are master &amp; slave. Node can use both.
roles:
- master
- slave</pre></div></li><li class="listitem">Then, replace <a class="indexterm" id="id505"/>the ID with a numerical value starting from 1; this ID is similar to the ZooKeeper ID that we used earlier.<p>Now, restart the minion via the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service salt-minion restart</strong></span>
</pre></div></li><li class="listitem">Public<a class="indexterm" id="id506"/> key authentication is used for <a class="indexterm" id="id507"/>authentication between the minion and the master. Execute the following command to do so:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo salt-key -A</strong></span>
</pre></div></li><li class="listitem">Once <a class="indexterm" id="id508"/>we are done with the preceding steps, we can run SaltStack with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo salt '*' state.highstate</strong></span>
</pre></div></li></ol></div><p>If everything is successfully executed, then Mesos services will be up and running in the cluster.</p></div></div>
<div class="section" title="Deploying and configuring a Mesos cluster using Chef"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec50"/>Deploying and configuring a Mesos cluster using Chef</h1></div></div></div><p>Chef is <a class="indexterm" id="id509"/>both the name of a company and the name of a configuration management tool written in Ruby and Erlang. It uses a pure Ruby <a class="indexterm" id="id510"/>domain-specific language (DSL) to write system configuration "recipes". This module will <a class="indexterm" id="id511"/>explain how to install and configure the Apache Mesos master and slave using the Chef cookbook. Chef is a <a class="indexterm" id="id512"/>configuration management tool to automate large-scale server and software application deployments. We will assume that the reader is already familiar with Chef. The following repository will be used for reference:</p><p>
<a class="ulink" href="https://github.com/everpeace/cookbook-mesos">https://github.com/everpeace/cookbook-mesos</a>.</p><p>The Chef cookbook <a class="indexterm" id="id513"/>version at the time of writing this book supports the Ubuntu and CentOS operating systems. The CentOS version is experimental and is not recommended for use in a production environment. Ubuntu 14.04 or higher is required to make use of the cgroups isolator or Docker container features. Only Mesos 0.20.0 and later supports Docker containerization.</p><p>This cookbook supports installation in both ways—that is, building Mesos from source and from the Mesosphere package. By <a class="indexterm" id="id514"/>default, this cookbook builds Mesos from source. One can <a class="indexterm" id="id515"/>switch between the source and Mesosphere by setting the following type variable:</p><div class="informalexample"><pre class="programlisting">node[:mesos][:type]</pre></div><div class="section" title="Recipes"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec88"/>Recipes</h2></div></div></div><p>The following are the <a class="indexterm" id="id516"/>recipes used by this cookbook to install and configure Mesos:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">mesos::default</code>: This installs Mesos using the source or Mesosphere recipe, depending on the type variable discussed before.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos::build_from_source</code>: This installs Mesos in the usual way—that is, download zip from GitHub, configure, make, and install.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos::mesosphere</code>: This variable installs Mesos using Mesosphere's <code class="literal">mesos</code> package. Along with it, we can use the following variable to install the ZooKeeper package.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:mesosphere][:with_zookeeper]</code></li></ul></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos::master</code>: This configures the Mesos master and cluster deployment configuration files and uses <code class="literal">mesos-master</code> to start the service. The following are the variables associated with the configurations:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:prefix]/var/mesos/deploy/masters</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:prefix]/var/mesos/deploy/slaves</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:prefix]/var/mesos/deploy/mesos-deploy-env.sh</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:prefix]/var/mesos/deploy/mesos-master-env.sh</code></li></ul></div></li></ul></div><p>If we select <code class="literal">mesosphere</code> as the type to build, then the default ":" prefix attribute location will be <code class="literal">/usr/local</code> as the package from Mesosphere installs Mesos in this directory. This recipe also configures the upstart files at the following locations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/mesos/zk</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/defaults/mesos</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/defaults/mesos-master</code></li></ul></div></div><div class="section" title="Configuring mesos-master"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec89"/>Configuring mesos-master</h2></div></div></div><p>The <code class="literal">mesos-master</code> command-line <a class="indexterm" id="id517"/>parameters can be used to configure the <code class="literal">node[:mesos][:master]</code> attribute. An example is given here:</p><div class="informalexample"><pre class="programlisting">node[:mesos][:master] = {
  :port    =&gt; "5050",
  :log_dir =&gt; "/var/log/mesos",
  :zk      =&gt; "zk://localhost:2181/mesos",
  :cluster =&gt; "MesosCluster",
  :quorum  =&gt; "1"
}</pre></div><p>The <code class="literal">mesos-master</code> command will be invoked with the options given in the configuration as follows:</p><div class="informalexample"><pre class="programlisting">mesos-master --zk=zk://localhost:2181/mesos --port=5050 --log_dir=/var/log/mesos --cluster=MesosCluster</pre></div><p>The <code class="literal">mesos::slave</code> command provides configurations for the Mesos slave and starts the <code class="literal">mesos-slave</code> instance. We can use the following variable to point to the <code class="literal">mesos-slave-env.sh</code> file:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">node[:mesos][:prefix]/var/mesos/deploy/mesos-slave-env.sh</code></li></ul></div><p>The upstart configuration files for <code class="literal">mesos-slave</code> are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/mesos/zk</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/defaults/mesos</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">/etc/defaults/mesos-slave</code></li></ul></div></div><div class="section" title="Configuring mesos-slave"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec90"/>Configuring mesos-slave</h2></div></div></div><p>The <code class="literal">mesos-slave</code> command-line <a class="indexterm" id="id518"/>options can be configured using the <code class="literal">node[:mesos][:slave]</code> hash. An example configuration is given here:</p><div class="informalexample"><pre class="programlisting">node[:mesos][:slave] = {
  :master    =&gt; "zk://localhost:2181/mesos",
  :log_dir   =&gt; "/var/log/mesos",
  :containerizers =&gt; "docker,mesos",
  :isolation =&gt; "cgroups/cpu,cgroups/mem",
  :work_dir  =&gt; "/var/run/work"
}</pre></div><p>The <code class="literal">mesos-slave</code> command is invoked as follows:</p><div class="informalexample"><pre class="programlisting">mesos-slave --master=zk://localhost:2181/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --isolation=cgroups/cpu,cgroups/mem --work_dir=/var/run/work</pre></div><p>Now, let's <a class="indexterm" id="id519"/>take a look at how we can put all this together in a vagrant file and launch a standalone Mesos cluster. Create a <code class="literal">Vagrantfile</code> with the following contents:</p><div class="informalexample"><pre class="programlisting"># -*- mode: ruby -*-
# vi: set ft=ruby:
# vagrant plugins required:
# vagrant-berkshelf, vagrant-omnibus, vagrant-hosts
Vagrant.configure("2") do |config|
  config.vm.box = "Official Ubuntu 14.04 daily Cloud Image amd64 (Development release,  No Guest Additions)"
  config.vm.box_url = "https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-amd64-vagrant-disk1.box"

#  config.vm.box = "chef/centos-6.5"

  # enable plugins
  config.berkshelf.enabled = true
  config.omnibus.chef_version = :latest
  # if you want to use vagrant-cachier,
  # please activate below.
  config.cache.auto_detect = true

  # please customize hostname and private ip configuration if you need it.
  config.vm.hostname = "mesos"
  private_ip = "192.168.1.10"
  config.vm.network :private_network, ip: private_ip
  config.vm.provision :hosts do |provisioner|
    provisioner.add_host private_ip , [ config.vm.hostname ]
  end
  # for mesos web UI.
  config.vm.network :forwarded_port, guest: 5050, host: 5050
  config.vm.provider :virtualbox do |vb|
    vb.name = 'cookbook-mesos-sample-source'
    # Use VBoxManage to customize the VM. For example, to change memory:
    vb.customize ["modifyvm", :id, "--memory", "#{1024*4}"]
    vb.customize ["modifyvm", :id,  "--cpus",  "2"]
  end

  config.vm.provision :shell do |s|
        s.path = "scripts/populate_sshkey.sh"
        s.args = "/home/vagrant vagrant"
  end

  # mesos-master doesn't create its work_dir.
  config.vm.provision :shell, :inline =&gt; "mkdir -p /tmp/mesos"

  # Mesos master depends on zookeeper emsamble since 0.19.0
  # for Ubuntu
  config.vm.provision :shell, :inline =&gt; "apt-get update &amp;&amp; apt-get install -y zookeeper zookeeperd zookeeper-bin"
  # For CentOS
  # config.vm.provision :shell, :inline =&gt; &lt;&lt;-EOH
  #   rpm -Uvh http://archive.cloudera.com/cdh4/one-click-install/redhat/6/x86_64/cloudera-cdh-4-0.x86_64.rpm
  #   yum install -y -q curl
  #   curl -sSfL http://archive.cloudera.com/cdh4/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera --output /tmp/cdh.key
  #   rpm --import /tmp/cdh.key
  #   yum install -y -q java-1.7.0-openjdk zookeeper zookeeper-server
  #   service zookeeper-server init
  #   service zookeeper-server start
  # EOH

  config.vm.provision :chef_solo do |chef|
#    chef.log_level = :debug
    chef.add_recipe "mesos"
    chef.add_recipe "mesos::master"
    chef.add_recipe "mesos::slave"

    # You may also specify custom JSON attributes:
    chef.json = {
      :java =&gt; {
        'install_flavor' =&gt; "openjdk",
        'jdk_version' =&gt; "7",
      },
      :maven =&gt; {
        :version =&gt; "3",
        "3" =&gt; {
          :version =&gt; "3.0.5"
        },
        :mavenrc =&gt; {
          :opts =&gt; "-Dmaven.repo.local=$HOME/.m2/repository -Xmx384m -XX:MaxPermSize=192m"
        }
      },
      :mesos =&gt; {
        :home         =&gt; "/home/vagrant",
        # command line options for mesos-master
        :master =&gt; {
          :zk =&gt; "zk://localhost:2181/mesos",
          :log_dir =&gt; "/var/log/mesos",
          :cluster =&gt; "MesosCluster",
          :quorum  =&gt; "1"
        },
        # command line options for mesos-slave
        :slave =&gt;{
          :master =&gt; "zk://localhost:2181/mesos",
          :isolation =&gt; "posix/cpu,posix/mem",
          :log_dir =&gt; "/var/log/mesos",
          :work_dir =&gt; "/var/run/work"
        },
        # below ip lists are for mesos-[start|stop]-cluster.sh
        :master_ips =&gt; ["localhost"],
        :slave_ips  =&gt; ["localhost"]
      }
    }
  end
end</pre></div><p>Now, type in<a class="indexterm" id="id520"/> the following command to have a fully functional standalone Mesos cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vagrant up</strong></span>
</pre></div></div></div>
<div class="section" title="Deploying and configuring a Mesos cluster using Terraform"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec51"/>Deploying and configuring a Mesos cluster using Terraform</h1></div></div></div><p>Terraform<a class="indexterm" id="id521"/> is an<a class="indexterm" id="id522"/> infrastructure building, changing, and versioning tool to handle the existing popular services as <a class="indexterm" id="id523"/>well as custom in-house <a class="indexterm" id="id524"/>solutions safely and efficiently that is <a class="indexterm" id="id525"/>owned by HashiCorp and written in Go language. In this module, we will first discuss how we can install Terraform, and then, we will consider how we can use Terraform to spin up a Mesos cluster.</p><div class="section" title="Installing Terraform"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec91"/>Installing Terraform</h2></div></div></div><p>Head <a class="indexterm" id="id526"/>to <a class="ulink" href="https://www.terraform.io/downloads.html">https://www.terraform.io/downloads.html</a>, download the appropriate version for your platform, and unzip it, as follows:</p><div class="informalexample"><pre class="programlisting">$ wget https://releases.hashicorp.com/terraform/0.6.9/terraform_0.6.9_linux_amd64.zip

$ unzip terraform_0.6.9_linux_amd64.zip</pre></div><p>You will note that the <code class="literal">terraform</code> archive is a bunch of binaries once you unzip them, which looks similar to the following:</p><div class="mediaobject"><img alt="Installing Terraform" src="graphics/B05186_05_04.jpg"/></div><p>Now, add the path to the directory in the <code class="literal">PATH</code> variable so that you can access the <code class="literal">terraform</code> command from any directory.</p><p>If everything goes well, then you can see the usage of <code class="literal">terraform</code> once you execute the <code class="literal">terraform</code> command in the Terminal:</p><div class="mediaobject"><img alt="Installing Terraform" src="graphics/B05186_05_05.jpg"/></div></div><div class="section" title="Spinning up a Mesos cluster using Terraform on Google Cloud"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec92"/>Spinning up a Mesos cluster using Terraform on Google Cloud</h2></div></div></div><p>To spin up a <a class="indexterm" id="id527"/>Mesos cluster in Google Cloud Engine (GCE) with Terraform, you are required to have a JSON key file to authenticate. Head to <a class="ulink" href="https://console.developers.google.com">https://console.developers.google.com</a> and then generate a new JSON key by navigating to the <span class="strong"><strong>Credentials</strong></span> <span class="strong"><strong>|</strong></span> <span class="strong"><strong>Service</strong></span> account. A file will then be downloaded, which will be needed later to launch machines.</p><p>We can now create a <code class="literal">terraform</code> configuration file for our Mesos cluster. Create a <code class="literal">mesos.tf</code> file with the following contents:</p><div class="informalexample"><pre class="programlisting">module "mesos" {
  source = "github.com/ContainerSolutions/terraform-mesos"
  account_file = "/path/to/your/downloaded/key.json"
  project = "your google project"
  region = "europe-west1"
  zone = "europe-west1-d"
  gce_ssh_user = "user"
  gce_ssh_private_key_file = "/path/to/private.key"
  name = "mymesoscluster"
  masters = "3"
  slaves = "5"
  network = "10.20.30.0/24"
  domain = "example.com"
  image = "ubuntu-1404-trusty-v20150316"
  mesos_version = "0.22.1"
}</pre></div><p>As we <a class="indexterm" id="id528"/>can note, a few of these configurations can be used to control versions, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">mesos_version</code>: This specifies the version of Mesos</li><li class="listitem" style="list-style-type: disc"><code class="literal">image</code>: This is the Linux system image</li></ul></div><p>Now, execute the following commands to start the deployment:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Download the modules</strong></span>
<span class="strong"><strong>$ terraform get</strong></span>

<span class="strong"><strong># Create a terraform plan and save it to a file</strong></span>
<span class="strong"><strong>$ terraform plan -out my.plan -module-depth=1</strong></span>

<span class="strong"><strong># Create the cluster</strong></span>
<span class="strong"><strong>$ terraform apply my.plan</strong></span>
</pre></div></div><div class="section" title="Destroying the cluster"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec93"/>Destroying the cluster</h2></div></div></div><p>We can <a class="indexterm" id="id529"/>execute the following command to destroy the cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ terraform destroy</strong></span>
</pre></div></div></div>
<div class="section" title="Deploying and configuring a Mesos cluster using Cloudformation"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec52"/>Deploying and configuring a Mesos cluster using Cloudformation</h1></div></div></div><p>In this <a class="indexterm" id="id530"/>module, we <a class="indexterm" id="id531"/>will discuss how we can use Cloudformation scripts to launch a Mesos cluster on Amazon AWS. Before<a class="indexterm" id="id532"/> jumping into it, first make sure you install and configure aws-cli <a class="indexterm" id="id533"/>on the machines where you want to launch the cluster. Take a look at the instructions from the following repository to set up aws-cli:</p><p>
<a class="ulink" href="https://github.com/aws/aws-cli">https://github.com/aws/aws-cli</a>.</p><p>The next thing that we need after setting up aws-cli is the <code class="literal">cloudformation-zookeeper</code> template for an exhibitor-managed ZooKeeper cluster.</p><div class="section" title="Setting up cloudformation-zookeeper"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec94"/>Setting up cloudformation-zookeeper</h2></div></div></div><p>We first <a class="indexterm" id="id534"/>need to clone the following repository as it contains the JSON file that has the parameters, descriptors, and configuration values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/mbabineau/cloudformation-zookeeper</strong></span>
</pre></div><p>Log in to the AWS console and open up the following ports for security group:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SSH Port: 22</li><li class="listitem" style="list-style-type: disc">ZooKeeper Client Port: 2181</li><li class="listitem" style="list-style-type: disc">Exhibitor HTTP Port: 8181</li></ul></div><p>We can now use <code class="literal">aws-cli</code> to launch the cluster using the following command:</p><div class="informalexample"><pre class="programlisting">aws cloudformation create-stack \
  --template-body file://cloudformation-zookeeper/zookeeper.json \
  --stack-name &lt;stack&gt; \
  --capabilities CAPABILITY_IAM \
  --parameters \
    ParameterKey=KeyName,ParameterValue=&lt;key&gt; \
    ParameterKey=ExhibitorS3Bucket,ParameterValue=&lt;bucket&gt; \
    ParameterKey=ExhibitorS3Region,ParameterValue=&lt;region&gt; \
    ParameterKey=ExhibitorS3Prefix,ParameterValue=&lt;cluster_name&gt; \
    ParameterKey=VpcId,ParameterValue=&lt;vpc_id&gt; \
    ParameterKey=Subnets,ParameterValue='&lt;subnet_id_1&gt;\,&lt;subnet_id_2&gt;' \
    ParameterKey=AdminSecurityGroup,ParameterValue=&lt;sg_id&gt;</pre></div></div><div class="section" title="Using cloudformation-mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec95"/>Using cloudformation-mesos</h2></div></div></div><p>You can clone<a class="indexterm" id="id535"/> the project repository from the following URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/mbabineau/cloudformation-mesos</strong></span>
</pre></div><p>The project primarily includes three templates in the JSON format, which defines the parameters, configurations, and descriptors, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">mesos-master.json</code>: This is used to launch a set of Mesos masters, which run Marathon in an autoscaling group.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos-slave.json</code>: Similar to the preceding one, this launches a set of Mesos slaves in an autoscaling group.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos.json</code>: This creates both the <code class="literal">mesos-master</code> and <code class="literal">mesos-slave</code> stacks from the corresponding templates, which were listed previously. This is the general template used to launch a Mesos cluster.</li></ul></div><p>A few configurable properties from <code class="literal">master.json</code> are listed as follows:</p><div class="informalexample"><pre class="programlisting">"MasterInstanceCount" : {
    "Description" : "Number of master nodes to launch",
    "Type" : "Number",
    "Default" : "1"
  },
  "MasterQuorumCount" : {
    "Description" : "Number of masters needed for Mesos replicated log registry quorum (should be ceiling(&lt;MasterInstanceCount&gt;/2))",
    "Type" : "Number",
    "Default" : "1"
  },</pre></div><p>
<code class="literal">MasterInstanceCount</code> and <code class="literal">MasterQuorumCount</code> control the number of master machines <a class="indexterm" id="id536"/>that are required in the cluster. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting">"SlaveInstanceCount" : {
    "Description" : "Number of slave nodes to launch",
    "Type" : "Number",
    "Default" : "1"
  },</pre></div><p>Similarly, <code class="literal">SlaveInstanceCount</code> is used to control the number of slave instances in the cluster.</p><p>Cloudformation updates the autoscaling groups, and Mesos then transparently handles the scaling up by adding more nodes and scaling down by removing nodes. Take a look:</p><div class="informalexample"><pre class="programlisting">"SlaveInstanceType" : {
  "Description" : "EC2 instance type",
  "Type" : "String",
  "Default" : "t2.micro",
  "AllowedValues" : ["t2.micro", "t2.small", "t2.medium",
    "m3.medium", "m3.large", "m3.xlarge", "m3.2xlarge","c3.large", "c3.xlarge", "c3.2xlarge", "c3.4xlarge", "c3.8xlarge", "c4.large", "c4.xlarge", "c4.2xlarge", "c4.4xlarge", "c4.8xlarge","r3.large", "r3.xlarge", "r3.2xlarge", "r3.4xlarge", "r3.8xlarge","i2.xlarge", "i2.2xlarge", "i2.4xlarge", "i2.8xlarge","hs1.8xlarge", "g2.2xlarge"],
  "ConstraintDescription" : "must be a valid, HVM-compatible EC2 instance type."
  },</pre></div><p>We can also use the <code class="literal">InstanceType</code> configuration property for both the master (<code class="literal">MasterInstanceType</code>) and slave (<code class="literal">SlaveInstanceType</code>) to control the size of the machine in the AWS cloud.</p><p>Again, in <a class="indexterm" id="id537"/>the security group that we created earlier, open up the following ports for Mesos communication:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mesos master port: 5050</li><li class="listitem" style="list-style-type: disc">Marathon port : 8080</li></ul></div><p>Once we configure the values in the <code class="literal">mesos-master.json</code>, <code class="literal">mesos-slave.json</code> files, we can upload these files into S3 using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws s3 cp mesos-master.json s3://cloudformationbucket/</strong></span>
<span class="strong"><strong>$ aws s3 cp mesos-slave.json s3://cloudformationbucket/</strong></span>
</pre></div><p>We can now use aws-cli to launch our Mesos cluster with the following command:</p><div class="informalexample"><pre class="programlisting">aws cloudformation create-stack \
  --template-body file://mesos.json \
  --stack-name &lt;stack&gt; \
  --capabilities CAPABILITY_IAM \
  --parameters \
    ParameterKey=KeyName,ParameterValue=&lt;key&gt; \
    ParameterKey=ExhibitorDiscoveryUrl,ParameterValue=&lt;url&gt; \
    ParameterKey=ZkClientSecurityGroup,ParameterValue=&lt;sg_id&gt; \
    ParameterKey=VpcId,ParameterValue=&lt;vpc_id&gt; \
    ParameterKey=Subnets,ParameterValue='&lt;subnet_id_1&gt;\,&lt;subnet_id_2&gt;' \
    ParameterKey=AdminSecurityGroup,ParameterValue=&lt;sg_id&gt; \
    ParameterKey=MesosMasterTemplateUrl,ParameterValue=https://s3.amazonaws.com/cloudformationbucket/mesos-master.json \
    ParameterKey=MesosSlaveTemplateUrl,ParameterValue=https://s3.amazonaws.com/cloudformationbucket/mesos-slave.json</pre></div></div></div>
<div class="section" title="Creating test environments using Playa Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec53"/>Creating test environments using Playa Mesos</h1></div></div></div><p>Apache Mesos test <a class="indexterm" id="id538"/>environments can be quickly <a class="indexterm" id="id539"/>created using Playa Mesos. You can take a look at the official repository from the following URL:</p><p>
<a class="ulink" href="https://github.com/mesosphere/playa-mesos">https://github.com/mesosphere/playa-mesos</a>.</p><p>Before using this project, make sure in your environment that you install and configure VirtualBox, Vagrant, and an Ubuntu machine image that contains Mesos and Marathon preinstalled.</p><div class="section" title="Installations"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec96"/>Installations</h2></div></div></div><p>Follow the below instructions to get started with Playa Mesos</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Install VirtualBox</strong></span>: You <a class="indexterm" id="id540"/>can navigate to <a class="ulink" href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a> and download and install the appropriate version for your environment</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Install Vagrant</strong></span>: You <a class="indexterm" id="id541"/>can use the methods described in the <span class="emphasis"><em>Installing Aurora</em></span> section of <a class="link" href="ch04.html" title="Chapter 4. Service Scheduling and Management Frameworks">Chapter 4</a>, <span class="emphasis"><em>Service Scheduling and Management Frameworks</em></span>, to get started with Vagrant</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Playa</strong></span>: You<a class="indexterm" id="id542"/> can clone the repository with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/mesosphere/playa-mesos</strong></span>
<span class="strong"><strong># Make sure the tests are passed</strong></span>
<span class="strong"><strong>$ cd playa-mesos</strong></span>
<span class="strong"><strong>$ bin/test</strong></span>
<span class="strong"><strong># Start the environment</strong></span>
<span class="strong"><strong>$ vagrant up</strong></span>
</pre></div></li></ul></div><p>If everything<a class="indexterm" id="id543"/> goes well, we will be able to see the Mesos master Web UI by pointing the browser to <code class="literal">10.141.141.10:5050</code> and the Marathon Web UI from <code class="literal">10.141.141.10:8080</code>.</p><p>Once the <a class="indexterm" id="id544"/>machine is started, then we can use <code class="literal">ssh</code> to log in to the machine with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vagrant ssh</strong></span>
</pre></div><p>We can also use the following command to halt and terminate the test environment:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Halting the machine</strong></span>
<span class="strong"><strong>$ vagrant halt</strong></span>

<span class="strong"><strong>#Destroying the VM</strong></span>
<span class="strong"><strong>$ vagrant destroy</strong></span>
</pre></div><p>Apart from this, if you wish to tweak the configurations a bit, then you can do this by editing the <code class="literal">config.json</code> file located at the root of the <code class="literal">playa-mesos</code> repository.</p><p>We can use the following configuration properties in the <code class="literal">config.json</code> file:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">platform</code>: This is the virtualization platform. We will use VirtualBox, although VMware Fusion and VMware workstation can also be used.</li><li class="listitem" style="list-style-type: disc"><code class="literal">box_name</code>: This is the name of the vagrant instance.</li><li class="listitem" style="list-style-type: disc"><code class="literal">base_url</code>: This is the base URL where Vagrant images are stored.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ip_address</code>: This is the private network IP address of the VM.</li><li class="listitem" style="list-style-type: disc"><code class="literal">mesos_release</code>: This parameter is optional, which specifies the version of Mesos. This should be the full string returned by <code class="literal">apt-cache policy mesos</code>. An example would be<code class="literal"> 0.22.1-1.0.ubuntu1404</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">vm_ram</code>: This is the memory allocated to the Vagrant VM.</li><li class="listitem" style="list-style-type: disc"><code class="literal">vm_cpus</code>: This is the number of cores allocated to the Vagrant VM.</li></ul></div><p>We can create<a class="indexterm" id="id545"/> a sample <code class="literal">config.json</code> file <a class="indexterm" id="id546"/>by putting up all these configuration parameters, and it would look similar to the following:</p><div class="informalexample"><pre class="programlisting">{

  "platform": "virtualbox",

  "box_name": "playa_mesos_ubuntu_14.04_201601041324",

  "base_url": "http://downloads.mesosphere.io/playa-mesos",

  "ip_address": "10.141.141.10",

  "vm_ram": "2048",

  "vm_cpus": "2"

}</pre></div><p>As you can note here, we allocated 2,040 MB memory and two cores, and the machine will run on the <code class="literal">10.141.141.10</code> IP address.</p></div></div>
<div class="section" title="Monitoring the Mesos cluster using Nagios"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec54"/>Monitoring the Mesos cluster using Nagios</h1></div></div></div><p>Monitoring is<a class="indexterm" id="id547"/> a vital part of keeping the infrastructure <a class="indexterm" id="id548"/>up and running. Mesos integrates well with the existing monitoring solutions <a class="indexterm" id="id549"/>and has plugins for most monitoring solutions, such as <span class="strong"><strong>Nagios</strong></span>. This module will give you a walkthrough of how to install Nagios on your cluster and enable monitoring to send you e-mail alerts whenever something goes wrong in your cluster.</p><div class="section" title="Installing Nagios 4"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl3sec40"/>Installing Nagios 4</h2></div></div></div><p>The <a class="indexterm" id="id550"/>first thing we need to do before installing Nagios is to add an Nagios user to the system from which the Nagios process can run and send out alerts. We can create a new user and a new usergroup for Nagios by executing the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo useradd nagios</strong></span>
<span class="strong"><strong>$ sudo groupadd nagcmd</strong></span>
<span class="strong"><strong>$ sudo usermod -a -G nagcmd nagios</strong></span>
</pre></div><p>Here, we <a class="indexterm" id="id551"/>created a user, Nagios, and a user group, <code class="literal">nagcmd</code>, which is assigned to the user Nagios in the third command listed before.</p><p>Now, install the dependency packages with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get install build-essential libgd2-xpm-dev openssl libssl-dev xinetd apache2-utils unzip</strong></span>
</pre></div><p>Once the dependencies are installed and the user is added, we can start downloading and installing <code class="literal">nagios</code> by executing the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#Download the nagios archive.</strong></span>
<span class="strong"><strong>$ wget https://assets.nagios.com/downloads/nagioscore/releases/nagios-4.1.1.tar.gz</strong></span>
<span class="strong"><strong># Extract the archive.</strong></span>
<span class="strong"><strong>$ tar xvf nagios-*.tar.gz</strong></span>
<span class="strong"><strong># Change the working directory to nagios</strong></span>
<span class="strong"><strong>$ cd nagios*</strong></span>

<span class="strong"><strong># Configure and build nagios</strong></span>
<span class="strong"><strong>$ ./configure --with-nagios-group=nagios --with-command-group=nagcmd --with-mail=/usr/sbin/sendmail</strong></span>
<span class="strong"><strong>$ make all</strong></span>

<span class="strong"><strong># Install nagios, init scripts and sample configuration file</strong></span>
<span class="strong"><strong>$ sudo make install</strong></span>
<span class="strong"><strong>$ sudo make install-commandmode</strong></span>
<span class="strong"><strong>$ sudo make install-init</strong></span>
<span class="strong"><strong>$ sudo make install-config</strong></span>
<span class="strong"><strong>$ sudo /usr/bin/install -c -m 644 sample-config/httpd.conf /etc/apache2/sites-available/nagios.conf</strong></span>
</pre></div><p>Once <code class="literal">nagios</code> is installed, we can install the <code class="literal">nagios</code> plugin by downloading and building it by issuing the following commands:</p><div class="informalexample"><pre class="programlisting">$ wget http://nagios-plugins.org/download/nagios-plugins-2.1.1.tar.gz

$ tar xvf nagios-plugins-*.tar.gz

$ cd nagios-plugins-*

$ ./configure --with-nagios-user=nagios --with-nagios-group=nagios --with-openssl

$ make

$ sudo make install</pre></div><p>Once the <a class="indexterm" id="id552"/>plugins are installed, we can install <span class="strong"><strong>NRPE</strong></span> (<span class="strong"><strong>Nagios Remote Plugin Executor</strong></span>) to<a class="indexterm" id="id553"/> take status updates from remote machines. It can be installed by executing the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://downloads.sourceforge.net/project/nagios/nrpe-2.x/nrpe-2.15/nrpe-2.15.tar.gz</strong></span>

<span class="strong"><strong>$ tar xf nrpe*.tar.gz</strong></span>
<span class="strong"><strong>$ cd nrpe*</strong></span>

<span class="strong"><strong>$ ./configure --enable-command-args --with-nagios-user=nagios --with-nagios-group=nagios --with-ssl=/usr/bin/openssl --with-ssl-lib=/usr/lib/x86_64-linux-gnu</strong></span>

<span class="strong"><strong>$ make all</strong></span>
<span class="strong"><strong>$ sudo make install</strong></span>
<span class="strong"><strong>$ sudo make install-xinetd</strong></span>
<span class="strong"><strong>$ sudo make install-daemon-config</strong></span>
</pre></div><p>For security reasons, edit the file located at <code class="literal">/etc/xinetd.d/nrpe</code> with the following content:</p><div class="informalexample"><pre class="programlisting">only_from = 127.0.0.1 10.132.224.168</pre></div><p>Replace the IP address with our <code class="literal">nagios</code> server IP address so that only our <code class="literal">nagios</code> server is able to make remote calls. Once this is done, save the file and exit and then restart the <code class="literal">xintend</code> service by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service xinetd restart</strong></span>
</pre></div><p>Now that <code class="literal">nagios</code> is installed, we can configure the contact e-mail addresses to which the notifications will be sent by editing the following file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi /usr/local/nagios/etc/objects/contacts.cfg</strong></span>
</pre></div><p>Find and replace the following line with your own e-mail address:</p><div class="informalexample"><pre class="programlisting">email nagios@localhost ; &lt;&lt; ** Change this to your email address **</pre></div><p>Add a user to <code class="literal">nagios</code> so that we can log in from the browser and see the activities by executing the following command. Here, we used <code class="literal">nagiosadmin</code> as the username and password, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin</strong></span>
</pre></div><p>Now, restart the <code class="literal">nagios</code> service by issuing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service nagios restart</strong></span>
</pre></div><p>We can now log in to the <code class="literal">nagios</code> admin panel by accessing the following URL from the browser:</p><p>
<code class="literal">http://MachineIP/nagios</code>
</p><p>
<code class="literal">MachineIP</code> is the <a class="indexterm" id="id554"/>IP address of the machine on which we installed <code class="literal">nagios</code>, and it will prompt you with an authentication form in which you can give the username and password as <code class="literal">nagiosadmin</code>.</p><div class="mediaobject"><img alt="Installing Nagios 4" src="graphics/B05186_05_06.jpg"/></div><p>Upon authentication, you will reach the Nagios home page. To view which hosts are being monitored by Nagios, click on the <span class="strong"><strong>Hosts</strong></span> link present on the left-hand side, as shown in the following screenshot (source: <a class="ulink" href="https://www.digitalocean.com/community/tutorials/how-to-install-nagios-4-and-monitor-your-servers-on-ubuntu-14-04">https://www.digitalocean.com/community/tutorials/how-to-install-nagios-4-and-monitor-your-servers-on-ubuntu-14-04</a>):</p><div class="mediaobject"><img alt="Installing Nagios 4" src="graphics/B05186_05_07.jpg"/></div><p>Now, we will discuss how we can use NRPE to monitor the nodes in our Mesos cluster.</p><p>The <a class="indexterm" id="id555"/>following section will add a machine to Nagios to monitor, and we can repeat the same steps to add as many machines as required. For the time being, we will choose the Mesos master node to be monitored, and it will trigger an e-mail if the disk usage for a particular drive exceeds the given amount.</p><p>Now, on the master machine, install the Nagios plugins and <code class="literal">nrpe-server</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get install nagios-plugins nagios-nrpe-server</strong></span>
</pre></div><p>As mentioned earlier, for security reasons, edit the <code class="literal">/etc/nagios/nrpe.cfg</code> file and put the <code class="literal">nagios</code> server IP address for communication under the <code class="literal">allowed_hosts</code> property.</p><p>Now, edit the <code class="literal">nrpe</code> configuration file with the properties to monitor the disk usage via the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi /etc/nagios/nrpe.cfg</strong></span>
</pre></div><p>Then, add the following content:</p><div class="informalexample"><pre class="programlisting">server_address=client_private_IP
allowed_hosts=nagios_server_private_IP
command[check_hda1]=/usr/lib/nagios/plugins/check_disk -w 20% -c 10% -p /dev/vda</pre></div><p>Here, <code class="literal">server_address</code> is the IP address of the machine, <code class="literal">allowed_hosts</code> is the <code class="literal">nagios</code> server address, and the command is the actual command to pull the disk usage. We used the <code class="literal">check_disk</code> plugin that comes with <code class="literal">nagios</code> and passes the arguments to the command as <code class="literal">-w 20%</code> <code class="literal">-c 10%</code>. Whenever the server exceeds 20% disk usage, Nagios will trigger an e-mail alert.</p><p>Once we <a class="indexterm" id="id556"/>edit the file, restart the <code class="literal">nrpe</code> server with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service nagios-nrpe-server restart</strong></span>
</pre></div><p>Now that we have configured the Mesos master to check the disk usage, we also need to add this Mesos master to the <code class="literal">nagios</code> server so that it can keep on checking the disk usage and alert the administrators when it exceeds the quota.</p><p>Add a new configuration file on the <code class="literal">nagios</code> server to monitor, and we can add the file in <code class="literal">/usr/local/nagios/etc/servers/</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi /usr/local/nagios/etc/servers/mesos-master.cfg</strong></span>
</pre></div><p>Then, add the following content:</p><div class="informalexample"><pre class="programlisting">    define host {
            use                             linux-server
            host_name                       mesos-master
            alias                           Mesos master server
            address                         10.132.234.52
            max_check_attempts              5
            check_period                    24x7
            notification_interval           30
            notification_period             24x7
    }</pre></div><p>This configuration will keep monitoring the Mesos master machine to check whether it is still running. The administrator (or others, as specified in the e-mail list) will get an e-mail notification if the Mesos master machine goes down.</p><p>We can also enable the network usage checking by adding the following service:</p><div class="informalexample"><pre class="programlisting">    define service {
            use                             generic-service
            host_name                       mesos-master
            service_description             PING
            check_command                   check_ping!100.0,20%!500.0,60%

    }</pre></div><p>Once we set the configurations for the new host, we need to restart <code class="literal">nagios</code> by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo service nagios reload</strong></span>
</pre></div><p>We can<a class="indexterm" id="id557"/> create new configuration files for slave nodes, as well, by following the steps listed previously.</p></div></div>
<div class="section" title="Monitoring the Mesos cluster using Satellite"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Monitoring the Mesos cluster using Satellite</h1></div></div></div><p>Satellite <a class="indexterm" id="id558"/>is another <a class="indexterm" id="id559"/>tool for monitoring Mesos, and the Satellite project is maintained by Two Sigma Investments, which<a class="indexterm" id="id560"/> is written in Clojure. The Satellite master instance monitors the Mesos masters and receives the monitoring information from Mesos slaves through the Satellite slaves. For each Mesos master and Mesos slave, there exists a Satellite master and slave process, with the <code class="literal">satellite-slave</code> processes sending one type of message to all <code class="literal">satellite-masters</code> in the cluster.</p><p>Aggregate statistics of the cluster, such as the utilization of resources, the number of lost tasks and events specific to the master, such as how many leaders are currently active and so on, are usually pulled. Satellite also provides a <a class="indexterm" id="id561"/>
<span class="strong"><strong>Representational State Transfer</strong></span> (<span class="strong"><strong>REST)</strong></span> interface to interact with the Mesos master whitelist. Whitelists are text files that contain the list of hosts to which the master will consider sending tasks. It also provides a REST interface to access the cached Mesos tasks metadata. Satellite itself never caches any of this information and only provides an interface to retrieve it if it is cached. This is an optional feature, but it is useful if we have persisted tasks metadata within Mesos itself.</p><p>Satellite adds two additional conceptual whitelists to the mix:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Managed whitelist</strong></span>: These<a class="indexterm" id="id562"/> are the hosts that are are entered automatically</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Manual whitelist</strong></span>: If a <a class="indexterm" id="id563"/>host is present in this whitelist, then its status overrides that in the managed whitelist discussed previously. These are the REST endpoints taking the <code class="literal">PUT</code> and <code class="literal">DELETE</code> requests.</li></ul></div><p>At intervals, a merge operation actually merges these two into the whitelist file.</p><div class="section" title="Satellite installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec98"/>Satellite installation</h2></div></div></div><p>Satellite<a class="indexterm" id="id564"/> needs to be installed on all the machines that are present in the Mesos cluster. We will need to install <code class="literal">satellite-master</code> on the Mesos master machines and the <code class="literal">satellite-slave</code> on the Mesos slave machines. Run the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Install lein on all the machines</strong></span>
<span class="strong"><strong>$ wget https://raw.githubusercontent.com/technomancy/leiningen/stable/bin/lein</strong></span>
<span class="strong"><strong>$ chmod +x lein</strong></span>
<span class="strong"><strong>$ export PATH=$PATH:/path/to/lein</strong></span>

<span class="strong"><strong># Clone the satellite repository</strong></span>
<span class="strong"><strong>$ git clone https://github.com/twosigma/satellite</strong></span>

<span class="strong"><strong># Compile the satellite-master jar</strong></span>
<span class="strong"><strong>$ cd satellite/satellite-master</strong></span>
<span class="strong"><strong>$ lein release-jar</strong></span>
</pre></div><p>The preceding command will create a <code class="literal">jar</code> in the target directory, which we can copy to all the Mesos master machines.</p><p>Executing the following command by passing the configuration will run the satellite process on the machines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ java -jar ./target/satellite.jar ./config/satellite-config.clj</strong></span>
</pre></div></div></div>
<div class="section" title="Common deployment issues and solutions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec56"/>Common deployment issues and solutions</h1></div></div></div><p>This module <a class="indexterm" id="id565"/>contains a few common issues that are faced while installing or setting up the tools and modules described in this chapter:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For the Ansible python-setup tools, take a look at the following screenshot:<div class="mediaobject"><img alt="Common deployment issues and solutions" src="graphics/B05186_05_08.jpg"/></div><p>If your Ansible installation shows the preceding message, then execute the following command to resolve it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo pip install setuptools</strong></span>
</pre></div></li><li class="listitem">SSH <a class="indexterm" id="id566"/>runs on a different port, and <code class="literal">nagios</code> shows a <code class="literal">connection refused</code> error.<p>You will get the following exception if you run your <code class="literal">ssh</code> server on a different port:</p><div class="informalexample"><pre class="programlisting">SERVICE ALERT: localhost;SSH;CRITICAL;HARD;4;Connection refused</pre></div><p>This can be fixed by editing the following line from <code class="literal">/etc/nagios/conf.d/services_nagios.cfg</code>:</p><div class="informalexample"><pre class="programlisting"># check that ssh services are running
define service {
  hostgroup_name          ssh-servers
  service_description     SSH
  check_command           check_ssh_port!6666!server
  use                     generic-service
  notification_interval   0 ; set &gt; 0 if you want to be renotified</pre></div><p>Here, we used <code class="literal">6666</code> as the <code class="literal">ssh</code> port, rather than <code class="literal">22</code>, to get rid of the error message.</p></li><li class="listitem">Chef fails to unzip packages.<p>Sometimes, the Chef setup fails to retrieve the packages and gives the following error stack:</p><div class="informalexample"><pre class="programlisting">        ==&gt; master1: [2015-12-25T22:28:39+00:00] INFO: Running queued delayed notifications before re-raising exception
        ==&gt; master1: [2015-12-25T22:28:39+00:00] ERROR: Running exception handlers
        ==&gt; master1: [2015-12-25T22:28:39+00:00] ERROR: Exception handlers complete
        ==&gt; master1: [2015-12-25T22:28:39+00:00] FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out
        ==&gt; master1: [2015-12-25T22:28:39+00:00] ERROR: packageunzip had an error: Mixlib::ShellOut::ShellCommandFailed: Expected process to exit with [0], but received '100'
        ==&gt; master1: ---- Begin output of apt-get -q -y install unzip=6.0-8ubuntu2 ----
        ==&gt; master1: STDOUT: Reading package lists...
        ==&gt; master1: Building dependency tree...
        ==&gt; master1: Reading state information...
        ==&gt; master1: The following packages were automatically installed and are no longer required:
        ==&gt; master1: erubis ohai ruby-bunny ruby-erubis ruby-highline ruby-i18n ruby-ipaddress
        ==&gt; master1: ruby-mime-types ruby-mixlib-authentication ruby-mixlib-cli
        ==&gt; master1: ruby-mixlib-config ruby-mixlib-log ruby-mixlib-shellout ruby-moneta
        ==&gt; master1: ruby-net-ssh ruby-net-ssh-gateway ruby-net-ssh-multi ruby-polyglot
        ==&gt; master1: ruby-rest-client ruby-sigar ruby-systemu ruby-treetop ruby-uuidtools
        ==&gt; master1: ruby-yajl
        ==&gt; master1: Use 'apt-get autoremove' to remove them.
        ==&gt; master1: Suggested packages:
        ==&gt; master1: zip
        ==&gt; master1: The following NEW packages will be installed:
        ==&gt; master1: unzip
        ==&gt; master1: 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
        ==&gt; master1: Need to get 192 kB of archives.
        ==&gt; master1: After this operation, 394 kB of additional disk space will be used.
        ==&gt; master1: WARNING: The following packages cannot be authenticated!
        ==&gt; master1: unzip
        ==&gt; master1: STDERR: E: There are problems and -y was used without --force-yes
        ==&gt; master1: ---- End output of apt-get -q -y install unzip=6.0-8ubuntu2 ----
        ==&gt; master1: Ran apt-get -q -y install unzip=6.0-8ubuntu2 returned 100
        ==&gt; master1: [2015-12-25T22:28:40+00:00] FATAL: Chef::Exceptions::ChildConvergeError: Chef run process exited unsuccessfully (exit code 1)
        Chef never successfully completed! Any errors should be visible in the
        output above. Please fix your recipes so that they properly complete.</pre></div><p>This error shows up when your apt's key is old. To resolve this issue, you need to update the keys by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-key update</strong></span>
</pre></div></li><li class="listitem">ZooKeeper<a class="indexterm" id="id567"/> throws an error "no masters are currently leading".<p>This is a known ZooKeeper error due to the incorrect configuration of the ZooKeeper properties. We can resolve this error by editing the ZooKeeper configuration file located at <code class="literal">/etc/zookeeper/conf/zoo.cfg</code> properly. Add the following properties beside the server IP listed in the file:</p><div class="informalexample"><pre class="programlisting">tickTime=2000
dataDir=/var/zookeeper
clientPort=2181</pre></div></li></ol></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec57"/>Summary</h1></div></div></div><p>After reading this chapter, you should now be able to use any standard deployment tool to launch and configure a Mesos cluster on a distributed infrastructure. You would also be able to understand various security, multitenancy, and maintenance features supported by Mesos and learn how to implement them for production-grade setups.</p><p>In the next chapter, we will explore Mesos frameworks in greater detail. We will discuss the various features of frameworks and the process of porting an existing framework on Mesos and understand how to develop custom frameworks on top of Mesos to address specific applications.</p></div></body></html>