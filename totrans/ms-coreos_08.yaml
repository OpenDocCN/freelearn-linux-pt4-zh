- en: Chapter 8. Container Orchestration
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章 容器编排
- en: As Containers became the basis of modern application development and deployment,
    it is necessary to deploy hundreds or thousands of Containers to a single data
    center cluster or data center clusters. The cluster could be an on-premises cluster
    or in a cloud. It is necessary to have a good Container orchestration system to
    deploy and manage Containers at scale.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着容器成为现代应用开发和部署的基础，部署数百个或数千个容器到单一数据中心集群或数据中心集群变得必要。集群可以是本地集群或云端集群。为了在大规模下部署和管理容器，必须有一个良好的容器编排系统。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: The basics of modern application deployment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代应用部署的基础
- en: Container orchestration with Kubernetes, Docker Swarm, and Mesos and their core
    concepts, installation, and deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes、Docker Swarm 和 Mesos 进行容器编排及其核心概念、安装和部署
- en: Comparison of popular orchestration solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的编排解决方案比较
- en: Application definition with Docker Compose
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Compose 进行应用定义
- en: Packaged Container Orchestration solutions—the AWS container service, Google
    container engine, and CoreOS Tectonic
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打包的容器编排解决方案——AWS 容器服务、Google 容器引擎和 CoreOS Tectonic
- en: Modern application deployment
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用部署
- en: We covered the basics of Microservices in [Chapter 1](index_split_023.html#filepos77735),
    CoreOS Overview. In cloud-based application development, infrastructure is treated
    as cattle rather than pet ([http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story](http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story)).
    What this means is that the infrastructure is commonly a commodity hardware that
    can easily go bad and high availability needs to be handled at either the application
    layer or application Orchestration layer. High availability can be taken care
    of by having a combination of the load balancer and Orchestration system that
    monitors the health of services taking necessary actions such as respawning the
    service if it dies. Containers have the nice property of isolation and packaging
    that allows independent teams to develop individual components as Containers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](index_split_023.html#filepos77735)中介绍了微服务的基础，CoreOS 概述。在基于云的应用开发中，基础设施被视为“牲畜”而非“宠物”([http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story](http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story))。这意味着基础设施通常是商品化的硬件，可能会出现故障，高可用性需要在应用层或应用编排层进行处理。高可用性可以通过负载均衡器和编排系统的组合来实现，这些系统监控服务的健康状况，采取必要的行动，例如在服务崩溃时重新启动服务。容器具有隔离和封装的优良特性，使得独立的团队能够作为容器开发各自的组件。
- en: 'Companies can adopt a pay-as-you-grow model where they can scale their Containers
    as they grow. It is necessary to manage hundreds or thousands of Containers at
    scale. To do this efficiently, we need a Container Orchestration system. The following
    are some characteristics of a Container Orchestration system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 企业可以采用按需增长模式，根据需要扩展容器。在大规模管理数百或数千个容器时，必须使用容器编排系统。以下是容器编排系统的一些特点：
- en: It treats disparate infrastructure hardware as a collection and represents it
    as one single resource to the application
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将不同的基础设施硬件视为一个集合，并将其表示为应用程序的单一资源
- en: It schedules Containers based on user constraints and uses the infrastructure
    in the most efficient manner
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它根据用户约束调度容器，并以最有效的方式利用基础设施
- en: It scales out containers dynamically
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它动态地扩展容器
- en: It maintains high availability of services
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它保持服务的高可用性
- en: 'There is a close relation between the application definition and Container
    Orchestration. The application definition is typically a manifest file describing
    the Containers that are part of the application and the services that the Container
    exposes. Container Orchestration is done based on the application definition.
    The Container Orchestrator operates on resources that could be a VM or bare metal.
    Typically, the nodes where Containers run are installed with Container-optimized
    OSes, such as CoreOS, DCOS, and Atomic. The following image shows you the relationship
    between the application definition, Container Orchestration, and Container-optimized
    nodes along with some examples of solutions in each category:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序定义与容器编排之间有着密切的关系。应用程序定义通常是一个清单文件，描述了应用程序中的容器以及容器暴露的服务。容器编排是基于应用程序定义进行的。容器编排器在资源上运行，这些资源可以是虚拟机或裸机。通常，容器运行的节点安装有容器优化的操作系统，例如
    CoreOS、DCOS 和 Atomic。以下图像展示了应用程序定义、容器编排和容器优化节点之间的关系，并给出了每个类别中一些解决方案的示例：
- en: '![](img/00458.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00458.jpg)'
- en: Container Orchestration
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排
- en: A basic requirement of Container orchestration is to efficiently deploy M containers
    into N compute resources.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排的基本要求是高效地将 M 个容器部署到 N 个计算资源中。
- en: 'The following are some problems that a Container Orchestration system should
    solve:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是容器编排系统应该解决的一些问题：
- en: It should schedule containers efficiently, giving enough control to the user
    to tweak scheduling parameters based on their need
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该高效地调度容器，并为用户提供足够的控制，以便根据需要调整调度参数
- en: It should provide Container networking across the cluster
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该提供跨集群的容器网络
- en: Services should be able to discover each other dynamically
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务应该能够动态发现彼此
- en: Orchestration system should be able to handle service failures
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排系统应该能够处理服务故障
- en: We will cover Kubernetes, Docker Swarm, and Mesos in the following sections.
    Fleet is used internally by CoreOS for Container orchestration. Fleet has very
    minimal capabilities and works well for the deployment of critical system services
    in CoreOS. For very small deployments, Fleet can be used for Container orchestration,
    if necessary.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中介绍 Kubernetes、Docker Swarm 和 Mesos。Fleet 是 CoreOS 内部用于容器编排的工具。Fleet
    功能非常简约，适合在 CoreOS 中部署关键系统服务。对于非常小的部署，如果需要，也可以使用 Fleet 进行容器编排。
- en: Kubernetes
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is an open source platform for Container Orchestration. This was
    initially started by Google and now multiple vendors are working together in this
    open source project. Google has used Containers to develop and deploy applications
    in their internal data center and they had a system called Borg ([http://research.google.com/pubs/pub43438.html](http://research.google.com/pubs/pub43438.html))
    for cluster management. Kubernetes uses a lot of the concepts from Borg combined
    with modern technologies available now. Kubernetes is lightweight, works across
    almost all environments, and has a lot of industry traction currently.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源的容器编排平台。最初由谷歌发起，现在多个供应商正在共同参与这个开源项目。谷歌一直使用容器来开发和部署应用程序，且在其内部数据中心有一个叫
    Borg的系统（[http://research.google.com/pubs/pub43438.html](http://research.google.com/pubs/pub43438.html)）用于集群管理。Kubernetes
    借鉴了 Borg 的许多概念，并结合了现有的现代技术。Kubernetes 轻量级，能够在几乎所有环境中运行，目前在行业中有着广泛的应用。
- en: Concepts of Kubernetes
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的概念
- en: Kubernetes has some unique concepts, and it will be good to understand them
    before diving deep into the architecture of Kubernetes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有一些独特的概念，深入了解这些概念会对理解 Kubernetes 的架构非常有帮助。
- en: Pods
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Pods
- en: Pods are a set of Containers that are scheduled together in a single node and
    need to work closely with each other. All containers in a Pod share the IPC namespace,
    network namespace, UTS namespace, and PID namespace. By sharing the IPC namespace,
    Containers can use IPC mechanisms to talk to each other.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是一组在同一个节点上调度并需要紧密协作的容器。Pod 中的所有容器共享 IPC 命名空间、网络命名空间、UTS 命名空间和 PID 命名空间。通过共享
    IPC 命名空间，容器可以使用 IPC 机制相互通信。
- en: 'By sharing the network namespace, Containers can use sockets to talk to each
    other, and all Containers in a Pod share a single IP address. By sharing the UTS
    namespace, volumes can be mounted to a Pod and all Containers can see these volumes.
    The following are some common application deployment patterns with Pods:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过共享网络命名空间，容器可以使用套接字相互通信，并且Pod中的所有容器共享一个IP地址。通过共享UTS命名空间，卷可以挂载到Pod上，所有容器都可以访问这些卷。以下是一些常见的应用部署模式与Pods结合使用的例子：
- en: 'Sidecar pattern: An example is an application container and logging container
    or application synchronizer container such as a Git synchronizer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sidecar模式：一个例子是应用容器和日志容器，或者像Git同步器这样的应用同步容器。
- en: 'Ambassador pattern: In this pattern, the application container and proxy container
    work together. When the application container changes, external services can still
    talk to the proxy container as before. An example is a redis application container
    with the redis proxy.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ambassador模式：在这种模式下，应用容器和代理容器一起工作。当应用容器发生变化时，外部服务仍然可以像以前一样与代理容器通信。一个例子是一个带有redis代理的redis应用容器。
- en: 'Adapter pattern: In this pattern, there is an application container and adapter
    container that adapts to different environments. An example is a logging container
    that works as an adapter and changes with different cloud providers but the interface
    to the adapter container remains the same.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adapter模式：在这种模式下，存在一个应用容器和适配器容器，适配不同的环境。一个例子是一个日志容器，它作为适配器，随着不同云服务提供商的变化而变化，但适配器容器的接口保持不变。
- en: The smallest unit in Kubernetes is a Pod and Kubernetes takes care of scheduling
    the Pods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中最小的单元是Pod，Kubernetes负责调度Pods。
- en: 'The following is a Pod definition example with the NGINX Container and Git
    helper container:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个包含NGINX容器和Git助手容器的Pod定义示例：
- en: '`apiVersion: v1 kind: Pod metadata:   name: www spec:   containers:   - name: nginx
        image: nginx   - name: git-monitor     image: kubernetes/git-monitor     env:
        - name: GIT_REPO       value: http://github.com/some/repo.git`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: www spec:   containers:   - name: nginx
        image: nginx   - name: git-monitor     image: kubernetes/git-monitor     env:
        - name: GIT_REPO       value: http://github.com/some/repo.git`'
- en: Networking
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 网络
- en: Kubernetes has the one IP per Pod approach. This approach was taken to avoid
    the pains associated with NAT to access Container services when Containers shared
    the host IP address. All Containers in a pod share the same IP address. Pods across
    nodes can talk to each other using different techniques such as cloud-based routing
    from cloud providers, Flannel, Weave, Calico, and others. The end goal is to have
    Networking as a plugin within Kubernetes and the user can choose the plugin based
    on their needs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes采用每个Pod一个IP的方式。这种方式避免了容器共享主机IP地址时，使用NAT访问容器服务的痛苦。Pod中的所有容器共享相同的IP地址。跨节点的Pods可以使用不同的技术进行通信，例如云提供商的云路由、Flannel、Weave、Calico等。最终目标是将网络作为Kubernetes中的插件，用户可以根据需求选择插件。
- en: Services
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 服务
- en: Services are an abstraction that Kubernetes provides to logically combine Pods
    that provide similar functionality. Typically, Labels are used as selectors to
    create services from Pods. As Pods are ephemeral, Kubernetes creates a service
    object with its own IP address that always remains permanent. Kubernetes takes
    care of load balancing for multiple pods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是Kubernetes提供的一种抽象，用于逻辑上组合提供相似功能的Pods。通常，标签（Labels）作为选择器，用来从Pods创建服务。由于Pods是短暂的，Kubernetes会创建一个服务对象，并为其分配一个永远保持不变的IP地址。Kubernetes负责多个Pod的负载均衡。
- en: 'The following is an example service:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例服务：
- en: '`{     "kind": "Service",     "apiVersion": "v1",     "metadata": {         "name": "my-service"
        },     "spec": {         "selector": {             "app": "MyApp"         },
            "ports": [             {                 "protocol": "TCP",                 "port": 80,
                    "targetPort": 9376             }         ]     } }`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`{     "kind": "Service",     "apiVersion": "v1",     "metadata": {         "name": "my-service"
        },     "spec": {         "selector": {             "app": "MyApp"         },
            "ports": [             {                 "protocol": "TCP",                 "port": 80,
                    "targetPort": 9376             }         ]     } }`'
- en: In the preceding example, we created a `my-service` service that groups all
    pods with a `Myapp` label. Any request to the `my-service` service's IP address
    and port number `80` will be load balanced to all the pods with the `Myapp` label
    and redirected to port number `9376`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了一个`my-service`服务，将所有带有`Myapp`标签的Pod组合在一起。任何访问`my-service`服务的IP地址和端口号`80`的请求都会被负载均衡到所有带有`Myapp`标签的Pods，并被重定向到端口号`9376`。
- en: Services need to be discovered internally or externally based on the type of
    service. An example of internal discovery is a web service needing to talk to
    a database service. An example of external discovery is a web service that gets
    exposed to the outside world.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据服务类型，需要在内部或外部发现服务。内部发现的示例是Web服务需要与数据库服务通信。外部发现的示例是将Web服务暴露给外部世界。
- en: 'For internal service discovery, Kubernetes provides two options:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内部服务发现，Kubernetes提供两种选项：
- en: 'Environment variable: When a new Pod is created, environment variables from
    older services can be imported. This allows services to talk to each other. This
    approach enforces ordering in service creation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量：创建新Pod时，可以从旧服务导入环境变量。这允许服务之间进行通信。此方法强制执行服务创建中的顺序。
- en: 'DNS: Every service registers to the DNS service; using this, new services can
    find and talk to other services. Kubernetes provides the `kube-dns` service for
    this.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS：每个服务都注册到DNS服务；通过此服务，新服务可以找到并与其他服务通信。Kubernetes为此提供了`kube-dns`服务。
- en: 'For external service discovery, Kubernetes provides two options:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于外部服务发现，Kubernetes提供两种选项：
- en: 'NodePort: In this method, Kubernetes exposes the service through special ports
    (30000-32767) of the node IP address.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort：通过此方法，Kubernetes通过节点IP地址的特殊端口（30000-32767）公开服务。
- en: 'Loadbalancer: In this method, Kubernetes interacts with the cloud provider
    to create a load balancer that redirects the traffic to the Pods. This approach
    is currently available with GCE.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器：通过此方法，Kubernetes与云提供商交互以创建负载均衡器，将流量重定向到Pod。目前此方法在GCE上可用。
- en: Kubernetes architecture
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: 'The following diagram shows you the different software components of the Kubernetes
    architecture and how they interact with each other:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Kubernetes架构的不同软件组件及其相互交互方式：
- en: '![](img/00460.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00460.jpg)'
- en: 'The following are some notes on the Kubernetes architecture:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于Kubernetes架构的一些注释：
- en: The master node hosts the Kubernetes control services. Slave nodes run the pods
    and are managed by master nodes. There can be multiple master nodes for redundancy
    purposes and to scale master services.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点托管Kubernetes控制服务。从节点运行Pod并由主节点管理。可以有多个主节点以实现冗余和扩展主服务。
- en: Master nodes run the critical services such as the Scheduler, Replication controller,
    and API server. Slave nodes run the critical services such as Kubelet and Kube-proxy.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点运行关键服务，例如调度器、复制控制器和API服务器。从节点运行关键服务，例如Kubelet和Kube-proxy。
- en: User interaction with Kubernetes is through Kubectl, which uses standard Kubernetes-exposed
    APIs.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户与Kubernetes的交互通过Kubectl进行，它使用标准的Kubernetes公开的API。
- en: The Kubernetes scheduler takes care of scheduling the pods in the nodes based
    on the constraints specified in the Pod manifest.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes调度器负责根据Pod清单中指定的约束在节点上调度Pod。
- en: The replication controller is necessary to maintain high availability of Pods
    and create multiple instances of pods as specified in the replication controller
    manifest.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制控制器是维护Pod的高可用性和根据复制控制器清单创建多个Pod实例所必需的。
- en: The API server in the master node talks to Kubelet of each slave node to provision
    the pods.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点中的API服务器与每个从节点的Kubelet进行通信，以部署Pod。
- en: Kube-proxy takes care of service redirection and load balancing the traffic
    to the Pods.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube-proxy负责服务重定向和负载均衡流量到Pod。
- en: Etcd is used as a shared data repository for all nodes to communicate with each
    other.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd用作所有节点之间通信的共享数据存储库。
- en: DNS is used for service discovery.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS用于服务发现。
- en: Kubernetes installation
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes安装
- en: Kubernetes can be installed on baremetal, VM, or in cloud providers such as
    AWS, GCE, and Azure. We can decide on the choice of the host OS on any of these
    systems. In this chapter, all the examples will use CoreOS as the host OS. As
    Kubernetes consists of multiple components such as the API server, scheduler,
    replication controller, kubectl, and kubeproxy spread between master and slave
    nodes, the manual installation of the individual components would be complicated.
    There are scripts provided by Kubernetes and its users that automate some of the
    node setup and software installation. The latest stable version of Kubernetes
    as of October 2015 is 1.0.7 and all examples in this chapter are based on the
    1.0.7 version.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Non-Coreos Kubernetes installation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'For non-Coreos-based Kubernetes installation, the procedure is straightforward:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Find the Kubernetes release necessary from [https://github.com/kubernetes/kubernetes/releases](https://github.com/kubernetes/kubernetes/releases).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download `kubernetes.tar.gz` for the appropriate version and unzip them.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `KUBERNETES_PROVIDER` as one of these (AWS, GCE, Vagrant, and so on)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the cluster size and any other configuration parameter in the `cluster`
    directory.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run `cluster/kube-up.sh`.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubectl installation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl is the CLI client to interact with Kubernetes. Kubectl is not installed
    by default. Kubectl can be installed in either the client machine or the kubernetes
    master node.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command can be used to install kubectl. It is needed to match
    kubectl version with Kubernetes version:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '`ARCH=linux; wget https://storage.googleapis.com/kubernetes-release/release/v1.0.7/bin/$ARCH/amd64/kubectl`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'If Kubectl is installed in the client machine, we can use the following command
    to proxy the request to the Kubernetes master node:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '`ssh -f -nNT -L 8080:127.0.0.1:8080 core@<control-external-ip>`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant installation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: I used the procedure at [https://github.com/pires/kubernetes-vagrant-coreos-cluster](https://github.com/pires/kubernetes-vagrant-coreos-cluster)
    to create a Kubernetes cluster running on the Vagrant environment with CoreOS.
    I initially tried this in Windows. As I faced the issue mentioned in [https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158](https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158),
    I moved to the Vagrant environment running on Ubuntu Linux.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the commands:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '`Git clone https://github.com/pires/kubernetes-vagrant-coreos-cluster.git``Cd coreos-container-platform-as-a-service/vagrant``Vagrant up`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the two running Kubernetes nodes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00462.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: GCE installation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: I used the procedure at [https://github.com/rimusz/coreos-multi-node-k8s-gce](https://github.com/rimusz/coreos-multi-node-k8s-gce)
    to create a Kubernetes cluster running in GCE with CoreOS.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the commands:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/rimusz/coreos-multi-node-k8s-gce``cd coreos-multi-node-k8s-gce`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In the `settings` file, change `project`, `zone`, `node count`, and any other
    necessary changes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following three scripts in the same order:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '`1-bootstrap_cluster.sh``2-get_k8s_fleet_etcd.sh``3-install_k8s_fleet_units.sh`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the cluster composed of three nodes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00464.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the Kubernetes client and server versions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00465.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the Kubernetes services running in master and slave
    nodes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00468.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: The script used in this example uses Fleet to orchestrate Kubernetes services.
    As we can see in the preceding image, the API server, controller, and scheduler
    run in the master node and kubelet and proxy run in the slave nodes. There are
    three copies of kubelet and kube-proxy, one each for every slave node.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: AWS installation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: I used the procedure at [https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html)
    to create the Kubernetes CoreOS cluster running on AWS.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to install the kube-aws tool:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '`Git clone https://github.com/coreos/coreos-kubernetes/releases/download/v0.1.0/kube-aws-linux-amd64.tar.gz`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Unzip and copy kube-aws to an executable path. Make sure that `~/.aws/credentials`
    is updated with your credentials.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a default `cluster.yaml` file:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '`curl --silent --location https://raw.githubusercontent.com/coreos/coreos-kubernetes/master/multi-node/aws/cluster.yaml.example > cluster.yaml`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Modify `cluster.yaml` with your `keyname`, `region`, and `externaldnsname`;
    `externaldnsname` matters for external access only.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy the cluster, we can perform the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '`Kube-aws up`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the two nodes that are part of the Kubernetes cluster:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00470.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: An example of a Kubernetes application
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the guestbook example that we will use to
    illustrate the different Kubernetes concepts discussed in the previous sections.
    This example is based on the reference at [http://kubernetes.io/v1.1/examples/guestbook/README.html](http://kubernetes.io/v1.1/examples/guestbook/README.html):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00472.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on this guestbook application:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: This application uses the php frontend with the redis master and slave backend
    to store the guestbook database
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frontend RC creates three instances of the `kubernetes/example-guestbook-php-redis`
    container
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis-master RC creates one instance of the redis container
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redis-slave RC creates two instances of the `kubernetes/redis-slave` container
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this example, I used the cluster created in the previous section with Kubernetes
    running on AWS with CoreOS. There is one master node and two slave nodes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the nodes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00474.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'In this example, the Kubernetes cluster uses flannel to communicate across
    pods. The following output shows the flannel subnet allocated to each node in
    the cluster:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00477.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'The following are the commands necessary to start the application:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl create -f redis-master-controller.yaml``kubectl create --validate=false -f redis-master-service.yaml``kubectl create -f redis-slave-controller.yaml``kubectl create --validate=false -f redis-slave-service.yaml``kubectl create -f frontend-controller.yaml``kubectl create --validate=false -f frontend-service.yaml`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the list of pods:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00478.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows three instances of the php frontend, one instance
    of the redis master, and two instances of the redis slave.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the list of RC:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00480.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows the replication count per pod. Frontend has three
    replicas, `redis-master` has one replica, and `redis-slave` has two replicas,
    as we requested.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the list of services:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00481.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: In the preceding output, we can see the three services comprising the guestbook
    application.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'For internal service discovery, this example uses `kube-dns`. The following
    output shows the `kube-dns` RC running:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00484.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'For external service discovery, I modified the example to use the `NodePort`
    mechanism, where one of the internal ports gets exposed. The following is the
    new `frontend-service.yaml` file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion: v1 kind: Service metadata:   name: frontend   labels:     name: frontend
    spec:   # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.   type: NodePort   ports:
        # the port that this service should serve on   - port: 80   selector:     name: frontend`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output when we start the frontend service with the `NodePort`
    type. The output shows that the service is exposed using port `30193`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00486.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Once we expose port `30193` using the AWS firewall, we can access the guestbook
    application as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00488.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the application containers in `Node1`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00489.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the application containers in `Node2`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00492.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: The preceding output accounts for three instances of frontend, one instance
    of redis master, and two instances of redis slave.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how the replication controller maintains the pod replication
    count, I went and stopped the guestbook frontend Docker Container in one of the
    nodes, as shown in the following image:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00493.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Kubernetes RC detects that the Pod is not running and restarts the Pod. This
    can be seen in the restart count for one of the guestbook pods, as shown in the
    following image:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00495.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'To do some basic debugging, we can log in to the pods or containers themselves.
    The following example shows you how we can get inside the Pod:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00496.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows the IP address in the guestbook pod, which agrees
    with the flannel subnet allocated to that node, as shown in the Flannel output
    in the beginning of this example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Another command that''s useful for the debugging is `kubectl logs` as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00476.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
- en: Kubernetes with Rkt
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: By default, Kubernetes works with Container runtime Docker. The architecture
    of Kubernetes allows other Container runtime such as Rkt to work with Kubernetes.
    There is active work going on ([https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt](https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt))
    to integrate Kubernetes with Rkt and CoreOS.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes 1.1 update
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes released 1.1 version ([http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html](http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html))
    in November 2015\. Significant additions in 1.1 are increased performance, auto-scaling,
    and job objects for the batching tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Swarm is Docker''s native Orchestration solution. The following are some properties
    of Docker Swarm:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Rather than managing individual Docker nodes, the cluster can be managed as
    a single entity.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm has a built-in scheduler that decides the placement of Containers in the
    cluster. Swarm uses user-specific constraints and affinities ([https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/))
    to decide the Container placement. Constraints could be CPU and memory, and affinity
    are parameters to group related Containers together. Swarm also has the provision
    to take its scheduler out and work with other schedulers such as Kubernetes.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows the Docker Swarm architecture:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00188.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'The following are some notes on the Docker Swarm architecture:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The Swarm Master takes care of scheduling Docker Containers based on the scheduling
    algorithm, constraints, and affinities. Supported algorithms are spread, `binpack`,
    and `random`. The default algorithm is spread. Multiple Swarm masters can be run
    in parallel to provide high availability. The Spread scheduling is used to distribute
    workloads evenly. The binpack scheduling is used to utilize each node fully before
    scheduling on another node.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Swarm Agent runs in each node and communicates to the Swarm Master.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different approaches available for Swarm worker nodes to discover
    the Swarm Master. Discovery is necessary because the Swarm Master and agents run
    on different nodes and Swarm agents are not started by the Swarm Master. It is
    necessary for Swarm agents and the Swarm Master to discover each other in order
    to understand that they are part of the same cluster. Available discovery mechanisms
    are Docker hub, Etcd, Consul, and others.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm integrates with the Docker machine to ease the creation of Docker
    nodes. Docker Swarm integrates with Docker compose for multicontainer application
    orchestration.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the Docker 1.9 release, Docker Swarm integrates with multi-host Docker
    networking that allows Containers scheduled across hosts to talk to each other.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker Swarm installation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'A prerequisite for this example is to install Docker 1.8.1 and Docker-machine
    0.5.0\. I used the procedure at [https://docs.docker.com/swarm/install-w-machine/](https://docs.docker.com/swarm/install-w-machine/)
    to create a single Docker Swarm master with two Docker Swarm agent nodes. The
    following are the steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Create a discovery token.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Swarm master node with the created discovery token.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create two Swarm agent nodes with the created discovery token.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By setting the environment variable to `swarm-master`, as shown in the following
    command we can control the Docker swarm cluster using regular Docker commands:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`eval $(docker-machine env --swarm swarm-master)`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the `docker info` output on the Swarm cluster:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00056.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows that three nodes (one master and two agents) are
    in the cluster and that four containers are running in the cluster. The `swarm-master`
    node has two containers and the `swarm-agent` node has one container each. These
    containers are used to manage the Swarm service. Application containers are scheduled
    only in Swarm agent nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the individual containers in the master node. This shows the
    master and agent services running:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00191.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the container running in the agent node. This shows the `swarm
    agent` running:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00193.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: An example of Docker Swarm
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the Docker Swarm Container orchestration, let''s start four `nginx`
    containers:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '`docker run -d --name nginx1 nginx``docker run -d --name nginx2 nginx``docker run -d --name nginx3 nginx``docker run -d --name nginx4 nginx`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'From the following output, we can see that the four containers are spread equally
    between `swarm-agent-00` and `swarm-agent-01`. The default `spread` scheduling
    strategy has been used here:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00195.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the overall Container count across the cluster that
    includes the master and two agent nodes. The container count eight includes Swarm
    service containers as well as nginx application containers:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00252.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Mesos
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos is an open source clustering software. Mesosphere's DCOS is the
    commercial version of Apache Mesos. Mesos combines the Clustering OS and Cluster
    manager. Clustering OS is responsible for representing resources from multiple
    disparate computers in one single resource over which applications can be scheduled.
    The cluster manager is responsible for scheduling the jobs in the cluster. The
    same cluster can be used for different workloads such as Hadoop and Spark. There
    is a two-level scheduling within Mesos. The first-level scheduling does resource
    allocation among frameworks and the framework takes care of scheduling the jobs
    within that particular framework. Each framework is an application category such
    as Hadoop, Spark, and others. For general purpose applications, the best framework
    available is Marathon. Marathon is a distributed INIT and HA system to schedule
    containers. The Chronos framework is like a Cron job and this framework is suitable
    to run shorter workloads that need to be run periodically. The Aurora framework
    provides you with a much more fine-grained control for complex jobs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows you the different layers in the Mesos architecture:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00198.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Comparing Kubernetes, Docker Swarm, and Mesos
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though all these solutions (Kubernetes, Docker Swarm, and Mesos) do application
    Orchestration, there are many differences in their approach and use cases. I have
    tried to summarize the differences based on their latest available release. All
    these Orchestration solutions are under active development, so the feature set
    can vary going forward. This table is updated as of October 2015:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Kubernetes | Docker Swarm | Mesos |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| Deployment unit | Pods | Container | Process or Container |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| Container runtime | Docker and Rkt. | Docker. | Docker; there is some discussion
    ongoing on Mesos with Rkt. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| Networking | Each container has an IP address, and can use external network
    plugins. | Initially, this did Port forwarding with a common agent IP address.
    With Docker 1.9, it uses Overlay networking and per container IP address. It can
    use external network plugins. | Initially, this did Port forwarding with a common
    agent IP address. Currently, it works on per Container IP. integration with Calico.
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Workloads | Homogenous workload. With namespaces, multiple virtual clusters
    can be created. | Homogenous workload. | Multiple frameworks such as Marathon,
    Aurora, and Hadoop can be run in parallel. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Service discovery | This can use either environment variable-based discovery
    or Kube-dns for dynamic discovery. | Static with modification of `/etc/hosts`.
    A DNS approach is planned in the future. | DNS-based approach to discover services
    dynamically. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| High availability | With a replication controller, services are highly available.
    Service scaling can be done easily. | Service high availability is not yet implemented.
    | Frameworks take care of service high availability. For example, Marathon has
    the `Init.d` system to run containers. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Maturity | Relatively new. The first production release was done a few months
    before. | Relatively new. The first production release was done a few months before.
    | Pretty stable and used in big production environments. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| Complexity | Easy. | Easy. | This is a little difficult to set up. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Use case | This is more suitable for homogenous workloads. | Presenting Docker
    frontend makes it attractive for Docker users not needing to learn any new management
    interface. | Suitable for heterogeneous workloads. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: Kubernetes can be run as a framework on top of Mesos. In this case, Mesos does
    the first-level scheduling for Kubernetes and Kubernetes schedules and manages
    applications scheduled. This project ([https://github.com/mesosphere/kubernetes-mesos](https://github.com/mesosphere/kubernetes-mesos))
    is dedicated to running Kubernetes on top of Mesos.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: There is work ongoing to integrate Docker Swarm with Kubernetes so that Kubernetes
    can be used as a scheduler and process manager for the cluster while users can
    still use the Docker interface to manage containers using Docker Swarm.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Application definition
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: When an application is composed of multiple Containers, it is useful to represent
    each Container property along with its dependencies in a single JSON or YAML file
    so that the application can be instantiated as a whole rather than instantiating
    each Container of the application separately. The application definition file
    takes care of defining the multicontainer application. Docker-compose defines
    both the application file and runtime to instantiate containers based on the application
    file.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Docker-compose
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Docker-compose provides you with an application definition format, and when
    we run the tool, Docker-compose takes care of parsing the application definition
    file and instantiating the Containers taking care of all the dependencies.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker-compose has the following advantages and use cases:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: It gives a simple approach to specify an application's manifest that contains
    multiple containers along with their constraints and affinities
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It integrates well with Dockerfile, Docker Swarm, and multihost networking
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same compose file can be adapted to different environments using environment
    variables
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single-node application
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows you how to build a multicontainer WordPress application
    with a WordPress and MySQL container.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `docker-compose.yml` file defining the Containers and
    their properties:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '`wordpress:   image: wordpress   ports:    - "8080:80"   environment:     WORDPRESS_DB_HOST: "composeword_mysql_1:3306"
        WORDPRESS_DB_PASSWORD: mysql mysql:   image: mysql   environment:     MYSQL_ROOT_PASSWORD: mysql`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command shows you how to start the application using `docker-compose`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-compose –p composeword –f docker-compose.yml up -d`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding command:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00200.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Containers are prefixed with a keyword specified in the `-p` option. In the
    preceding example, we have used `composeword_mysql_1` as the hostname, and the
    IP address is derived dynamically from the container using this and updated in
    `/etc/hosts`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the running containers of the wordpress application:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00202.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'The following output is the `/etc/hosts` output in the wordpress container;
    the one which shows that the IP address of the MySQL container is dynamically
    updated:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00220.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: A multinode application
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: I used the example at [https://docs.docker.com/engine/userguide/networking/get-started-overlay/](https://docs.docker.com/engine/userguide/networking/get-started-overlay/)
    to create a web application spanning multiple nodes using `docker-compose`. In
    this case, `docker-compose` is integrated with Docker Swarm and Docker multihost
    networking.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The prerequisite for this example is to have a working Docker Swarm cluster
    and Docker version 1.9+.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command creates the multihost counter application. This application
    has a web container as the frontend and a mongo container as the backend. These
    commands have to be executed against the Swarm cluster:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '`docker-compose –p counter –x-networking up -d`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding command:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00204.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the overlay network `counter` created as part of
    this application:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00206.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the running Containers in the Swarm cluster:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00207.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the Swarm cluster information. There are in total
    five containers—three of them are Swarm service containers and two of them are
    the preceding application containers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00209.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: 'The following output shows the working web application:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00211.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Packaged Container Orchestration solutions
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many components necessary for the deployment of a distributed microservice
    application at scale. The following are some of the important components:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: An infrastructure cluster
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Container-optimized OS
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Container orchestrator with a built-in scheduler, service discovery, and networking
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage integration
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multitenant capability with authentication
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An API at all layers to ease management
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud providers such as Amazon and Google already have an ecosystem to manage
    VMs, and their approach has been to integrate Containers and Container orchestration
    into their IaaS offering so that Containers play well with their other tools.
    The AWS Container service and Google Container engine fall in this category. The
    focus of CoreOS has been to develop a secure Container-optimized OS and open source
    tools for distributed application development. CoreOS realized that integrating
    their offering with Kubernetes would give their customers an integrated solution,
    and Tectonic provides this integrated solution.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other projects such as OpenStack Magnum ([https://github.com/openstack/magnum](https://github.com/openstack/magnum))
    and Cisco's Mantl ([https://mantl.io/](https://mantl.io/)) that falls under this
    category of managed Container Orchestration. We have not covered these in this
    chapter.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Container service
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'The AWS EC2 Container Service (ECS) is a Container Orchestration service from
    AWS. The following are some of the key characteristics of this service:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: ECS creates and manages the node cluster where containers are launched. The
    user needs to specify only the cluster size.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container health is monitored by container agents running on the node. The Container
    agent communicates to the master node that makes all service-related decisions.
    This allows for high availability of Containers.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ECS takes care of scheduling the containers across the cluster. A scheduler
    API is implemented as a plugin and this allows integration with other schedulers
    such as Marathon and Kubernetes.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ECS integrates well with other AWS services such as Cloudformation, ELB, logging,
    Volume management, and others.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing ECS and an example
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: ECS can be controlled from the AWS console or using the AWS CLI or ECS CLI.
    For the following example, I have used the ECS CLI, which can be installed using
    the procedure in this link ([http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: I used the following example ([http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html))
    to create a WordPress application with two containers (WordPress and MySQL) using
    the compose YML file.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Create an ECS cluster.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the application as a service over the cluster.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster size or service size can be dynamically changed later based on the
    requirements.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following command shows the running containers of the WordPress application:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '`ecs-cli ps`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00213.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: 'We can scale the application using the `ecs-cli` command. The following command
    scales each container to two:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '`ecs-cli compose --file hello-world.yaml scale 2`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the running containers at this point. As we can
    see, containers have scaled to two:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00214.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The MySQL container is scaled typically using a single master and multiple
    slaves.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also log in to each AWS node and look at the running containers. The
    following output shows three containers in one of the AWS nodes. Two of them are
    application containers, and the third one is the ECS agent container that does
    container monitoring and talks to the master node:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00216.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: To log in to each node, we need to use `ec2-user` as the username along
    with the private key used while creating the cluster.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate HA, I tried stopping containers or nodes. Containers got rescheduled
    because the Container agent monitors containers in each node.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Google Container Engine
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Container Engine is the cluster manager and container orchestration
    solution from Google that is built on top of Kubernetes. The following are the
    differences or benefits that we get from GCE compared to running a container cluster
    using Kubernetes as specified in the Kubernetes installation section:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: A node cluster is created automatically by Google Container engine. The user
    needs to specify only the cluster size and the CPU and memory requirement.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes is composed of multiple individual services such as an API server,
    scheduler, and agents that need to be installed for the Kubernetes system to work.
    Google Container engine takes care of creating the Kubernetes master with appropriate
    services and installing other Kubernetes services in agent nodes.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Container engine integrates well with other Google services such as VPC
    networking, Logging, autoscaling, load balancing, and so on.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Docker hub, Google container registry, or on-premise registry can be used
    to store Container images.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing GCE and an example
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The procedure at [https://cloud.google.com/container-engine/docs/before-you-begin](https://cloud.google.com/container-engine/docs/before-you-begin)
    can be used to install the gcloud container components and kubectl. Containers
    can also be managed using the GCE dashboard.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: I used the procedure at [https://cloud.google.com/container-engine/docs/tutorials/guestbook](https://cloud.google.com/container-engine/docs/tutorials/guestbook)
    to create a guestbook application containing three services. This application
    is the same as the one used in the An example of Kubernetes application section
    specified earlier.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Create a node cluster with the required cluster size. This will automatically
    create a Kubernetes master and appropriate agent services will be installed in
    the nodes.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the application using a replication controller and service files.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cluster can be dynamically resized later based on the need.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the cluster that I created. There are four nodes in the cluster
    as specified by `NUM_NODES`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00217.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: 'The following command shows the running services that consist of frontend,
    redis-master, and redis-slave. The Kubernetes service is also running in the master
    node:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '`Kubectl get services`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00342.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'As the frontend service is integrated with the GCE load balancer, there is
    also an external IP address. Using the external IP address, guestbook service
    can be accessed. The following command shows the list of endpoints associated
    with the load balancer:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '`Kubectl describe services frontend`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the preceding command:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00222.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'To resize the cluster, we need to first find the instance group associated
    with the cluster and resize it. The following command shows the instance group
    associated with the guestbook:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00224.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: 'Using the instance group, we can resize the cluster as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00225.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: The initial set of outputs that show the cluster size as four were done after
    the resizing of the cluster.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'We can log in to the individual nodes and see the containers launched in the
    node using regular Docker commands. In the following output, we see one instance
    of `redis-slave` and one instance of front end running in this node. Other Containers
    are Kubernetes infrastructure containers:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00227.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: CoreOS Tectonic
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Tectonic is the commercial offering from CoreOS where they have integrated CoreOS
    and the open source components of CoreOS (Etcd, Fleet, Flannel, Rkt, and Dex)
    along with Kubernetes. With Tectonic, CoreOS is integrating their other commercial
    offerings such as CoreUpdate, Quay repository, and Enterprise CoreOS into Tectonic.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The plan is to expose the Kubernetes API as it is in Tectonic. Development in
    CoreOS open source projects will continue as it is, and the latest software will
    be updated to Tectonic.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the different components of Tectonic:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00228.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: 'Tectonic provides you with Distributed Trusted Computing (DTM), where security
    is provided at all layers including hardware and software. The following are some
    unique differentiators:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: At the firmware level, the customer key can be embedded, and this allows customers
    to verify all the software running in the system.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure keys embedded in the firmware can verify the bootloader as well as CoreOS.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers such as Rkt can be verified with their image signature.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs can be made tamper-proof using the TPM hardware module embedded in the
    CPU motherboard.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered the importance of Container Orchestration along
    with the internals of popular container orchestration solutions, such as Kubernetes,
    Docker Swarm, and Mesos. There are many companies offering integrated Container
    orchestration solutions, and we covered a few popular ones such as the AWS Container
    service, Google Container Engine, and CoreOS Tectonic. For all the technologies
    covered in this chapter, installation and examples have been provided so that
    you can try them out. Customers have a choice of picking between integrated Container
    Orchestration solutions and manually integrating the Orchestration solution in
    their infrastructure. The factors affecting the choice would be flexibility, integration
    with in-house solutions, and cost. In the next chapter, we will cover OpenStack
    integration with Containers and CoreOS.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kubernetes page: [http://kubernetes.io/](http://kubernetes.io/)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesos: [http://mesos.apache.org/](http://mesos.apache.org/) and [https://mesosphere.com/](https://mesosphere.com/)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Swarm: [https://docs.docker.com/swarm/](https://docs.docker.com/swarm/)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes on CoreOS: [https://coreos.com/kubernetes/docs/latest/](https://coreos.com/kubernetes/docs/latest/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Container Engine: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS ECS: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker Compose: [https://docs.docker.com/compose](https://docs.docker.com/compose)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker machine: [https://docs.docker.com/machine/](https://docs.docker.com/machine/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tectonic: [https://tectonic.com](https://tectonic.com)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tectonic Distributed Trusted Computing: [https://tectonic.com/blog/announcing-distributed-trusted-computing/](https://tectonic.com/blog/announcing-distributed-trusted-computing/)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Container Orchestration with Kubernetes and CoreOS: [https://www.youtube.com/watch?v=tA8XNVPZM2w](https://www.youtube.com/watch?v=tA8XNVPZM2w)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparing Orchestration solutions: [http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html](http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html),
    [http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1](http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1),
    and [https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca](https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesosphere introduction: [https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker and AWS ECS: 0[https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga](https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
