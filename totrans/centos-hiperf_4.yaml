- en: Chapter 4. Real-world Implementations of Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use your cluster in real-life scenarios
    by deploying a web server and a database server. Before we do this, we will need
    to review some fundamental concepts related to these key components, configure
    replicated storage so that files are kept in sync between nodes, and then finally,
    populate our database with sample data, which we will then query using a simple
    PHP application.
  prefs: []
  type: TYPE_NORMAL
- en: Since the programming side of things is out of the scope of this book, feel
    free to use some other programming language of your choice if you want to do so.
    I have chosen PHP for simplicity. Keep in mind that this book is not aimed at
    teaching you how to build web-based applications for use in a CentOS 7 cluster,
    but rather how to use it in order to provide high availability for those applications.
  prefs: []
  type: TYPE_NORMAL
- en: During the course of this chapter, you will notice that we will rely on the
    concepts introduced and the services configured in previous chapters as we dive
    into taking advantage of the cluster architecture that we have already put in
    place.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we started discussing the fundamental concepts of clustering, we mentioned
    that high availability clusters aim, in simple terms, to minimize downtime of
    services by providing failover capabilities. As we begin the journey of installing
    a web server and a database server in our cluster, we can't help but wonder how
    will we synchronize between nodes the content that those services should make
    available to us. We need to find a way for nodes to share a piece of common storage
    where data will be saved. If one node fails to provide access to it, the other
    node will take client requests from then on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Linux, a common and cost-free method of dealing with this question is an
    open source technology known as **Distributed Replicated Block Device** (**DRBD**),
    which makes it possible to mirror or replicate individual storage devices (such
    as hard disks or partitions) from one node to the other(s) over a network connection.
    In a somewhat high-level explanation, you can think of the functionality offered
    by DRBD as a network-based RAID-1\. Its basic structure and data flow are illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up storage](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All replicated data sets, such as a shared storage device, are called a resource
    in DRBD and should not be confused with a PCS resource, as discussed in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install DRBD, you will need to enable the ELRepo repository on
    both nodes, because this software package is not distributed through the standard
    CentOS repositories. Here is a brief explanation of the purpose and contents of
    the ELRepo repository:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step consists of importing the GPG key that is used to sign the `rpm`
    package, which represents the foundation to the repository. Should you try to
    install the package using rpm before importing the key, the installation will
    fail as a security measure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands on both nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can verify that ELRepo has been added to your configured repositories with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the one shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Setting up storage](img/00034.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, you can explicitly disable ELRepo after installing the `rpm`
    packages that add it to your system and enable it only to install the necessary
    packages (for precaution, make sure you make a copy of the original repository
    configuration file first):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It will install the necessary management utilities, along with the corresponding
    kernel module for DRBD. Once this process is complete, you will need to check
    whether the module is loaded, using this command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If it is not loaded automatically, you can load the module to the kernel on
    both nodes, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Note that `modprobe` command will take care of loading the kernel module for
    the time being on your current session. However, in order for it to be loaded
    during boot, you have to make use of the systemd-modules-load service by creating
    a file inside `/etc/modules-load.d/` so that the DRBD module is loaded properly
    each time the system boots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ELRepo repository and DRBD availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELRepo is a community repository for Linux distributions that are compatible
    with Red Hat Enterprise Linux, which CentOS and Scientific Linux are derivatives
    of. ELRepo has hardware-related packages (especially drivers) as the primary focus
    in order to enhance or provide functionality that is not present in the current
    kernel. Thus, by installing the corresponding package, you save yourself from
    the pain of having to recompile the kernel only to add a certain feature, or having
    to wait for it to be supported by upstream repositories, or for the feature to
    be included in a later kernel release. The ELRepo repository is maintained by
    active members of the related distributions (RHEL, CentOS, and Scientific Linux).
  prefs: []
  type: TYPE_NORMAL
- en: DBRD, as made available by ELRepo, is intended primarily to evaluate and get
    experience with DRBD on RHEL-based platforms, but is not officially supported
    by Red Hat and LINBIT, the creators of DRBD. However, following the procedures
    outlined in this chapter and throughout the rest of this book, you can ensure
    that all of the necessary functionality will be available in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have installed the packages mentioned earlier, we need to allocate the
    physical space that will be used to store the replicated contents on both servers.
    With scalability in mind, we will use the **Logical Volume Manager** (**LVM**)
    technology to create dynamic hard disk partitions that are easily resizable down
    the road if we need to.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, we will add a 2 GB hard disk to each node. The purpose of this
    hard disk is to serve as the underlying filesystem for a PHP application accessed
    by the Apache web server.
  prefs: []
  type: TYPE_NORMAL
- en: I chose this size because it will be enough to store all the necessary files
    to be replicated, and because Virtualbox allows you to pick arbitrary sizes for
    storage disks. If you happen to be using real hardware as you follow along with
    this book, you may want to choose a different size accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a virtual hard disk to an existing virtual machine in Virtualbox, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Turn off the **VM**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on it in Virtualbox's initial screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the contextual menu, choose **Settings** and then **Storage**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Controller: SATA**, and click on **Add hard disk** and then click
    on **Create new disk**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Virtual Disk Image (VDI)** and **Dynamically Allocated** and proceed
    to next step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, assign a name for the device and choose 2 GB as size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After starting and booting up each node, we should issue the following command
    in order to identify the newly added disk (the new disk will be, in our case,
    the one that is not partitioned yet):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will identify the newly added disk with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `/dev/sdb` is the new disk ID, as returned by listing the contents of
    the `/dev` directory earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a partition on the disk, the corresponding physical volume,
    a volume group (`drbd_vg`), and finally, a logical volume (`drbd_vol`) on top.
    Make sure you repeat these steps on each node, changing the device (`dev/sdX`)
    as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can check the status of the newly created logical volume with `lvdisplay
    /dev/drbd_vg/drbd_vol`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring DRBD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After having successfully created and partitioned our DRBD disks on each node,
    the main configuration file for DRBD is located in `/etc/drbd.conf`, which consists
    only of the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Both lines include relative paths, starting at `/etc/`, of the actual configuration
    files. In the `global_common.conf` file, you will find the global settings for
    your DRBD installation, along with the common section (which defines those settings
    that should be inherited by every resource) of the DRBD configuration. On the
    other hand, in the `.res` files, you will find the specific configuration for
    each DRBD resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now rename the existing `global_common.conf` file as `global_common.conf.orig`
    (as a backup copy of the original settings) with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create a new `global_common.conf` file with the following contents
    by opening the file with your preferred text editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you created the preceding file on one node (say, `node01`), you can easily
    copy it to the another node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should make it a habit to make backup copies of the original configuration
    files so that you can roll back to previous settings, should something go wrong
    at any time.
  prefs: []
  type: TYPE_NORMAL
- en: The `usage-count no` line in the global section skips sending a notice to the
    DRBD team each time a new version of the software is installed in your system.
    You could change it to `yes` if you want to submit information from your system.
    Alternatively, you could change it to `ask` if you want to be prompted for a decision
    each time you do an upgrade. Either way, you should know that they use this information
    for statistical analysis only, and their reports are always available to the public
    at [http://usage.drbd.org/cgi-bin/show_usage.pl](http://usage.drbd.org/cgi-bin/show_usage.pl).
  prefs: []
  type: TYPE_NORMAL
- en: The `protocol C` line tells the DRBD resource to use a fully synchronous replication,
    which means that local write operations on the node that is functioning as primary
    are considered completed only after both the local and remote disk writes have
    been confirmed. Thus, if you run into the loss of a single node, that should not
    lead to any data loss under normal circumstances, unless both nodes (or their
    storage subsystems) are irreversibly destroyed at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will need to create a specific new configuration file file (called
    `/etc/drbd.d/drbd0.res`) for our resource, which we will name `drbd0`, with the
    following contents (where `192.168.0.2` and `192.168.0.3` are the IP addresses
    of our two nodes, and `7789` is the port used for communication):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can look up the meaning of each directive (and the rest as well) in the
    resource configuration file at Linbit's website at [http://drbd.linbit.com/users-guide-8.4/](http://drbd.linbit.com/users-guide-8.4/).
  prefs: []
  type: TYPE_NORMAL
- en: TCP port `7789` is the typical port number used in most DRBD installations.
    However, the official documentation states that DRBD (by convention) uses TCP
    ports from `7788` upwards, with every resource listening on a separate port. In
    this chapter, since we are dealing with only one resource, we will only use port
    `7789`—both in the only resource configuration file and in the firewall settings
    on both nodes. It is essential that you remember to open this port in the firewall,
    because otherwise, the resources will not be able to synchronize later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the `7789` TCP port in the firewall configuration, execute the following
    commands on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, you can copy this file to the other node, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When we installed DRBD earlier, a utility called `drbdadm` was installed as
    well, which, as you will be able to guess from its name, is intended to be used
    for the administration of DRBD resources, such as our newly configured volume
    The first step in starting and bringing a DRBD resource online is to initialize
    its metadata (you may need to change the resource name if you set a different
    name in the configuration file previously). Note that the `/var/lib/drbd` directory
    is needed beforehand. If it was not created previously when you installed DRBD,
    create it manually before proceeding, using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines should result in the following output, with the corresponding confirmation
    message that indicates a successful creation of the metadata for the device:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring DRBD](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The word "metadata" has been defined as data about the data. In the context
    of DRBD resources, the metadata of a resource consists of several pieces of information
    about the device and the data that is kept in it. The `drbdadm create-md [drbd
    resource]` command will return useful debugging information if something does
    not work as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step consists of enabling `drbd0` in order to finish allocating both
    disk and network resources for its operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify the status of the resource by taking a look at the `/proc` virtual
    filesystem, which allows you to view the system''s resources as the kernel sees
    them, as you can see in the following screenshot. However, make sure you have
    followed the instructions outlined earlier on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring DRBD](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the status of the device shows as unknown and inconsistent since we
    haven't indicated yet which of the DRBD devices (one in each node) will act as
    a primary device and which one as a secondary device. At this point, given our
    current scenario where we have set up two DRBD devices from scratch, it does not
    matter which one you choose to be primary. However, if we had used one device
    with data already residing in it, it is crucial that you select that one device
    as the primary resource. Otherwise, you run the serious risk of losing your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run this command in order to mark one device as primary and to perform the
    initial synchronization. You only need to do this in the node that has the primary
    resource (in our example, this means `node01`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you did earlier, you can check the current status of the synchronization
    while it''s being performed. The cat `/proc/drbd` command displays the creation
    and synchronization progress of the resource, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring DRBD](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, with the help of `drbd-overview` command, as its name implies, you can
    see an overview of the currently configured DRBD resources. In this case, you
    should see that `node01` is acting as primary and `node02` as secondary, as indicated
    by running the command on both nodes (which can also be seen in the following
    screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `node01` : the drbd-overview command should return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Whereas in `node02` you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Configuring DRBD](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we need to create a filesystem on `/dev/drbd0` in `node01`. You can
    choose whatever suits your needs or requirements, if any. `Ext4` is a good choice
    if you have not decided which one to use. XFS is the default filesystem for CentOS
    7 out of the box. However, it is not possible to resize it if we need to do so
    at a later time, should we run into a more complex setup for the underlying storage
    needed for the operation of the web and database servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command on the primary node to create an `ext4` filesystem
    on `/dev/drbd0` and wait until it completes, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![Configuring DRBD](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, your DRBD resource is ready to be used as usual. You can now mount it and
    start saving files to it. However, we still need to add it as a cluster resource
    before we can start using it as a highly available and fail-safe component. This
    is what we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important that you create the filesystem on the resource from `node01`,
    our primary node. Otherwise, you will run into a mounting issue that is caused
    when you try to add a filesystem from a node that is not the primary member of
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Adding DRBD as a PCS cluster resource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will recall how in [Chapter 2](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 2. Installing Cluster Services and Configuring Network Components"),
    *Installing Cluster Services and Configuring Network Components*, we added a virtual
    IP address to the cluster. Now, it's time to do the same with the DRBD resource
    that we have just created and configured.
  prefs: []
  type: TYPE_NORMAL
- en: Before doing that, however, we must point out that one of the most distinguishing
    features of the PCS command-line tool that we first introduced back in [Chapter
    2](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0 "Chapter 2. Installing
    Cluster Services and Configuring Network Components"), *Installing Cluster Services
    and Configuring Network Components*, is its ability to save the current cluster
    configuration to a file, to which you can add further settings using command-line
    tools. Then, you can use the resulting file to update the running cluster configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve the cluster configuration from the **Cluster Information Base**
    (**CIB**) and save it to a file named `drbd0_conf` in the current working directory,
    use the following command to make sure you started the cluster first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then save the cluster configuration to the file mentioned earlier (`drbd0_conf`
    will be created automatically):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will add the DRBD device as a PCS cluster resource. Note the `-f`
    switch, which indicates that changes resulting from the following command should
    be appended to the `drbd0_conf` file. The following command must be executed from
    the same directory as the previous command (meaning the directory where the `drbd0_conf`
    file is located):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to make sure that the resource will run on both nodes simultaneously
    by adding a clone resource (a special type of resource that should be active on
    multiple hosts at the same time) for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can update the cluster configuration using the `drbd0_conf`
    file. However, a quick inspection of the cluster status and its resources will
    allow us to better visualize the changes if we run `pcs status` command before
    and after updating the global configuration, in that order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The last command should result in the following message if the update was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s check the current cluster configuration again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case the last PCS status indicates some failure event (most likely related
    to SELinux policies and less likely with regular file permissions), you should
    inspect the `/var/log/audit/audit.log` file to start your troubleshooting. Lines
    starting with AVC will point out the places where you need to look first. Here
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding error message seems to indicate that SELinux is denying the `drbdsetup-84`
    executable read/write access to the temporary `tmpfs` filesystem. Its corresponding
    denied system call supports this theory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NSA Security-Enhanced Linux (SELinux) is an implementation of a flexible mandatory
    access control architecture in Linux. You can disable it to perform the following
    steps (but it is strongly recommended that you don't) if you experience several
    issues with it at first. If you choose to disable SELinux by editing `/etc/sysconfig/selinux`,
    do not forget to clean the resource error count with `pcs resource cleanup [resource_id]`,
    where `resource_id` is the name of the resource as returned by `pcs resource show`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clear all doubts, install the `policycoreutils-python` package (which contains
    the management tools used to manage an SELinux environment):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `audit2allow` utility included in it to view the reason of access denied
    in human-readable form and then generate an SELinux policy-allow rule based on
    logs of denied operations. The following command will output the last line in
    the `audit.log` file where the word AVC appears and then pipe it to `audit2allow`
    to produce the result in human-readable form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following screenshot, we can confirm that access was denied
    due to a missing type enforcement rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding DRBD as a PCS cluster resource](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we know what is causing the problem, let''s create a policy package
    in order to implement the necessary type enforcement rule into a module whose
    name is specified in the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do `ls -l` in your current working directory, you will find that the
    preceding command created a type enforcement file (`drbd_access_0.te`) and compiled
    it into a policy package (`drbd_access_0.pp`), which you will need to activate
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command can take about a minute to complete, so do not worry
    if this is the case for you, as you can see in the following screenshot, no output
    means a successful operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding DRBD as a PCS cluster resource](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we need to copy the module to `node02` and install it there. This is one
    of the reasons why we set up key-based authentication between nodes in [Chapter
    1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0 "Chapter 1. Cluster
    Basics and Installation on CentOS 7"), *Cluster Basics and Installation on CentOS
    7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following command in `node02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can execute the following command in `node01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the SELinux `daemons_enable_cluster_mode` policy should be set
    to true on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Then, you may need to repeat this process more than once if the output of `pcs
    status` shows further errors. If you find that you have to repeat it several times,
    you may want to consider setting SELinux to permissive so that it will still issue
    warnings instead of blocking the cluster resource. Then, you can continue with
    the setup for the time being and debug later.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that both nodes are online, and the cluster resources are properly
    started, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding DRBD as a PCS cluster resource](img/00042.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's give DRBD a rest for a brief moment, and let's focus on the installation
    of the web and database servers. Note that we will also revisit this topic in
    [Chapter 5](part0041_split_000.html#173721-1d2c6d19b9d242db82da724021b51ea0 "Chapter 5. Monitoring
    the Cluster Health"), *Monitoring the Cluster Health*, where we will simulate
    and troubleshoot issues. Note that if you reboot a node or both of them, nodes
    may detect a split-brain situation at this point, which we will fix manually (as
    that is the method that is recommended by LINBIT) later during the next chapter,
    when we troubleshoot the most common issues that may come up during the cluster
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the web and database servers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As of the time of writing this book, the Apache HTTP server (or just Apache
    for short) remains the world's most widely used web server and is often used within
    what is called a **LAMP stack**. In this stack, a Linux distribution is used as
    the operating system, Apache as the web server, MySQL/MariaDB as the database
    server, and PHP as the server-side programming language for applications. Each
    one of these components is free, and these technologies are widely spread and
    thus easy to learn/get help on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the Apache and MariaDB (a free and open source fork of MySQL) servers,
    run the following commands on each node. Note that this will install PHP as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon successful installation, we will proceed as we did earlier. To begin,
    let''s enable and start the web server on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to make sure that Apache is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Allow traffic through TCP port `80` in the firewall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can fire up a web browser and point it to the individual
    IP addresses of the nodes (remember that we haven''t added Apache as a cluster
    resource, and thus, we can''t access the web server on the virtual IP that is
    common to both nodes). You should see Apache''s welcome page, as shown in the
    following figure, where we can see that web server is running correctly on `node02`
    (`192.168.0.3` as per our initial setup):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Installing the web and database servers](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, it is time to take a small step back. We will disable and stop Apache
    on both nodes so that the cluster will manage it when PCS is moving forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for Apache to listen on the virtual IP (to which we assigned `192.168.0.4`
    as the IP address) and the loopback address (we will see why in just a minute),
    we need to modify the main configuration file `(/etc/httpd/conf/httpd.conf`),
    as follows (you may want to make a backup of this file first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, restart Apache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note that while restarting the web server in the second node, an error is to
    be expected since there is already a service running in that socket. However,
    that is normal, and now, you should be able to access the Apache welcome page
    by pointing your browser to the virtual IP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fun part is finding out which is the node in which the virtual IP was started,
    as shown in the following screenshot. If you get an error here instead, make sure
    `virtual_ip` is started by PCS first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Installing the web and database servers](img/00044.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s stop the cluster in that node, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Then, on the other node, it should still indicate that the resource is active.
  prefs: []
  type: TYPE_NORMAL
- en: However, even when the virtual IP is failed over to `node02`, the web server
    is not accessible through that resource because it wasn't started there in the
    first place. For this reason, we still need to configure Apache as a cluster resource
    so that it can be managed as such.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the web server as a cluster resource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will recall from when we configured the virtual IP in [Chapter 2](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 2. Installing Cluster Services and Configuring Network Components"),
    *Installing Cluster Services and Configuring Network Components*, and when we
    added replicated storage earlier during this chapter that we must indicate a way
    for PCS to check on a periodic basis whether the resource is available or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we will use the server status page (`http://node0[1-2]/server-status`),
    which is the preferred Apache web page as it provides information about how well
    the server will be performing PCS will query this page once per minute. This is
    accomplished by creating a file named `status.conf` inside `/etc/httpd/conf.d`
    on both nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, with the following command, we will add Apache as a cluster resource.
    The status of the resource will be checked by PCS once every minute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: By default, pacemaker will try to balance the resource usage over the cluster.
    However, at certain times, our setup will require that two related resources (as
    it is in the case of the web server and the virtual IP) need to run on the same
    host.
  prefs: []
  type: TYPE_NORMAL
- en: The web server should always run on the host on which the virtual IP is active.
    This also means that if the virtual IP resource is not active on any node, the
    web server should not run at all. In addition, since we need the web server to
    listen on the virtual IP address as well as on the loopback device on each host,
    it goes without saying that
  prefs: []
  type: TYPE_NORMAL
- en: We must ensure that the virtual IP resource is started before the web server
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can accomplish both requirements through the use of the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the second command, you should see the following message on your
    screen. Note that starting the virtual IP resource before the web server is a
    mandatory requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s check the status of the cluster and focus on its assigned resources,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the web server as a cluster resource](img/00045.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can now simulate a failover by forcing `node01` to go offline. To do so,
    you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The resources should be automatically started on `node02`, as indicated in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the web server as a cluster resource](img/00046.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The last step consists of mounting the DRBD resource on the `/var/html/www directory`
    and adding in it a simple PHP page to display the PHP configuration of the cluster.
    You will then be able to build on that simple example to add more sophisticated
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before attempting to use `/dev/drbd0`, we should check its status on both nodes
    with `drbd-overview`. If the output shows StandAlone or WFConnection, we are looking
    at a split-brain situation, which can be confirmed in the output of the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This will result in a `Split-Brain detected, dropping connection!` error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linbit recommends to manually resolve such cases by choosing a node whose modifications
    will be discarded and then issuing the following commands in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then connect the DRBD resource on the other node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also start or stop DRBD and get an overview with the following commands
    in `node01`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Review the DRBD documentation carefully before choosing a recovery method after
    a split-brain situation. Since there is no one-size-fits-all answer to this issue,
    I have chosen to cover the recommended method in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Mounting the DRBD resource and using it with Apache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before using the DRBD resource, you must define a filesystem on it and mount
    it on a local directory. We will use Apache's document root directory (`/var/www/html`),
    but given the case, you could use a virtual host directory as well. As we did
    earlier, we will add these changes in a configuration file, step by step, and
    we will push it to the running CIB later on `node01` (or whatever the DC is).
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, create a new configuration file named `fs_dbrd0_cfg` (feel free to
    change the name if you want):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll create the filesystem resource itself (again, change the variable
    values if needed). This is another special type of resource provided out of the
    box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'It indicates that the filesystem should always be available on the master DRBD
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in order for the filesystem to be started properly, `/dev/drbd0`
    must be started first, so we will have to add a constraint for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, ensure that Apache needs to run on the same node as the filesystem
    resource, which also needs to come online before the web server resource can be
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'You can review the configuration with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mounting the DRBD resource and using it with Apache](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If everything is correct, then push it to the running CIB with this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command should show CIB updated on successful completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you now run `pcs status`, you should see the newly added resources, as you
    can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mounting the DRBD resource and using it with Apache](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, you don''t need to manually mount `/dev/drbd0` in `/var/www/html`, because
    the cluster will take care of it. You can verify that the DRBD device has been
    mounted in `/var/www/html` using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that any original contents present in `/var/html/www` will not be available
    while `/dev/drbd0` is mounted.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the DRBD resource along with Apache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a simple test, we will display the information about the PHP installation.
    Create a file named `info.php` inside `/var/www/html` on `node01` with the following
    contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, point your browser to `192.168.0.4/info.php` and verify that the output
    is similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the DRBD resource along with Apache](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, stop the cluster (`pcs cluster stop`) on `node01` or put it into the
    standby mode (`pcs cluster standby node01`) and refresh the browser. The only
    thing that should change on the output is the system name, as shown in the following
    screenshot, since the `phinfo()` PHP function returns the local hostname along
    with the information about the PHP installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the DRBD resource along with Apache](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, if you list the contents of `/var/www/html` on `node02`, you will
    see that the `info.php` file that was created originally on `node01` now shows
    on `node02` as well, as indicated in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the DRBD resource along with Apache](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before proceeding, remember to return `node01` to normal mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Setting up a high-availability database with replicated storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last part of this chapter focuses on setting up a HA MariaDB database with
    replicated storage. To begin, we will have to set up another DRBD resource as
    we did earlier. We will review the necessary steps here for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: Add another virtual disk to each virtual machine (a 2 GB disk will do).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a partition on the newly added disk and then go through the process
    of creating a **Physical Volume** (**PV**) on `/dev/``sdc1`, a **Volume Group**
    (**VG**, named `drbd_db_vg`), and finally a **Logical Volume** (**LV**, `drbd_db_vol`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a configuration file (`/etc/drbd.d/drbd1.res`) for the new DRBD resource
    (`drbd1`), and based on the configuration file for the first replicated storage
    resource, edit the settings accordingly and use a different port:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The, add a firewall rule to allow traffic:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat the previous steps on the second node. Initialize the metadata for the
    new DRBD resource on both nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enable the replicated storage resource in order to allocate disk and network
    resources for its operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Mark the DRBD device on the DC node as primary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the new DRBD device as cluster resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When this process is complete, the overview of all configured DRBD resources
    up until this point should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the cluster should now include the new DRBD resource and its clone
    (`db_drbd` and `db_drbd_clone`, respectively) as well as the filesystem resource,
    as you can see in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a high-availability database with replicated storage](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now divide the MariaDB files into two separate sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Binaries, socket, and `.pid` files will be placed inside a directory on a regular
    partition, independent on each node `(/var/lib/mysql` by default). These are files
    we don't need to be highly available or fail-safe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database and configuration files (`my.cnf`) will be stored in a DRBD resource,
    which will be mounted under `/var/lib/mariadb_drbd1`, inside a directory named
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we need to add the database server as a cluster resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This we will add the same constraints that we did with Apache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will add a firewall rule to allow traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We will begin by creating an `ext4` filesystem on `drbd1` and mounting it in
    the directory that was created previously. Only perform this step on the DC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to move the database server configuration file to the mount point
    of `drbd1` (perform all of the following steps on both nodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit it so that the `datadir` variable will point to the right directory inside
    the mount point of the DRBD resource and at the same time, specify that the database
    server should listen for TCP connections on a defined address (in this case, the
    IP address of our virtual IP resource):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to initialize the database data directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, log on to the database server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, grant all permissions to the root user identified by the defined password:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This permission set is only for testing and should be modified with the necessary
    security parameters before moving the cluster to a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can create an empty database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, make sure the `mysql` user can access the `/var/lib/mariadb_drbd1`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: If we now failover, from the active node to the passive one, the actual database
    files within `datadir` will be replicated by DRBD to the same directory on the
    other node.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained previously, the output of `pcs status` under `Failed actions` will
    show you whether there are problems with the cluster resources and provide information
    as to what you should do in order to fix them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`exit-reason=''Config /var/lib/mariadb_drbd1/my.cnf doesn''t exist''`: Make
    sure the configuration file for MariaDB exists and is identical on both nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exit-reason=''Couldn''t find device [/dev/drbd1]. Expected /dev/??? to exist''`:
    The DRBD device was not created correctly. Review the instructions and try to
    create it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, the exit reason will give you valuable information to troubleshoot
    and fix the issues you may have. If, after verifying the conditions outlined in
    the error messages, you are still experiencing issues with a particular resource,
    it is useful to clean up the operation history of a resource and redetect its
    current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'From Kamran, a real world problem scenario, which happens when the reader follows
    (or gets lost following) instructions in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explained how to set up real-world applications of clusters:
    a database server and a web server. Both applications build upon a replicated
    storage device in a setup that increases availability by providing failover storage
    for regular and database files.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next two chapters, we will build upon the concepts and resources that
    we introduced here, troubleshoot common issues in cluster-based web and database
    servers, and prevent common bottlenecks in order to ensure the high availability
    of applications.
  prefs: []
  type: TYPE_NORMAL
