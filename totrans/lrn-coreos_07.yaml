- en: Chapter 7. Creating a Virtual Tenant Network and Service Chaining Using OVS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 使用OVS创建虚拟租户网络和服务链
- en: In the previous chapter, we saw how different services running inside the CoreOS
    cluster can be linked with each other. The chapter described in detail how the
    services deployed by different customers/tenants across the CoreOS cluster can
    be linked/connected using OVS.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到如何将不同的服务通过CoreOS集群中的链接相互连接。本章详细描述了如何使用OVS将不同客户/租户在CoreOS集群中部署的服务链接/连接起来。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Introduction to OpenVSwitch/OVS
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenVSwitch/OVS介绍
- en: Introduction to overlay and underlay networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖网络和基础网络介绍
- en: Introduction to virtual tenant networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟租户网络简介
- en: Docker networking using OVS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OVS的Docker网络
- en: As OVS is a production-quality, widely deployed software switch with a wide
    range of feature sets, we are going to see how OVS can be used to provide service
    chaining, which can differentiate between different customer services.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OVS是一个生产级别、广泛部署的软件交换机，具有广泛的功能集，我们将看看如何使用OVS提供服务链，这可以区分不同的客户服务。
- en: Introduction to OVS
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OVS介绍
- en: '**OpenVSwitch** (**OVS**) is a production-quality open source virtual switch
    application that can be run on any Unix-based systems. Typically, OVS is used
    in a virtualization environment to provide communication between the virtual machines/containers
    that are running inside the servers. OVS acts as a software switch that provides
    layer2 connectivity between the VMs running inside a server. Linux Bridge can
    also be used for providing communication between the VMs inside the server. However,
    OVS provides all the bells and whistles that are required in a typical server
    virtualization environment. The following diagram depicts how OVS provides connectivity
    across the VMs running inside the server:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenVSwitch**（**OVS**）是一个生产级别的开源虚拟交换机应用程序，可以在任何Unix类系统上运行。通常，OVS用于虚拟化环境中，为在服务器内部运行的虚拟机/容器提供通信。OVS充当一个软件交换机，在服务器内部的虚拟机之间提供二层连接。Linux
    Bridge也可以用于提供虚拟机之间的通信。然而，OVS提供了在典型的服务器虚拟化环境中所需的所有功能。以下图示展示了OVS如何提供服务器内虚拟机之间的连接：'
- en: '![Introduction to OVS](img/00030.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![OVS介绍](img/00030.jpeg)'
- en: In the diagram, there are three VMs that are running in a server. One end of
    the VM's virtual NIC is connected to **Open vSwitch**. Here, **Open vSwitch**
    provides connectivity across all the VMs in the server. **Open vSwitch** is also
    connected to the physical NIC to provide communication to and from the VMs to
    the external world.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示中，服务器中有三个虚拟机正在运行。每个虚拟机的虚拟网卡一端连接到**Open vSwitch**。在这里，**Open vSwitch**提供了服务器内所有虚拟机之间的连接。**Open
    vSwitch**还连接到物理网卡，以提供虚拟机与外部世界之间的通信。
- en: The OVS offers **Security** by providing traffic **isolation** using **VLAN**
    and **traffic filtering** based on various packet headers. OVS provides a way
    for **Monitoring** the packets that are exchanged across the VMs in the server
    using protocols like **sFlow**, **SPAN**, **RSPAN**, and so on. OVS also supports
    **QoS** (quality of service) with **traffic queuing and shaping** along with **OpenFlow**
    support.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: OVS通过使用**VLAN**和基于各种数据包头部的**流量过滤**，提供**安全性**，并通过提供**流量隔离**来确保网络安全。OVS还提供了使用**sFlow**、**SPAN**、**RSPAN**等协议监控服务器内虚拟机之间交换的包的方法。OVS还支持具有**流量排队和整形**的**QoS**（服务质量）以及**OpenFlow**支持。
- en: OVS architectural overview
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OVS架构概述
- en: This section describes the high-level architectural overview of OVS and its
    components.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了OVS及其组件的高层架构概述。
- en: '![OVS architectural overview](img/00031.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![OVS架构概述](img/00031.jpeg)'
- en: 'The main components of OVS are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: OVS的主要组件如下：
- en: '`ovs-vsctl`: This is the utility provided by OVS for configuring and querying
    the `ovs-vswitchd` daemon via `ovsdb-server`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-vsctl`：这是OVS提供的用于通过`ovsdb-server`配置和查询`ovs-vswitchd`守护进程的工具'
- en: '`ovs-appctl`: This is a utility for managing the logging level of OVS'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-appctl`：这是用于管理OVS日志级别的工具'
- en: '`ovs-ofctl`: This is the utility provided by OVS for managing OpenFlow entries
    in the switch'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-ofctl`：这是OVS提供的用于管理交换机中OpenFlow条目的工具'
- en: '`ovs-dpctl`: This is the data-path management utility that is used to configure
    the data path of OVS'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-dpctl`：这是用于配置OVS数据路径的路径管理工具'
- en: '`ovsdb-server`: This is the DB that stores persistently all the configurations
    of OVS'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovsdb-server`：这是存储OVS所有配置信息的数据库'
- en: '`ovs-vswitchd`: This is the OVS switchd module that provides the core functionality,
    such as bridging, VLAN segregation, and so on of OVS'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-vswitchd`：这是 OVS 的交换模块，提供核心功能，如桥接、VLAN 隔离等。'
- en: '`Openvswitch.ko`: This is the data-path module for handling fast switching
    and tunneling of traffic'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Openvswitch.ko`：这是处理流量快速切换和隧道传输的数据路径模块。'
- en: Advantages of using OVS in CoreOS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 CoreOS 中使用 OVS 的优点
- en: 'In a CoreOS environment, OVS can replace docker0 bridge and can provide connectivity
    across the different containers in the CoreOS instance. docker0 bridge can only
    provide connectivity across the containers running in the same CoreOS instance.
    However, along with providing connectivity across the containers running in the
    same CoreOS instance, OVS can be used to provide connectivity across the containers
    running in different CoreOS instances. The following are the key advantages provided
    by OVS compared to other techniques mentioned in the previous chapter:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CoreOS 环境中，OVS 可以替代 docker0 桥接，并为不同 CoreOS 实例中的容器提供连接。docker0 桥接只能为同一 CoreOS
    实例中运行的容器提供连接。然而，除了为同一 CoreOS 实例中的容器提供连接外，OVS 还可以为不同 CoreOS 实例中的容器提供连接。与上一章提到的其他技术相比，OVS
    提供的关键优点如下：
- en: As the name implies, OpenVSwitch/OVS does layer2 bridging/switching of data
    from one container to other containers. It does typical layer2 processing, such
    as flooding, learning, forwarding, traffic segregation based on VLAN tag, providing
    loop-free topology using spanning tree protocol, and so on.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顾名思义，OpenVSwitch/OVS 执行数据从一个容器到其他容器的层2桥接/交换。它执行典型的层2处理，如洪泛、学习、转发、基于 VLAN 标签的流量隔离、使用生成树协议提供无环拓扑等。
- en: OVS supports tunneling protocols, such as GRE, VxLAN, and so on. These are the
    tunneling protocols that are used to carry layer2 traffic over a layer3 network.
    These tunnels are used to provide connectivity for containers running in different
    CoreOS instances. The VxLAN protocol is defined in detail in RFC 7348 and the
    GRE protocol is defined in detail in RFC 2784\. These tunnels provide the virtual
    infrastructure for laying out the overlay network over the physical underlay network.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVS 支持隧道协议，例如 GRE、VxLAN 等。这些隧道协议用于在层3网络上传输层2流量。这些隧道用于为运行在不同 CoreOS 实例中的容器提供连接。VxLAN
    协议在 RFC 7348 中有详细定义，而 GRE 协议则在 RFC 2784 中有详细定义。这些隧道为在物理基础网络上构建覆盖网络提供了虚拟基础设施。
- en: OVS also supports the OpenFlow protocol that can be programmed by an external
    SDN controller like OpenDayLight Controller, RYU Controller, ONOS controller,
    and so on. This means the CoreOS cluster can be managed easily by a centralized
    controller in a typical SDN deployment.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVS 还支持 OpenFlow 协议，可以通过外部 SDN 控制器（如 OpenDayLight 控制器、RYU 控制器、ONOS 控制器等）进行编程。这意味着
    CoreOS 集群可以通过典型的 SDN 部署中的集中式控制器轻松管理。
- en: Before looking in detail at how OVS can be used to provide connectivity across
    containers and hence can provide service chaining, we may need to look into some
    of the core concepts and features, such as overlay network, underlay network,
    and Virtual Tenant Network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细了解 OVS 如何提供跨容器连接并实现服务链条之前，我们需要了解一些核心概念和特性，如覆盖网络、基础网络和虚拟租户网络。
- en: Introduction to overlay and underlay networks
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 覆盖网络和基础网络简介
- en: 'The following diagram represents the typical service provided by OVS in a virtual
    machine environment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示 OVS 在虚拟机环境中提供的典型服务：
- en: '![Introduction to overlay and underlay networks](img/00032.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![介绍覆盖网络和基础网络](img/00032.jpeg)'
- en: Server1 and Server2 are the two physical servers wherein the customer applications
    are deployed inside the VM. There are two VMs in each server as VM1 and VM2\.
    The green VM belongs to one customer and the orange VM belongs to another customer.
    A single instance of OVS is running in each of the servers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Server1 和 Server2 是两个物理服务器，客户应用程序部署在虚拟机（VM）内。每个服务器上有两个虚拟机，分别是 VM1 和 VM2。绿色虚拟机属于一个客户，橙色虚拟机属于另一个客户。每台服务器上都运行一个
    OVS 实例。
- en: 'In a typical virtualization environment, there are two kinds of network devices:
    the soft switch, which provides connectivity to the virtualization layer, and
    the physical switch, which provides connectivity to the physical infrastructure
    (such as servers, switches, and routers).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的虚拟化环境中，有两种类型的网络设备：软交换，它为虚拟化层提供连接；物理交换机，它为物理基础设施（如服务器、交换机和路由器）提供连接。
- en: The OVS switch provides connectivity to the VMs/containers running inside the
    server instance. These server instances are also connected to each other physically
    in order to provide connectivity for all the servers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OVS交换机为运行在服务器实例内的虚拟机/容器提供连接。这些服务器实例还通过物理方式相互连接，以便为所有服务器提供连接。
- en: The physical network that provides connectivity for the servers is termed the
    underlay network. This underlay network will have the physical infrastructure
    that comprises physical switches and routers, which provides connectivity for
    the servers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提供服务器连接的物理网络被称为底层网络。这个底层网络将拥有包括物理交换机和路由器在内的物理基础设施，提供服务器之间的连接。
- en: Now, the complexity comes in providing connectivity across the containers that
    are running in the server to other containers that are running in different server
    instances. There are multiple solutions to solve this problem. One of the major
    and widely deployed solutions is using OVS to provide the overlay network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，复杂性出现在如何为在服务器中运行的容器提供连接，以便与在不同服务器实例中运行的其他容器进行通信。解决这个问题有多种方法。一个主要且广泛部署的解决方案是使用OVS来提供覆盖网络。
- en: As the term implies, an overlay network is a network that is overlaid on top
    of another network. Unlike physical underlay networks, overlay networks are virtual
    networks that comprise virtual links that share an underlying physical network
    (underlay network), allowing deployment of containers/virtual machines to provide
    connectivity with each other without the need to modify the underlying network.
    The virtual link here refers to the tunnels that provide connectivity across OVS.
    OVS supports multiple tunneling protocols; widely used tunnels are GRE and VxLAN.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个术语所暗示的，覆盖网络是覆盖在另一个网络之上的网络。与物理底层网络不同，覆盖网络是虚拟网络，由虚拟链路组成，虚拟链路共享底层的物理网络（底层网络），允许容器/虚拟机的部署在不修改底层网络的情况下互相连接。这里的虚拟链路指的是提供OVS之间连接的隧道。OVS支持多种隧道协议，常用的隧道包括GRE和VxLAN。
- en: 'The key benefits of the overlay network are:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖网络的关键好处是：
- en: As it is a logical network, it is possible to create and destroy the overlay
    network very easily without any change in the underlay networks. To create an
    overlay network between two nodes, just create a tunnel between the nodes, and
    to destroy the overlay network, unconfigure the tunnel interface.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于它是一个逻辑网络，因此可以很容易地创建和销毁覆盖网络，而无需对底层网络进行任何更改。要在两个节点之间创建覆盖网络，只需在节点之间创建一个隧道，而要销毁覆盖网络，则需要取消配置隧道接口。
- en: It is possible to create multiple overlay networks across the nodes. For instance,
    there is a possibility to create multiple overlay networks based on the number
    of customers deployed in a server instance. This provides a way of virtualizing
    the network similar to server virtualization. Let us look into the details of
    network virtualization.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在节点之间创建多个覆盖网络。例如，可以根据在服务器实例中部署的客户数量创建多个覆盖网络。这为网络虚拟化提供了一种方式，类似于服务器虚拟化。让我们深入了解网络虚拟化的细节。
- en: Introduction to network virtualization
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络虚拟化简介
- en: Network virtualization is one of the most widely discussed topics in the recent
    past in the networking industry. To understand network virtualization better,
    think of server virtualization wherein the physical infrastructures are logically
    segregated into multiple virtual devices, each assigning to different containers
    for performing its workload. Similar to server virtualization, there is a requirement
    to virtualize the networking layer that provides connectivity for different virtual
    machines/containers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 网络虚拟化是最近在网络行业中广泛讨论的话题之一。为了更好地理解网络虚拟化，可以想象服务器虚拟化，其中物理基础设施被逻辑上分隔成多个虚拟设备，每个虚拟设备分配给不同的容器以执行其工作负载。与服务器虚拟化类似，网络层也需要进行虚拟化，以便为不同的虚拟机/容器提供连接。
- en: As in server virtualization, wherein the customer will have full access to the
    virtualized server infrastructure, customers may also want to virtualize the networking
    infrastructure to secure data traffic between their VMs or containers. They don't
    want others to expose the data exchange that is happening between their applications
    to other customers' VMs or containers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与服务器虚拟化类似，客户可以完全访问虚拟化的服务器基础设施，客户也可能希望虚拟化网络基础设施，以确保虚拟机或容器之间的数据流量安全。他们不希望其他人看到他们应用程序之间的数据交换，尤其是其他客户的虚拟机或容器。
- en: Network virtualization as a concept is not new to the networking world. Network
    virtualization is realized in existing networks using technologies or concepts
    such as VLAN, VRF, L2VPN, L3VPN, and so on. These network virtualization techniques
    provide a mechanism for isolating traffic from one customer to another customer.
    VLAN provides a way of logically segregating the layer2 broadcast domain based
    on the VLAN tag.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 网络虚拟化作为一个概念，在网络领域并不新鲜。现有网络通过 VLAN、VRF、L2VPN、L3VPN 等技术或概念实现网络虚拟化。这些网络虚拟化技术提供了一种机制，用于隔离一个客户与另一个客户的流量。VLAN
    提供了一种基于 VLAN 标签逻辑隔离二层广播域的方法。
- en: These technologies also define the necessary protocol support to have overlapping
    address spaces across different customers. Say, for instance, using VRF, it is
    possible for two or more customers to use and share their IP address across different
    sites.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术还定义了必要的协议支持，以便在不同客户之间实现重叠的地址空间。例如，使用 VRF 时，两个或更多客户可以在不同站点之间共享其 IP 地址。
- en: However, these technologies are not providing true network virtualization throughout
    the network. These technologies also have their own limitations. The 1026 number
    of VLAN limits the number of tenants in the network. Similarly for VPN support,
    protocols like MPLS may be required that are typically deployed in a service provider
    network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些技术并没有在整个网络中提供真正的网络虚拟化。这些技术也有其自身的局限性。例如，1026个 VLAN 限制了网络中租户的数量。对于 VPN 支持，可能需要像
    MPLS 这样的协议，而 MPLS 通常在服务提供商网络中部署。
- en: As more and more operators and cloud providers are deploying **Software Defined
    Networking** (**SDN**) and **Network Function Virtualization** (**NFV**), it is
    necessary to provide a mechanism to provide network virtualization and traffic
    isolation in a better way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的运营商和云服务提供商部署**软件定义网络**（**SDN**）和**网络功能虚拟化**（**NFV**），有必要提供一种机制，以更好地提供网络虚拟化和流量隔离。
- en: The overlay network described in the previous chapter can provide an effective
    mechanism to isolate data traffic across different tenants or customers. As multiple
    overlay networks (one for each different customer) can be laid out over the underlay
    physical infrastructure, we should be able to provide the required traffic isolation
    between different customer traffic. Hence, the overlay network infrastructure
    provides an easy way of providing network virtualization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中描述的覆盖网络可以提供一种有效的机制来隔离不同租户或客户之间的数据流量。由于可以在下层物理基础设施上布置多个覆盖网络（每个客户一个），我们应该能够提供不同客户流量之间所需的隔离。因此，覆盖网络基础设施提供了一种简便的方式来提供网络虚拟化。
- en: To create an overlay network for a customer or tenant, we need to create a tunnel
    across all the nodes wherein the customer's/tenant's application is deployed.
    OVS helps in creating a tunnel across the different OVS instances and hence supports
    the creation of VTNs and underlay networks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要为客户或租户创建覆盖网络，我们需要在客户/租户的应用程序部署的所有节点之间创建一个隧道。OVS 有助于在不同的 OVS 实例之间创建隧道，从而支持创建
    VTN 和下层网络。
- en: 'Going back to the previous diagram, there are two customers, shown as green
    and orange. Both customers'' VMs are running in both server1 and server2\. In
    order to provide network virtualization and isolate the traffic across these two
    customers, the following steps can be used:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到之前的示意图，有两个客户，分别以绿色和橙色显示。两个客户的虚拟机都运行在 server1 和 server2 中。为了提供网络虚拟化并隔离这两个客户之间的流量，可以使用以下步骤：
- en: Create two bridge instances in OVS, one for each customer, as Greenbr and Orangebr.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 OVS 中为每个客户创建两个桥接实例，分别为 Greenbr 和 Orangebr。
- en: Attach the VM's virtual NIC interface (veth) to the corresponding bridge instance.
    For example, the green VM's virtual NIC should be attached to Greenbr and the
    orange VM's virtual NIC interface should be attached to Orangebr.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将虚拟机的虚拟网卡接口（veth）附加到相应的桥接实例。例如，绿色虚拟机的虚拟网卡应附加到 Greenbr，橙色虚拟机的虚拟网卡接口应附加到 Orangebr。
- en: Create two tunnels, say `Green_tun` and `Orange_tun`, between server1 and server2\.
    The two server instances can be part of the same network or different networks.
    If they are part of different networks, one or more routers should be deployed
    to provide physical connectivity between these servers.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 server1 和 server2 之间创建两个隧道，例如`Green_tun`和`Orange_tun`。这两个服务器实例可以位于同一网络中，也可以位于不同的网络中。如果它们位于不同的网络中，则应部署一个或多个路由器来提供这些服务器之间的物理连接。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To create a tunnel between two nodes, there should be IP reachability between
    these two nodes. IP reachability will be provided by the underlay network.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了在两个节点之间创建隧道，这两个节点之间应该具有 IP 可达性。IP 可达性由底层网络提供。
- en: Attach these two tunnels to the respective bridge instances.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这两个隧道附加到相应的桥接实例。
- en: 'With these simple steps, it is possible to create a virtual network for different
    customers. This is illustrated in the following diagram:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些简单的步骤，可以为不同的客户创建虚拟网络。如下图所示：
- en: '![Introduction to network virtualization](img/00033.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![网络虚拟化简介](img/00033.jpeg)'
- en: OpenFlow support in OVS
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OVS 中的 OpenFlow 支持
- en: One of the key advantages of using OVS is that it supports the OpenFlow protocol
    and supports flow-based switching. **OpenFlow** is a protocol defined by ONF to
    manage the network infrastructure centrally with standard interfaces between the
    controller (traditionally called the control plane) and the actual packet-forwarding
    entity (traditionally called the data plane). Enabling the network to be programmed
    centrally makes the whole system more agile and flexible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 OVS 的主要优势之一是它支持 OpenFlow 协议并支持基于流的交换。**OpenFlow** 是由 ONF 定义的协议，用于通过标准接口集中管理网络基础设施，控制器（传统上称为控制平面）与实际的数据转发实体（传统上称为数据平面）之间的接口。使网络能够集中编程，能让整个系统更加灵活和敏捷。
- en: OpenFlow promises to ease the way of provisioning large data centers and server
    clusters that can be managed centrally using OpenFlow controllers. With large
    data centers and server clusters, there is a clear necessity of changing the traditional
    control plane and data plane paradigm to move toward flow-based switching, which
    is more generic and can be adoptable for different avenues. Software Defined Networking
    (SDN) is a new paradigm shift in networking.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFlow 承诺简化大型数据中心和服务器集群的配置方式，这些数据中心和服务器集群可以通过 OpenFlow 控制器进行集中管理。随着大型数据中心和服务器集群的出现，传统的控制平面和数据平面范式的改变变得尤为必要，转向基于流的交换，这种方式更加通用，并且可以适应不同的领域。软件定义网络（SDN）是网络领域中的一种新范式。
- en: The OpenFlow specification defines three different components in an OpenFlow-based
    network as follows.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFlow 规范定义了基于 OpenFlow 的网络中的三个不同组件，如下所示。
- en: OpenFlow switch
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenFlow 交换机
- en: An **OpenFlow switch** consists of one or more flow tables, meter table, group
    table, and OpenFlow channels to the external controller. The flow tables and group
    table are used during the lookup or forwarding phase of packet pipeline processing
    in order to forward the packet to the appropriate port, whereas the meter table
    is used to perform simple QoS operations, such as rate-limiting to complex QOS
    operations, such as DiffServ and so on. The switch communicates with the controller
    and the controller manages the switch via the OpenFlow protocol using OpenFlow
    messages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 **OpenFlow 交换机** 由一个或多个流表、计量表、组表以及与外部控制器的 OpenFlow 通道组成。在包处理的查找或转发阶段，流表和组表用于将数据包转发到适当的端口，而计量表用于执行简单的
    QoS 操作，如速率限制，以及更复杂的 QoS 操作，如 DiffServ 等。交换机与控制器通信，控制器通过 OpenFlow 协议使用 OpenFlow
    消息管理交换机。
- en: OpenFlow controller
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenFlow 控制器
- en: An **OpenFlow controller** typically manages one or more OpenFlow switches remotely
    via OpenFlow channels. Similarly, a single switch can be managed by multiple controllers
    for better reliability and better load balancing. The OpenFlow controller acts
    in a similar way to the control plane of typical traditional switches or routers.
    The controller is responsible for programming various tables, such as flow table,
    group table, and meter table using OpenFlow protocol messages to provide network
    connectivity or network functions across various nodes in the system
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 **OpenFlow 控制器** 通常通过 OpenFlow 通道远程管理一个或多个 OpenFlow 交换机。类似地，单个交换机可以由多个控制器管理，以提高可靠性和负载均衡。OpenFlow
    控制器的作用类似于传统交换机或路由器的控制平面。控制器负责使用 OpenFlow 协议消息编程各种表格，如流表、组表和计量表，以在系统中提供网络连接或网络功能。
- en: OpenFlow channel
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenFlow 通道
- en: 'An **OpenFlow channel** is used to exchange OpenFlow messages between an OpenFlow
    switch and an OpenFlow controller. The switch must be able to create an OpenFlow
    channel by initiating a connection to the controller:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 **OpenFlow 通道** 用于在 OpenFlow 交换机和 OpenFlow 控制器之间交换 OpenFlow 消息。交换机必须能够通过启动与控制器的连接来创建
    OpenFlow 通道：
- en: '![OpenFlow channel](img/00034.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![OpenFlow 通道](img/00034.jpeg)'
- en: With OVS, the entire CoreOS cluster's overlay network can be centrally managed
    by a controller with very simple configurations. The ofctl utility provided by
    OVS is helpful in programming the flow tables using a command-line argument without
    being controlled by an external controller.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OVS，可以通过控制器以非常简单的配置集中管理整个CoreOS集群的覆盖网络。OVS提供的ofctl实用工具有助于通过命令行参数编程流表，而无需由外部控制器进行控制。
- en: Running OVS in CoreOS
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CoreOS中运行OVS
- en: 'There are two ways to run or install OVS in a CoreOS environment:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoreOS环境中，有两种方法可以运行或安装OVS：
- en: Build a CoreOS image with OVS
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建带有OVS的CoreOS镜像
- en: Run OVS inside a Docker container with the `–net=host` option
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`–net=host`选项在Docker容器中运行OVS
- en: As we have already seen in [Chapter 1](part0014_split_000.html#DB7S1-31555e2039a14139a7f00b384a5a2dd8
    "Chapter 1. CoreOS, Yet Another Linux Distro?"), *CoreOS, Yet Another Linux Distro*
    in CoreOS there is no way to install an application. Any service/application should
    be deployed in a container. So the simple way to run OVS is to run OVS inside
    a Docker container. Let us see how we can install an OVS docker in CoreOS.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](part0014_split_000.html#DB7S1-31555e2039a14139a7f00b384a5a2dd8 "第1章
    CoreOS，另一个Linux发行版？")中已经看到的那样，*CoreOS，另一个Linux发行版*，在CoreOS中没有安装应用程序的方法。任何服务/应用程序都应部署在容器中。因此，运行OVS的简单方法是将OVS运行在Docker容器内。让我们看看如何在CoreOS中安装OVS
    Docker。
- en: 'There is already a docker image available with OVS (coreos-ovs). Download this
    docker image from [https://github.com/theojulienne/coreos-ovs](https://github.com/theojulienne/coreos-ovs)
    github link. Use the following `cloud-config` to start this container:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一个带有OVS的docker镜像（coreos-ovs）。可以从[https://github.com/theojulienne/coreos-ovs](https://github.com/theojulienne/coreos-ovs)的GitHub链接下载此docker镜像。使用以下`cloud-config`启动此容器：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This starts a docker container that has OVS installed. Along with that, it removes
    the IP address of docker0 bridge and assigns it to OVS bridge (bridge0). docker0
    bridge will be attached to bridge0 as a link.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一个安装了OVS的Docker容器。同时，它会移除docker0桥接器的IP地址并将其分配给OVS桥接器（bridge0）。docker0桥接器将作为链接连接到bridge0。
- en: As we are using the `–net=host` option, any OVS command we are executing inside
    this container will result in changing the network configuration of the host OS,
    which is the CoreOS network stack.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了`–net=host`选项，我们在容器内执行的任何OVS命令都将导致修改主机操作系统的网络配置，即CoreOS网络堆栈。
- en: 'This section describes in detail how to provide a virtual tenant network between
    docker containers that are running in two different CoreOS instances. There are
    multiple ways to provide the solution. We are going to see the two most common
    and simple ways of providing the solution:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述了如何在两个不同的CoreOS实例中运行的docker容器之间提供虚拟租户网络。提供此解决方案有多种方法。我们将介绍提供此解决方案的两种最常见且简单的方法：
- en: Attaching docker0 bridge to OVS
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将docker0桥接器连接到OVS
- en: Attaching the container's veth interface to OVS
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将容器的veth接口连接到OVS
- en: Attaching docker0 bridge to OVS
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将docker0桥接器连接到OVS
- en: This is a simple way of providing connectivity across different containers using
    OVS. In this case, OVS should be connected to docker0 bridge (which is already
    connected to all the containers) using a veth interface. Refer to the previous
    chapter for more detail about docker0 bridge and how it provides connectivity
    for the containers in a system.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种通过OVS提供不同容器之间连接的简单方法。在这种情况下，OVS应通过veth接口连接到docker0桥接器（该桥接器已连接到所有容器）。有关docker0桥接器的更多详细信息以及它如何为系统中的容器提供连接，请参见前一章节。
- en: The docker bridge is intern connected to the OVS bridge. The OVS bridge provides
    connectivity to the other CoreOS instances using GRE/VxLAN tunnels.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Docker桥接器通过内部连接到OVS桥接器。OVS桥接器通过GRE/VxLAN隧道提供与其他CoreOS实例的连接。
- en: '![Attaching docker0 bridge to OVS](img/00035.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![将docker0桥接器连接到OVS](img/00035.jpeg)'
- en: 'The step-by-step procedure with configuration is described in detail as follows.
    This consists of the following major steps on both the CoreOS instances:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下详细描述了带有配置的逐步过程。此过程包含在两个CoreOS实例上的以下主要步骤：
- en: Configurations during the instantiation of a CoreOS node in a cluster
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中实例化CoreOS节点期间的配置
- en: Configurations during the creation of a container
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建容器期间的配置
- en: Configuration in CoreOS Instance 1
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreOS实例1中的配置
- en: This section describes in detail the operations to be performed on the coreos-ovs
    docker of CoreOS node1 to provide this solution.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述了在CoreOS节点1的coreos-ovs Docker上执行的操作，以提供此解决方案。
- en: Configurations during the instantiation of a CoreOS node 1 in a cluster
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在集群中实例化CoreOS节点1时的配置
- en: 'At the time of CoreOS server boot-up, OVS needs to be started and the procedure
    to start OVS is as follows. Note that the way in which the OVS command will be
    executed depends on whether OVS is deployed inside a docker container or the CoreOS
    host instance. However, in both cases, there is no change in the list of OVS commands
    to be used:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在CoreOS服务器启动时，必须启动OVS，启动OVS的过程如下。请注意，执行OVS命令的方式取决于OVS是部署在docker容器内还是CoreOS主机实例中。不过在这两种情况下，使用的OVS命令列表是相同的：
- en: 'Run the OVS data-path module using the command:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS数据路径模块：
- en: '[PRE1]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a configuration, `db`, using the default schema file with the following
    command:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，使用默认的schema文件创建一个配置`db`：
- en: '[PRE2]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Run the OVS DB server using the following command:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS DB服务器：
- en: '[PRE3]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run OVS-VSCTL using the following command:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS-VSCTL：
- en: '[PRE4]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Run the OVS switchd daemon using the following command:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS switchd守护进程：
- en: '[PRE5]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a bridge instance:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建桥接实例：
- en: '[PRE6]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.103`. Here, the assumption
    is the etho IP of CoreOS instance 2 is `172.17.8.103`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用远程节点`172.17.8.103`创建GRE隧道。这里假设CoreOS实例2的etho IP是`172.17.8.103`：
- en: '[PRE7]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The key needs to be different for each tunnel.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个隧道的密钥需要不同。
- en: 'Create a veth interface to provide a connection between docker0 bridge and
    OVS:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建veth接口以提供docker0桥接和OVS之间的连接：
- en: 'Create the veth pair:'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建veth对：
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Attach one end of the veth pair to docker0 bridge:'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将veth对的一端附加到docker0桥接：
- en: '[PRE9]'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Attach the other end of the veth pair to OVS:'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将veth对的另一端附加到OVS：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Configurations during the creation of a container for CoreOS Instance 1
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在为CoreOS实例1创建容器时的配置
- en: This section describes the configuration to be done when a new container is
    created in the CoreOS instance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述在CoreOS实例中创建新容器时需要进行的配置。
- en: Tip
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As by default, the eth0 (one end of the veth pair) interface of the container
    is attached to docker0 bridge, we need not explicitly attach the container veth
    interface to docker0 bridge.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，容器的eth0（veth对的一端）接口附加在docker0桥接上，因此我们不需要显式地将容器的veth接口附加到docker0桥接上。
- en: 'Set the IP address of the eth0 interface of the docker container. It is not
    possible to set the IP address of the docker container inside the docker instance.
    We need to use the `nsenter` utility for this. To do this, follow these steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 设置docker容器eth0接口的IP地址。无法在docker实例内部设置docker容器的IP地址。我们需要使用`nsenter`工具来完成此操作。请按照以下步骤进行操作：
- en: 'Execute the following command and get the `pid`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取`pid`：
- en: '[PRE11]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Execute the following command and get the `pid`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取`pid`：
- en: '[PRE12]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Configuration in CoreOS Instance 2
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreOS实例2中的配置
- en: This section describes in detail the operations to be performed on the coreos-ovs
    docker of CoreOS node2 to provide this solution.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述在CoreOS节点2的coreos-ovs Docker上执行的操作，以提供此解决方案。
- en: Configurations during the instantiation of CoreOS node 2 in a cluster
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在集群中实例化CoreOS节点2时的配置
- en: 'This section describes the list of operations to be performed during the initialization
    of the CoreOS instance. During initialization, OVS needs to be started and the
    procedure to start OVS is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述在CoreOS实例初始化过程中需要执行的操作列表。在初始化过程中，必须启动OVS，启动OVS的过程如下：
- en: 'Run the OVS data-path module using the command:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS数据路径模块：
- en: '[PRE13]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a configuration, `db`, using the default schema file with the following
    command:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令，使用默认的schema文件创建一个配置`db`：
- en: '[PRE14]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Run the OVS DB server using the following command:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS DB服务器：
- en: '[PRE15]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run OVS-VSCTL using the following command:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS-VSCTL：
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run the OVS switchd daemon using the following command:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令运行OVS switchd守护进程：
- en: '[PRE17]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create a bridge instance:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建桥接实例：
- en: '[PRE18]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.101`. Here, the assumption
    is the etho IP of CoreOS instance 1 is `172.17.8.101`:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用远程节点`172.17.8.101`创建GRE隧道。这里假设CoreOS实例1的etho IP是`172.17.8.101`：
- en: '[PRE19]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tip
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The key needs to be different for each tunnel.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个隧道的密钥需要不同。
- en: 'Now we need to create a veth interface to provide a connection between docker0
    bridge and OVS:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个veth接口来提供docker0桥接和OVS之间的连接：
- en: 'Create the veth pair:'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建veth对：
- en: '[PRE20]'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Attach one end of the veth pair to docker0 bridge:'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将veth对的一端附加到docker0桥接：
- en: '[PRE21]'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Attach the other end of the veth pair to OVS:'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将veth对的另一端附加到OVS：
- en: '[PRE22]'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Configurations during the creation of a container for CoreOS Instance 2
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CoreOS 实例 2 中创建容器时的配置
- en: This section describes the configuration to be done when a new container is
    created in the CoreOS instance.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在 CoreOS 实例中创建新容器时需要进行的配置。
- en: 'Set the IP address of the eth0 interface of the docker container. It is not
    possible to set the IP address of the docker container inside the docker instance.
    We need to use the `nsenter` utility for this. To do this, follow these steps:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Docker 容器的 eth0 接口的 IP 地址。无法在 Docker 实例内部设置 Docker 容器的 IP 地址。我们需要使用 `nsenter`
    工具来实现。按照以下步骤操作：
- en: 'Execute the following command and get the `pid`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取 `pid`：
- en: '[PRE23]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Execute the following command and get the `pid`:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取 `pid`：
- en: '[PRE24]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now you should be able to ping from the docker container running in CoreOS instance
    1 to a docker container running in CoreOS instance 2\. The main disadvantage of
    this solution is tha it is not possible to provide a virtual tenant network using
    this solution. This is because all the docker containers are attached to docker0
    bridge, which is connected to OVS. OVS acts as a way to provide communication
    between different server instances.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能够从在 CoreOS 实例 1 中运行的 Docker 容器 ping 到在 CoreOS 实例 2 中运行的 Docker 容器。这个解决方案的主要缺点是无法为租户提供虚拟网络。这是因为所有
    Docker 容器都连接到 docker0 网桥，而该网桥又连接到 OVS。OVS 作为不同服务器实例之间提供通信的方式。
- en: Attaching container's veth interface to OVS
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将容器的 veth 接口连接到 OVS
- en: In this case, all the docker containers in the CoreOS instance are attached
    directly to the OVS bridge. There will be multiple instance of bridge running
    inside OVS, each mapping to different customers/tenants. A new bridge needs to
    be created and provisioned for each tenant in the system. On the subsequent creation
    of containers (for the same tenant), the container's interface should be connected
    to the corresponding bridge instance. The OVS bridge provides connectivity to
    the other CoreOS instances using GRE/VxLAN tunnels.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，CoreOS 实例中的所有 Docker 容器都直接连接到 OVS 网桥。OVS 内部将运行多个网桥实例，每个实例映射到不同的客户/租户。每个租户在系统中需要创建并配置一个新的网桥。在后续创建容器时（对于相同租户），容器的接口应连接到相应的网桥实例。OVS
    网桥通过 GRE/VxLAN 隧道为其他 CoreOS 实例提供连接。
- en: '![Attaching container''s veth interface to OVS](img/00036.jpeg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![将容器的 veth 接口连接到 OVS](img/00036.jpeg)'
- en: 'The step-by-step procedure to configure this kind of solution is described
    in detail as follows. This consists of the following major steps to be performed
    on both the CoreOS instances:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 配置这种解决方案的逐步程序如下所述，主要包括以下步骤，这些步骤需要在两个 CoreOS 实例上执行：
- en: Configurations during the instantiation of a CoreOS node in a cluster
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中实例化 CoreOS 节点时的配置
- en: Configurations during the creation of the first container for a tenant
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为租户创建第一个容器时的配置
- en: Configurations during the creation of subsequent containers for a tenant
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为租户创建后续容器时的配置
- en: Configuration in CoreOS Instance 1
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreOS 实例 1 中的配置
- en: This section describes in detail the operations to be performed on the coreos-ovs
    docker of CoreOS node1 to provide this solution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述了在 CoreOS 节点 1 的 coreos-ovs Docker 中执行的操作，以提供该解决方案。
- en: Configurations during the instantiation of a CoreOS node in a cluster
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在集群中实例化 CoreOS 节点时的配置
- en: During initialization, OVS needs to be started and the procedures to start OVS
    are as follows. Note that the way in which the OVS command will be executed depends
    on whether OVS is deployed inside a docker container or the CoreOS host instance.
    However, in both cases, there is no change in the list of OVS commands to be used.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化期间，OVS 需要启动，启动 OVS 的程序如下。请注意，OVS 命令的执行方式取决于 OVS 是部署在 Docker 容器内还是 CoreOS
    主机实例中。然而，在这两种情况下，所需使用的 OVS 命令列表没有变化。
- en: 'Run the OVS data-path module using the command:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS 数据路径模块：
- en: '[PRE25]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a configuration, `db`, using the default schema file with the following
    command:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，使用默认模式文件创建配置 `db`：
- en: '[PRE26]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Run the OVS DB server using the following command:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS DB 服务器：
- en: '[PRE27]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run OVS-VSCTL using the following command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS-VSCTL：
- en: '[PRE28]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the OVS switchd daemon using the following command:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS switchd 守护进程：
- en: '[PRE29]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Configurations during the creation of the first container for a tenant
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为租户创建第一个容器时的配置
- en: 'When a container is created for a tenant for the first time, a new bridge needs
    to be created and this container should be connected to OVS. The procedure to
    do this is described in detail as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当为租户首次创建容器时，需要创建一个新的桥接，并将此容器连接到 OVS。具体的操作流程如下：
- en: 'Bring down the docker0 bridge instance (the default bridge created by docker):'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 docker0 桥接实例关闭（docker 创建的默认桥接）：
- en: '[PRE30]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Detach the virtual interface that is created for the container from docker0
    bridge. The virtual interface starts with the name as veth:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将为容器创建的虚拟接口从 docker0 桥接中分离。该虚拟接口的名称以 veth 开头：
- en: '[PRE31]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create a bridge instance for a tenant:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为租户创建一个桥接实例：
- en: '[PRE32]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add the port that is created in docker. This interface starts with veth:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Docker 中添加已创建的端口。此接口以 veth 开头：
- en: '[PRE33]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Set the IP address of the eth0 interface of the docker container. It is not
    possible to set the IP address of the docker container inside the docker instance.
    We need to use the `nsenter` utility for this. To do this, follow these steps:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Docker 容器的 eth0 接口的 IP 地址。不能在 Docker 实例内设置 Docker 容器的 IP 地址。我们需要使用 `nsenter`
    工具来实现。按照以下步骤操作：
- en: 'Execute the following command and get the `pid`:'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取 `pid`：
- en: '[PRE34]'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Execute the following command and get the `pid`:'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取 `pid`：
- en: '[PRE35]'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Create a GRE tunnel with the remote node as `172.17.8.103`. Here, the assumption
    is the eth0 IP of CoreOS instance 2 is `172.17.8.103`
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 GRE 隧道，远程节点 IP 为 `172.17.8.103`。此处假设 CoreOS 实例 2 的 eth0 IP 为 `172.17.8.103`
- en: '[PRE36]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tip
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The key needs to be different for each tunnel.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个隧道的密钥需要不同。
- en: Configurations during the creation of subsequent containers for a tenant
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在为租户创建后续容器时的配置
- en: This section describes the configuration to be done when subsequent containers
    are being created in the CoreOS instance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在 CoreOS 实例中创建后续容器时需要进行的配置。
- en: 'Add the port that is created in docker. This interface starts with veth:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Docker 中添加已创建的端口。此接口以 veth 开头：
- en: '[PRE37]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.103`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 GRE 隧道，远程节点 IP 为 `172.17.8.103`：
- en: '[PRE38]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Configuration in CoreOS Instance 2
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreOS 实例 2 中的配置
- en: This section describes in detail the operations to be performed on the coreos-ovs
    docker of CoreOS node2 to provide this solution.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述了在 CoreOS 节点 2 的 coreos-ovs Docker 中执行的操作，以提供此解决方案。
- en: Configurations during the instantiation of a CoreOS node in a cluster
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在集群中实例化 CoreOS 节点时的配置
- en: 'This section describes the list of operations to be performed during the initialization
    of the CoreOS instance. During initialization, OVS needs to be started and the
    procedure to start OVS is as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在初始化 CoreOS 实例时需要执行的操作列表。在初始化过程中，需要启动 OVS，启动 OVS 的步骤如下：
- en: 'Run the OVS data-path module using the command:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS 数据路径模块：
- en: '[PRE39]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create a configuration, `db`, using the default schema file with the following
    command:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认模式文件通过以下命令创建配置 `db`：
- en: '[PRE40]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Run the OVS DB server using the following command:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS DB 服务器：
- en: '[PRE41]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Run OVS-VSCTL using the following command:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS-VSCTL：
- en: '[PRE42]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Run the OVS switchd daemon using the following command:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行 OVS switchd 守护进程：
- en: '[PRE43]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create a bridge instance:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个桥接实例：
- en: '[PRE44]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.101`. Here, the assumption
    is the etho IP of CoreOS instance 1 is `172.17.8.101`:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 GRE 隧道，远程节点 IP 为 `172.17.8.101`。此处假设 CoreOS 实例 1 的 eth0 IP 为 `172.17.8.101`：
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Tip
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The key needs to be different for each tunnel.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个隧道的密钥需要不同。
- en: Configurations during the creation of the first container for a tenant
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在为租户创建第一个容器时的配置
- en: 'When a container is created for a tenant for the first time, a new bridge needs
    to be created and this container should be connected to OVS. The procedure to
    do this is described in detail as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当为租户首次创建容器时，需要创建一个新的桥接，并将此容器连接到 OVS。具体的操作流程如下：
- en: 'Bring down the docker0 bridge instance (the default bridge created by docker):'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 docker0 桥接实例关闭（docker 创建的默认桥接）：
- en: '[PRE46]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Detach the virtual interface that is created for the container from docker0
    bridge. The virtual interface starts with the name as veth:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将为容器创建的虚拟接口从 docker0 桥接中分离。该虚拟接口的名称以 veth 开头：
- en: '[PRE47]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create a bridge instance for a tenant:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为租户创建一个桥接实例：
- en: '[PRE48]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Add the port that is created in docker. This interface starts with veth:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Docker 中添加已创建的端口。此接口以 veth 开头：
- en: '[PRE49]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Set the IP address of the eth0 interface of the docker container. It is not
    possible to set the IP address of the docker container inside the docker instance.
    We need to use the `nsenter` utility for this. To do this, follow these steps:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Docker 容器 eth0 接口的 IP 地址。无法在 Docker 实例内设置 Docker 容器的 IP 地址。我们需要使用 `nsenter`
    工具来完成此操作。请按照以下步骤操作：
- en: 'Execute the following command and get the `pid`:'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取`pid`：
- en: '[PRE50]'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Execute the following command and get the `pid`:'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行以下命令并获取`pid`：
- en: '[PRE51]'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.103`. Here, the assumption
    is the etho IP of CoreOS instance 2 is `172.17.8.103`:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 GRE 隧道，远程节点为`172.17.8.103`。这里假设 CoreOS 实例 2 的 eth0 IP 为`172.17.8.103`：
- en: '[PRE52]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Tip
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The key needs to be different for each tunnel.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个隧道的密钥需要不同。
- en: Configurations during the creation of subsequent containers for a tenant
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为租户创建后续容器时的配置
- en: This section describes the configuration to be done when subsequent containers
    are being created in the CoreOS instance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了在 CoreOS 实例中创建后续容器时需要进行的配置。
- en: 'Add the port that is created in docker. This interface starts with veth:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加在 Docker 中创建的端口。此接口以 veth 开头：
- en: '[PRE53]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create a GRE tunnel with the remote node as `172.17.8.103`:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 GRE 隧道，远程节点为`172.17.8.103`：
- en: '[PRE54]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now you should be able to ping from the docker container running in CoreOS instance
    1 to a docker container running in CoreOS instance 2\. The main advantage of this
    solution is that it is possible to provide a virtual tenant network using this
    solution.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够从在 CoreOS 实例 1 中运行的 Docker 容器 ping 到在 CoreOS 实例 2 中运行的 Docker 容器。这个解决方案的主要优点是，使用此方案可以提供虚拟租户网络。
- en: Looping issue
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环问题
- en: Everything works fine so far. However, when the number of CoreOS instances running
    in the cluster increases, we may need to create a mesh of tunnels between CoreOS
    instances for each customer/tenant. This ends up creating a loop in the network
    that will result in a traffic black hole. Let us look into this issue in detail
    and discuss the solution.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切正常。然而，当集群中运行的 CoreOS 实例数量增加时，我们可能需要为每个客户/租户在 CoreOS 实例之间创建隧道网格。这将导致网络中出现环路，从而导致流量黑洞。让我们详细了解这个问题并讨论解决方案。
- en: 'Consider a topology wherein you have three CoreOS instances running in the
    CoreOS cluster. In each of these instances, the green and orange customers'' applications
    are deployed as a container. To provide VTN for each customer, we need to create
    tunnels across these CoreOS instances. In this case, we need to create two tunnels
    for each customer from every CoreOS instance. From CoreOS instance 1, we need
    to create two tunnels for each customer: one toward CoreOS instance 2 and the
    other toward CoreOS instance 3\. Similarly, from CoreOS instance 2, we need to
    create two tunnels and so on. This will result in forming a layer2 loop in the
    customer''s bridge instance.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种拓扑结构，其中在 CoreOS 集群中运行着三个 CoreOS 实例。在这些实例中，绿色和橙色客户的应用程序作为容器进行部署。为了为每个客户提供
    VTN，我们需要在这些 CoreOS 实例之间创建隧道。在这种情况下，我们需要为每个客户从每个 CoreOS 实例创建两个隧道。从 CoreOS 实例 1
    开始，我们需要为每个客户创建两个隧道：一个指向 CoreOS 实例 2，另一个指向 CoreOS 实例 3。同样，从 CoreOS 实例 2 开始，我们需要创建两个隧道，依此类推。这将导致在客户的桥接实例中形成一个层
    2 环路。
- en: Tip
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The total number of tunnels required to create a complete mesh in the topology
    is 2n-1, where n is the number of CoreOS instances wherein the tenant's service
    is deployed as a container.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在拓扑中创建完整网格所需的隧道总数为 2n-1，其中 n 是租户服务作为容器部署的 CoreOS 实例数量。
- en: 'As the bridge instance is a layer2 device, this results in forwarding the same
    packet multiple times in the loop:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于桥接实例是一个层 2 设备，这将导致在环路中多次转发相同的数据包：
- en: '![Looping issue](img/00037.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![循环问题](img/00037.jpeg)'
- en: A simple way to avoid this looping problem is by running **Spanning Tree Protocol**
    (**STP**) in OVS. STP is defined and standardized as IEEE 802.1D. STP identifies
    a loop-free topology, considering all the links in the topology based on different
    metrics. Once it identifies the loop-free topology, it will block one or more
    ports (in this case, tunnels) that are not part of the loop-free topology. The
    ports that are in a blocking state won't forward the traffic and hence avoid the
    traffic black hole.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这个循环问题的一个简单方法是通过在 OVS 中运行**生成树协议**（**STP**）。STP 已被定义并标准化为 IEEE 802.1D。STP
    会根据不同的度量标准，识别出无环拓扑结构。一旦它识别出无环拓扑，它将阻塞一个或多个不属于无环拓扑的端口（在这种情况下，是隧道）。处于阻塞状态的端口将不会转发流量，从而避免了流量黑洞。
- en: 'In the preceding topology, when we run the spanning tree based on the priority
    or configured bridge-id, STP blocks one port, in this case blocks the port from
    CoreOS 3 to CoreOS 2:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述拓扑中，当我们根据优先级或配置的桥接 ID 运行生成树时，STP 会阻止一个端口，在本例中阻止从 CoreOS 3 到 CoreOS 2 的端口：
- en: '![Looping issue](img/00038.jpeg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![环路问题](img/00038.jpeg)'
- en: 'The list of commands to enable and configure the spanning tree in OVS are as
    follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 启用和配置 OVS 中生成树的命令列表如下：
- en: 'Enable the spanning tree on a bridge instance:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在桥接实例上启用生成树：
- en: '[PRE55]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Set the bridge priority:'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置桥接优先级：
- en: '[PRE56]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Set the path cost of the port:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置端口的路径成本：
- en: '[PRE57]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Tip
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The bridge priority and path cost configurations are not mandatory configurations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接优先级和路径成本配置不是强制性配置。
- en: The spanning tree needs to be enabled on all the bridge instances of OVS to
    avoid any loop in the network.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在 OVS 的所有桥接实例上启用生成树，以避免网络中出现任何环路。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen the importance of OVS in container communications
    and the various advantages provided by OVS. As there are multiple communication
    mechanisms available for container communications, while deploying the CoreOS
    cluster, based on the advantages, ease of use, and network management tools, you
    should cautiously choose one or more communication mechanisms in your deployment.
    In the next chapter, we are going to see some of the latest developments in CoreOS
    and advanced topics such as security, orchestration, container data volume management,
    and so on.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了 OVS 在容器通信中的重要性以及 OVS 提供的各种优势。由于容器通信有多种通信机制可供选择，在部署 CoreOS 集群时，基于优势、易用性和网络管理工具，您应该在部署中谨慎选择一种或多种通信机制。在下一章，我们将看到
    CoreOS 的一些最新发展以及安全性、编排、容器数据卷管理等高级主题。
