<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Managing Filesystems and Storage"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Managing Filesystems and Storage</h1></div></div></div><p>This chapter contains the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Viewing the size of files and available storage</li><li class="listitem" style="list-style-type: disc">Setting storage limits for users and groups</li><li class="listitem" style="list-style-type: disc">Creating a RAM disk</li><li class="listitem" style="list-style-type: disc">Creating a RAID</li><li class="listitem" style="list-style-type: disc">Replacing a device in a RAID</li><li class="listitem" style="list-style-type: disc">Creating a new LVM volume</li><li class="listitem" style="list-style-type: disc">Removing an existing LVM volume</li><li class="listitem" style="list-style-type: disc">Adding storage and growing an LVM volume</li><li class="listitem" style="list-style-type: disc">Working with LVM snapshots</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Introduction</h1></div></div></div><p>The recipes in this chapter focus on leveraging your CentOS system's storage to maintain availability, increase reliability, and to keep your data safe against inevitable disk failures. You'll learn how to determine how much space your files take up and how much storage is still available. Then, you'll see how to put limits in place to ensure that users use the system's storage resources equitably. We'll also create a RAM disk, a memory-based low latency storage for frequently accessed data. Then you'll learn how to create and manage RAID arrays to provide reliable storage, and how to work with LVM volumes to allocate logical drives from storage pools to better utilize your system's total storage capacity.</p></div></div>
<div class="section" title="Viewing the size of files and available storage"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Viewing the size of files and available storage</h1></div></div></div><p>Programs and services can behave unexpectedly or stop working entirely when storage space runs tight, so it's important to know how much space is available on our system. This recipe introduces a handful of commands used to determine how large your files and directories are and how much storage is used and is available.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec113"/>Getting ready</h2></div></div></div><p>This recipe requires a working CentOS system. Administrative privileges may be needed depending on the permissions of the directories and files you want to inspect.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec114"/>How to do it...</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To display the storage capacity of a mounted filesystem, use the <code class="literal">df</code> command:<pre class="programlisting">
<span class="strong"><strong>df -h /</strong></span>
</pre></li><li class="listitem" style="list-style-type: disc">To view the size of a file, use the <code class="literal">ls</code> command:<pre class="programlisting">
<span class="strong"><strong>ls -sh file.txt</strong></span>
</pre></li><li class="listitem" style="list-style-type: disc">To determine the size of a directory (the sum of sizes of all of its files), use the <code class="literal">du</code> command:<pre class="programlisting">
<span class="strong"><strong>du -sh ~</strong></span>
</pre></li></ul></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec115"/>How it works...</h2></div></div></div><p>The <code class="literal">df</code> command returns information about how much free space is available on a mounted filesystem. The preceding example asked for details about the root filesystem.</p><pre class="programlisting">
<span class="strong"><strong>df -h /</strong></span>
</pre><p>The <code class="literal">-h</code> argument formats the information in a human-readable format, listing the values as megabytes, gigabytes, and so on, as opposed to block counts. When invoked without any arguments, <code class="literal">df</code> displays its information in 512-byte block counts for all mounted filesystems. We can specify one or more mount points with this command, in which case <code class="literal">df</code> reports only on those filesystems.</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_001.jpg"/><div class="caption"><p>Values presented as megabytes and gigabytes are more informative than when given in block counts</p></div></div><p>The output's first column, labeled <code class="literal">Filesystem</code>, and the last, labeled <code class="literal">Mounted on</code>, identifies the filesystem and mount point it's been made available on, respectively. The <code class="literal">Size</code> column shows the total amount of space the filesystem provides. The <code class="literal">Used</code> column shows how much of that space is occupied and the <code class="literal">Avail</code> column shows how much is still available. <code class="literal">Use%</code> shows how much space is occupied as a percentage.</p><p>While <code class="literal">df</code> gives us a high-level view of our overall storage usage, to view the size of individual files we can use <code class="literal">ls</code>. The command supports a large number of arguments that show meta information for files and directories, such as their ownership details, create time, and size.</p><p>This recipe used the <code class="literal">-s</code> argument to return the file's size and <code class="literal">-h</code> to again display the value in a human-readable format:</p><pre class="programlisting">
<span class="strong"><strong>ls -hs filename.txt</strong></span>
</pre><p>If you use <code class="literal">ls</code> to show the size of a directory, it will likely report 4.0 K regardless of which directory you choose. This is because directories aren't really containers holding files like we usually imagine; a directory is really a special file that contains an index listing the files that are within it. This index occupies a block's worth of storage. <code class="literal">ls</code> reports the amount of space the directory occupies as a file, not the sum of the sizes of its files.</p><p>To view the total size of all of the files in a directory, which is usually what we want when talking about directory size, we need to use the <code class="literal">du</code> command:</p><pre class="programlisting">
<span class="strong"><strong>du -hs ~</strong></span>
</pre><p>The <code class="literal">-s</code> argument prints only the value for the current directory and <code class="literal">-h</code> formats the value in a human-readable format. Without any arguments, <code class="literal">du</code> also displays 512-byte block counts for all files and directories within the current directory. However, directories are treated as containers so the values reflect the block count of all of their contained files. We can also list one or more files or directories, in which case <code class="literal">du</code> reports back only on those targets. By targeting all of the files/directories within a directory and piping the output through <code class="literal">sort</code>, we can use <code class="literal">du</code> to identify targets that consume the most storage:</p><pre class="programlisting">
<span class="strong"><strong>du -hs ./* | sort -hr</strong></span>
</pre><p>sort's <code class="literal">-h</code> argument organizes the human-readable numbers correctly (for example, <code class="literal">4.0K</code> is less than <code class="literal">3M</code> even though 3 is less than 4 in a numerical sort) and <code class="literal">-r</code> reverses the order to display the largest entries first:</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_002.jpg"/><div class="caption"><p>Sorting can help identify what consumes the most storage</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec116"/>See also</h2></div></div></div><p>For more information on the commands mentioned in this recipe, refer to their respective man pages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">df</code> manual page (<code class="literal">man 1 df</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">du</code> manual page (<code class="literal">man 1 du</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">ls</code> manual page (<code class="literal">man 1 ls</code>)</li></ul></div></div></div>
<div class="section" title="Setting storage limits for users and groups"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Setting storage limits for users and groups</h1></div></div></div><p>Imposing limits on the amount of storage a user can consume is an effective way to manage resources and ensure they are made available to everyone fairly, especially in a multiuser environment. This recipe shows you how to enable quotas and set limits by users and groups.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec117"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>. It assumes <code class="literal">/home</code> mounts its own filesystem.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec118"/>How to do it...</h2></div></div></div><p>Follow these steps to set up quotas and specify storage limits:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the <code class="literal">/etc/fstab</code> file for editing:<pre class="programlisting">
<span class="strong"><strong>vi /etc/fstab</strong></span>
</pre></li><li class="listitem">To enable user quotas, which enforce usage limits based on user accounts, add <code class="literal">uquota</code> to the mount options for <code class="literal">/home</code>. For group quotas, add <code class="literal">gquota</code>. Both <code class="literal">uquota</code> and <code class="literal">gquota</code> can be given together to enable both:<pre class="programlisting">
<span class="strong"><strong>/dev/mapper/centos-home /home xfs defaults,uquota,gquota 0  0</strong></span>
</pre></li><li class="listitem">Save your changes and close the file.</li><li class="listitem">Reboot the system:<pre class="programlisting">
<span class="strong"><strong>shutdown -r +5 'Reboot required for system maintenance'</strong></span>
</pre></li><li class="listitem">When the system reboots, launch the <code class="literal">xfs_quota</code> shell in expert mode:<pre class="programlisting">
<span class="strong"><strong>xfs_quota -x /home</strong></span>
</pre></li><li class="listitem">Set limits for a user account using the <code class="literal">limit</code> command:<pre class="programlisting">
<span class="strong"><strong>limit bsoft=5g bhard=6g tboronczyk</strong></span>
</pre></li><li class="listitem">Use the <code class="literal">quota</code> command to verify that the user's limits have been set:<pre class="programlisting">
<span class="strong"><strong>quota -h tboronczyk</strong></span>
</pre></li><li class="listitem">Set limits for a group using <code class="literal">limit -g</code>:<pre class="programlisting">
<span class="strong"><strong>limit -g bsoft=20g bhard=21g users&#13;
  </strong></span>
</pre></li><li class="listitem">Use <code class="literal">quota -g</code> to verify that the group's limits have been set:<pre class="programlisting">
<span class="strong"><strong>quota -gh users</strong></span>
</pre></li><li class="listitem">Type <code class="literal">quit</code> or press <span class="emphasis"><em><span class="strong"><strong>Ctrl</strong></span></em></span> + <span class="emphasis"><em><span class="strong"><strong>D</strong></span></em></span> to exit the shell:<pre class="programlisting">
<span class="strong"><strong>quit</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec119"/>How it works...</h2></div></div></div><p>Quotas are not enabled by default and must be enabled explicitly in the filesystem's mount options; so, we updated <code class="literal">/etc/fstab</code> and added the <code class="literal">uquota</code> and/or <code class="literal">gquota</code> option for <code class="literal">/home</code>:</p><pre class="programlisting">
<span class="strong"><strong>/dev/mapper/centos-home /home xfs defaults,uquota,gquota 0 0</strong></span>
</pre><p>We should never unmount a filesystem that's in use because we don't want to risk corrupting or losing data. So, it's important that no one else is logged in when we remount <code class="literal">/home</code>. If you're logged in as <code class="literal">root</code> and you're certain you're the only user logged in, you can remount the filesystem with <code class="literal">umount</code> immediately followed by <code class="literal">mount</code>. But if others are logged on, it's best to perform a reboot as the recipe suggests. When the system reboots, it will have automatically mounted <code class="literal">/home</code> and the quota options will be in effect:</p><pre class="programlisting">
<span class="strong"><strong>shutdown -r +5 'Reboot required for server maintenance'</strong></span>
</pre><p>Next, we ran <code class="literal">xfs_quota</code> as an interactive shell to enter commands to manage our quotas. We used the <code class="literal">-x</code> argument to start the shell in expert mode (the commands we need to manage quotas are only available in expert mode) and gave the filesystem's mount point on which we're going to set quotas:</p><pre class="programlisting">
<span class="strong"><strong>xfs_quota -x /home</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note29"/>Note</h3><p>The traditional quota utilities can be used to manage basic quotas, but <code class="literal">xfs_quota</code> lets us take advantage of the additional quota functionality unique to XFS. For example, using <code class="literal">xfs_quota</code> we can also manage project quotas.</p></div></div><p>The two commands with the most interest for us are <code class="literal">limit</code> and <code class="literal">quota</code>. <code class="literal">limit</code> is used to set the quota limits and <code class="literal">quota</code> is used to report the quota information.</p><p>We can set four limits with <code class="literal">limit</code>. They are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">isoft</code>: This sets a soft limit on the number of inodes used</li><li class="listitem" style="list-style-type: disc"><code class="literal">ihard</code>: This sets a hard limit on the number of inodes used</li><li class="listitem" style="list-style-type: disc"><code class="literal">bsoft</code>: This sets a soft limit on the number of blocks used</li><li class="listitem" style="list-style-type: disc"><code class="literal">bhard</code>: This sets a hard limit on the number of blocks used</li></ul></div><p>An inode is a data structure used by filesystems to track files and directories. Each file and directory are represented by an inode, so setting a limit on the number of inodes a user can have essentially limits the number of files/directories they can have.</p><p>Blocks represent the physical storage, and setting a quota on the number of blocks for a user limits the amount of storage space their files can consume. The typical block size is 512 bytes, meaning two blocks are used to store 1 KB of data. The recipe's examples set a soft block limit of 5 GB for the user account and a hard limit of 6 GB. The suffixes <code class="literal">k</code>, <code class="literal">m</code>, and <code class="literal">g</code> are used to specify values as kilobytes, megabytes, and gigabytes, respectively:</p><pre class="programlisting">
<span class="strong"><strong>limit bsoft=5g bhard=5500m tboronczyk</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>Commands can be run in <code class="literal">xfs_quota</code> without entering the interactive shell by using <code class="literal">-c</code>:</p><p><code class="literal"><span class="strong"><strong>xfs_quota -x -c 'limit -u bsoft=5g tboronczyk' /home</strong></span></code></p></div></div><p>A hard limit specifies a value that the user absolutely cannot surpass. For example, a user with a hard limit of 100 inodes and having 99 files will only be able to create one more file. An attempt to create a file beyond that will be met with an error.</p><p>On the other hand, a soft limit defines a limit a user can surpass for a small amount of time. Once the limit is exceeded, the user enters a grace period. A user with a soft block limit of 5 GB will be able to consume more than 5 GB of storage, but only for a certain amount of time. If they're still violating the limit by the end of the grace period, the soft limit will be treated as a hard limit and they won't be able to save any more data.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>The grace period is 7 days by default. We can change this with the <code class="literal">timer</code> command, using <code class="literal">-i</code> to change the inodes timer and <code class="literal">-b</code> to change the block timer:
<code class="literal">
<span class="strong"><strong>timer -b 3d tboronczyk</strong></span>
</code></p></div></div><p>To review the current quotas, the <code class="literal">quota</code> command is used. <code class="literal">-h</code> presents the values in human-readable values:</p><pre class="programlisting">
<span class="strong"><strong>quota -h tboronczyk</strong></span>
</pre><p>The default output shows the filesystem and its mount point and the user's block quota details: the number of blocks consumed (under the <span class="strong"><strong>Blocks</strong></span> header), soft limit (<span class="strong"><strong>Quota</strong></span>), hard limit (<span class="strong"><strong>Limit</strong></span>), and the elapsed time of a soft-limit violation's grace period (<span class="strong"><strong>Warn/Time</strong></span>). <code class="literal">-i</code> will retrieve the same information for inode quotas, and <code class="literal">-b</code> and <code class="literal">-i</code> can be used together to display both sets of information at the same time:</p><pre class="programlisting">
<span class="strong"><strong>quota -bih tboronczyk</strong></span>
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_003.jpg"/><div class="caption"><p>Block and inode quotas can be displayed at the same time</p></div></div><p>The <code class="literal">limit</code> and <code class="literal">quota</code> commands all default to working with a user's quota, although we can explicitly manage a user's quota using the <code class="literal">-u</code> argument. To manage a group's quota, we use <code class="literal">-g</code>:</p><pre class="programlisting">
<span class="strong"><strong>quota -gh users</strong></span>
</pre><p>As mentioned earlier, <code class="literal">xfs_quota</code> also allows us to manage project quotas. These are essentially limits placed on specific directories that are enforced regardless of user or group ownership. To use project quotas, use the <code class="literal">pquota</code> mount option:</p><pre class="programlisting">
<span class="strong"><strong>/dev/mapper/centos-home /home xfs defaults,uquota,pquota 0 0</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>Project quotas and group quotas cannot be used together; <code class="literal">mount</code> will fail to mount the filesystem if both <code class="literal">pquota</code> and <code class="literal">gquota</code> are given. Depending on the filesystem, this may prevent your system from booting.</p></div></div><p>Next, create the file <code class="literal">/etc/projid</code>. Each line is an entry made up of an arbitrary project name and a unique ID number separated by a colon:</p><pre class="programlisting">
<span class="strong"><strong>echo "my_project:42" &gt;&gt; /etc/projid</strong></span>
</pre><p>Then, create the file <code class="literal">/etc/projects</code>. Its entries are made up of the project ID, a separating colon, and the project's directory. Together, the <code class="literal">projects</code> and <code class="literal">projid</code> files define the relationship between the project's name and its directory:</p><pre class="programlisting">
<span class="strong"><strong>echo "42:/home/dev/project" &gt;&gt; /etc/projects</strong></span>
</pre><p>With the two configuration files in place, the final step is to initialize the project's quota tracking in <code class="literal">xfs_quota</code> using <code class="literal">project -c</code>:</p><pre class="programlisting">
<span class="strong"><strong>project -c my_project</strong></span>
</pre><p>With the initial setup steps complete, you can use the <code class="literal">limit</code> and <code class="literal">quota</code> commands to manage the project's quotas using the <code class="literal">-p</code> argument:</p><pre class="programlisting">
<span class="strong"><strong>limit -p bsoft=10g bhard=11g my_project</strong></span>
</pre></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec120"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with quotas:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">xfs_quota</code> manual page (<code class="literal">man 8 xfs_quota</code>)</li><li class="listitem" style="list-style-type: disc">Enable User and Group Disk Quota on CentOS 7 (<a class="ulink" href="http://www.linuxtechi.com/enable-user-group-disk-quota-on-centos-7-rhel-7/">http://www.linuxtechi.com/enable-user-group-disk-quota-on-centos-7-rhel-7/</a>)</li></ul></div></div></div>
<div class="section" title="Creating a RAM disk"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Creating a RAM disk</h1></div></div></div><p>This recipe teaches you how to take advantage of RAM's low latency using a RAM disk, a section of memory made available as if it were a standard storage device. RAM disks often store volatile data that is constantly read and updated in memory. For example, on desktop systems they're used for storing a browser's cache to speed up web surfing. In server environments, RAM disks can store cache data for high-load proxy services to reduce latency.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec121"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec122"/>How to do it...</h2></div></div></div><p>Perform the following steps to create and use a RAM disk:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Check whether there is sufficient memory available for the RAM disk using <code class="literal">free</code> command (a practical RAM disk will need to be smaller than the amount of free memory):<pre class="programlisting">
<span class="strong"><strong>free -h</strong></span>
</pre></li><li class="listitem">Use <code class="literal">mount</code> to mount a <code class="literal">tmpfs</code> filesystem at the desired mount point, giving the target size as a mount option:<pre class="programlisting">
<span class="strong"><strong>mount -t tmpfs -o size=512M tmpfs /mnt</strong></span>
</pre></li><li class="listitem">When the RAM disk is no longer needed, unmount the filesystem:<pre class="programlisting">
<span class="strong"><strong>umount /mnt</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec123"/>How it works...</h2></div></div></div><p>Whenever we access data on a hard drive, its motors must first spin up the storage platters and position the magnetic head at the correct location. These mechanical actions make access painfully slow compared to accessing data already resident in system memory (RAM). Exact measurements depend on the individual system and its hardware, but disk access takes somewhere in the neighborhood of 10 milliseconds or 10,000,000 nanoseconds. Memory access only takes about 200 nanoseconds, so it's safe to say accessing RAM is at least 10,000 times faster than disk even as a low estimate.</p><p>Before creating the RAM disk, you should first review the amount of free memory available on your system using the <code class="literal">free</code> command:</p><pre class="programlisting">
<span class="strong"><strong>free -h</strong></span>
</pre><p><code class="literal">free</code> command responds with how much memory is available and how much memory is in use. The <code class="literal">-h</code> argument formats the output in a human-readable format (listing the values in megabytes and gigabytes instead of bytes). We can see numbers for RAM, swap disks, and any special buffers used by the kernel, but we're really interested in the amount of used and free memory listed by the <code class="literal">Mem</code> and <code class="literal">Swap</code> entries. A low amount of free memory and a high amount of used swap is an indication that we probably won't have sufficient memory for a practical RAM disk:</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_004.jpg"/><div class="caption"><p>With only 1 GB of RAM, this system has resources to support only a relatively small RAM disk</p></div></div><p>Next, we used <code class="literal">mount</code> to make the desired amount of memory available at the given mount point. The recipe used <code class="literal">/mnt</code>, but you're free to use whatever mount point you see fit:</p><pre class="programlisting">
<span class="strong"><strong>mount -t tmpfs -o size=512M tmpfs /mnt</strong></span>
</pre><p>The invocation specifies <code class="literal">tmpfs</code> as the mount device and <code class="literal">/mnt</code> as the mount point. <code class="literal">-t</code> specifies the underlying filesystem, in this case, <code class="literal">tmpfs</code> and <code class="literal">-o</code> specifies our mount options for the filesystem. A list of possible options for the <code class="literal">tmpfs</code> filesystem can be found in the <code class="literal">mount</code> man page, but the most important option is <code class="literal">size</code>, which sets the desired size of the filesystem.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>It's possible to specify a value for <code class="literal">size</code> that's greater than the amount of available RAM but most of the time this isn't desirable. The extra data is marshaled to swap once RAM is exhausted and this will increase latency, negating the benefits of using a RAM disk in the first place.</p></div></div><p>Remember, RAM disks serve as low latency temporary storage for volatile data. Because its data is stored in memory, the contents of the disk are lost when either the system shuts down or the disk is unmounted. Never store persistent data to your RAM disk.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec124"/>See also</h2></div></div></div><p>Refer to the following resources for more information about RAM disks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">mount</code> manual page (<code class="literal">man 8 mount</code>)</li><li class="listitem" style="list-style-type: disc">How to create a RAM disk in Linux (<a class="ulink" href="http://www.jamescoyle.net/how-to/943-create-a-ram-disk-in-linux">http://www.jamescoyle.net/how-to/943-create-a-ram-disk-in-linux</a>)</li><li class="listitem" style="list-style-type: disc">What is <code class="literal">/dev/shm</code> and its practical usage? (<a class="ulink" href="http://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html">http://www.cyberciti.biz/tips/what-is-devshm-and-its-practical-usage.html</a>)</li></ul></div></div></div>
<div class="section" title="Creating a RAID"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Creating a RAID</h1></div></div></div><p>In this recipe, you'll learn how to configure a redundant array of disks (RAID). Configuring an array of disks to provide redundant storage is an excellent way to protect your data from drive failures. For example, if your data resides on a single disk and that drive fails, then the data is lost. You'll have to replace the drive and restore the data from your latest backup. But if two disks are in a RAID-1 configuration, your data is mirrored and can still be accessed from the working drive when the other fails. The failure doesn't impact access to the data and you can replace the faulty drive at a more convenient time.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec125"/>Getting ready</h2></div></div></div><p>This recipe requires a working CentOS system and elevated privileges. It assumes that at least two new disks have been installed (identified as <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code>) and we will partition and configure them.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec126"/>How to do it...</h2></div></div></div><p>Perform the following steps to create a RAID:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use <code class="literal">lsblk</code> to identify the new storage devices.</li><li class="listitem">Launch <code class="literal">cfdisk</code> to partition the first drive:<pre class="programlisting">
<span class="strong"><strong>cfdisk -z /dev/sdb</strong></span>
</pre><div class="mediaobject"><img alt="How to do it..." src="graphics/image_05_005.jpg"/><div class="caption"><p>cfdisk presents a user-friendly interface for partitioning storage devices</p></div></div></li><li class="listitem">To create a single partition that occupies the entire disk, use the left and right arrow keys to select <code class="literal">New</code> and press <span class="emphasis"><em>Enter.</em></span> Then select <code class="literal">Primary</code> and accept the default size.</li><li class="listitem">Select <code class="literal">Write</code> and confirm the action by typing <code class="literal">yes</code> when prompted. Select <code class="literal">Quit</code> to exit <code class="literal">cfdisk</code>.</li><li class="listitem">Repeat steps 1 to 4 to partition the second drive.</li><li class="listitem">Install the <code class="literal">mdadm</code> package:<pre class="programlisting">
<span class="strong"><strong>yum install mdadm</strong></span>
</pre></li><li class="listitem">Use <code class="literal">mdadm -C</code> to create a new array using the two partitions. The following example creates a RAID-1 (mirroring) configuration:<pre class="programlisting">
<span class="strong"><strong>mdadm -C md0 -l 1 -n 2 /dev/sdb1 /dev/sdc1</strong></span>
</pre></li><li class="listitem">Use the <code class="literal">-D</code> option to examine the RAID:<pre class="programlisting">
<span class="strong"><strong>mdadm -D /dev/md/md0</strong></span>
</pre></li><li class="listitem">Format the RAID using the XFS filesystem with <code class="literal">mkfs.xfs</code>:<pre class="programlisting">
<span class="strong"><strong>mkfs.xfs /dev/md/md0</strong></span>
</pre></li><li class="listitem">Mount the RAID for use:<pre class="programlisting">
<span class="strong"><strong>mount /dev/md/md0 /mnt</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec127"/>How it works...</h2></div></div></div><p>There are many ways to configure disks to work together, especially when it comes to things like data mirroring, striping, and parity checking. Some configurations are implemented at the hardware level and others can be implemented using software. This recipe used <code class="literal">mdadm</code> to set up multiple disks in a RAID configuration, specifically RAID-1.</p><p>The Storage Networking Industry Association has standardized several different RAID configurations. Some of the more common configurations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RAID-0</strong></span>: Data is distributed evenly across two or more disks. This configuration offers no redundancy, and the failure of a single disk in the array will result in data loss. However, it offers increased performance since data can be read and written to different disks simultaneously.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RAID-1</strong></span>: Data is duplicated between disks. Write activity is slower because the same data must be written to each disk, but this configuration offers excellent redundancy; the data remains accessible as long as there is at least one functioning disk.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RAID-5</strong></span>: Blocks of data and parity information are split between two or more disks. If a member of the array fails, parity information on another disk can be used to reconstruct the missing data. Write performance is slower, but read performance is increased since data can be read simultaneously from different disks. This configuration can withstand the failure of a single disk, although the failure of a second disk will result in data loss.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>RAID-6</strong></span>: This configuration is similar to RAID-5, but maintains an extra parity block. The array can withstand two disk failures before data is lost.</li></ul></div><p>There are other standard configurations as well (RAID-2, RAID-3, and so on), and even non-standard configurations, but these are rarely used in practice. As with everything in life, there are trade-offs between the different RAID configurations, and selecting the right configuration for you will depend on how you want to balance redundancy, fault-tolerance, and latency.</p><p><code class="literal">lsblk</code> prints information for the block devices (storage disks) attached to our CentOS system, and it should be relatively easy to identify the names of the new devices simply by looking at the drive sizes and lack of partitions. This recipe assumes that the new devices are <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code>; you'll need to use whatever is appropriate for your system when invoking the <code class="literal">cfdisk</code> and <code class="literal">mdadm</code> commands:</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_006.jpg"/><div class="caption"><p>Several unconfigured drives are installed on the system</p></div></div><p>A new primary partition is created on each disk that occupies its entire capacity. The recipe uses <code class="literal">cfdisk</code>, a program that offers a console-based graphical interface to manipulate partitions. However, there are other partitioning utilities installed in CentOS that you can use instead if you're comfortable with them, such as <code class="literal">fdisk</code>, <code class="literal">sfdisk</code>, and <code class="literal">parted</code>.</p><p>Once the disks are partitioned, we're ready to configure the RAID. The <code class="literal">mdadm</code> program used to set up and administer RAIDs is installed using <code class="literal">yum</code>:</p><pre class="programlisting">
<span class="strong"><strong>yum install mdadm</strong></span>
</pre><p><code class="literal">mdadm -C</code> creates a new RAID configuration and requires a name to identify it. <code class="literal">md0</code> is used in the recipe which results in creating the device <code class="literal">/dev/md/md0</code>. The other arguments describe the desired configuration:</p><pre class="programlisting">
<span class="strong"><strong>mdadm -C md0 -l 1 -n 2 /dev/sdb1 /dev/sdc1</strong></span>
</pre><p>The <code class="literal">-l</code> (a lower-case L) option specifies the standard RAID level, in this case 1 (the number <code class="literal">1</code>) represents RAID-1. If you wanted to set up RAID-5 instead, you'd use <code class="literal">-l 5</code>. The <code class="literal">-n</code> option specifies the number of partitions the RAID will use, and then we list the partitions. The recipe configures two partitions, <code class="literal">/dev/sdb1</code> and <code class="literal">/dev/sdc1</code>.</p><p><code class="literal">mdadm -D</code> displays information for a given array that's useful in examining the configuration and verifying its health. The output lists details such as the RAID level, available storage size, which partitions make up the array, whether any partitions/devices are failing, resync status, and other useful information:</p><pre class="programlisting">mdadm -D /dev/md/md0 &#13;
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_007.jpg"/><div class="caption"><p>mdadm displays the status of the new RAID configuration</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p><code class="literal">mdadm -E</code> retrieves information for one or more partitions that make up the array:</p><p>
</p><p><code class="literal">
<span class="strong"><strong>mdadm -E /dev/sdb1 /dev/sdc1</strong></span>
</code></p></div></div><p>Next, the storage space is formatted with an XFS filesystem using the <code class="literal">mkfs.xfs</code> command:</p><pre class="programlisting">
<span class="strong"><strong>mkfs.xfs /dev/md/md0</strong></span>
</pre><p>Finally, the RAID-backed storage space is ready for use. The recipe demonstrates mounting it manually with the <code class="literal">mount</code> command, although you can also add an entry to <code class="literal">/etc/fstab</code> for the filesystem to be mounted automatically whenever the system boots up.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec128"/>See also</h2></div></div></div><p>For more information on setting up RAIDs, refer to the following resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">cfdisk</code> manual page (<code class="literal">man 8 cfdisk</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">mdadm</code> manual page (<code class="literal">man 8 mdadm</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">mkfs.xfs</code> manual page (<code class="literal">man 8 mkfs.xfs</code>)</li><li class="listitem" style="list-style-type: disc">Linux RAID Wiki: Linux RAID (<a class="ulink" href="https://raid.wiki.kernel.org/index.php/Linux_Raid">https://raid.wiki.kernel.org/index.php/Linux_Raid</a>)</li><li class="listitem" style="list-style-type: disc">Mdadm Cheat Sheet (<a class="ulink" href="http://www.ducea.com/2009/03/08/mdadm-cheat-sheet/">http://www.ducea.com/2009/03/08/mdadm-cheat-sheet/</a>)</li><li class="listitem" style="list-style-type: disc">Introduction to RAID (<a class="ulink" href="http://www.tecmint.com/understanding-raid-setup-in-linux/">http://www.tecmint.com/understanding-raid-setup-in-linux/</a>)</li><li class="listitem" style="list-style-type: disc">Standard RAID levels (<a class="ulink" href="https://en.wikipedia.org/wiki/Standard_RAID_levels">https://en.wikipedia.org/wiki/Standard_RAID_levels</a>)</li></ul></div></div></div>
<div class="section" title="Replacing a device in a RAID"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec43"/>Replacing a device in a RAID</h1></div></div></div><p>When an array member fails, it's important to replace it as soon as possible because the failure of additional drives increases the chance of data loss. This recipe teaches you how to properly replace a bad drive and rebuild the array.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec129"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>. It assumes that a RAID-1 configuration has been set up as described in the previous recipe and the drive that will be replaced is <code class="literal">/dev/sdb</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec130"/>How to do it...</h2></div></div></div><p>Follow these steps to replace a failed disk in a RAID:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Mark the failed partition as faulty with <code class="literal">mdadm</code> using the <code class="literal">-f</code> option:<pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -f /dev/sdb1</strong></span>
</pre></li><li class="listitem">Remove the partition from the RAID's configuration with <code class="literal">-r</code>:<pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -r /dev/sdb1</strong></span>
</pre></li><li class="listitem">Physically replace the faulty disk.</li><li class="listitem">Partition the new drive with <code class="literal">cfdisk</code>:<pre class="programlisting">
<span class="strong"><strong>cfdisk -z /dev/sdb</strong></span>
</pre></li><li class="listitem">Use the <code class="literal">-a</code> option to add the partition to the RAID:<pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -a /dev/sdb1</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec131"/>How it works...</h2></div></div></div><p>It's important to replace bad members as soon you become aware of the failure because, depending on the fault tolerance of your configuration, the failure of a second device may result in full data loss.</p><p>A member must be marked faulty before we can safely remove it, so the first step is to fail the partition. To do this, we used <code class="literal">mdadm</code>. The <code class="literal">-f</code> argument specifies the partition we want failed:</p><pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -f /dev/sdb1</strong></span>
</pre><p>Then, to remove the partition from the RAID, we used the <code class="literal">-r</code> argument:</p><pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -r /dev/sdb1</strong></span>
</pre><p>Now that the device is no longer in use, we can replace the physical drive. Whether the drive can be hot-swapped while the system is running or if a system shutdown is necessary depends on your hardware.</p><p>Once the replacement partition was ready, we added it to the RAID with the <code class="literal">-a</code> argument. The RAID will begin to rebuild itself, distributing data and parity information to the new partition, as soon as the partition is added:</p><pre class="programlisting">
<span class="strong"><strong>mdadm /dev/md/md0 -a /dev/sdb1</strong></span>
</pre><p>The last recipe showed how the <code class="literal">-D</code> (and <code class="literal">-E</code>) argument of <code class="literal">mdadm</code> is used to retrieve status information about the RAID. You can review the output to monitor the rebuild's progress, but a more concise report is available via <code class="literal">/proc/mdstat</code>. The contents show the speed at which the rebuild is being processed and estimate the time it will take for it to complete. Using <code class="literal">watch</code> to repeatedly display <code class="literal">/proc/mdstat</code>, you can create a make-shift dashboard to monitor the process:</p><pre class="programlisting">
<span class="strong"><strong>watch -n 10 -x cat /proc/mdstat</strong></span>
</pre><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_008.jpg"/><div class="caption"><p>The estimated time for this RAID's rebuild to complete is about an hour and a half</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec132"/>See also</h2></div></div></div><p>Refer to the following resources for more information on replacing failed drives in a RAID:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">mdadm</code> manual page (<code class="literal">man 8 mdadm</code>)</li><li class="listitem" style="list-style-type: disc">Replacing a failed hard drive in a software RAID (<a class="ulink" href="https://www.howtoforge.com/replacing_hard_disks_in_a_raid1_array">https://www.howtoforge.com/replacing_hard_disks_in_a_raid1_array</a>)</li><li class="listitem" style="list-style-type: disc">Five tips to speed up RAID re-building and re-syncing (<a class="ulink" href="http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html">http://www.cyberciti.biz/tips/linux-raid-increase-resync-rebuild-speed.html</a>)</li></ul></div></div></div>
<div class="section" title="Creating a new LVM volume"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec44"/>Creating a new LVM volume</h1></div></div></div><p>Logical Volume Manager (LVM) abstracts data storage away from the physical hardware, which lets us configure the partitions on one or more physical drives to act as one logical device. We also have the freedom to later add or remove physical partitions and grow or shrink the logical device. This recipe show's you how to create a new LVM group and a logical device from the group's storage.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec133"/>Getting ready</h2></div></div></div><p>This recipe requires a working CentOS system and elevated privileges. It assumes that at least two new disks have been installed (identified as <code class="literal">/dev/sdb </code>and <code class="literal">/dev/sdc</code>) and we will partition and configure them.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec134"/>How to do it...</h2></div></div></div><p>Perform these steps to set up a new LVM group and create a volume:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use <code class="literal">lsblk</code> to identify the new storage devices.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note35"/>Note</h3><p>You can set up LVM with RAID storage as well. Skip to step 5 and replace the partitions with RAID devices (for example, <code class="literal">/dev/md/md0</code>) in the given commands.</p></div></div></li><li class="listitem">Launch <code class="literal">cfdisk</code> to partition the first drive and create a single partition that occupies the entire disk:<pre class="programlisting">
<span class="strong"><strong>cfdisk -z /dev/sdb</strong></span>
</pre></li><li class="listitem">Repeat step 2 to partition the second drive.</li><li class="listitem">Use <code class="literal">pvcreate</code> to register the new partitions as physical volumes:<pre class="programlisting">
<span class="strong"><strong>pvcreate /dev/sdb1 /dev/sdc1</strong></span>
</pre></li><li class="listitem">Verify that the physical volumes are listed in the output of <code class="literal">pvs</code>:<pre class="programlisting">
<span class="strong"><strong>pvs</strong></span>
</pre></li><li class="listitem">Using <code class="literal">vgcreate</code>, group the physical volumes to form a volume group:<pre class="programlisting">
<span class="strong"><strong>vgcreate vg0 /dev/sdb1 /dev/sdc1</strong></span>
</pre></li><li class="listitem">Verify that the group is listed in the output of <code class="literal">vgs</code>:<pre class="programlisting">
<span class="strong"><strong>vgs</strong></span>
</pre></li><li class="listitem">Using <code class="literal">lvcreate</code>, create a logical volume from the storage pool provided by the volume group:<pre class="programlisting">
<span class="strong"><strong>lvcreate -n myvol -L 500G vg0</strong></span>
</pre></li><li class="listitem">Format the volume using the XFS filesystem:<pre class="programlisting">
<span class="strong"><strong>mkfs.xfs /dev/vg0/myvol</strong></span>
</pre></li><li class="listitem">Mount the volume for use:<pre class="programlisting">
<span class="strong"><strong>mount /dev/vg0/myvol /mnt</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec135"/>How it works...</h2></div></div></div><p>LVM is another approach to configure multiple storage units to work together, focusing on pooling their resources together in a flexible way. These units can be disk partitions, as well as RAID arrays, and so the generic term <span class="emphasis"><em>volume</em></span> is used.</p><p>The recipe starts with the assumption that we have two new disks as our storage volumes and provides steps for identifying the devices and partitioning them using <code class="literal">lsblk</code> and <code class="literal">cfdisk</code>. It uses <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code> as the devices, but you should use whatever is appropriate for your system. Once the disks are partitioned, we're ready to register the partitions as physical volumes with <code class="literal">pvcreate</code>. The term <span class="emphasis"><em>physical volume</em></span> describes storage available as a physical partition or RAID.</p><pre class="programlisting">
<span class="strong"><strong>pvcreate /dev/sdb1 /dev/sdc1</strong></span>
</pre><p>Next, the physical volumes are grouped as a volume group using <code class="literal">vgcreate</code>. The recipe created a volume group name <code class="literal">vg0</code> using the <code class="literal">sdb1</code> and <code class="literal">sdc2</code> partitions.</p><pre class="programlisting">
<span class="strong"><strong>vgcrate vg0 /dev/sdb1 /dev/sdc1</strong></span>
</pre><p>The desired name for the volume group is passed first to <code class="literal">vgcreate</code>, followed by the physical volumes we want to group together. If <code class="literal">sdb1</code> and <code class="literal">sdc1</code> both have a capacity of 1 TB each, their storage is combined and the volume group will have 2 TB. If we were to later add a 500 GB volume to the group, the group's storage capacity would increase to 2.5 TB.</p><p>The <code class="literal">pvs</code> and <code class="literal">vgs</code> commands return basic information about physical volumes or volume groups, respectively, and the recipe uses them to verify that each registration was successful. <code class="literal">pvs</code> reports the physical volumes that are registered and which group they are assigned to, any attributes, and their storage capacity. <code class="literal">vgs</code> lists the groups, the number of physical volumes that make up each group's pool, the number of logical volumes using storage from the group, and the groups' capacities.</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_009.jpg"/><div class="caption"><p>pvs and vgs are used to review the status of physical volumes and volume groups</p></div></div><p>A new logical volume is created from the pooled storage of the volume group using the <code class="literal">lvcreate</code> command:</p><pre class="programlisting">
<span class="strong"><strong>lvcreate -n myvol -L 500G vg0</strong></span>
</pre><p>The <code class="literal">-n</code> option provides the name for the logical volume and <code class="literal">-L</code> provides the amount of storage to allocate the volume from the pool. The final argument is the name of the volume group used to support the volume. The values given in the recipe's example creates a volume named <code class="literal">myvol</code> with a capacity of 500 GB backed by the <code class="literal">vg0</code> group. Logical volumes are organized under <code class="literal">/dev</code> by group, so the volume is available as <code class="literal">/dev/vg0/myvol</code>.</p><p>Finally, the volume is formatted with the XFS filesystem using <code class="literal">mkfs.xfs</code>:</p><pre class="programlisting">
<span class="strong"><strong>mkfs.xfs /dev/vg0/myvol</strong></span>
</pre><p>The logical volume is now ready for use and can be mounted manually with <code class="literal">mount</code> and/or an entry can be made in /<code class="literal">etc/fstab</code> to mount the volume automatically at system boot time.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec136"/>See also</h2></div></div></div><p>For more information on getting started with LVM, refer to the following resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">lvcreate</code> manual page (<code class="literal">man 8 lvcreate</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">pvcreate</code> manual page (<code class="literal">man 8 pvcreate</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">vgcreate</code> manual page (<code class="literal">man 8 vgcreate</code>)</li><li class="listitem" style="list-style-type: disc">Linux Partition HOWTO (<a class="ulink" href="http://tldp.org/HOWTO/Partition/index.html">http://tldp.org/HOWTO/Partition/index.html</a>)</li><li class="listitem" style="list-style-type: disc">LVM made easy (<a class="ulink" href="http://www.tuxradar.com/content/lvm-made-easy">http://www.tuxradar.com/content/lvm-made-easy</a>)</li><li class="listitem" style="list-style-type: disc">Manage LVM volumes with System Storage Manager (<a class="ulink" href="http://xmodulo.com/manage-lvm-volumes-centos-rhel-7-system-storage-manager.html">http://xmodulo.com/manage-lvm-volumes-centos-rhel-7-system-storage-manager.html</a>)</li></ul></div></div></div>
<div class="section" title="Removing an existing LVM volume"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec45"/>Removing an existing LVM volume</h1></div></div></div><p>The flexibility of LVM allows us to allocate the pooled storage of physical volumes however we see fit. This recipe shows us how to delete a logical volume and free its storage back to the volume group for use by other logical volumes.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec137"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>. It assumes that a logical volume has been created as described in the preceding recipe.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec138"/>How to do it...</h2></div></div></div><p>Perform the following steps to remove an LVM volume:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Unmount the filesystem with <code class="literal">umount</code>:<pre class="programlisting">
<span class="strong"><strong>       umount /mnt &#13;
</strong></span>
</pre></li><li class="listitem">Open <code class="literal">/etc/fstab</code> and verify that there isn't an entry to automatically mount the filesystem. If there is, remove the entry, save your changes, and close the file.</li><li class="listitem">Use <code class="literal">lvremove</code> to delete the logical volume:<pre class="programlisting">
<span class="strong"><strong>lvremove vg0/myvol</strong></span>
</pre></li><li class="listitem">Review the output of <code class="literal">vgs</code> to verify the removal.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec139"/>How it works...</h2></div></div></div><p>Deleting a volume frees its storage back to the volume group, which can then be used to create new logical volumes or support growing an existing volume. This recipe taught you how to destroy a logical volume using the <code class="literal">lvremove</code> command.</p><p>Because a volume can't be freed if it's in use, the first step is to make sure that its filesystem is unmounted. If the filesystem is mounted automatically, its entry in <code class="literal">/etc/fstab</code> should also be removed.</p><p>Next, <code class="literal">lvremove</code> is invoked with the name of the logical volume to free it:</p><pre class="programlisting">
<span class="strong"><strong>lvremove vg0/myvol</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>You can delete all of the volumes from a pool by providing just the pool name:</p><p>
</p><p><code class="literal">
<span class="strong"><strong>lvremove vg0</strong></span>
</code></p></div></div><p>The recipe suggests checking the output of <code class="literal">vgs</code> to verify that the logical volume was removed. In the output, the number of logical volumes under the <code class="literal">#LV</code> column should have decreased and the amount of free space under the <code class="literal">VFree</code> column increased appropriately.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec140"/>See also</h2></div></div></div><p>Refer to the following resources for more information on removing a volume:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">lvremove</code> manual page (<code class="literal">man 8 lvremove</code>)</li><li class="listitem" style="list-style-type: disc">The <code class="literal">vgs</code> manual page (<code class="literal">man 8 vgs</code>)</li></ul></div></div></div>
<div class="section" title="Adding storage and growing an LVM volume"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec46"/>Adding storage and growing an LVM volume</h1></div></div></div><p>The size of logical volumes doesn't need to be fixed and we're free to allocate more storage for one from its volume group. This recipe teaches us how to add more storage to the group and then grow the size of the logical volume to take advantage of it.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec141"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>. It assumes that a new disk has been installed and partitioned (identified as <code class="literal">/dev/sdd1</code>) and a logical group and volume have been configured as described in previous recipes.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec142"/>How to do it...</h2></div></div></div><p>Follow these steps to add storage and increase the size of an LVM volume:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Register the new partition as a physical volume:<pre class="programlisting">
<span class="strong"><strong>pvcreate /dev/sdd1</strong></span>
</pre></li><li class="listitem">Review the output of <code class="literal">pvs</code> to confirm that the volume was registered:<pre class="programlisting">
<span class="strong"><strong>pvs</strong></span>
</pre></li><li class="listitem">Use <code class="literal">vgextend</code> to add the physical volume to the desired volume group:<pre class="programlisting">
<span class="strong"><strong>vgextend vg0 /dev/sdd1</strong></span>
</pre></li><li class="listitem">Review the output of <code class="literal">vgs</code> to confirm that the volume was added to the group:<pre class="programlisting">
<span class="strong"><strong>vgs</strong></span>
</pre></li><li class="listitem">Use <code class="literal">lvextend</code> to increase the size of the desired logical volume:<pre class="programlisting">
<span class="strong"><strong>lvextend vg0/myvol -L+500G</strong></span>
</pre></li><li class="listitem">Review the output of <code class="literal">lvs</code> to confirm the new capacity:<pre class="programlisting">
<span class="strong"><strong>lvs</strong></span>
</pre></li><li class="listitem">Expand the filesystem with <code class="literal">xfs_grow</code> to use the new capacity:<pre class="programlisting">
<span class="strong"><strong>xfs_grow -d /mnt</strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>An XFS filesystem must be mounted to expand its size; if it's not already mounted, you'll need to do so before executing <code class="literal">xfs_grow</code>.</p></div></div></li><li class="listitem">Confirm the new size of the filesystem using <code class="literal">df</code>:<pre class="programlisting">
<span class="strong"><strong>df -h /mnt</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec143"/>How it works...</h2></div></div></div><p>The recipe assumed that a new partition has been prepared, which was then registered as a physical volume using the <code class="literal">pvcreate</code> command. Then the physical volume was assigned to the <code class="literal">vg0</code> volume group using <code class="literal">vgextend</code>, increasing the group's available storage:</p><pre class="programlisting">
<span class="strong"><strong>vgextend vg0 /dev/sdd1</strong></span>
</pre><p><code class="literal">lvextend</code> was invoked to grow the size of a logical volume, <code class="literal">vg0/myvol</code>:</p><pre class="programlisting">
<span class="strong"><strong>lvextend vg0/myvol -L+500G</strong></span>
</pre><p>The <code class="literal">-L</code> argument specifies the amount of storage to allocate from the pool. It's value can be an absolute value, for example, <code class="literal">-L 500G</code>, in which case the volume will be resized to have that much capacity. A relative value can also be used to increase the volume's current capacity by some amount. The recipe used <code class="literal">-L+500G</code> to grow the size of the logical volume by an additional 500 GB.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note38"/>Note</h3><p>You will receive an error if you provide a value for <code class="literal">-L</code> less than the logical volume's current capacity because <code class="literal">lvextend</code> only increases the capacity of a volume. The <code class="literal">lvreduce</code> command is used to reduce the size of logical volumes:</p><p>
</p><p><code class="literal">
<span class="strong"><strong>lvreduce vg0/myvol -L 500GB</strong></span>
</code></p><p>
</p><p>Given a straight value, <code class="literal">-L</code> specifies the total capacity for the volume. In the preceding command, the capacity for <code class="literal">vg0/myvol</code> is reduced to <code class="literal">500GB</code>. Given a relative value, for example <code class="literal">-L-500GB</code>, <code class="literal">lvreduce</code> reduces the volume's capacity by the specified amount.</p></div></div><p>When finished, the logical volume's capacity can be confirmed by inspecting the output of the <code class="literal">lvs</code> command. The command reports the logical volumes that exist and to which group they are assigned, their attributes, storage capacity, and other statistics.</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_010.jpg"/><div class="caption"><p>The capacity of the logical volume has increased but the filesystem needs to be resized to use it</p></div></div><p>Finally, the filesystem needs to be expanded to make use of the additional space available to it with <code class="literal">xfs_growfs</code>. Filesystems must be mounted for the utility to work, and the recipe assumes that it's mounted at <code class="literal">/mnt</code>. The <code class="literal">-d</code> argument instructs <code class="literal">xfs_grow</code> to increase the size of the filesystem as much as possible (the entire size of the volume).</p><pre class="programlisting">
<span class="strong"><strong>xfs_growfs -d /mnt</strong></span>
</pre><p>Alternatively, you can give a specific size with <code class="literal">-D</code>. Its value is given in block counts, so some math will be required to grow the filesystem to the desired size. For example, let's say you have a 1 TB filesystem and the block size is 4,096 bytes (the default). The block count will be 268,435,456 blocks. If you want to grow the filesystem an additional 500 GB, the target block count will be <code class="literal">399507456</code>:</p><pre class="programlisting">
<span class="strong"><strong>xfs_growfs -D 399507456 /mnt</strong></span>
</pre><p>To make life a little easier, here's a table that presents block counts for common sizes:</p><div class="mediaobject"><img alt="How it works..." src="graphics/image_05_011.jpg"/><div class="caption"><p>These block counts can be used with xfs_growfs to grow an XFS filesystem</p></div></div><p>While it's possible to reduce the size of a logical volume, it's only possible to grow an XFS filesystem. If you want to reduce the size of an XFS-supported volume you'll have to move its data to a safe location, remove and recreate the logical volume with a smaller size, and later move the data back.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec144"/>See also</h2></div></div></div><p>Refer to the following resources for more information on growing an LVM volume:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">xfs_growfs</code> manual page (<code class="literal">man 8 xfs_growfs</code>)</li><li class="listitem" style="list-style-type: disc">Linux guide to the XFS filesystem (<a class="ulink" href="http://landoflinux.com/linux_xfs_filesystem_introduction.html">http://landoflinux.com/linux_xfs_filesystem_introduction.html</a>)</li><li class="listitem" style="list-style-type: disc">Extend/Reduce LVM's in Linux (<a class="ulink" href="http://www.tecmint.com/extend-and-reduce-lvms-in-linux/">http://www.tecmint.com/extend-and-reduce-lvms-in-linux/</a>)</li><li class="listitem" style="list-style-type: disc">How to grow an XFS-formatted disk (<a class="ulink" href="http://superuser.com/questions/1000092/how-to-grow-xfs-formated-disk/1001486#1001486">http://superuser.com/questions/1000092/how-to-grow-xfs-formated-disk/1001486#1001486</a>)</li></ul></div></div></div>
<div class="section" title="Working with LVM snapshots"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec47"/>Working with LVM snapshots</h1></div></div></div><p>A logical volume, also called a linear volume, is just one type of volume we can create; LVM also lets us create snapshot volumes. A snapshot volume is associated with a logical volume and keeps track of changes made to the logical volume's data. We can then merge the snapshot back into the logical volume to roll back the data. This recipe will show you how to do just that.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec145"/>Getting ready</h2></div></div></div><p>This recipe requires a CentOS system with administrative privileges provided by logging in with the <code class="literal">root</code> account or using <code class="literal">sudo</code>. It assumes that a logical volume has been configured and sufficient storage exists in its volume group for the snapshot.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec146"/>How to do it...</h2></div></div></div><p>The following commands show you how to work with LVM snapshots. Before you begin, you should verify that there is sufficient storage available in the volume group to support the snapshot using <code class="literal">vgs</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use <code class="literal">lvcreate -s</code> to create a snapshot volume:<pre class="programlisting">
<span class="strong"><strong>lvcreate -s -L 100M -n myvolsnap vg0/myvol</strong></span>
</pre></li><li class="listitem">A snapshot volume may be deleted using <code class="literal">lvremove</code>:<pre class="programlisting">
<span class="strong"><strong>lvremove vg0/myvolsnap</strong></span>
</pre></li><li class="listitem">A snapshot volume may be mounted and accessed with <code class="literal">mount</code>:<pre class="programlisting">
<span class="strong"><strong>mount -o ro /dev/vg0/myvolsnap /mnt</strong></span>
</pre></li><li class="listitem">To restore a logical volume to the state it was in when the snapshot was made, make sure neither are mounted and use <code class="literal">lvconvert</code>:<pre class="programlisting">
<span class="strong"><strong>lvconvert -v --merge vg0/myvolsnap</strong></span>
</pre></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec147"/>How it works...</h2></div></div></div><p>This recipe presented commands to create a snapshot volume which then tracks the changes made to a logical volume and to merge the snapshot back into the logical volume.</p><p>Snapshots are created using the <code class="literal">lvcreate</code> command with the <code class="literal">-s</code> flag. <code class="literal">-n</code> gives the name for the snapshot and <code class="literal">-L</code> specifies how much storage will be allocated for it from the volume group. The final argument is the logical volume the snapshot is created from:</p><pre class="programlisting">
<span class="strong"><strong>lvcreate -s -L 100M -n myvolsnap vg0/myvol</strong></span>
</pre><p>The values given in the example create a snapshot of <code class="literal">vg0/myvol</code> named <code class="literal">myvolsnap</code> with a capacity of 100 MB. Storage for the snapshot volume is allocated from the same group as its logical volume so that there should be sufficient storage to support the snapshot. Luckily, snapshot volumes don't copy all of the data from the original volume. Instead, they use a copy-on-write strategy where only the differences are recorded to the snapshot when the data is modified.</p><p>If the deltas exceed the snapshot volume's capacity, LVM won't be able to continue to record changes and the snapshot will no longer be valid. For this reason, you should periodically monitor the snapshot's storage usage and either resize the snapshot or discard the snapshot and create a new one with a larger capacity if necessary. As with other volumes, <code class="literal">lvremove</code> is used to delete snapshot volumes:</p><pre class="programlisting">
<span class="strong"><strong>lvremove vg0/myvolsnap</strong></span>
</pre><p>A snapshot can also be mounted and accessed like other logical volumes. LVM transparently reads unmodified data from the original logical volume so that the data appears as a full copy. Depending on the your reasons for creating a snapshot, you may want to use the <code class="literal">ro</code> mount option to mount the volume read-only to prevent inadvertent changes from being introduced:</p><pre class="programlisting">
<span class="strong"><strong>mount -o ro /dev/vg0/myvolsnap /mnt</strong></span>
</pre><p><code class="literal">lvconvert</code> is used to change a volume's type and other characteristics. You should unmount both the logical and snapshot volumes before calling <code class="literal">lvconvert</code> so that the merge process can begin immediately. Otherwise, LVM will schedule the process to begin after both have been unmounted and either the logical or snapshot volume is mounted again.</p><p>To revert the logical volume's data, we target its snapshot volume and use the <code class="literal">--merge</code> option:</p><pre class="programlisting">
<span class="strong"><strong>lvconvert -v --merge vg0/myvolsnap</strong></span>
</pre><p>Merging the snapshot volume's data to its logical volume rolls back the changes to the logical volume's data, basically restoring it to the state it was in at the time the snapshot was created. When finished, the snapshot is automatically deleted. <code class="literal">-v</code> puts <code class="literal">lvconvert</code> into verbose mode, which is useful to monitor its progress and to know when the merge is complete and the snapshot has been deleted.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec148"/>See also</h2></div></div></div><p>Refer to the following resources for more information on working with snapshots:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">lvconvert</code> manual page (<code class="literal">man 8 lvconvert</code>)</li><li class="listitem" style="list-style-type: disc">How to take a snapshot logical volume and restore (<a class="ulink" href="http://www.tecmint.com/take-snapshot-of-logical-volume-and-restore-in-lvm/">http://www.tecmint.com/take-snapshot-of-logical-volume-and-restore-in-lvm/</a>)</li><li class="listitem" style="list-style-type: disc">How to take volume snapshots (<a class="ulink" href="http://www.unixarena.com/2013/08/linux-lvm-how-to-take-volume-snapshot.html">http://www.unixarena.com/2013/08/linux-lvm-how-to-take-volume-snapshot.html</a>)</li></ul></div></div></div></body></html>