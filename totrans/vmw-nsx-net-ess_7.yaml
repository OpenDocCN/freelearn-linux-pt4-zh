- en: Chapter 7. NSX Cross vCenter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章：NSX 跨 vCenter
- en: 'In this chapter, we will have a detailed discussion of the NSX cross vCenter
    feature. Starting from NSX 6.2, we can manage multiple vCenter Servers from a
    single pane of glass by leveraging NSX features across vCenter environments. We
    will primarily cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细讨论 NSX 跨 vCenter 的功能。从 NSX 6.2 开始，我们可以通过跨 vCenter 环境利用 NSX 功能，从一个统一的界面管理多个
    vCenter Servers。我们将主要覆盖以下主题：
- en: Understanding NSX cross vCenter Server
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 NSX 跨 vCenter Server
- en: Components of NSX cross vCenter Server
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 跨 vCenter Server 组件
- en: Cross vCenter universal logical switches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨 vCenter 通用逻辑交换机
- en: Cross vCenter universal logical router
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨 vCenter 通用逻辑路由器
- en: Network choke points in NSX cross vCenter Server
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 跨 vCenter Server 的网络瓶颈
- en: Understanding NSX cross vCenter Server
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 NSX 跨 vCenter Server
- en: 'Earlier versions of NSX had a **1:1** relationship with vCenter Server and
    that means NSX features and functionalities were limited to that specific vCenter
    Server. So whenever we are scaling vCenter Servers, it demands separate installation
    and configuration of NSX Manager and each of these environments has to be managed
    separately. When VMware released NSX 6.2 in August 2015, security and cross vCenter
    Server networking were exciting features that were announced. With cross vCenter
    networking and security features, we can extend logical switches across vCenter
    boundaries and, adding to that, we can extend distributed routing and distributed
    firewalling seamlessly across VCs to provide a true network hybridity between
    both the sites. There are numerous use cases customers can benefit from with this
    cross VC NSX integration. The following are a few of them:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 早期版本的 NSX 与 vCenter Server 存在 **1:1** 的关系，这意味着 NSX 功能仅限于特定的 vCenter Server。因此，每当我们扩展
    vCenter Server 时，需要单独安装和配置 NSX Manager，并且每个环境都必须单独管理。当 VMware 在 2015 年 8 月发布 NSX
    6.2 时，安全性和跨 vCenter Server 网络功能成为了令人兴奋的新增功能。通过跨 vCenter 的网络和安全功能，我们可以在 vCenter
    边界之间扩展逻辑交换机，此外，我们可以无缝地跨 VCs 扩展分布式路由和分布式防火墙，为两个站点提供真正的网络混合性。客户可以通过这一跨 VC NSX 集成功能受益的使用案例有很多，以下是其中的一些：
- en: By leveraging cross vCenter Server, we can have a primary and secondary vSphere
    environment and can easily configure disaster recovery sites.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过利用跨 vCenter Server，我们可以拥有一个主 vSphere 环境和一个备份 vSphere 环境，并且可以轻松配置灾难恢复站点。
- en: Seamless migration of workloads from one vSphere environment to another.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无缝地将工作负载从一个 vSphere 环境迁移到另一个环境。
- en: Simplified data center routing across vCenter Server sites; centralized security
    policy management; firewall rules can be managed from one centralized location.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化的跨 vCenter Server 站点数据中心路由；集中式安全策略管理；防火墙规则可以从一个集中位置进行管理。
- en: NSX features and functionalities can be locally deployed (single vCenter Server);
    also we can deploy it across vCenter Server (cross vCenter Server). That way,
    local objects can be managed locally and global objects can be managed globally.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 功能可以在本地部署（单个 vCenter Server）；也可以跨 vCenter Server 部署（跨 vCenter Server）。这样，本地对象可以在本地管理，而全局对象可以在全球管理。
- en: 'The following figure depicts a cross vCenter Server NSX environment:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了一个跨 vCenter Server 的 NSX 环境：
- en: '![Understanding NSX cross vCenter Server](img/B03244_07_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![理解 NSX 跨 vCenter Server](img/B03244_07_01.jpg)'
- en: 'I know we are all excited to know how NSX cross VC works. But let''s be very
    clear with prerequirements and a few points before discussing this feature:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道大家都很想了解 NSX 跨 VC 的工作原理。但在讨论此功能之前，让我们先明确一些前提条件和几个要点：
- en: The VSphere environment should be version 6.0.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere 环境应该是 6.0 版本。
- en: Each vCenter Server should be registered with a unique NSX Manager. Okay! That
    is an interesting point, isn't it? When I first heard about cross VC NSX architecture,
    I thought all we needed would be one NSX Manager moving forward. But further reading
    and lab sessions proved my understanding was wrong.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 vCenter Server 应该与一个唯一的 NSX Manager 注册。好吧！这个点很有趣，不是吗？当我第一次听说跨 VC NSX 架构时，我以为我们将来只需要一个
    NSX Manager。但通过进一步的阅读和实验，证明我的理解是错误的。
- en: We have to promote one NSX Manager as **primary** and the others will be **secondary**.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须将一个 NSX Manager 提升为 **主**，其他的将是 **备**。
- en: We can certainly demote NSX roles based on business requirements. For example,
    a secondary NSX Manager can be demoted to a standalone NSX Manager and that way,
    we are going back to where NSX was initially when it started (pre 6.2 NSX version).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们当然可以根据业务需求降级 NSX 角色。例如，一个次级 NSX Manager 可以降级为独立的 NSX Manager，这样我们就回到了 NSX
    最初开始时的状态（6.2 版本之前的 NSX）。
- en: Even though cross VC NSX is a great architecture, I would still call it a new
    kid in town and it lacks all the features that were possible in a standalone NSX
    Manager instance. Keeping that negativity aside, I strongly believe newer versions
    of NSX will start supporting all the features across vCenter Servers.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管跨 VC NSX 是一个很好的架构，但我仍然认为它是一个新兴的解决方案，并且缺乏独立 NSX Manager 实例中可能具有的所有功能。抛开这些负面看法，我坚信
    NSX 的新版本将会开始支持跨 vCenter 服务器的所有功能。
- en: Carefully plan and integrate cross VC NSX environments; to be precise, watch
    out for what features we need in the primary and secondary sites and how we need
    to manage those features. For example, if we need just a Distributed Firewall
    feature in the secondary site, I wouldn't recommend cross VC NSX integration unless
    we want to manage these firewall policies from a single pane of glass.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细规划并集成跨 VC 的 NSX 环境；具体来说，关注我们在主站点和次站点中需要的功能，以及如何管理这些功能。例如，如果我们仅在次站点需要分布式防火墙功能，我不建议进行跨
    VC 的 NSX 集成，除非我们希望从一个统一界面管理这些防火墙策略。
- en: Components of NSX cross vCenter Server
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 跨 vCenter 服务器的组件
- en: 'Cross vCenter NSX includes the following components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 跨 vCenter NSX 包括以下组件：
- en: Universal controller cluster
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用控制器集群
- en: Universal transport zone
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用传输区域
- en: Universal logical switch
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用逻辑交换机
- en: Universal distributed logical router
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用分布式逻辑路由器
- en: Universal IP set
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用 IP 集合
- en: Universal MAC set
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用 MAC 集合
- en: Universal security group
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用安全组
- en: 'The following table depicts NSX cross vCenter Server deployment options; based
    on these points, we will have a detailed explanation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了 NSX 跨 vCenter 服务器部署选项；基于这些要点，我们将进行详细解释：
- en: '![Components of NSX cross vCenter Server](img/image_07_002.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![NSX 跨 vCenter 服务器的组件](img/image_07_002.jpg)'
- en: In the preceding table, I have updated key NSX features that customers would
    be ideally configuring in a cross vCenter NSX environment. Any other features,
    such as load balancing and L2 bridging, do not have any global level fitting between
    the NSX sites so they always remain local to the vCenter Server environment. Before
    exploring universal NSX features, we need to know what roles are available for
    NSX Manager.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，我已更新了客户理想情况下会在跨 vCenter NSX 环境中配置的关键 NSX 功能。其他功能，如负载均衡和 L2 桥接，在 NSX 站点之间没有全局级别的适配，因此它们始终局限于
    vCenter Server 环境。 在探讨通用 NSX 功能之前，我们需要了解 NSX Manager 可用的角色。
- en: 'The NSX Manager instance has the following roles and a synchronization module
    will be running on the primary NSX Manager to ensure universal objects are synchronized
    to the secondary NSX Manager. The NSX Manager instance has the following roles:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: NSX Manager 实例具有以下角色，并且在主 NSX Manager 上将运行同步模块，以确保通用对象能够同步到次级 NSX Manager。NSX
    Manager 实例具有以下角色：
- en: '**Standalone**:Before configuring NSX roles, all the NSX Managers are standalone
    NSX Managers. A fresh installation of NSX Manager or an upgraded version of NSX
    Manager from VCNS are perfect examples of standalone NSX Managers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立**：在配置 NSX 角色之前，所有的 NSX Manager 都是独立的 NSX Manager。NSX Manager 的全新安装或从 VCNS
    升级的 NSX Manager 都是独立 NSX Manager 的典型例子。'
- en: '**Primary**:There will be only **one Primary NSX Manager** in a cross vCenter
    NSX environment and we will be creating all universal objects in the primary NSX
    Manager. To be more specific, any deployment, modification, or deletion tasks
    will be done on the primary NSX Manager.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主级**：在跨 vCenter NSX 环境中，只有**一个主级 NSX Manager**，我们将在主 NSX Manager 中创建所有通用对象。更具体地说，任何部署、修改或删除任务都将在主
    NSX Manager 上进行。'
- en: '**Secondary**: Whenever a standalone NSX Manager is added to the primary NSX
    Manager instance, it is called a secondary NSX Manager. All the universal objects
    are read-only on the secondary NSX Manager instance. A secondary NSX Manager instance
    cannot have its own controllers. We can have a total of seven secondary NSX Managers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**次级**：当一个独立的 NSX Manager 被添加到主 NSX Manager 实例时，它称为次级 NSX Manager。在次级 NSX Manager
    实例中，所有的通用对象都是只读的。次级 NSX Manager 实例不能拥有自己的控制器。我们最多可以拥有七个次级 NSX Manager。'
- en: '**Transit**:There will be instances wherein we need to remove/change primary
    and secondary roles for the NSX Manager and this is where the transit role is
    important. However, if the NSX Manager instance has universal objects, it cannot
    be assigned the standalone role by definition. In such cases, the NSX Manager
    instance is assigned the transit role. In the transit role, a universal object
    can only be deleted. An NSX Manager instance can be assigned the secondary role
    after all the universal objects are deleted.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中转**：有时我们需要移除或更改NSX Manager的主节点和次节点角色，这时中转角色就显得很重要。然而，如果NSX Manager实例拥有通用对象，它就不能被定义为独立角色。在这种情况下，NSX
    Manager实例将被分配为中转角色。在中转角色下，通用对象只能被删除。当所有通用对象被删除后，NSX Manager实例可以被分配为次节点角色。'
- en: 'The following diagram explains the various NSX roles that we have explained
    so far and the process to promote and demote NSX roles based on universal objects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示解释了我们迄今为止讲解的各种NSX角色以及根据通用对象提升和降级NSX角色的过程：
- en: '![Components of NSX cross vCenter Server](img/image_07_003.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![NSX跨vCenter服务器的组件](img/image_07_003.jpg)'
- en: Before moving ahead with further discussion of cross vCenter, we need to know
    the importance of the **Universal Synchronization Service**. The Universal Synchronization
    Service is the heart of cross vCenter NSX communication.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论跨vCenter之前，我们需要了解**通用同步服务**的重要性。通用同步服务是跨vCenter NSX通信的核心。
- en: Universal Synchronisation Service
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用同步服务
- en: The **Universal Synchronization Service** is responsible for synchronizing configuration
    changes from the primary NSX Manager instance to all the secondary NSX Manager
    instances. These are inbuilt services that are running in primary NSX Manager
    and they do REST API calls to secondary NSX Managers for synchronization. Okay,
    let's do a few quick tests to see what the state of the service is before we start
    assigning roles to NSX Managers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用同步服务**负责将主NSX Manager实例的配置更改同步到所有次NSX Manager实例。这些是内建服务，运行在主NSX Manager中，通过REST
    API调用次NSX Manager进行同步。好了，让我们做一些快速测试，看看在我们开始分配角色之前，服务的状态如何。'
- en: 'The following figure shows a GUI connection to NSX 6.2 and in the highlighted
    column we can see the NSX Universal Synchronization Service status:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了与NSX 6.2的GUI连接，在高亮列中我们可以看到NSX通用同步服务的状态：
- en: '![Universal Synchronisation Service](img/image_07_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![通用同步服务](img/image_07_004.jpg)'
- en: 'In our setup, we have two NSX Managers and vCenter Servers are registered with
    a common **Platform Service Controller** (**PSC**). The PSC was introduced in
    vSphere 6 and handles functions such as vCenter Single Sign-On, licensing, certificate
    management, and server reservation. Controllers are already deployed in the NSX
    Manager 192.168.110.15, which will be our primary NSX Manager in a short while.
    We all know how to register NSX Manager with an individual vCenter Server since
    we have already discussed that in [Chapter 3](ch03.html "Chapter 3. NSX Manager
    Installation and Configuration") , *NSX Manager Installation and Configuration*.
    Assuming that we have already done that registration successfully, it''s time
    to promote the **192.168.110.15** NSX Manager as primary and **192.168.210.15**
    as secondary:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设置中，我们有两个NSX Manager和与之注册的vCenter Server，它们使用一个共同的**平台服务控制器**（**PSC**）。PSC是在vSphere
    6中引入的，负责处理如vCenter单点登录、许可证、证书管理和服务器预留等功能。控制器已经在NSX Manager 192.168.110.15上部署，这将在短时间内成为我们的主NSX
    Manager。我们都知道如何将NSX Manager与单独的vCenter Server注册，因为我们在[第3章](ch03.html "第3章. NSX
    Manager 安装与配置")中已经讨论过，*NSX Manager 安装与配置*。假设我们已经成功完成了注册，现在是时候将**192.168.110.15**的NSX
    Manager提升为主节点，并将**192.168.210.15**设为次节点：
- en: Select the **Management** tab and highlight **NSX Manager**s.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**管理**选项卡并高亮显示**NSX Manager**。
- en: Select the **Actions** icon.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**操作**图标。
- en: 'Select **Assign Primary Role**:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**分配主角色**：
- en: '![Universal Synchronisation Service](img/image_07_005.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![通用同步服务](img/image_07_005.jpg)'
- en: Any guess what will happen when we promote NSX Manager to primary? If your thoughts
    match the output, I'm going to show that result right away. Congratulations if
    your answers are correct.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你猜，当我们将NSX Manager提升为主节点时会发生什么？如果你的想法与结果一致，我马上就展示这个结果。如果你的答案正确，恭喜你。
- en: 'The following figure depicts NSX Manager primary role registration; the **Replicator
    Service** is automatically starting:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了NSX Manager主节点角色注册；**复制服务**正在自动启动：
- en: '![Universal Synchronisation Service](img/image_07_006.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![通用同步服务](img/image_07_006.jpg)'
- en: 'The preceding screenshot is the NSX Manager logs from the primary NSX Manager.
    As we can see in the GUI as well, **Replicator Service** is automatically started
    since the registration is successful. The following figure shows the Universal
    Synchronization Service in a running state:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图是来自主NSX Manager的NSX Manager日志。正如我们在GUI中看到的，**复制服务**已自动启动，因为注册成功。以下图示显示了通用同步服务的运行状态：
- en: '![Universal Synchronisation Service](img/image_07_007.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![通用同步服务](img/image_07_007.jpg)'
- en: So watch out for this output and verify in the GUI whether the output matches
    the output in the logs. It is extremely important and useful for troubleshooting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以请留意这个输出，并在GUI中验证输出是否与日志中的输出匹配。这对于故障排除来说极其重要和有用。
- en: Universal segment ID
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用段ID
- en: The universal segment ID pool is used to assign VNIs to universal logical switches,
    to ensure that we don't use the same segment ID pool for local and global logical
    switches; there would be overlapping segment IDs in that eventually.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通用段ID池用于将VNI分配给通用逻辑交换机，以确保我们不会为本地和全局逻辑交换机使用相同的段ID池；否则最终会出现段ID重叠的情况。
- en: 'The following figure shows **Universal Segment ID pool** creation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了**通用段ID池**的创建：
- en: '![Universal segment ID](img/B03244_07_08.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![通用段ID](img/B03244_07_08.jpg)'
- en: The whole purpose of creating universal logical switches is to span the logical
    network across vCenter sites without doing traditional complex routing and switching.
    That way, universal logical switches will be available on all the vCenter Servers
    in the cross domain NSX site and we can simply connect virtual machines to those
    logical switches. The virtual machine logical switches will always remain as port
    groups and NSX will take care of cross vCenter switching. Haven't we configured
    a more simplified Layer 2 switching than this? First of all, was it possible do
    a Layer 2 switching like this in the past? I strongly believe we have all already
    moved away from legacy network design thinking with the amount of awareness that
    we have added so far in this book.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 创建通用逻辑交换机的整个目的是为了在vCenter站点之间跨越逻辑网络，而不需要进行传统的复杂路由和交换。这样，通用逻辑交换机将可以在跨域的所有vCenter服务器上使用，我们可以简单地将虚拟机连接到这些逻辑交换机。虚拟机的逻辑交换机将始终保持为端口组，NSX将处理跨vCenter的交换。难道我们没有配置比这更简化的第二层交换吗？首先，以前是否能够像这样进行第二层交换？我坚信，在本书中我们所增加的意识量足以让我们远离传统网络设计思维。
- en: Let's go ahead and add a secondary role to our second NSX Manager so that we
    can start creating universal transport zones and universal logical switches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为第二个NSX Manager添加一个次级角色，以便我们可以开始创建通用传输区域和通用逻辑交换机。
- en: 'The procedure for adding a secondary NSX Manager is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 添加次级NSX Manager的步骤如下：
- en: Log in to the vCenter linked to the primary NSX Manager.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到与主NSX Manager链接的vCenter。
- en: Navigate to **Home** | **Networking & Security** | **Installation** and select
    the **Management** tab.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**主页** | **网络与安全** | **安装**，然后选择**管理**选项卡。
- en: Click the primary NSX Manager. Then select **Actions** | **Add Secondary NSX
    Manager**.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击主NSX Manager。然后选择**操作** | **添加次级NSX Manager**。
- en: Enter the IP address, username, and password of the secondary NSX Manager.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入次级NSX Manager的IP地址、用户名和密码。
- en: Click **OK**.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确定**。
- en: 'The following figure depicts the **Add Secondary NSX Manager** option:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了**添加次级NSX Manager**选项：
- en: '![Universal segment ID](img/image_07_009.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![通用段ID](img/image_07_009.jpg)'
- en: 'A successful addition of a secondary NSX Manager will show the role as secondary
    as depicted in the following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 成功添加次级NSX Manager后，角色会显示为次级，正如以下截图所示：
- en: '![Universal segment ID](img/image_07_010.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![通用段ID](img/image_07_010.jpg)'
- en: Universal transport zone
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用传输区域
- en: Since we have a primary and secondary NSX Manager, let's go ahead and create
    a universal transport zone. First and foremost, there can only be **one universal
    transport zone in a cross vCenter NSX environment**. During the NSX Manager roles
    in this chapter, we have gone through the difference between primary, secondary
    and transit roles and there is no exception while creating a universal transport
    zone. Universal objects are always created from the primary NSX Manager.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有主要和次要 NSX 管理器，接下来让我们创建一个通用传输区域。首先，**在跨 vCenter NSX 环境中只能有一个通用传输区域**。在本章中关于
    NSX 管理器角色的内容中，我们已经讨论了主要、次要和传输角色的区别，创建通用传输区域时也没有例外。通用对象始终从主要 NSX 管理器中创建。
- en: 'The following figure shows universal logical switch creation and we have added
    a primary NSX-VC pairing vSphere cluster:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了通用逻辑交换机的创建，并且我们已经添加了一个主要 NSX-VC 配对 vSphere 集群：
- en: '![Universal transport zone](img/image_07_011.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![通用传输区域](img/image_07_011.jpg)'
- en: 'To add clusters from the secondary NSX-VC pairing vSphere site, we need to
    change the manager settings to secondary and click on **Connect Cluster Option**,
    which will display all clusters in the secondary NSX-VC site. Whenever we add
    new clusters, all we need to do is connect those newly added clusters to the universal
    transport zone and in my view this is the simplest way we can scale a software-defined
    data center:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要从次要 NSX-VC 配对 vSphere 站点添加集群，我们需要将管理器设置更改为次要，并点击**连接集群选项**，这样会显示次要 NSX-VC 站点中的所有集群。每当我们添加新的集群时，我们所需要做的就是将这些新添加的集群连接到通用传输区域，在我看来，这也是我们扩展软件定义数据中心最简单的方法：
- en: '![Universal transport zone](img/image_07_012.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![通用传输区域](img/image_07_012.jpg)'
- en: 'Let''s do a quick test: we will go ahead and create a universal logical switch
    and will check if logical switches are getting populated in the primary and secondary
    NSX sites. Sounds great? Let''s get started then.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做个快速测试：我们将创建一个通用逻辑交换机，并检查逻辑交换机是否在主要和次要 NSX 站点中被填充。听起来不错吗？那就开始吧。
- en: Cross vCenter universal logical switch creation
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨 vCenter 通用逻辑交换机创建
- en: 'Okay! We need to check a few prerequirements for logical switch creation to
    ensure that we are able to create the logical switch at the same time and it is
    functioning as expected. The following are the key points that should be followed
    while creating a universal logical switch:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！我们需要检查几个前提条件，以确保我们能够同时创建逻辑交换机并确保其按预期工作。创建通用逻辑交换机时应遵循以下关键点：
- en: A VSphere distributed switch should be configured
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应配置 VSphere 分布式交换机
- en: Controllers must be deployed in the primary NSX Manager
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器必须部署在主要 NSX 管理器中
- en: VSphere Host clusters must be prepared for NSX
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须为 NSX 准备 VSphere 主机集群
- en: VXLAN must be configured
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须配置 VXLAN
- en: A universal segment ID pool must be configured (should not overlap with local
    segment ID)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须配置通用段 ID 池（不应与本地段 ID 重叠）
- en: A universal transport zone must be created
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须创建一个通用传输区域
- en: 'Let''s go ahead and create a **universal logical switch**:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个**通用逻辑交换机**：
- en: In vSphere web client, navigate to **Home** | **Networking & Security** | **Logical
    Switches**.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 vSphere Web 客户端中，导航到**首页**|**网络与安全**|**逻辑交换机**。
- en: Select the NSX Manager on which you want to create a logical switch (this should
    be the primary NSX Manager; if we select the secondary NSX Manager, it won't be
    universal object selection).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你希望在其上创建逻辑交换机的 NSX 管理器（这应该是主要 NSX 管理器；如果选择了次要 NSX 管理器，将无法选择通用对象）。
- en: Click the **New Logical Switch** icon.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建逻辑交换机**图标。
- en: 'This being a universal logical switch, we certainly need to have a universal
    transport zone and segment ID created; however, we have created that already.
    Assuming that we have met all the pre requirements, let''s move on:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个通用逻辑交换机，我们当然需要创建一个通用传输区域和段 ID；不过我们已经创建了这些。假设我们已经满足所有前提条件，接下来我们继续：
- en: Type the universal logical switch name; in our example, we are naming it ****Universal****
     switch.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入通用逻辑交换机的名称；在我们的例子中，我们将其命名为****Universal****交换机。
- en: Select the **Transport Zone**; this should be the **Universal Transport Zone** created
    earlier in this chapter.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**传输区域**；这应该是本章之前创建的**通用传输区域**。
- en: 'The following figure represents universal logical switch creation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示通用逻辑交换机的创建：
- en: '![Cross vCenter universal logical switch creation](img/image_07_013.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![跨 vCenter 通用逻辑交换机创建](img/image_07_013.jpg)'
- en: Adding virtual machines to universal logical switches
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将虚拟机添加到通用逻辑交换机
- en: 'Since we have already created universal logical switches, we will go ahead
    and attach two virtual machines from two vCenter sites and perform a basic ping
    test. Considering the amount of knowledge that we have added so far, this lab
    task will be a cakewalk for all of us. Let''s get started:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经创建了通用逻辑交换机，我们将继续将两个vCenter站点的两台虚拟机连接，并执行基本的ping测试。考虑到我们到目前为止所掌握的知识，这个实验任务对我们所有人来说都将是轻松的。让我们开始吧：
- en: In logical switches, we need to select the logical switch to which you want
    to add virtual machines.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在逻辑交换机中，我们需要选择您要添加虚拟机的逻辑交换机。
- en: Click the **Add Virtual Machine** icon.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加虚拟机**图标。
- en: Select the virtual machines you want to add to the logical switch. In our case,
    we are selecting the **Web-Site A** machine, which is preconfigured with IP 1**72.17.10.11**
    as shown in the following figure:![Adding virtual machines to universal logical
    switches](img/image_07_014.jpg)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您要添加到逻辑交换机的虚拟机。在我们的例子中，我们选择的是**Web-Site A**虚拟机，它已经预配置了IP 1**72.17.10.11**，如下面的图所示：![将虚拟机添加到通用逻辑交换机](img/image_07_014.jpg)
- en: Select the vNICs that you want to connect as shown in the following figure:![Adding
    virtual machines to universal logical switches](img/image_07_015.jpg)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您要连接的vNIC，如下图所示：![将虚拟机添加到通用逻辑交换机](img/image_07_015.jpg)
- en: Click **Next** and **Finish** the connection configuration task for **Web-Site
    A.**
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**并**完成**对**Web-Site A**的连接配置任务。
- en: 'The following screenshot portrays the Web-Site A machine and its IP details:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图显示了**Web-Site A**虚拟机及其IP详细信息：
- en: '![Adding virtual machines to universal logical switches](img/image_07_016.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![将虚拟机添加到通用逻辑交换机](img/image_07_016.jpg)'
- en: If we don't see the correct virtual machine in the available objects section,
    there's a strong chance we are in the wrong NSX Manager. We have to be in the
    correct NSX Manager (primary/secondary) to see the VC-virtual machine inventory
    list.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们在可用对象部分看不到正确的虚拟机，很有可能是我们处于错误的NSX Manager中。我们必须在正确的NSX Manager（主/次）中，以查看VC虚拟机的库存列表。
- en: Since we have done with the Site-A, virtual machines have been added to the
    universal logical switch, we need to switch our NSX Manager role to secondary
    so that we can add machines from the secondary NSX Manager VC inventory to the
    same universal logical switch. How do we do that? It's a simple switch and is
    demonstrated in the following screenshot:![Adding virtual machines to universal
    logical switches](img/image_07_017.jpg)
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们已经完成了Site-A，虚拟机已被添加到通用逻辑交换机中，我们需要将NSX Manager角色切换到次要角色，这样我们就可以从次要NSX Manager的VC库存中将虚拟机添加到同一通用逻辑交换机中。我们该如何操作？这只是一个简单的切换，下面的截图演示了这一过程：![将虚拟机添加到通用逻辑交换机](img/image_07_017.jpg)
- en: We need to repeat Steps 1,2, and 3 and add virtual machine **Web-Site B** from
    the second vCenter Server, which is preconfigured with IP **172.17.10.12**.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重复步骤1、2和3，并从第二个vCenter服务器添加虚拟机**Web-Site B**，它预配置了IP **172.17.10.12**。
- en: Click the **Add Virtual Machine** icon.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加虚拟机**图标。
- en: Select the virtual machines you want to add to the logical switch. In our case,
    we are selecting the **Web-Site B** machine.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您要添加到逻辑交换机的虚拟机。在我们的例子中，我们选择的是**Web-Site B**虚拟机。
- en: 'The following screenshot shows the **Web-Site B** machine and its IP details:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了**Web-Site B**虚拟机及其IP详细信息：
- en: '![Adding virtual machines to universal logical switches](img/image_07_018.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![将虚拟机添加到通用逻辑交换机](img/image_07_018.jpg)'
- en: Now we have created a universal logical switch and connected the **Web-Site
    A** and **Web-Site B** machines residing in two vCenter Servers. Traditionally,
    we need a Layer 2 switch for such circumstances since virtual machines are on
    two vCenter Servers and the same subnet. However, the cross vCenter NSX universal
    logical switch is a game-changer for data center Layer 2 switching. This is certainly
    a great use case not only for cross vCenter virtual machine connectivity we can
    easily design an active-active,active-passive vSphere data center for disaster
    recovery configuration with VMware SRM.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个通用逻辑交换机，并连接了位于两个vCenter服务器中的**Web-Site A**和**Web-Site B**虚拟机。传统上，在这种情况下我们需要一个二层交换机，因为虚拟机位于两个vCenter服务器上并且在相同的子网中。然而，跨vCenter的NSX通用逻辑交换机改变了数据中心二层交换的游戏规则。这无疑是一个很好的使用案例，不仅是为了跨vCenter虚拟机连接，我们还可以轻松设计一个活动-活动、活动-被动的vSphere数据中心用于灾难恢复配置，并与VMware
    SRM一起使用。
- en: 'Let''s do a simple ping test to confirm if these web servers are communicating
    as expected. The following screenshot shows **Web-Site B** virtual machine connectivity
    from **Web-Site A**:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个简单的ping测试，以确认这些Web服务器是否按预期进行通信。以下截图显示了**Web-Site B**虚拟机与**Web-Site A**的连接：
- en: '![Adding virtual machines to universal logical switches](img/image_07_019.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![将虚拟机添加到通用逻辑交换机](img/image_07_019.jpg)'
- en: 'Okay! So we have Layer 2 connectivity between two vCenter sites by leveraging
    universal logical switches. If this entire network flow sounds complex or confusing,
    let''s focus on the following figure, which portrays the entire configuration
    that we did so far for establishing universal logical switching:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！通过利用通用逻辑交换机，我们已经在两个vCenter站点之间建立了二层连接。如果整个网络流看起来复杂或令人困惑，请让我们关注以下图示，它展示了我们迄今为止为了建立通用逻辑交换所做的全部配置：
- en: '![Adding virtual machines to universal logical switches](img/image_07_020.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![将虚拟机添加到通用逻辑交换机](img/image_07_020.jpg)'
- en: Cross vCenter Server Universal Logical Routers
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨vCenter Server通用逻辑路由器
- en: 'Universal Logical Routers provides a optimized routing between vCenter Server
    sites for East-West data center traffic. For now, we can call this router a **Global
    NSX Router**, which will ease management tasks such as configuring and creating
    routes (static/dynamic) and firewall rules from a single pane of glass. Once again,
    I will re-emphasize: creation/deletion and all management activity relative to
    universal logical routers can be only done from the primary NSX Manager. We will
    go ahead and configure a cross vCenter Server universal logical router and establish
    a routing between two vCenter Server sites. We are taking the same virtual machines
    that we used in universal logical switching for this configuration; however, I
    have changed the IP/subnet of **Web-Site B**, which demands a routing between
    **Web-Site A** and **Web-Site B**. Let''s get started then:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逻辑路由器提供了东-西数据中心流量之间的vCenter Server站点优化路由。目前，我们可以将此路由器称为**全球NSX路由器**，它将简化管理任务，如从单一管理界面配置和创建路由（静态/动态）和防火墙规则。我再次强调：与通用逻辑路由器相关的创建/删除以及所有管理活动只能在主NSX管理器上进行。接下来，我们将配置一个跨vCenter
    Server的通用逻辑路由器，并在两个vCenter Server站点之间建立路由。我们将使用在通用逻辑交换中使用的相同虚拟机进行此配置；然而，我已更改了**Web-Site
    B**的IP/子网，这需要在**Web-Site A**和**Web-Site B**之间进行路由。让我们开始吧：
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_021.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![跨vCenter Server通用逻辑路由器](img/image_07_021.jpg)'
- en: 'The procedure for deploying a universal logical router is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 部署通用逻辑路由器的步骤如下：
- en: Log in to vSphere web client.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到vSphere Web客户端。
- en: Click on **Networking & Security** and then click **NSX Edges**.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**网络与安全**，然后点击**NSX边缘**。
- en: Select **Universal Logical (Distributed) Router** (we will discuss local egress
    in the *Network choke points* section of this chapter):![Cross vCenter Server
    Universal Logical Routers](img/image_07_022.jpg)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**通用逻辑（分布式）路由器**（我们将在本章的*网络瓶颈*部分讨论本地出口）：![跨vCenter Server通用逻辑路由器](img/image_07_022.jpg)
- en: Enter the **User Name** and P**assword** for the **Universal Distributed Logical
    Router** (**UDLR**) as shown in the following figure:![Cross vCenter Server Universal
    Logical Routers](img/image_07_023.jpg)
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入**用户名称**和**密码**，用于**通用分布式逻辑路由器**（**UDLR**），如以下图所示：![跨vCenter Server通用逻辑路由器](img/image_07_023.jpg)
- en: 'Select the **Datacenter** and** NSX  EdgeAppliance** details as shown in the
    following screenshot:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**数据中心**和**NSX Edge Appliance**详细信息，如以下截图所示：
- en: Note
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that **NSX Edge Appliance** is not mandatory if we are leveraging
    only static routes. However, **Appliance deployment** is a must for dynamic routing
    and firewall.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，如果仅使用静态路由，则**NSX Edge Appliance**不是强制性的。然而，**Appliance部署**对于动态路由和防火墙是必须的。
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_024.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![跨vCenter Server通用逻辑路由器](img/image_07_024.jpg)'
- en: For High Availability interface configuration, we connect the interface to the
    vSphere distributed port group and they will communicate over the APIPA range
    (169.250.0.0/26) IP address as shown in the following screenshot:![Cross vCenter
    Server Universal Logical Routers](img/image_07_025.jpg)
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于高可用性接口配置，我们将接口连接到vSphere分布式端口组，并且它们将通过APIPA范围（169.250.0.0/26）IP地址进行通信，如以下截图所示：![跨vCenter
    Server通用逻辑路由器](img/image_07_025.jpg)
- en: Finally, we will add logical interfaces to the UDLR.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将向UDLR添加逻辑接口。
- en: 'The following screenshot shows the universal switch connection from Site-A
    to connect the Web-Site-A VM to the UDLR:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图展示了从 Site-A 到通用逻辑路由器（UDLR）的连接，目的是将 Web-Site-A 虚拟机连接到 UDLR：
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_026.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![跨 vCenter 服务器通用逻辑路由器](img/image_07_026.jpg)'
- en: 'We need to repeat Step 7 to add the Web-Site-B (VM residing in the second vCenter)
    to the UDLR as shown in the following screenshot:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重复步骤 7，将 Web-Site-B（位于第二个 vCenter 的虚拟机）添加到 UDLR，如下截图所示：
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_027.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![跨 vCenter 服务器通用逻辑路由器](img/image_07_027.jpg)'
- en: 'Now that we have connected both the universal switches to the UDLR, let''s
    go ahead and verify the routing table. No rocket science here, if we have followed
    all the steps so far diligently. Our UDLR should show those two logical networks
    as directly connected networks. The following screenshot depicts the output for
    the following command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将两个通用交换机连接到 UDLR，让我们继续验证路由表。没有什么高深的技术，如果我们迄今为止都按照步骤认真操作，UDLR 应该会显示那两个逻辑网络作为直接连接的网络。以下截图展示了执行以下命令的输出结果：
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_028.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![跨 vCenter 服务器通用逻辑路由器](img/image_07_028.jpg)'
- en: Since the UDLR is showing **172.16.20.0** and **172.17.10.0** networks connected,
    we should be able to perform a simple **ICMP** ping between these machines, considering
    we have appropriate firewall rules added in the router. In our example, the default
    rule is to allow all the traffic so there should not be any choke points here.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 UDLR 显示连接了 **172.16.20.0** 和 **172.17.10.0** 网络，我们应该能够在这些机器之间执行简单的 **ICMP**
    Ping 操作，前提是我们在路由器中添加了适当的防火墙规则。在我们的例子中，默认规则是允许所有流量，因此这里不应该存在任何瓶颈。
- en: 'The following screenshot portrays a successful ICMP ping from **Web-Site-B**
    (172.16.10.11) to **Web-Site-A** (172.17.10.11):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了从 **Web-Site-B**（172.16.10.11）到 **Web-Site-A**（172.17.10.11）的 ICMP Ping
    成功：
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_029.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![跨 vCenter 服务器通用逻辑路由器](img/image_07_029.jpg)'
- en: 'Let''s take an example, to be clear with the overall learning process, showing
    how routes are getting pushed to the underlying ESXi host; we have a multi-tenant
    topology as an example which I have shown in the following figure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确整体学习过程，让我们通过一个例子来看清楚路由是如何推送到底层 ESXi 主机的；我们以一个多租户拓扑为例，如下图所示：
- en: Dynamic routing protocol (OSPF) is running between Site-A and B Edges to their
    respective data center routers.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态路由协议（OSPF）在 Site-A 和 B 边缘设备之间以及它们各自的数据中心路由器之间运行。
- en: Site-A and Site-B NSX Edges are connected with Universal Distributed Logical
    Routers.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Site-A 和 Site-B 的 NSX 边缘设备与通用分布式逻辑路由器（UDLR）连接。
- en: Static routes are created on NSX Edges to reach the 172.16.10.0 series network
    (we can certainly leverage dynamic routing protocols as well, based on business
    requirements).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 静态路由在 NSX 边缘设备上创建，用于到达 172.16.10.0 系列网络（根据业务需求，我们当然也可以利用动态路由协议）。
- en: UDLR control VMs will send the learnt route to the NSX Controller cluster for
    distribution. Just to reiterate, controllers are running only in the primary NSX
    Manager since there is a cross VC NSX solution and all NSX Managers are well aware
    of universal NSX objects through the universal synchronization service.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UDLR 控制的虚拟机将把学习到的路由发送到 NSX 控制器集群进行分发。重申一下，由于这是跨 vCenter 的 NSX 解决方案，控制器只在主 NSX
    管理器中运行，并且所有 NSX 管理器都通过通用同步服务充分了解通用 NSX 对象。
- en: The primary NSX Controllers will send those routes to the underlying ESXi hosts.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主 NSX 控制器将把这些路由发送到底层的 ESXi 主机。
- en: The ESXi host kernel routing module will update its routing table and will take
    care of data path traffic for those networks which it has learnt from the controllers.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ESXi 主机的内核路由模块将更新其路由表，并负责处理它从控制器学习到的那些网络的数据路径流量。
- en: 'The preceding mentioned six steps are basic routing learning processes in an
    NSX environment:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面提到的六个步骤是 NSX 环境中基本的路由学习过程：
- en: '![Cross vCenter Server Universal Logical Routers](img/image_07_030.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![跨 vCenter 服务器通用逻辑路由器](img/image_07_030.jpg)'
- en: Network choke points
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络瓶颈
- en: 'Let me get started by saying: *Never offer or implement any design unless you
    are well aware that it addresses all the customer''s needs*.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从一句话开始：*除非你非常清楚设计能够满足客户所有需求，否则绝不要提供或实施任何设计*。
- en: 'That rule is not specific to marketing or sales folk. It''s general advice
    for anyone who deals with technology. If not, we will hear the feedback: *I''m
    having a nightmare; all the problems started happening after that design change.
    Can you please get rid of that?* We have designed or seen various types of vSphere
    networks. Every network topology will have a loophole. Nothing is perfect in this
    world and all we can do is ensure that we are better prepared for failures, this
    seems more like an advantage than a failure? I want all of you to pause for a
    minute and have a look at the preceding topology; make a note of all failure scenarios
    that might interrupt data traffic. Adding to that, if we have carefully observed
    the topology, we will see that universal control VMs and Edges are running on
    two different sites. So how will the UDLR control VM ensure that whatever routes
    it is learning from that specific NSX Edge are the only routes learnt by the underlying
    ESXi host that are specific to that site? I know that is a slightly confusing
    statement. Never mind, all we need is that routes learned by Site-A appliances
    (Edges/Control VM) are sent to the Site-A ESXi host and vice versa for Site-B.
    Okay! Let''s get started then and keep reading the following useful points to
    ensure that our design satisfies our customer requirement without inviting any
    further problems:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这条规则并不限于市场营销或销售人员，它是给所有与技术打交道的人的通用建议。如果没有遵循，我们会听到这样的反馈：*我遇到了噩梦般的情况；所有问题都是在那次设计变更后开始的。你能把这个去掉吗？*
    我们设计过或见过各种类型的 vSphere 网络。每种网络拓扑都会有一个漏洞。这个世界上没有完美的东西，我们能做的就是确保自己更好地为故障做好准备，这更像是一种优势而非失败？我希望大家暂停一分钟，看看前面的拓扑图；记录下可能中断数据流量的所有故障场景。再者，如果我们仔细观察拓扑图，会发现通用控制
    VM 和边缘设备运行在两个不同的站点。那么，UDLR 控制 VM 如何确保它从特定的 NSX Edge 学到的路由是该站点下的 ESXi 主机所学习的唯一路由呢？我知道这个说法有点困惑。没关系，我们需要的只是，站点
    A 的设备（边缘设备/控制 VM）学到的路由会发送到站点 A 的 ESXi 主机，站点 B 则反之亦然。好吧！那么我们开始吧，继续阅读以下有用的要点，确保我们的设计满足客户需求，避免引发任何进一步的问题：
- en: 'Both the data centers have two NSX appliances running and we need to ensure
    they are running in HA mode with vSphere HA also configured. That way, single
    appliance/host failure will have less impact:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个数据中心都运行着两台 NSX 设备，我们需要确保它们以 HA 模式运行，并且 vSphere HA 已配置。这样，单个设备/主机故障的影响会更小：
- en: NSX Edge
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Edge
- en: UDLR Control VM
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: UDLR 控制 VM
- en: Ensure that we are using **LOCALE-ID (by default, this value is set to the NSX
    Managers-UUID)**. With the Locale-ID configuration, NSX Controllers will send
    routes to ESXi hosts matching Locale-ID. Going via our topology, each site ESXi
    host will maintain a site-specific local routing table. We can set Locale-ID per
    cluster, host level and UDLR level. This would perfectly fit in a multi-tenant
    network/cloud environment wherein each tenant wants to maintain routes which are
    specific to that tenant.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保我们使用 **LOCALE-ID（默认情况下，该值设置为 NSX 管理器的 UUID）**。通过 Locale-ID 配置，NSX 控制器会将路由发送到匹配
    Locale-ID 的 ESXi 主机。在我们的拓扑中，每个站点的 ESXi 主机会维护一个特定站点的本地路由表。我们可以在每个集群、主机级别和 UDLR
    级别设置 Locale-ID。这非常适合多租户网络/云环境，其中每个租户都希望维护特定于该租户的路由。
- en: All **South-North** traffic is handled by **Site A NSX Edge** and **SITE B NSX
    Edge** in their respective sites. Starting from NSX 6.1, **Equal Cost Multi Path** (**ECMP**)
    is supported. Hence, we can deploy multiple NSX Edges and the ECMP algorithm will
    HASH the traffic based on source and destination IP. ECMP can be enabled on Edges
    and DLR. That way, if there is a failure, it will recalculate the HASH and will
    route the traffic to the active edge/DLR. In addition to that, there will be designs
    that might demand back-to-back ECMP configuration, Distributed Logical Router
    to NSX Edge and NSX Edge to physical routers.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有 **南北向** 流量由 **站点 A NSX Edge** 和 **站点 B NSX Edge** 在各自的站点中处理。从 NSX 6.1 开始，**等价多路径**（**ECMP**）得到了支持。因此，我们可以部署多个
    NSX Edge，ECMP 算法会基于源和目标 IP 对流量进行哈希处理。ECMP 可以在边缘设备和 DLR 上启用。这样，如果发生故障，它会重新计算哈希，并将流量路由到活动的边缘设备/DLR。此外，某些设计可能需要连续的
    ECMP 配置，分布式逻辑路由器到 NSX Edge，以及 NSX Edge 到物理路由器。
- en: 'We should never design something that breaks a working topology. While we deal
    with ECMP, we need to be aware that NSX Edge has a stateful firewall. There is
    a good chance we might have asymmetric routing issues; basically, a packet that
    travels from source to destination uses one path and while replying, takes another
    path, because at any time only one Edge will be aware of the traffic flow. No
    worries: while we enable ECMP in NSX 6.1, we will get a message that says enabling
    this feature will **disable Edge firewall**. **Don''t worry**, we are not compromising
    on firewall rules in such designs. Either we can deploy any third-party physical
    firewall (as shown in the figure) between NSX Edges and Upstream router or we
    need to leverage Distributed Firewall, which will filter the traffic at the VNIC
    level. However, starting from NSX 6.1.3, ECMP and logical firewall can work together
    and for the same reason it won''t get disabled by default when we enable ECMP.
    However, starting from NSX 6.1.3, ECMP and logical firewall can work together
    and for the same reason it won''t get disabled by default when we enable ECMP
    (Active/Standby NSX Edges). The following diagram depicts an **ECMP**-added configuration
    for the same topology:![Network choke points](img/image_07_031.jpg)'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们绝不应该设计会破坏现有拓扑的方案。当我们处理 ECMP 时，需要注意的是，NSX Edge 拥有有状态防火墙。我们很可能会遇到非对称路由问题；基本上，从源到目标的包使用一条路径，而回复时则走另一条路径，因为任何时候只有一个
    Edge 会知道流量路径。别担心：当我们在 NSX 6.1 中启用 ECMP 时，会收到一条消息，提示启用此功能将**禁用 Edge 防火墙**。**别担心**，我们并没有在这种设计中妥协防火墙规则。我们可以在
    NSX Edge 和上游路由器之间部署任何第三方物理防火墙（如图所示），或者我们需要利用分布式防火墙，在 VNIC 层过滤流量。然而，从 NSX 6.1.3
    开始，ECMP 和逻辑防火墙可以共同工作，且出于同样的原因，在启用 ECMP 时，逻辑防火墙默认不会被禁用（主动/备用 NSX Edge）。下图展示了同一拓扑中添加
    **ECMP** 配置后的情况：![网络瓶颈](img/image_07_031.jpg)
- en: Since we have NSX Edges running on each site, **overlapping** IP addresses are
    supported by configuring a NAT on the site-specific Edge. Again, a highly demanding
    use case, especially for cloud providers.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于每个站点都有运行中的 NSX Edge，因此通过在特定站点的 Edge 上配置 NAT，支持**重叠**的 IP 地址。再次强调，这是一个需求非常高的使用场景，尤其是对于云服务提供商。
- en: We can have eight NSX Edges participating in ECMP configuration at a time; in
    our case, eight ECMP edges per site with a total of 16 Edges we can run in that
    way.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以同时让八个 NSX Edge 参与 ECMP 配置；在我们的情况下，每个站点有八个 ECMP 边缘，总共有 16 个 Edge 可以以这种方式运行。
- en: How about a worst case scenario of complete Site A or Site B failure one at
    a time? There is certainly a solution for any problem, but in this case, the solution
    will be slightly tedious and based on the physical network design. Reconfiguring
    all NSX components in another site may not work.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果发生 Site A 或 Site B 完全故障的最坏情况会怎么样？当然，任何问题都有解决方案，但在这种情况下，解决方案会稍显繁琐，并且取决于物理网络设计。在另一个站点重新配置所有
    NSX 组件可能不起作用。
- en: Site-B is where we have the primary NSX Manager and Site-B had a complete failure.
    Starting from NSX Manager role changing, we need to deploy Edges and appliances
    and can bring the environment back to normal only if the physical network design
    is equally matching for both the sites. I know this is a tedious process,so if
    we want to automate such tasks,we need to leverage NSX API and VRO workflow's
    so that that would ease lot of manual tasks. In a rare case, we may have to reconfigure
    the physical network so that Site-B machines can communicate with the physical
    network while they are residing in Site-A during the outage.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Site-B 是我们拥有主 NSX Manager 的地方，而 Site-B 完全故障。从 NSX Manager 角色更改开始，我们需要部署 Edges
    和设备，并且只有在物理网络设计在两个站点之间完全匹配时，才能将环境恢复到正常状态。我知道这是一个繁琐的过程，因此如果我们想自动化这些任务，我们需要利用 NSX
    API 和 VRO 工作流，以便简化大量手动操作。在极少数情况下，我们可能需要重新配置物理网络，以便 Site-B 的机器在故障期间能够与物理网络通信，尽管它们当时处于
    Site-A。
- en: There is a lot to be discussed about types of failures, such as NSX components
    in network sites failing at the same time or virtual environment and physical
    network partial/full failure scenarios. There are also routing protocol (OSPF/BGP)
    specific failure scenarios that also bring up some good points for discussion.
    It is extremely hard to cover all such failure scenarios and carry out precautionary
    steps based on the type of design in just one book. Luckily, the NSX product documentation
    from VMware is not limited to installation, configuration, and general designs.
    There are a few design guides they have released specific to vendor integration,
    such as NSX+UCS design, NSX+CISCO ACI, and so on. It's worth reading such documents
    once we have mastered the basics and, hopefully, what we have learnt so far in
    all seven chapters is a foundation step for climbing the network virtualization
    ladder. It's been a remarkable discussion so far on VMware NSX topologies and,
    optimistically, by this time, we all are clear on how VMware NSX is reinventing
    data center networking.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于故障类型有很多内容可以讨论，比如网络站点中的NSX组件同时发生故障，或者虚拟环境与物理网络部分/完全故障的场景。还有一些路由协议（OSPF/BGP）特定的故障场景，这些问题也引发了一些值得讨论的要点。要在一本书中涵盖所有这样的故障场景并根据设计类型采取预防措施是非常困难的。幸运的是，VMware的NSX产品文档并不限于安装、配置和一般设计内容。他们发布了一些专门针对厂商集成的设计指南，比如NSX+UCS设计、NSX+CISCO
    ACI等。等我们掌握了基础知识后，读一读这些文档是很有价值的。希望到目前为止，我们在七章中学到的内容已经为登上网络虚拟化的阶梯奠定了基础。到目前为止，我们对VMware
    NSX拓扑的讨论非常精彩，乐观地说，到现在为止，我们都已经清楚了VMware NSX是如何重塑数据中心网络的。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter with an introduction to NSX cross vCenter Server followed
    by cross vCenter Server components, and universal object creation, and we ended
    by discussing a few design decisions that we should be well aware of during cross
    vCenter Server NSX deployment. Way back, network troubleshooting was single-handedly
    done by network architects and support engineers, which made life easy for vSphere
    folk. NSX being a network software layer running on top of vSphere, people often
    believe that it might make their life somewhat threatening since they have a clear
    visibility on both for both hypervisor and network virtualization layers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从介绍NSX跨vCenter Server开始，接着讨论了跨vCenter Server组件和通用对象创建，最后讨论了一些在跨vCenter Server部署NSX时需要特别注意的设计决策。早些时候，网络故障排除主要由网络架构师和支持工程师单独完成，这让vSphere用户的生活变得更加轻松。由于NSX是运行在vSphere上的网络软件层，人们通常认为它可能会让他们的生活变得有些复杂，因为他们需要清晰地了解超管和网络虚拟化层的情况。
- en: For me, troubleshooting is an art; if we follow a systematic procedure for checking
    a problem, resolving the problem is a cakewalk. There is no secret or straightforward
    automation that will help us analyze and fix a problem in network virtualization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，故障排除是一门艺术；如果我们遵循一个系统的检查流程，解决问题简直是小菜一碟。没有任何秘密或简单的自动化可以帮助我们分析和修复网络虚拟化中的问题。
- en: In the next chapter, we will have a detailed discussion on NSX troubleshooting.
    So let's ensure that we recall whatever we learnt and get our hands dirty by applying
    those points based on the situation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将详细讨论NSX故障排除。因此，让我们确保回顾一下所学内容，并通过根据具体情况应用这些知识来实践。
