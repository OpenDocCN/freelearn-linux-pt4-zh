<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0; NSX Troubleshooting"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8.   NSX Troubleshooting  </h1></div></div></div><p>Let me start this chapter with a famous quote from Antisthenes:</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>"Not to unlearn what you have learned is the most necessary kind of learning"</em></span>
</p></blockquote></div><p>I couldn't find a better quote than that for giving everyone a heads-up on how vital it is to ensure that we recollect what we have learned so far in previous chapters about how to approach a problem to see what the best solution is. For the best solution to also be the quickest, we truly need to know how to approach a scenario, where to start looking, what logs are useful, and lastly, when to engage the vendor for further troubleshooting. As we all know, our course is focused on NSX with vSphere. NSX is tightly integrated with vSphere.</p><p>Taking a real example, even a well-constructed building will not stand on a weak foundation. A bad vSphere design will have a direct impact on NSX components, no matter how good the NSX design is. This rule of thumb is the same for any VMware solution that runs on top of vSphere. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">NSX installation and registration issues</li><li class="listitem" style="list-style-type: disc">The log collection process and steps</li><li class="listitem" style="list-style-type: disc">VXLAN troubleshooting</li></ul></div><div class="section" title="NSX Manager installation and registration issues"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>NSX Manager installation and registration issues</h1></div></div></div><p>Installing NSX Manager is one of the easiest tasks, and the bitter truth is that anyone who is familiar with vSphere OVA/OVF deployment can easily deploy an NSX Manager without any prior knowledge of NSX products. We know for sure, that in a production environment, no one will follow that method. However, I still want to educate you all about the importance of NSX installation. Let's carefully go through the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There should not be any vCloud networking security (VCNS/vShield Manager) registered with the same vCenter when we are trying to register NSX Manager. If we find any such environments, we must ensure that we are unregistering one of the solutions; definitely VCNS/vShield, since that is an outdated solution compared with NSX Manager. That doesn't mean we can have two NSX Managers registered with the same vCenter Server. However, we can upgrade VCNS to NSX and I will be sharing the upgrade guide link in the chapter's final section.</li><li class="listitem" style="list-style-type: disc">Never import any previously used NSX Manager instance to a new environment and register it as a solution with a new vCenter.</li><li class="listitem" style="list-style-type: disc">Always check if NSX Manager is registered with how many vSphere solutions. For example, we might have a <span class="strong"><strong>vCloudAutomation Center </strong></span>(<span class="strong"><strong>VCAC</strong></span>) and<span class="strong"><strong> vCloud Director</strong></span> (<span class="strong"><strong>VCD</strong></span>) registered with NSX Manager A, which is also registered with a vCenter Server environment. The reason why I'm more curious about such solutions is that careful planning and design is required not only for installation but also for uninstallation of NSX products during break fix time. Each solution's integration demands separate steps while unregistering NSX Manager.</li><li class="listitem" style="list-style-type: disc">Always take a backup of NSX Manager after initial deployment of the software. Never depend on the vSphere snapshot feature for this backup activity.</li><li class="listitem" style="list-style-type: disc">NSX Manager can be treated as a normal vSphere virtual machine for troubleshooting any network-related issues. For example, we can migrate NSX Manager from one host to another host, or check the ESXTOP command to know Tx and Rx counts for isolating a network issue.</li><li class="listitem" style="list-style-type: disc">While registering with vCenter Server, we have two options:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Lookup service registration</strong></span>: Lookup service registration is an optional feature for importing SSO users. However, if we are integrating with an SSO identity source, we need to follow all vendor-specific best practices for identity source availability. But, it's worth remembering that if SSO is down except for login to NSX Manager, it won't have any impact on NSX components and their features.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>vCenter Server Registration</strong></span>: vCenter Server registration is the first and most critical integration. Hence, we need to ensure that we have proper connectivity and configuration for the following points:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">DNS resolution should be configured between <span class="strong"><strong>NSX Manager</strong></span> and <span class="strong"><strong>vCenter Server</strong></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NTP</strong></span> should be configured properly; this point might be very familiar for most of the experts, but I will still reiterate it: The impact of wrong NTP is very high when we integrate the lookup service (SSO) and try to leverage SSO-based authentication.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Firewall </strong></span>ports should be opened between NSX Manager and vCenter Server. Always verify VMware <span class="strong"><strong>Knowledge Base</strong></span> (<span class="strong"><strong>KB</strong></span>) article for port requirements. The following link leads to a VMware KB article, which talks about all the port requirements:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">https://kb.vmware.com/selfservice/search.do?cmd=displayKC&amp;docType=kc&amp;docTypeID=DT_KB_1_1&amp;externalId=2079386</code></li><li class="listitem" style="list-style-type: disc">Ensure that we are using vCenter Server administrative user rights while registering with NSX Manager. We can certainly use the administrator@vsphere.local account to register NSX with vCenter, vCloud Director, and vRealize Automation products.</li></ul></div><p>
</p></li></ul></div><p>
</p></li></ul></div><p>
</p></li></ul></div></div></div>
<div class="section" title="Troubleshooting NSX Manager"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Troubleshooting NSX Manager</h1></div></div></div><p>Based on the situation, we may have to collect diagnostic information for NSX Manager for VMware Support. Keep the following steps handy for such scenarios.</p><div class="section" title="Collecting NSX Manager logs via GUI"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec49"/>Collecting NSX Manager logs via GUI</h2></div></div></div><p>The steps to collect NSX Manager logs via GUI are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to the <span class="strong"><strong>
<span class="strong"><strong>NSX Manager</strong></span>
</strong></span> virtual appliance through a web browser.</li><li class="listitem">In <span class="strong"><strong>NSX Manager Virtual Appliance Management</strong></span>, click <span class="strong"><strong>Download Tech Support Log</strong></span>.</li><li class="listitem">Click <span class="strong"><strong>
<span class="strong"><strong>Download </strong></span>
</strong></span>|<span class="strong"><strong>
<span class="strong"><strong> Save</strong></span>
</strong></span>. The following screenshot depicts the NSX Manager logs download:</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/image_08_001.jpg" alt="Collecting NSX Manager logs via GUI"/></div><p>
</p></div><div class="section" title="Collecting NSX Manager logs via CLI"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec50"/>Collecting NSX Manager logs via CLI</h2></div></div></div><p>There might be instances when the NSX Manager GUI is not working and we might need to depend on the CLI for collecting logs. For CLI haters, there is no escape this time; we need to go through the following steps to capture NSX Manager logs:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to the <span class="strong"><strong>NSX Manager</strong></span> virtual appliance through a SSH session.</li><li class="listitem">Go to <span class="strong"><strong>
<span class="strong"><strong>Enable Mode</strong></span>
</strong></span>, by typing <code class="literal">enable</code><span class="strong"><strong>.</strong></span></li><li class="listitem">Issue the following command in <span class="strong"><strong>Enable Mode</strong></span>, which will save the NSX Manager logs in a remote location based on the host name that we selected:</li></ol></div><p>                <code class="literal">export tech-support scp USERNAME@HOSTNAME:FILENAME</code>
</p><p>The following screenshot illustrates NSX CLI log capturing:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_002.jpg" alt="Collecting NSX Manager logs via CLI"/></div><p>
</p></div></div>
<div class="section" title="VMware Installation Bundle"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec62"/>VMware Installation Bundle</h1></div></div></div><p>Hypervisors are basically the backbone of network virtualization. Virtual machines are able to leverage NSX features primarily because the ESXi host is a network-virtualized host. One of the most critical pillars of an NSX installation is ESXi host preparation. If we don't have the right modules running in the ESXi host, the whole purpose of leveraging NSX features will be defeated. Symptoms would be that we might not be able to install feature <span class="emphasis"><em>X</em></span>, or we can configure feature <span class="emphasis"><em>X</em></span>, but the functionality is impacted. Watch out for the following VIBs in the ESXi host:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">esx-vxlan</li><li class="listitem" style="list-style-type: disc">esx-vsip</li><li class="listitem" style="list-style-type: disc">esx-dvfilter-switch-security (starting from NSX 6.2.0, esx-dvfilter-switch-security is part of esx-vxlan vibs)</li></ul></div><p>This is the command to check if VIB is installed in ESXi host:</p><pre class="programlisting">
<span class="strong"><strong>esxcli software vib list | grep vibname</strong></span>
</pre><p>Since these are VIBs, we can manually uninstall and install the same during break fix scenarios. But the real question is, who is pushing these VIBs? That's where I have seen the majority of issues. Behind the scenes, vCenter Server ESX Agent Managers (<span class="strong"><strong>EAM</strong></span>)<span class="strong"><strong> </strong></span> are responsible for installing these VIBs. So, first and foremost, the EAM service should be up and running. The following steps are useful for collecting EAM based upon the operating system and vCenter Server flavor.</p><div class="section" title="EAM log location"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec51"/>EAM log location</h2></div></div></div><p>Following are the EAM log locations for respective vCenter Server and operating system versions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VMware vSphere 5.1.x/5.5.x (EAM is a part of the common Tomcat server):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Windows 2003: <code class="literal">C:\Documents and Settings\All Users\Application Data\VMware\VMware VirtualCenter\Logs\eam.log</code></li><li class="listitem" style="list-style-type: disc">Windows 2008: Same as Windows 2003, the VC log directory is located at <code class="literal">C:\ProgramData\VMware\VMware VirtualCenter\Logs\</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>vCenter Server Virtual Appliance</strong></span> (<span class="strong"><strong>VCVA</strong></span>): <code class="literal">/storage/log/vmware/vpx/eam.log</code></li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc">VMware vSphere 6.x (EAM is a standalone service and has embedded tcserver):<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Windows 2003: <code class="literal">C:\Documents and Settings\All Users\Application Data\VMware\CIS\logs\eam\eam.log</code></li><li class="listitem" style="list-style-type: disc">Windows 2008: Same as Windows 2003, the VC log directory is located at <code class="literal">C:\ProgramData\VMware\VMware VirtualCenter\Logs\</code></li><li class="listitem" style="list-style-type: disc">CloudVM: <code class="literal">/storage/log/vmware/eam/eam.log</code></li></ul></div><p>
</p></li></ul></div><p>I have seen a lot of issues, especially when a vCenter Server installation is a Windows-based installation, with EAM trying to use port <code class="literal">80</code> for downloading VIBs. At times, we might have other applications or services running in VC, which might be leveraging port <code class="literal">80</code> and will cause VIB download failures, so we have to change the default EAM ports. However, starting from VMware vSphere 6.0, VIB downloads over port <code class="literal">443</code> (instead of port <code class="literal">80</code>) are supported. This port is opened and closed dynamically. The intermediate devices (firewalls) between the ESXi hosts and vCenter Server must allow traffic using this port. With that, we will move on to our next topic: control plane and data plane log collection.</p></div></div>
<div class="section" title="Control plane and data plane log collection"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec63"/>Control plane and data plane log collection</h1></div></div></div><p>Log collection is vital for proactive and root cause analysis. How many times have we ended up collecting the wrong set of logs or received feedback that we have to enable or increase certain logging levels to ensure that we have the right set of logs to analyze the root cause? Technically, that type of feedback is digestible. However, when it comes to production impact, it would be a disappointment to find that there is nothing conclusive, even after going through the logs. There is only one solution for this issue: we should know what logs need to be collected and most importantly, from which locality. First and foremost, we need to get some background knowledge on the following points:</p><div class="section" title="Understanding the physical topology"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec52"/>Understanding the physical topology</h2></div></div></div><p>Understanding the physical topology is not only important for overall NSX design and feature configuration, it is equally important to share effective feedback if there is a better way to approach the overall design. The following mentioned points are something that we need to keep handy:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Physical Network design - Spine-leaf/layer2 architecture</li><li class="listitem" style="list-style-type: disc">Existing firewall deployments and rules</li><li class="listitem" style="list-style-type: disc">Overall datacenter Routing and Switching topology</li><li class="listitem" style="list-style-type: disc">vSphere cluster design (placement in racks) and topology. In addition to that, how many clusters (single-site, multi-site), data centers and active-active or active-passive physical data center designs required currently or future configuration.</li><li class="listitem" style="list-style-type: disc">vSphere distributed Switch Uplink policy</li></ul></div><p>The following are some recommendations based on the preceding points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Physical Network design - Spine-leaf/layer2 architecture</strong></span>: Spine/Leaf architecture is the best and most widely used connectivity now a days because of full mesh connectivity, less latency, high bandwidth, ECMP routing and most importantly easy expansion of network is achievable.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Existing firewall deployments and rules</strong></span>: This is an important check primarily because of NSX Edge Firewall and Microsegmentation capabilities. My suggestion would be try to integrate vRealize Network-insight software to understand overall traffic growth in North-South and East-West direction and then decide what firewall policies to be configured at which points. In the closing section of the chapter i have given a small summary on vRealize Network-insight software.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Overall datacenter routing and switching topology</strong></span>: <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Here we are mainly focusing on routing protocols used in physical network - for example OSPF, BGP, ISIS. </li><li class="listitem" style="list-style-type: disc">For example based on the AREA types configured for OSPF, assuming that upstream router is <span class="strong"><strong>Area Border Router (ABR)</strong></span> we need to know what routes should be injected to Upstream router from NSX Edge and appropriate firewall policies for allowing the traffic.</li><li class="listitem" style="list-style-type: disc">In the Layer 2 side ideally ESXI, VXLAN traffic is the most important parameters that we need to know for assigning/configuring VLAN-ID (Management, VMotion, Storage, VXLAN).</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>vSphere cluster design (placement in racks) and topology</strong></span>: <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cluster design should ideally be Management &amp; Edge Cluster Separated from Compute Cluster with minimum of 4 Host in Management &amp; Edge Cluster and maximum of 64 host in Computer Cluster (only if environment is running vSphere 6.0)</li><li class="listitem" style="list-style-type: disc">For disaster recovery environment configured with SRM and NSX.It is recommended to maintain similar physical and virtual design in the DR site. Situations might demand workloads to be running in DR site for long time, hence we need to follow that strict rule.</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>vSphere distributed Switch Uplink policy</strong></span>: This is the same topic that we have discussed in <a class="link" href="ch03.html" title="Chapter 3. NSX Manager Installation and Configuration">Chapter 3</a>, <span class="emphasis"><em>NSX Manager Installation and Configuration</em></span>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Here we need to select and configure NIC teaming and failover policies. For example if we choose LACP configuration, we are limited with Single VTEP configuration, however route based on port and source MAC hash support multi VTEP configuration. Prevention is better that cure, it is recommended to load balance VXLAN traffic across all available uplinks rather than inviting performance issues.</li></ul></div><p>
</p></li></ul></div><p>The idea behind these minor point discussion is to ensure all configurations/design aspects are taken care well in advance rather than spending time later to do configuration and performance tweaking.</p></div></div>
<div class="section" title="NSX Controller log collection"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec64"/>NSX Controller log collection</h1></div></div></div><p>Controller is the real game changer component in the overall architecture of NSX. For the same reason, it remains a critical piece when it comes to troubleshooting. As we all know, controllers are deployed from NSX Manager in an <span class="strong"><strong>Open Virtualization Appliance</strong></span> (<span class="strong"><strong>OVA</strong></span>). In a worst-case scenario, even the deployment of controllers might fail, and that would be a showstopper for any NSX implementation. The majority of such failures happen for the following two reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">DNS</li><li class="listitem" style="list-style-type: disc">NTP</li></ul></div><p>There should be proper DNS/NTP configuration between ESXi hosts, vCenter Server, and NSX Manager for a successful deployment of NSX Controller. Apart from this point, a successful deployment of any virtual machine in vSphere certainly needs enough compute and storage capacity, and NSX Controller is no exception, primarily because these are virtual machines from an ESXi host perspective. For collecting NSX Controller logs, we need to complete the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Firstly, we need to log in to vCenter Server using the vSphere web client.</li><li class="listitem">Click on <span class="strong"><strong>Networking and Security</strong></span>.</li><li class="listitem">Click <span class="strong"><strong>Installation</strong></span> on the left-hand pane.</li><li class="listitem">Under the <span class="strong"><strong>Manage</strong></span> tab, select the controller you want to download logs from.</li><li class="listitem">Click <span class="strong"><strong>Download Tech support logs</strong></span>.</li><li class="listitem">The following screenshot depicts the NSX Controller log collection process:</li></ol></div><p>
</p><div class="mediaobject"><img src="graphics/image_08_003.jpg" alt="NSX Controller log collection"/></div><p>
</p><p>How about if the web client is down, or we don't have access to the web client? What method we can follow to collect the logs? There are two options in those cases:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Using a vSphere client session, we can connect to vCenter Server or the ESXi host where the controller is running, and we can take a VM console session to controller to leverage the CLI command for log collection.</li><li class="listitem">Take an SSH session directly to controller and execute a CLI command for log collection.</li></ol></div><div class="section" title="Collecting NSX Controller logs using CLI steps"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec53"/>Collecting NSX Controller logs using CLI steps</h2></div></div></div><p>Firstly, log into the NSX Controller for which you want to gather logs using any of the previous steps, and execute the following command, as shown in the screenshot:</p><pre class="programlisting">
<span class="strong"><strong>save status-report filename</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_08_004.jpg" alt="Collecting NSX Controller logs using CLI steps"/></div><p>
</p><p>After the controller logs are captured, we can go ahead and copy the same set of logs to any machine that has got IP connectivity with the controller.</p><p>In the following example, I have copied the controller logs to one of the management ESXi host TMP locations:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" alt="Collecting NSX Controller logs using CLI steps"/></div><p>
</p><p>With that, we will move to NSX Edge and DLR log collection, and we will finish off with data plane log collection and a few important service statuses.</p><p>The processes for collecting Edge and DLR logs are almost the same.</p></div><div class="section" title="Collecting Edge and Distributed Logical Router logs through the web client"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec54"/>Collecting Edge and Distributed Logical Router logs through the web client</h2></div></div></div><p>The following are the steps for collecting distributed logical router logs:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Firstly, log into vCenter Server using the vSphere web client.</li><li class="listitem">Click the <span class="strong"><strong>Networking &amp; Security</strong></span> icon.</li><li class="listitem">Click <span class="strong"><strong>Edges</strong></span> on the left-hand pane.</li><li class="listitem">On the right-hand pane, select the edge (<span class="strong"><strong>DLR/EDGE</strong></span>) we want to download from.</li><li class="listitem">Click <span class="strong"><strong>Actions</strong></span> and select <span class="strong"><strong>
<span class="strong"><strong>Download Tech support logs</strong></span>
</strong></span>.</li></ol></div><p>In the following screenshot, we can see <span class="strong"><strong>Download Tech support log</strong></span> highlighted for Distributed Logical Router, and I mentioned earlier that this is the same process as for collecting NSX Edge logs:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" alt="Collecting Edge and Distributed Logical Router logs through the web client"/></div><p>
</p><p>For collecting logs via CLI, we need to execute the following command by executing any of the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Using a vSphere client session, we can connect to vCenter Server or the ESXi host where controller is running, and we can take a VM console session to controller to leverage the CLI command for log collection.</li><li class="listitem">Take an SSH session directly to controller and execute the CLI command for log collection:</li></ol></div><pre class="programlisting">
<span class="strong"><strong>  export tech-support scp user@scpserver:file</strong></span>
</pre><p>We have already discussed what EAM is and the role it plays in an NSX environment. Apart from that vSphere troubleshooting piece, we need to the status and logging level of two user world agents, which will be running in an NSX-prepared ESXi host.</p></div></div>
<div class="section" title="NSX user world agents"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec65"/>NSX user world agents</h1></div></div></div><p>NSX Manager is responsible for deploying the NSX Controller cluster, ESXi hosts preparation by pushing <span class="strong"><strong>vSphere Installation Bundles</strong></span> (<span class="strong"><strong>VIBs</strong></span>) to enable VXLAN, distributed routing, distributed firewall, and a user world agent used to communicate at the control-plane level. The functionality of the user world agent is highly critical and any failures will have a direct impact on the control plane learning, which eventually affects data plane traffic. So, let's discuss these agents, along with basic health checks and log locations.</p><div class="section" title="netcpa"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec55"/>netcpa</h2></div></div></div><p>It is a user world agent that communicates with NSX control plane, and the netcpa service should be up and running on the NSX-prepared ESXi host. If the functionality is impacted, we will certainly experience routing and switching issues in the NSX environment, and the ESXi host won't learn new routes from the time the netcpa service was down. So, this is extremely important: creating the routes alone on an NSX Edge VM won't do the trick; unless the netcpa service is up and running, ESXi host won't learn those routes. Complete the following steps in order to check for netcpa-related issues:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Check if the netcpa service is running on the host (this needs to be checked on the host where we are experiencing network or control-plane related issues).</li><li class="listitem">Use the following command to check the netcpa service:</li></ol></div><pre class="programlisting">
<span class="strong"><strong>
<span class="strong"><strong>/etc/init.d/netcpad status</strong></span>
</strong></span>
</pre><p>The following screenshot depicts the netcpa agent's status:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_007.jpg" alt="netcpa"/></div><p>
</p><p>Check if the netcpa configuration file is showing all the controllers. Use the following command to check controller details in the configuration file:</p><pre class="programlisting">
<span class="strong"><strong>cat  /etc/vmare/netcpa/config-by-vsm.xml</strong></span>
</pre><p>The following screenshot depicts the config file output, with controller IP and SSL certificate thumbprint information:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_008.jpg" alt="netcpa"/></div><p>
</p><p>In the <code class="literal">/var/log/netcpa.log</code> file on the ESXi host, we can see the complete netcpa logs. The following screenshot depicts controller registration information, which is populated in netcpa logs:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_009.jpg" alt="netcpa"/></div><p>
</p><p>Anytime we are facing issues with the netcpa service, I would strongly recommend restarting the service to confirm if that fixes the issue. To restart the netcpa service, we need to complete the following steps in order:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in as root to the ESXi host through SSH or through the DCUI console.</li><li class="listitem">Run the <code class="literal">/etc/init.d/netcpad</code> restart command to restart the netcpa agent on the ESXi host.</li></ol></div></div><div class="section" title="Vsfwd"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec56"/>Vsfwd</h2></div></div></div><p>NSX distributed firewall is a hypervisor integrated firewall and apart from the point that the host should have a firewall <code class="literal">vib</code> installed, there should be a vsfwd daemon process up and running for proper message bus communication with NSX Manager.</p><p>The following command and screenshot shows a stateful firewall status on the ESXi host:</p><pre class="programlisting">
<span class="strong"><strong>etc/init.d/vShield-Stateful-Firewall status</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_08_010.jpg" alt="Vsfwd"/></div><p>
</p><p>To check the active message bus session with NSX Manager the following command and screenshot depicts an active session with NSX Manager (172.16.1.5):</p><pre class="programlisting">
<span class="strong"><strong>esxcli network ip connection list | grep 5671</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_08_011.jpg" alt="Vsfwd"/></div><p>
</p><p>A potential failure can happen if port <code class="literal">
<span class="strong"><strong>5671</strong></span>
</code> is not opened between the ESXi host and NSX Manager.</p><div class="section" title="Vsfwd log location and collection process"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec26"/>Vsfwd log location and collection process</h3></div></div></div><p>NSX Distributed Firewall is a new generation firewall in vSphere environment used primarily because of its ability to filter traffic at the virtual machine NIC level. Hence, it is important to understand the log collection and a few troubleshooting steps related to this feature. So, let's get started.</p><p>Firstly, we need to start with the prerequisites to run <span class="strong"><strong>Distributed Firewall</strong></span> (<span class="strong"><strong>DFW</strong></span>). There is no need for log collection, even if the following requirements are not met. The prerequisites for running DFW are as follows:
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VMware vCenter Server should be version 5.5 minimum</li><li class="listitem" style="list-style-type: disc">VMware ESXi version should be at 5.1, 5.5, 6.0</li><li class="listitem" style="list-style-type: disc">VMware NSX for vSphere 6.0 and later</li></ul></div><p>All logs related to vsfwd will be at the following location, and their representation is shown in the figure:
</p><pre class="programlisting">
<span class="strong"><strong>/var/log/vsfwd.log file on the ESXi host</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_08_012.jpg" alt="Vsfwd log location and collection process"/></div><p>
</p></div></div></div>
<div class="section" title="Collecting centralized logs from NSX Manager"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec66"/>Collecting centralized logs from NSX Manager</h1></div></div></div><p>Firstly, we need to log in to NSX Manager using the admin credentials and execute the following command:</p><pre class="programlisting">
<span class="strong"><strong>export host-tech-support host-id scp uid@ip:/path</strong></span>
</pre><p>With the introduction of NSX 6.2.3, VMware has come up with an export host-tech-support command, which can be executed on the NSX Manager to collect the following information. I strongly believe they will be adding more and more log collection options, since this is a centralized way of collecting the logs, but a lot depends upon the type of failure. If we encounter an NSX Manager failure scenario, centralized logging functionality is also impacted, hence it is important to understand the next plan for such scenarios, which is the whole purpose of me explaining the log collection process so far. Following is the list of logs that are included in the centralized logs as of now:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">vmkernel and vsfwd log files</li><li class="listitem" style="list-style-type: disc">A list of filters</li><li class="listitem" style="list-style-type: disc">A list of dfw rules</li><li class="listitem" style="list-style-type: disc">A list of containers</li><li class="listitem" style="list-style-type: disc">Spoofguard details</li><li class="listitem" style="list-style-type: disc">Host-related information</li><li class="listitem" style="list-style-type: disc">ipdiscovery-related information</li><li class="listitem" style="list-style-type: disc">RMQ command outputs</li><li class="listitem" style="list-style-type: disc">Security group and services profile and instance details</li><li class="listitem" style="list-style-type: disc">esxcli-related outputs</li></ul></div></div>
<div class="section" title="VXLAN troubleshooting"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec67"/>VXLAN troubleshooting</h1></div></div></div><p>VXLAN is the overlay technology that is used in the VMware NSX environment when it comes to first-time testing and implementation, and most likely, we will end up with a few connectivity issues. Some of the common issues that we might face because of misconfiguration in both virtual and physical networks are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Virtual machines have no network connectivity, either between other machines in the same VXLAN network, or no egress connection with the physical world</li><li class="listitem" style="list-style-type: disc">Frequent packet drops getting reported and the applications team facing poor performance issues</li><li class="listitem" style="list-style-type: disc">Virtual Machines have proper network connectivity on some of the ESXi hosts, but when placed on another set of hosts, there is no connectivity</li><li class="listitem" style="list-style-type: disc">Normal ping tests are fine, but when checking with VXLAN packets, packet drops occur</li><li class="listitem" style="list-style-type: disc">Virtual machines in VLAN networks have proper network connectivity; however, VXLAN networks are not working</li></ul></div><p>The majority of network-related issues related to VXLAN will be around the preceding list of issues; however, only by applying our knowledge will we have a clear picture of what type of networks customers have and what type of network issues they are facing. So, let's get started with learning what the possible symptoms for such issues are and what the necessary actions to resolve the issue are.</p><p>After implementing a VXLAN solution, I would strongly recommend to checking GUI-level PING and VXLAN tests between all the NSX-prepared ESXi hosts, which is the best way to confirm if initial requirements are met for sending a VXLAN packet from one hypervisor to another hypervisor.</p><p>We need to select one of the logical switches. Go to <span class="strong"><strong>Monitor</strong></span> page, and there we will have two options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ping</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Broadcast</strong></span></li></ul></div><p>The following screenshot shows the <span class="strong"><strong>Ping</strong></span> and <span class="strong"><strong>Broadcast</strong></span> test options:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_013.jpg" alt="VXLAN troubleshooting"/></div><p>
</p>First, we will do a <span class="strong"><strong>Ping</strong></span> test between the <code class="literal">172.16.1.94</code> and <code class="literal">172.16.1.96</code> ESXi hosts, and we will follow that with a VXLAN test. The following screenshot shows a successful Ping test:
<p>
</p><div class="mediaobject"><img src="graphics/image_08_014.jpg" alt="VXLAN troubleshooting"/></div><p>
</p><p>The following screenshot shows a VXLAN test between the same ESXi host, and we can confirm that VXLAN packets sent from host <code class="literal">172.16.1.94</code> are successfully accepted by <code class="literal">172.16.1.96</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_015.jpg" alt="VXLAN troubleshooting"/></div><p>
</p><p>In addition to the previous test, we can also perform a VXLAN ping test through an SSH session, and I have captured the output of this test between two ESXi hosts. The command to perform the test is as follows:</p><pre class="programlisting">
<span class="strong"><strong>ping ++netstack=vxlan -d -s MTU VTEP-IP</strong></span>
</pre><p> The following screenshot depicts a VXLAN test performed through an SSH session to host 172.16.1.94:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_016.jpg" alt="VXLAN troubleshooting"/></div><p>
</p><p>The preceding output is a clear indication that MTU is set properly in the VXLAN environment, so we are able to MTU more than 1500. If there is an MTU misconfiguration issue, this test would fail, as shown in the following screenshot. I intentionally changed MTU in the distributed virtual switch from 1600 to 1500 to showcase the failure scenario:
</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_017.jpg" alt="VXLAN troubleshooting"/></div><p>
</p><p>Network troubleshooting isn't complete without a packet capture. This is our final topic for this chapter, and I will showcase how to collect VXLAN packets. We will also take a quick walk-through to see what information is in the packet. Considering the knowledge we have gained so far, it should be a cakewalk for everyone.</p></div>
<div class="section" title="Packet capturing and analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec68"/>Packet capturing and analysis</h1></div></div></div><p>Starting from ESXi 5.5, the pktcap-uw tool is embedded inside the hypervisor. Some of you will be familiar with the tcpdump tool, which was already available in ESXi; pktcap is a replacement for the same. The prime reason for integrating the pktcap tool captures packets are every layer which is extremely essential in NSX world. So, we are no longer limited by capturing packets at the vmkernel layer. I have been a big fan of this tool starting from the vCloud networking and security days and I strongly believe most of us will like this tool. Before jumping into packet capturing, let's be clear about the following points:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Pktcap</code>, by default, collects only incoming packets, and it is unidirectional. So, if we want to capture both ingress and egress traffic, we need to add certain parameters. If not, the whole purpose of capturing the packet will be defeated.</li><li class="listitem" style="list-style-type: disc">Traffic direction is mentioned as <code class="literal">-dir 0</code> for ingress packets.</li><li class="listitem" style="list-style-type: disc">Traffic direction is mentioned as <code class="literal">-dir 1</code> for egress packets.</li><li class="listitem" style="list-style-type: disc">We can capture a packet at the vmkernel, vmnic, and switch port levels (DVS).</li><li class="listitem" style="list-style-type: disc">Detailed command syntax is available if we issue the following command in ESXi host:</li></ul></div><pre class="programlisting">
<span class="strong"><strong>Pktcap-uw -h </strong></span>
</pre><p>Enough theory; let's get started by capturing the packet to analyze the VXLAN field. Before that, let me explain my lab setup and virtual machine details so that we can verify if those outputs match the captured packet details.
</p><div class="section" title="Lab environment details"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec57"/>Lab environment details</h2></div></div></div><p>In this lab, I have two vSphere clusters, and we have a VXLAN network 5001, which is stretched across these two clusters. Both clusters have their own distributed switch.</p><p>The virtual machine IP in cluster A is <code class="literal">192.16.10.12</code>, with a VTEP IP of <code class="literal">172.16.1.32</code> running on ESXi <code class="literal">172.16.1.97</code>.</p><p>The virtual machine IP in cluster B is <code class="literal">192.16.10.14</code>, with a VTEP IP of <code class="literal">172.16.1.33</code> running on ESXi <code class="literal">172.16.1.94</code>.</p></div><div class="section" title="VNIC packet capturing for egress traffic"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec58"/>VNIC packet capturing for egress traffic</h2></div></div></div><p>To start the packet capturing, we need to take an SSH session to host <code class="literal">172.16.1.97</code> and identify on which VNIC the VM is running:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Issue the <code class="literal">ESXTOP</code> command and press the <span class="emphasis"><em>n</em></span> key to show the network parameters. The following screenshot shows the ESXTOP screen. We have identified that our source virtual machine 192.16.10.12 is running on <code class="literal">vmnic0</code>:<p>
</p><div class="mediaobject"><img src="graphics/image_08_018.jpg" alt="VNIC packet capturing for egress traffic"/></div><p>
</p></li><li class="listitem">Issue the following command for egress traffic capturing. Output is saved in the ESXi host <span class="strong"><strong>tmp</strong></span> directory:<pre class="programlisting">
<span class="strong"><strong>pktcap-uw --uplink vmnic0 --dir 1 --stage 1 -o /tmp/webAvxlan.pcap</strong></span>
</pre></li><li class="listitem">Initiate a <code class="literal">ping</code> request from the source virtual machine to the destination virtual machine, as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_08_019.jpg" alt="VNIC packet capturing for egress traffic"/></div><p>
</p></li><li class="listitem">Stop the packet capture after some time by pressing <span class="emphasis"><em>Ctrl </em></span>+ <span class="emphasis"><em>C</em></span> in an ESXi putty session and see the following output:<p>
</p><div class="mediaobject"><img src="graphics/image_08_020.jpg" alt="VNIC packet capturing for egress traffic"/></div><p>
</p></li><li class="listitem">The saved packet <code class="literal">webAvxlan.pcap</code> needs to be imported to the Wireshark tool. I know for sure that everyone knows how to copy or download a file from the ESXi host.</li><li class="listitem">Once we import the file to Wireshark, we will get the following output:<p>
</p><div class="mediaobject"><img src="graphics/image_08_021.jpg" alt="VNIC packet capturing for egress traffic"/></div><p>
</p></li><li class="listitem">Expand the <span class="strong"><strong>Virtual eXentsible Local Area Network</strong></span> option, which will display the complete VXLAN header field as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_08_022.jpg" alt="VNIC packet capturing for egress traffic"/></div><p>
</p></li></ol></div><p>As we can see from the highlighted field, we got the following output, which matches perfectly with the lab environment details that we mentioned earlier:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The inner IP <code class="literal">Src</code> is <code class="literal">192.16.10.12</code> and <code class="literal">Dst</code> is <code class="literal">192.16.10.14</code> (two machines which we have tested the ping command)</li><li class="listitem" style="list-style-type: disc">The VXLAN network is <code class="literal">5001</code> (the logical switch where our virtual machines are running)</li><li class="listitem" style="list-style-type: disc">The VXLAN UDP port is <code class="literal">4789</code></li><li class="listitem" style="list-style-type: disc">The outer IP <code class="literal">SRC: 172.16.1.32</code> and <code class="literal">DST: 172.16.1.33</code> (VXLAN tunnel endpoint IP address)</li></ul></div><p>I strongly believe this is the most precise VXLAN output, which will help us in many scenarios, and pktcap-uw is a great tool, which is not used by many people, primarily because of lack of knowledge. With the captured output imported to the <span class="strong"><strong>Wireshark </strong></span>tool, it gives us granular-level information on all the fields. So, keep these steps handy and I bet this will be useful in a production environment.</p></div></div>
<div class="section" title="NSX upgrade checklist and planning order"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec69"/>NSX upgrade checklist and planning order</h1></div></div></div><p>It is always better to stop something from happening in the first instance than spend time repairing the damage after it has happened. Every product upgrade should follow a step-by-step process with proper planning, and an NSX upgrade is no exception. The following steps are the proper order that needs to be followed while planning to upgrade the NSX environment:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Upgrade NSX Manager</li><li class="listitem">Upgrade NSX Controllers</li><li class="listitem">Upgrade ESXi cluster prepared by NSX</li><li class="listitem">Upgrade Distributed Logical Router and Edge service gateway</li><li class="listitem">Upgrade data security and guest introspection services</li></ol></div><p>First and foremost, we need to ensure the following pre-checks are met before doing any upgrade.</p><p>The following is an NSX pre-upgrade checklist:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Take backup for NSX Manager. In a cross-VC NSX environment, we need to take backup from all the NSX Managers and complete the following process for all NSX Managers.</li><li class="listitem" style="list-style-type: disc">Take a snapshot of all NSX Managers. This is only for additional protection in case normal backups are not available or corrupted due to human error or catastrophic events.</li><li class="listitem" style="list-style-type: disc">Log in to NSX Manager and run show filesystems to show the <code class="literal">/dev/sda2</code> filesystem usage. If the filesystem usage is 100 percent, the upgrade process will certainly fail, and for such cases, we need to purge manager logs and NSX system commands and reboot the NSX Manager before starting with the upgrade.</li><li class="listitem" style="list-style-type: disc">Issue the following commands in NSX Manager to purge NSX logs and system commands:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">purge log manager</li><li class="listitem" style="list-style-type: disc">purge log system</li></ul></div><p>
</p></li><li class="listitem" style="list-style-type: disc">Reboot the NSX Manager appliance for the log cleanup to take effect.</li><li class="listitem" style="list-style-type: disc">NSX data security should be uninstalled before upgrading NSX Manager.</li><li class="listitem" style="list-style-type: disc">Ensure all controllers are connected and don't plan to deploy any new controllers during the existing controller upgrade phase.</li><li class="listitem" style="list-style-type: disc">Place the vSphere cluster ESXi host in maintenance mode, perform the upgrade, and reboot the host. Continue the same operation for the NSX-prepared ESXi host, so that new VIBS are pushed to all ESXi hosts, and finally, take the host out of maintenance mode.</li><li class="listitem" style="list-style-type: disc">Starting from NSX Manager 6.2.3 onwards the default VXLAN port is <code class="literal">4789</code>. Before NSX 6.2.3, the default VXLAN UDP port number was <code class="literal">8472</code>. So, if we are planning to continue with the new VXLAN port <code class="literal">4789</code>, please ensure that this port is allowed in your firewall.</li><li class="listitem" style="list-style-type: disc">NSX Edge and NSX Distributed Logical Router control VM can be upgraded in any order, and there is no repeated upgrade process for HA-enabled NSX Edge and control VM. Both appliances get upgraded at the same time.</li><li class="listitem" style="list-style-type: disc">Finally, we can go ahead and upgrade the guest introspection virtual machine and respective partner appliance.</li><li class="listitem" style="list-style-type: disc">Once the upgrade is complete, we should delete all snapshots taken for NSX Manager, after verifying all components and services are up and running.</li></ul></div><p>The following screenshot depicts all operationally-impacted tasks, and non-impacted tasks and services during respective NSX component upgrade phases:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>NSX Components</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong> Operational impact</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong> Not impacted</strong></span>
</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>NSX Manager</strong></span>
</p>
</td><td>
<p>NSX Manager GUI and API-related new tasks are blocked</p>
</td><td>
<p>Control plane and data plane continue to work</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>NSX Controllers</strong></span>
</p>
</td><td>
<p>No modification accepted for logical networks and no new logical network creation will be accepted</p>
</td><td>
<p>Management plane and data plane</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>vSphere Cluster</strong></span>
</p>
</td><td>
<p>No new VM provisioning will be accepted during this phase for that specific vSphere cluster.</p>
</td><td>
<p>Management plane, control plane, and data plane related tasks for other vSphere clusters will be working (if we are doing upgrades on one vSphere cluster at a time)</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>NSX Edge and Control VM</strong></span>
</p>
</td><td>
<p>All Edge and control VM services will be impacted during this operation.</p>
</td><td>
<p>Management plane, new Edges and control VM can be deployed. All existing configuration on old edges will be retained.</p>
</td></tr><tr><td>
<p>
<span class="strong"><strong>Guest introspection and data security</strong></span>
</p>
</td><td>
<p>Virtual machines will be unprotected during this phase</p>
</td><td>
<p>All other NSX related tasks and services will be intact.</p>
</td></tr></tbody></table></div><p>That concludes our final chapter, and I believe this network virtualization journey has been fantastic so far.</p></div>
<div class="section" title="The future of NSX"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec70"/>The future of NSX</h1></div></div></div><p>The current and future choice of IT will certainly be Software Defined Data Center, no doubt about it. One of the primary reasons I believe this product and technology is the cherry on top of the VMware product portfolio is the proven and larger ecosystem that VMware has, and the fact that customers can leverage NSX in private, public (vCloud Air), and with the mix of both, a truly hybrid cloud platform. There are millions of workloads, which are protected by vCloud Air disaster recovery data center, and behind the scenes, these are vSphere environments that are fully network-virtualized with NSX. It's no mistake that Gartner has recently recognized the vCloud Air disaster recovery solution as one of the best in the public Cloud market. The VMware NSX approach is very simple: <span class="emphasis"><em>Follow the virtual machines wherever they go: private, public, or hybrid will always remain secured</em></span>. From a cross-vendor perspective, there are definitely some hard decisions taken by VMware, especially with products such as NEXUS 1000V, which is no longer supported with VMware NSX. However, in a greenfield deployment scenario, there is only one question that might be raised while looking for an NSX solution: what is the cost and manpower involved in developing or remodeling existing applications if NSX is getting integrated? Does it demand an overall change in networking? Does it demand any specific models of switches or routers? We all have the answers now: NSX doesn't demand any significant change in overall physical networking. Already a recognized leader in software defined networking, VMware has made another wise move by acquiring Arkin on June 13, 2016, and that will further simplify a lot of NSX operational tasks. Arkin cross-domain visibility will help customers to get a granular visibility on physical to overlay mapping and other security parameters, and with that approach and output, day-to-day operational tasks will be much more simplified. How many times have we received questions about how a virtual machine is connected to the network? The traditional way of checking that would be by taking multiple connections to multiple products to get an end-to-end connectivity view. I strongly believe tasks like this will be simplified with the integration of Arkin with NSX, and adding to that, it will give us precise information on what types of traffic we have in the data center, as well as overall traffic percentages for East-West and North-South, Internet traffic, as a few examples.</p><p>VMware vRealize network insight (Arkin) is an intelligent security and operations management solution for the network, which provides 360-degree visibility across virtual and physical networks using network flow analytics. It is available for download from 1<sup>st</sup> August 2016. There was also a recent announcement from VMware regarding a new release of NSX multi hypervisor called NSX transformers, which supports hypervisors such as KVM and vSphere, and one could simply club them under a common NSX transport zone. It's too early to comment on how transformers will evolve, so for the time being, it will be a watch and wait game for all of us. NSX is certainly an evolution in software-defined networking, and it has got all the bits and fragments needed to reach further heights, which will enormously help all types of businesses. I appreciate you all being part of this journey and I would encourage everyone to start testing and implementing this great solution. Let's be part of this game-changing software. Based on your technical background and the pace at which we can understand and learn a technology, I believe there will be a few questions, and I would appreciate if you all can reach out to me via LinkedIn. Rest assured I will ensure queries are addressed at the earliest opportunity.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec71"/>Summary</h1></div></div></div><p>We started this chapter with Introduction to troubleshooting followed by NSX manager,Controller and Data plane log collection and major focus points when things go wrong. Finally we ended this chapter with Future of NSX followed by few links for documentation reading.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec72"/>References</h1></div></div></div><p>Lastly, as promised earlier, I'm posting all the articles that we all should read. You can trust me that the right way to climb the NSX ladder is by reading each and every document available; it will only multiply our knowledge:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The CISCO NEXUS 9000 design guide, which talks about a few design scenarios with UCS servers. A great guide for physical network design understanding with NSX: <a class="ulink" href="http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf">http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/whitepaper/products/nsx/design-guide-for-nsx-with-cisco-nexus-9000-and-ucs-white-paper.pdf</a></li><li class="listitem" style="list-style-type: disc">Advanced networking services offered through vCloud Air. It is worth reading this document to know how NSX is helping vCloud Air customers and what services are offered by providing zero trust security in the public Cloud: <a class="ulink" href="http://vcloud.vmware.com/service-offering/advanced-networking-services">http://vcloud.vmware.com/service-offering/advanced-networking-services</a></li><li class="listitem" style="list-style-type: disc">The VMware NSX design guide for vSphere is another knowledge hub for designing NSX in a vSphere environment: <a class="ulink" href="http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf">http://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/products/nsx/vmw-nsx-network-virtualization-design-guide.pdf</a></li><li class="listitem" style="list-style-type: disc">We should all go through this blog if we really want to keep ourselves updated with the technology. This is a network virtualization blog from VMware, and there is good amount of videos and use cases discussed: <a class="ulink" href="http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI">http://blogs.vmware.com/networkvirtualization/#.V5NU0Pl95QI</a></li><li class="listitem" style="list-style-type: disc">VMware integrated OpenStack with NSX configuration guide. This document demands a little bit of VIO knowledge; however, readers will find out how NSX works in the VIO world: <a class="ulink" href="https://communities.vmware.com/docs/DOC-30985">https://communities.vmware.com/docs/DOC-30985</a></li><li class="listitem" style="list-style-type: disc">The complete guide to upgrading VCNS to NSX. This guide contains step-by-step instructions to upgrade all vCloud network security solutions to NSX: <a class="ulink" href="https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf">https://pubs.vmware.com/NSX-62/topic/com.vmware.ICbase/PDF/nsx_62_upgrade.pdf</a></li><li class="listitem" style="list-style-type: disc">I'm going to shout practice, practice, practice! Well, practice makes perfect, so keep practicing the free VMware HOL labs. I would highly recommend starting with vSphere distributed switch from A to Z labs before starting NSX labs: <a class="ulink" href="http://labs.hol.vmware.com/HOL/catalogs/catalog/130">http://labs.hol.vmware.com/HOL/catalogs/catalog/130</a></li><li class="listitem" style="list-style-type: disc">The document is a deployment guide for implementing VMware NSX with Brocade VCS: <a class="ulink" href="http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html">http://www.brocade.com/content/html/en/deployment-guide/brocade-vcs-gateway-vmware-dp/GUID-329954A2-A957-4864-A0E0-FD29262D3352.html</a></li></ul></div></div></body></html>