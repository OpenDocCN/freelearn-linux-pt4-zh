<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Mesos Big Data Frameworks"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Mesos Big Data Frameworks</h1></div></div></div><p>This chapter is a <a class="indexterm" id="id787"/>guide to deploy important big data processing frameworks, such as Hadoop, Spark, Storm, and Samza, on top of Mesos.</p><div class="section" title="Hadoop on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec71"/>Hadoop on Mesos</h1></div></div></div><p>This <a class="indexterm" id="id788"/>section will introduce Hadoop, explain <a class="indexterm" id="id789"/>how to set up the Hadoop stack on Mesos, and discuss the problems commonly encountered while setting up the stack.</p><div class="section" title="Introduction to Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec127"/>Introduction to Hadoop</h2></div></div></div><p>Hadoop<a class="indexterm" id="id790"/> was developed by Mike Cafarella and Doug Cutting in 2006 to manage the distribution for the Nutch project. The project was named after Doug's son's toy elephant.</p><p>The<a class="indexterm" id="id791"/> following<a class="indexterm" id="id792"/> modules make up the Apache Hadoop framework:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hadoop Common</strong></span>: This <a class="indexterm" id="id793"/>has the common libraries and utilities required by other modules</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>): This<a class="indexterm" id="id794"/> is a distributed, scalable filesystem capable of storing petabytes of data on commodity hardware</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hadoop YARN</strong></span>: This<a class="indexterm" id="id795"/> is a resource manager to manage cluster resources (similar to Mesos)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hadoop MapReduce</strong></span>: This<a class="indexterm" id="id796"/> is a processing model for parallel data processing at scale</li></ul></div><div class="section" title="MapReduce"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec75"/>MapReduce</h3></div></div></div><p>MapReduce is<a class="indexterm" id="id797"/> a processing model using which large amounts <a class="indexterm" id="id798"/>of data can be processed in parallel on a distributed, commodity hardware-based infrastructure reliably and in a fault-tolerant way.</p><p>The word MapReduce is a combination of the Map and Reduce tasks, which are described here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The Map Task</strong></span>: In<a class="indexterm" id="id799"/> this step, an operation is performed on all the elements of the input dataset to transform it as needed (for example applying a filter condition)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The Reduce Task</strong></span>: The <a class="indexterm" id="id800"/>next step uses the output generated by the map task as its input and applies an aggregate operation on it to generate the final output (for example, summing all values)</li></ul></div><p>The scheduling, execution, and monitoring of the tasks is reliably handled by the framework without the application programmer having to worry about it.</p></div><div class="section" title="Hadoop Distributed File System"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec76"/>Hadoop Distributed File System</h3></div></div></div><p>Based <a class="indexterm" id="id801"/>on <a class="indexterm" id="id802"/>
<span class="strong"><strong>Google Filesystem</strong></span> or <span class="strong"><strong>GFS</strong></span>, Hadoop Distributed File System (HDFS) provides a scalable, distributed filesystem to store large amounts of data in a reliable, fault-tolerant way.</p><p>HDFS is based <a class="indexterm" id="id803"/>on a master/slave architecture, with the master consisting of a solitary <span class="strong"><strong>NameNode</strong></span>, which handles the metadata of the filesystem, and single or multiple slave nodes, which store the data and are also called <a class="indexterm" id="id804"/>
<span class="strong"><strong>DataNode</strong></span>.</p><p>Each file in HDFS is divided into multiple blocks, with each of these blocks being stored in DataNode. NameNode is responsible for maintaining information regarding which block is present in which DataNode. Operations such as read/write are handled by DataNode along with block management tasks such as the creation, removal, and replication of instructions from NameNode.</p><p>Interaction is through a shell, where a set of commands can be used to communicate with the filesystem.</p><div class="mediaobject"><img alt="Hadoop Distributed File System" src="graphics/B05186_08_01.jpg"/></div></div></div><div class="section" title="Setting up Hadoop on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec128"/>Setting up Hadoop on Mesos</h2></div></div></div><p>This <a class="indexterm" id="id805"/>section will explain how to set up Hadoop on Mesos. An existing Hadoop distribution can also be set up on top of Mesos. To run Hadoop on Mesos, our Hadoop distribution must contain <code class="literal">Hadoop-Mesos-0.1.0.jar</code> (the version at the time of writing this book). This is required for any Hadoop distribution that uses a protobuf version higher than 2.5.0. We will also set a few configuration properties in order to complete the setup, as will be explained subsequently. Note that at the time of writing this chapter, YARN and MRv2 are not supported.</p><p>Let's follow the steps mentioned here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open up the Terminal on your cluster and fire up the following commands to set up Hadoop on Mesos:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Install snappy-java package if not alr</strong></span>
<span class="strong"><strong>eady installed.</strong></span>
<span class="strong"><strong>$ sudo apt-get install libsnappy-dev</strong></span>

<span class="strong"><strong># Clone the repository</strong></span>
<span class="strong"><strong>$ git clone https://github.com/Mesos/Hadoop</strong></span>

<span class="strong"><strong>$ cd Hadoop</strong></span>

<span class="strong"><strong># Build the Hadoop-Mesos-0.1.0.jar</strong></span>
<span class="strong"><strong>$ mvn package</strong></span>
</pre></div></li><li class="listitem">Once<a class="indexterm" id="id806"/> the previous command is executed, it will create the <code class="literal">target/Hadoop-Mesos-0.1.0.jar</code> jar.<p>One thing to note here is that if you have an older version of Mesos and need to build the jar against this version, then you will have to edit the <code class="literal">pom.xml</code> file with the appropriate version. We can change the following versions in the <code class="literal">pom.xml</code> file:</p><div class="informalexample"><pre class="programlisting">  &lt;!-- runtime deps versions --&gt;
  &lt;commons-logging.version&gt;1.1.3&lt;/commons-logging.version&gt;
  &lt;commons-httpclient.version&gt;3.1&lt;/commons-httpclient.version&gt;
&lt;Hadoop-client.version&gt;2.5.0-mr1-cdh5.2.0&lt;/Hadoop-client.version&gt;
  &lt;Mesos.version&gt;0.23.1&lt;/Mesos.version&gt;
  &lt;protobuf.version&gt;2.5.0&lt;/protobuf.version&gt;
  &lt;metrics.version&gt;3.1.0&lt;/metrics.version&gt;
  &lt;snappy-java.version&gt;1.0.5&lt;/snappy-java.version&gt;</pre></div></li><li class="listitem">Now, we can download a Hadoop distribution. As you can see here, we have compiled <code class="literal">Hadoop-Mesos jar</code> with the <code class="literal">hadoop-2.5.0-mr1-cdh5.2.0</code> version. It can be downloaded with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget </strong></span>
<span class="strong"><strong>http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.5.0-cdh5.2.0.tar.gz</strong></span>
<span class="strong"><strong># Extract the contents</strong></span>
<span class="strong"><strong>$ tar zxf hadoop-2.5.0-cdh5.2.0.tar.gz</strong></span>
</pre></div></li><li class="listitem">Now, we need to copy <code class="literal">Hadoop-Mesos-0.1.0.jar</code> into the Hadoop <code class="literal">share/Hadoop/common/lib</code> directory. This is done as shown here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cp hadoop-Mesos-0.1.0.jar hadoop-2.5.0-cdh5.2.0/share/hadoop/common/lib/</strong></span>
</pre></div></li><li class="listitem">We now need to update the symlinks of the CHD5 distribution to point to the correct version (as it includes both MRv1 and MRv2 (YARN)) using the following set of commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd hadoop-2.5.0-cdh5.2.0</strong></span>
<span class="strong"><strong>$ mv bin bin-mapreduce2</strong></span>
<span class="strong"><strong>$ mv examples examples-mapreduce2 </strong></span>
<span class="strong"><strong>$ ln -s bin-mapreduce1 bin</strong></span>
<span class="strong"><strong>$ ln -s examples-mapreduce1 examples</strong></span>

<span class="strong"><strong>$ pushd etc</strong></span>
<span class="strong"><strong>$ mv hadoop hadoop-mapreduce2</strong></span>
<span class="strong"><strong>$ ln -s hadoop-mapreduce1 Hadoop</strong></span>
<span class="strong"><strong>$ popd</strong></span>
<span class="strong"><strong>$ pushd share/hadoop</strong></span>
<span class="strong"><strong>$ rm mapreduce</strong></span>
<span class="strong"><strong>$ ln -s mapreduce1 mapreduce</strong></span>
<span class="strong"><strong>$ popd </strong></span>
</pre></div></li><li class="listitem">All the <a class="indexterm" id="id807"/>configurations are now ready. We can archive and upload the Hadoop distribution to our existing Hadoop Distributed File System (HDFS) system, where it can be accessed by Mesos. Take a look at the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tar czf hadoop-2.5.0-cdh5.2.0.tar.gz hadoop-2.5.0-cdh5.2.0</strong></span>
<span class="strong"><strong>$ hadoop fs -put hadoop-2.5.0-cdh5.2.0.tar.gz /hadoop-2.5.0-cdh5.2.0.tar.gz</strong></span>
</pre></div></li><li class="listitem">Once done, we can configure <code class="literal">JobTracker</code> to launch each <code class="literal">TaskTracker</code> node on Mesos by editing the <code class="literal">mapred-site.xml</code> file, as follows:<div class="informalexample"><pre class="programlisting">&lt;property&gt;
  &lt;name&gt;mapred.job.tracker&lt;/name&gt;
  &lt;value&gt;localhost:9001&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapred.jobtracker.taskScheduler&lt;/name&gt;
  &lt;value&gt;org.apache.Hadoop.mapred.MesosScheduler&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapred.Mesos.taskScheduler&lt;/name&gt;
  &lt;value&gt;org.apache.Hadoop.mapred.JobQueueTaskScheduler&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapred.Mesos.master&lt;/name&gt;
  &lt;value&gt;localhost:5050&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapred.Mesos.executor.uri&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:9000/Hadoop-2.5.0-cdh5.2.0.tar.gz&lt;/value&gt;
&lt;/property&gt;</pre></div></li><li class="listitem">A few <a class="indexterm" id="id808"/>properties in the <code class="literal">mapred-site.xml</code> file are Mesos-specific, such as <code class="literal">mapred.Mesos.master</code> or <code class="literal">mapred.Mesos.executor.uri</code>.</li><li class="listitem">We can now start the <code class="literal">JobTracker</code> service by including the Mesos native library using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ MESOS_NATIVE_LIBRARY=/path/to/libMesos.so Hadoop jobtracker</strong></span>
</pre></div></li></ol></div></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec129"/>An advanced configuration guide</h2></div></div></div><p>More details <a class="indexterm" id="id809"/>regarding the configuration settings in<a class="indexterm" id="id810"/> Mesos can be found at <a class="ulink" href="https://github.com/mesos/hadoop/blob/master/configuration.md">https://github.com/mesos/hadoop/blob/master/configuration.md</a>.</p></div><div class="section" title="Common problems and solutions"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec130"/>Common problems and solutions</h2></div></div></div><p>The<a class="indexterm" id="id811"/> two most common problems encountered while setting up Hadoop on Mesos are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Inability to set the Mesos library in the environment</li><li class="listitem" style="list-style-type: disc">Build failures</li></ul></div><p>A solution for both these problems is described here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Missing the Mesos library in the environment: </strong></span>We will get an exception stack when we forget to set the Mesos library in the environment at the following URL: <a class="ulink" href="https://github.com/mesos/hadoop/issues/25">https://github.com/mesos/hadoop/issues/25</a>.<p>This can be resolved by setting the following environment variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export MESOS_NATIVE_LIBRARY=/usr/local/lib/libMesos.so</strong></span>
<span class="strong"><strong>$ export MESOS_NATIVE_JAVA_LIBRARY=/usr/local/lib/libMesos.so</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The Maven build failure:</strong></span> We won't be able to build the package on some occasions due to build failures. One example of a build failure can be found here: <a class="ulink" href="https://github.com/mesos/hadoop/issues/64">https://github.com/mesos/hadoop/issues/64</a>.<p>This can be avoided by removing the older Maven dependencies from the environment and rebuilding it.</p><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv ~/.m2 ~/.mv_back</strong></span>
<span class="strong"><strong>$ mvn package</strong></span>
</pre></div></li></ul></div></div></div></div>
<div class="section" title="Spark on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec72"/>Spark on Mesos</h1></div></div></div><p>Apache <span class="strong"><strong>Spark</strong></span> is a <a class="indexterm" id="id812"/>powerful open source processing engine built around speed, ease of use, and sophisticated analytics. It is currently one of the fastest growing big data technologies and is used by several leading companies in production.</p><p>Interestingly, Apache Spark <a class="indexterm" id="id813"/>was first started as a research project in 2009 at AmpLab, UC Berkeley, to prove that a distributed processing framework leveraging memory resources can run atop Apache Mesos. It was open sourced in 2010, entered the Apache incubator in 2013, and became an Apache top-level project in 2014. In its short existence, Apache Spark has managed to capture the attention of the developer community and is slowly finding its way into the lexicon of business decision makers as well. This, along with the fact that it is now in production in over 5000 organizations, speaks volumes about its versatility and utility.</p><div class="section" title="Why Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec131"/>Why Spark</h2></div></div></div><p>With earlier <a class="indexterm" id="id814"/>distributed parallel computation frameworks such as <span class="strong"><strong>Map Reduce</strong></span>, each <a class="indexterm" id="id815"/>computation step had to be read from and written to disk. For instance, consider the standard word count example of counting the number of occurrences of each word that appears in a set of text files. The first step here would be a map task that reads the text files from disk (breaking it up into smaller chunks if necessary), takes one line from it, splits it into individual words, and then outputs a key value pair of <code class="literal">&lt;&lt;word&gt;</code>,<code class="literal">1&gt;</code> (note that an intermediate combiner step can add the occurrences for each word from each mapper while it is still in memory for more efficiency). A number of mappers are spawned across the entire cluster to efficiently perform the preceding task in parallel over all the lines from each text file. The final output of all these map tasks is written to disk. In the next step, the reduce task needs to collect the same words in a single machine in order to add them all up and produce the final count. For this, there is a shuffle operation that reads the intermediate output generated from the map step and ensures that all the output for one word is sent to one and only one reducer. After the reduce step, the final output is collected and written to disk.</p><p>For iterative workloads, which involve multiple repetitions of the various preceding steps, this would lead to a lot of disk I/O. The mappers would read the first set of inputs and write out the intermediate output to disk. Then the reducers would read the intermediate output from disk and write out their outputs to disk, which would then be read by the mappers of stage 2, and so on. Disk reads are very slow, and for long, iterative computations, they would often be the bottleneck instead of the CPUs. This was one of the basic problems that Spark intended to resolve. Many iterative or latency-sensitive applications (interactive querying or stream processing, for example) weren't being adequately solved by batch processing frameworks such as Map Reduce due to fundamental design constraints. Spark set out to solve this problem by coming up with a novel architecture.</p><p>To minimize <a class="indexterm" id="id816"/>disk I/O, the creators of Spark decided to look at memory as a potential alternative. Trends demonstrated that memory cost was falling exponentially with each passing year. Affordable memory meant that more memory could be packed into commodity servers without bloating up the costs. In order to effectively handle the emerging class of business applications, such as iterative machine learning, interactive data mining, or stream processing, a memory-based framework also had to develop elegant solutions to the common problems of fault tolerance and partition control. For example, fault tolerance (or more accurately, high availability) can be achieved by replicating data across different machines or by updating the state at regular intervals in a database. However, this is a very time-consuming approach that utilizes a lot of network bandwidth and would have resulted in much slower job execution, something that the framework sought to solve in the first place. Partition control and having the flexibility to keep all the data required by a task as close to it as possible was also very important as without it, a high execution speed could not be achieved.</p><p>To tackle all these problems, a new higher-level abstraction called <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>) was <a class="indexterm" id="id817"/>developed. These new data structures allowed programmers to explicitly cache them in memory, place them in the desired partitions for optimal performance, and rebuild it based on a blueprint of how it was constructed if it is lost. Programmers need to simply write a driver program that encapsulates the logical workflow of their application and initiates the execution of the various comprising operations in parallel across the cluster.</p><p>The two main abstractions that Spark provides are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Resilient distributed datasets (or RDDs), as mentioned before</li><li class="listitem">The processing operations (map, filter, join, for each, collect, and so on) have to be applied on these datasets to generate the required output.</li></ol></div><p>RDDs also permit the application of the same operation/function to multiple data points in parallel. By logging the process used to build a dataset (lineage), it can effectively reconstruct the dataset at even partition-level granularity in case there is a failure by leveraging this stored blueprint.</p><div class="mediaobject"><img alt="Why Spark" src="graphics/B05186_08_02.jpg"/></div><p>Spark <a class="indexterm" id="id818"/>has been shown to be 10 times faster on disk and 100 times faster in-memory than MapReduce for certain kinds of applications. It has also brought about a drastic reduction in writing the logical workflow itself, with even complex application programs now no longer extending beyond a few 100 lines of code instead of 1,000 and 10,000 lines earlier.</p><div class="section" title="Logistic regression in Hadoop and Spark"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec77"/>Logistic regression in Hadoop and Spark</h3></div></div></div><p>Spark <a class="indexterm" id="id819"/>proves to be particularly useful in iterative machine learning, stream processing, and interactive querying use cases, in which the enhanced processing speeds and ease of programming brought by it is harnessed to drive more and more business or organizational value. Source: <a class="ulink" href="http://spark.apache.org">http://spark.apache.org</a>.</p><div class="mediaobject"><img alt="Logistic regression in Hadoop and Spark" src="graphics/B05186_08_03.jpg"/></div><p>Spark's generality <a class="indexterm" id="id820"/>not only makes it a great fit for the use cases mentioned earlier but also for the traditional batch applications. Spark's versatility also extends to the fact that it offers rich, expressive APIs in Python, Java, Scala, and SQL among others, along with other inbuilt libraries. It is also highly interoperable and can work off all standard data storage tools, such as HDFS and Cassandra.</p></div></div><div class="section" title="The Spark ecosystem"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec132"/>The Spark ecosystem</h2></div></div></div><p>The <a class="indexterm" id="id821"/>Spark ecosystem comprises multiple complementary components that are designed to work with each other seamlessly. The versatile and general structure of Spark allows for specialized libraries geared towards specific workloads to be built on top of it, such as <span class="strong"><strong>Spark SQL</strong></span> to <a class="indexterm" id="id822"/>query structured data through a SQL interface,<span class="strong"><strong> MLib</strong></span> <a class="indexterm" id="id823"/>for machine learning, <span class="strong"><strong>Spark Streaming</strong></span> <a class="indexterm" id="id824"/>to process data streams in motion, and <span class="strong"><strong>GraphX</strong></span> for <a class="indexterm" id="id825"/>graph computations. Underpinning each of these components is the Spark core engine that defines the basic structure of Spark including its core abstraction, the resilient distributed dataset (or RDD).</p><p>Spark's design principle of high interoperability and several tightly coupled components feeding off a common core has multiple advantages. All the higher-level libraries in the ecosystem are directly able to leverage any feature additions or improvements made to the base framework. The total cost of ownership reduces as only one software stack needs to be set up and maintained instead of having multiple disparate systems. It also acts as a unified data analysis stack for varied use cases, reducing the overall learning curve and deployment, testing, and running cycle. Applications that involve a combination of processing streaming data, applying machine learning algorithms, and querying the final output through a SQL interface can be easily built using all the different libraries of Spark. Moreover, the enhanced speed and lower infrastructure costs provided by Spark has unlocked <a class="indexterm" id="id826"/>newer use cases as well, ranging from processing streaming data in real time to developing applications involving complex machine learning algorithms. Source: <a class="ulink" href="http://spark.apache.org">http://spark.apache.org</a>.</p><div class="mediaobject"><img alt="The Spark ecosystem" src="graphics/B05186_08_04.jpg"/></div><p>The components <a class="indexterm" id="id827"/>of the Spark ecosystem are described in the following sections.</p><div class="section" title="Spark Core"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec78"/>Spark Core</h3></div></div></div><p>Spark Core<a class="indexterm" id="id828"/> is <a class="indexterm" id="id829"/>the fundamental component that comprises the general execution engine and covers the core Spark functionalities. It includes features such as memory management, failure recovery, connectivity, or interoperability with different storage systems, job scheduling, and rich, expressive APIs (including the one that contains the definition for RDDs) to construct application workflows.</p></div><div class="section" title="Spark SQL"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec79"/>Spark SQL</h3></div></div></div><p>Spark SQL is<a class="indexterm" id="id830"/> the <a class="indexterm" id="id831"/>component that allows analysts to use SQL to analyze and query structured datasets. It supports multiple data formats, including CSV, Parquet, JSON, and AVRO. As mentioned before, the integrated design of Spark also permits data engineers to intermingle complex analytics with Spark SQL queries, thus feeding the output of one component to others within a unified application through the common RDD abstraction.</p><p>Another<a class="indexterm" id="id832"/> important development in this area was the introduction of the DataFrame API (inspired by R and Python DataFrame), which was introduced in <a class="indexterm" id="id833"/>Spark 1.3. DataFrames are optimized, table-like, column-organized distributed datasets that allow a vast subsection of analysts and data scientists familiar with this concept to have the ability to leverage them using Spark. They can be generated from a lot of sources, such as structured files, Hive tables, or other RDDs, and can be operated upon using a provided DSL or domain-specific language.</p></div><div class="section" title="Spark Streaming"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec80"/>Spark Streaming</h3></div></div></div><p>Spark Streaming<a class="indexterm" id="id834"/> is a <a class="indexterm" id="id835"/>component that allows the processing of streaming data or data in motion, such as machine logs, server logs, social media feeds, and so on in real time. Its model for processing live streams of data is a logical extension of how the core API processes batch data. It operates on data in a minibatch mode; that is, it collects data for a window of time, applies the processing logic on this <span class="emphasis"><em>minibatch</em></span>, then collects the next minibatch, and so on. This makes it extremely easy to reuse code written for batch workflows and apply them to a streaming data scenario.</p></div><div class="section" title="MLlib"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec81"/>MLlib</h3></div></div></div><p>Spark comes <a class="indexterm" id="id836"/>with a scalable machine learning library containing <a class="indexterm" id="id837"/>a rich set of distributed algorithms for a wide range of use cases, such as classification, clustering, collaborative filtering, decomposition, and regression. It also includes a few deeper primitives, such <a class="indexterm" id="id838"/>as <span class="strong"><strong>gradient descent optimization</strong></span>.</p></div><div class="section" title="GraphX"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec82"/>GraphX</h3></div></div></div><p>The GraphX <a class="indexterm" id="id839"/>component is useful for operating on and processing <a class="indexterm" id="id840"/>graphs (for example, social network graphs) through a rich set of operators, such as mapVertices and subgraph, as well as standard algorithms for graph analytics (PageRank), community detection (triangle counting), and structured prediction (Gibbs sampling). The Spark API is extended (similarly to Spark SQL and Spark Streaming) to develop directed graphs in which customized properties can be assigned to each edge and vertex.</p><p>Spark's extendibility has also fostered the development of a multitude of third-party packages, connectors, and other libraries that further enhance its utility. Among some of the more popular ones are SparkR, launch scripts for different cloud infrastructure providers such as GCE developed by Sigmoid, connectors for Redshift and Elasticsearch, and a job execution server.</p></div></div><div class="section" title="Setting up Spark on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec133"/>Setting up Spark on Mesos</h2></div></div></div><p>This <a class="indexterm" id="id841"/>section explains how we can run Spark on top of Mesos in detail. Similar to the Hadoop setup, we will need the Spark binary package uploaded to a place accessible by Mesos and the Spark driver program configured to connect to Mesos.</p><p>Another alternative is to install Spark at the same location in all the Mesos slaves and set  <code class="literal">Spark.Mesos.executor.home</code> to point to this location.</p><p>The following steps can be performed to upload the Spark binary to a location accessible by Mesos.</p><p>Whenever Mesos runs a task on the Mesos slave for the first time, the slave must have a Spark binary package to run the Spark Mesos executor backend (which comes with Spark). A location that is accessible by Mesos can be HDFS, HTTP, S3, and so on. We can download the latest version of Spark Binary from the official website by navigating to the following website:</p><p>
<a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>.</p><p>For example, at the time of writing this book, Spark's latest version is 1.6.0, and we can download and upload it to HDFS with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ hadoop fs -put spark-1.6.0-bin-hadoop2.6.tgz /</strong></span>
</pre></div><p>In the driver program, we can now give the master URL as the Mesos master URL, which will be in the <code class="literal">Mesos://master-host:5050</code> form for a single master Mesos cluster. It could be similar to <code class="literal">Mesos://zk://host1:2181,host2:2181</code> for a multimaster Mesos cluster.</p><p>There are two modes of submitting a Spark job to Mesos, which are explained in the following sections.</p><div class="section" title="Submitting jobs in client mode"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec83"/>Submitting jobs in client mode</h3></div></div></div><p>The <a class="indexterm" id="id842"/>Spark Mesos framework is launched directly on the client machine, and it waits for the driver output in client mode. We need to set a few Mesos-specific configurations in the <code class="literal">Spark-env.sh</code> file to interact with Mesos, which are listed here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libMesos.so&gt;</strong></span>
<span class="strong"><strong>export SPARK_EXECUTOR_URI=&lt;URL of Spark-1.6.0.tar.gz uploaded above&gt;</strong></span>
</pre></div><p>Now, when starting a Spark application on the cluster, pass the <code class="literal">Mesos://</code> URL as the master while creating <code class="literal">SparkContext</code>. Here's an example:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf().setMaster("Mesos://HOST:5050").setAppName("My app").set("Spark.executor.uri", "&lt;path to Spark-1.6.0.tar.gz uploaded above&gt;")
val sc = new SparkContext(conf)</pre></div></div><div class="section" title="Submitting jobs in cluster mode"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec84"/>Submitting jobs in cluster mode</h3></div></div></div><p>In the <a class="indexterm" id="id843"/>cluster mode, the driver is launched in the cluster, and the client can find the results of the driver on the Mesos Web UI. We need to start <code class="literal">MesosClusterDispatcher</code> to use the cluster mode. The script to start <code class="literal">MesosClusterDispatcher</code> is located under the <code class="literal">sbin/start-Mesos-dispatcher.sh</code> script, which takes in the Mesos master URL. We can then submit the jobs to Mesos cluster by specifying the master URL to the URL of <code class="literal">MesosClusterDispatcher</code> (such as <code class="literal">Mesos://dispatcher:7077</code>). The driver status will be available on the Spark cluster Web UI.</p><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">./bin/Spark-submit \
  --class org.apache.Spark.examples.SparkPi \
  --master Mesos://207.184.161.138:7077 \
  --deploy-mode cluster
  --supervise
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000</pre></div><p>Note that the jars or Python files that are passed to Spark-submit should be URIs reachable by Mesos slaves, as the Spark driver doesn't automatically upload local jars.</p></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec85"/>An advanced configuration guide</h3></div></div></div><p>Spark<a class="indexterm" id="id844"/> currently supports two modes to run <a class="indexterm" id="id845"/>on Mesos: <span class="strong"><strong>coarse-grained</strong></span> <span class="strong"><strong>mode</strong></span> and <span class="strong"><strong>fine-grained</strong></span> <span class="strong"><strong>mode</strong></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Coarse-grained</strong></span> <span class="strong"><strong>mode</strong></span>: The coarse-grained mode is the default mode, and it will<a class="indexterm" id="id846"/> launch one long-running Spark task on each Mesos machine and dynamically schedule its own minitasks within it. This mode is usually used when we require a much lower startup overhead, but it comes with the cost of reserving the Mesos resources for the complete duration of the application. We can control the maximum number of resources that Spark acquires in the coarse-grained mode by setting the <code class="literal">Spark.cores.max</code> property in <code class="literal">SparkConf</code>. By default, it acquires all the resources available in the cluster.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fine-grained</strong></span> <span class="strong"><strong>mode</strong></span>: In the <a class="indexterm" id="id847"/>fine-grained mode, each Spark task runs as a separate Mesos task. This allows better sharing of the cluster resources among other frameworks at a very fine granularity, where each application gets additional or fewer machines as it ramps up and down, depending on the workload. The drawback is that it comes with an additional overhead in launching each task. This mode is not preferred for low-latency requirements, such as interactive queries or serving web requests. To run in the fine-grained mode, we can turn the coarse-grained mode off by setting the following property:<div class="informalexample"><pre class="programlisting">conf.set("Spark.Mesos.coarse", "false")</pre></div></li></ul></div><div class="section" title="Spark configuration properties"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec11"/>Spark configuration properties</h4></div></div></div><p>Mesos-specific Spark configuration <a class="indexterm" id="id848"/>properties can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/running-on-mesos.html#configuration">http://spark.apache.org/docs/latest/running-on-mesos.html#configuration</a>.</p></div></div></div></div>
<div class="section" title="Storm on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec73"/>Storm on Mesos</h1></div></div></div><p>
<span class="strong"><strong>Storm</strong></span> is <a class="indexterm" id="id849"/>a<a class="indexterm" id="id850"/> real-time <span class="strong"><strong>distributed data processing system</strong></span> for <a class="indexterm" id="id851"/>processing data coming in at high velocities. It can process millions of<a class="indexterm" id="id852"/> records per second and is particularly useful for applications where millisecond-level latency is essential (for example, security threat detection, fraud detection, operational monitoring, and so on).</p><div class="section" title="The Storm architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec134"/>The Storm architecture</h2></div></div></div><p>A typical <a class="indexterm" id="id853"/>Storm cluster has three types of nodes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Nimbus or master node</strong></span>: This<a class="indexterm" id="id854"/> is responsible for submitting and distributing the computations for execution apart from handling tasks such as launching slave nodes and monitoring the execution</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ZooKeeper node</strong></span>: This<a class="indexterm" id="id855"/> is responsible for coordinating the cluster</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Supervisor node</strong></span>: This<a class="indexterm" id="id856"/> is responsible for starting and stopping slave nodes based on the instructions sent by the Nimbus node<div class="mediaobject"><img alt="The Storm architecture" src="graphics/B05186_08_05.jpg"/></div></li></ul></div><p>Some important<a class="indexterm" id="id857"/> terms used in Storm are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Tuples</strong></span>: This<a class="indexterm" id="id858"/> is an ordered list of elements</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Streams</strong></span>: This <a class="indexterm" id="id859"/>is a sequence of tuples</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spouts</strong></span>: These<a class="indexterm" id="id860"/> are sources of streams in a computation (for example, the Twitter API)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bolts</strong></span>: These <a class="indexterm" id="id861"/>are process input streams and produce output streams</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Topologies</strong></span>: These <a class="indexterm" id="id862"/>are the overall calculation represented visually as a network of spouts and bolts, as follows:<div class="mediaobject"><img alt="The Storm architecture" src="graphics/B05186_08_06.jpg"/></div></li></ul></div></div><div class="section" title="Setting up Storm on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec135"/>Setting up Storm on Mesos</h2></div></div></div><p>This <a class="indexterm" id="id863"/>section explains how we can integrate Storm with the Mesos cluster resource manager. Let's follow the steps mentioned here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We can first explore the Storm on Mesos repository by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/Mesos/Storm</strong></span>
<span class="strong"><strong>$ cd Storm</strong></span>
</pre></div></li><li class="listitem">We can now edit the configuration file listed in <code class="literal">conf/Storm.yaml</code>:<div class="informalexample"><pre class="programlisting">  # Please change these for your cluster to reflect your cluster settings
  # -----------------------------------------------------------
  Mesos.master.url: "zk://localhost:2181/Mesos"
  Storm.zookeeper.servers:
  - "localhost"
  # -----------------------------------------------------------

  # Worker resources
  topology.Mesos.worker.cpu: 1.0
  
  # Worker heap with 25% overhead
  topology.Mesos.worker.mem.mb: 512
  worker.childopts: "-Xmx384m"

  # Supervisor resources
  topology.Mesos.executor.cpu: 0.1
  topology.Mesos.executor.mem.mb: 500 # Supervisor memory, with 20% overhead
  supervisor.childopts: "-Xmx400m"

# The default behavior is to launch the 'logviewer' unless 'autostart' is false. If you enable the logviewer, you'll need to add memory overhead to the executor for the logviewer.

  logviewer.port: 8000
  logviewer.childopts: "-Xmx128m"
  logviewer.cleanup.age.mins: 10080
  logviewer.appender.name: "A1"
  supervisor.autostart.logviewer: true

# Use the public Mesosphere Storm build. Please note that it won't work with other distributions. You may want to make this empty if you use `Mesos.container.docker.image` instead.
  # Mesos.executor.uri: "file:///usr/local/Storm/Storm-Mesos-0.9.6.tgz"

# Alternatively, use a Docker image instead of URI. If an image is specified, Docker will be used instead of Mesos containers.
  Mesos.container.docker.image: "Mesosphere/Storm"

  # Use Netty to avoid ZMQ dependencies
  Storm.messaging.transport: "backtype.Storm.messaging.netty.Context"
  Storm.local.dir: "Storm-local"

  # role must be one of the Mesos-master's roles defined in the --roles flag
  Mesos.framework.role: "*"
  Mesos.framework.checkpoint: true
  Mesos.framework.name: "Storm"

# For setting up the necessary Mesos authentication see Mesos authentication page and set the Mesos-master flags --credentials, --authenticate, --acls, and --roles.
  
  Mesos.framework.principal: "Storm"

  # The "secret" phrase cannot be followed by a NL
  
  Mesos.framework.secret.file: "Storm-local/secret"

  #Mesos.allowed.hosts:
    - host1
  #Mesos.disallowed.hosts:
    - host1</pre></div></li><li class="listitem">Edit<a class="indexterm" id="id864"/> the properties according to the cluster that we have and execute the following command to start the nimbus:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bin/Storm-Mesos nimbus</strong></span>
</pre></div></li><li class="listitem">We need to start the UI on the same machine as the nimbus with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bin/Storm ui</strong></span>
</pre></div></li></ol></div><p>Topologies are submitted to a Storm/Mesos cluster in the exact same way that they are submitted to <a class="indexterm" id="id865"/>a regular Storm cluster. Storm/Mesos provides resource isolation between topologies. So, you don't need to worry about topologies interfering with one another.</p><div class="section" title="Running a sample topology"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec86"/>Running a sample topology</h3></div></div></div><p>Once<a class="indexterm" id="id866"/> nimbus is running, we can launch one of the Storm-starter topologies with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./bin/Storm jar -c nimbus.host=10.0.0.1 -c nimbus.thrift.port=32001 examples/Storm-starter/Storm-starter-topologies-0.9.6.jar Storm.starter.WordCountTopology word-count</strong></span>
</pre></div><p>Here, we specified the nimbus host and thrift port as parameters.</p></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec87"/>An advanced configuration guide</h3></div></div></div><p>If we <a class="indexterm" id="id867"/>want to build the release against a different version of Mesos or Storm, then we can use the <code class="literal">build-release.sh</code> script to download the appropriate version by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>STORM_RELEASE=x.x.x MESOS_RELEASE=y.y.y bin/build-release.sh</strong></span>
</pre></div><p>Here <code class="literal">x.x.x</code> and <code class="literal">y.y.y</code> are the appropriate versions of Storm and Mesos that we will build against. This command will build a Mesos executor package.</p><p>The <code class="literal">bin/build-release.sh</code> script takes in the following subcommands:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Subcommand</p>
</th><th style="text-align: left" valign="bottom">
<p>Usage</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">main</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is used to build a Storm package with the Mesos scheduler. The output of this command can be used as the package for <code class="literal">mesos.executor.uri</code>.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clean</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This attempts to clean the working files and directories created when building.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">downloadStormRelease</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a utility function to download the Storm release tarball for the targeted Storm release. Set the <code class="literal">MIRROR</code> environment variable to configure the download mirror.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">mvnPackage</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This runs the Maven targets necessary to build the Storm Mesos framework.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">prePackage</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This prepares the working directories to be able to package the Storm Mesos framework and is an optional argument specifying the Storm release tarball to package against.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">package</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This packages the Storm Mesos framework.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dockerImage</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This builds a Docker image from the current code.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">help</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This prints out usage information about the <code class="literal">build-release.sh</code> script.</p>
</td></tr></tbody></table></div></div></div><div class="section" title="Deploying Storm through Marathon"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec136"/>Deploying Storm through Marathon</h2></div></div></div><p>We can run <a class="indexterm" id="id868"/>Storm on Mesos with Marathon easily by setting the <code class="literal">MESOS_MASTER_ZK</code> environment variable to point to our ZooKeeper node of the cluster. The repository also includes a script, <code class="literal">bin/run-with-marathon.sh</code>, which sets the required parameters and starts the UI and nimbus. As Storm writes stateful data to the disk, we need to make sure that <code class="literal">storm.local.dir config</code> is set. We can run this from Marathon by submitting the following JSON data:</p><div class="informalexample"><pre class="programlisting">{
  "id": "storm-nimbus",
  "cmd": "./bin/run-with-marathon.sh",
  "cpus": 1.0,
  "mem": 1024,
  "ports": [0, 1],
  "instances": 1,
  "container": {
    "type": "DOCKER",
    "docker": {
      "image": "mesosphere/storm",
      "network": "HOST",
      "forcePullImage":true
    }
  },
  "healthChecks": [
    {
      "protocol": "HTTP",
      "portIndex": 0,
      "path": "/",
      "gracePeriodSeconds": 120,
      "intervalSeconds": 20,
      "maxConsecutiveFailures": 3
    }
  ]
}</pre></div><p>We can save the preceding JSON code as <code class="literal">storm-mesos.json</code> and send a <code class="literal">curl</code> request to the Marathon API <a class="indexterm" id="id869"/>endpoint to deploy with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -X POST -H "Content-Type: application/json" -d storm-mesos.json http://marathon-machine:8080/v2/apps</strong></span>
</pre></div><p>Reference: <a class="ulink" href="https://github.com/mesos/storm">https://github.com/mesos/storm</a>.</p></div></div>
<div class="section" title="Samza on Mesos"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec74"/>Samza on Mesos</h1></div></div></div><p>Samza<a class="indexterm" id="id870"/> is an open source distributed stream processing<a class="indexterm" id="id871"/> framework <a class="indexterm" id="id872"/>originally developed at LinkedIn. It has the following features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A simple API</li><li class="listitem" style="list-style-type: disc">State management</li><li class="listitem" style="list-style-type: disc">Fault tolerance</li><li class="listitem" style="list-style-type: disc">Durability</li><li class="listitem" style="list-style-type: disc">Scalability</li><li class="listitem" style="list-style-type: disc">Pluggability</li><li class="listitem" style="list-style-type: disc">Processor isolation</li></ul></div><div class="section" title="Important concepts of Samza"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec137"/>Important concepts of Samza</h2></div></div></div><p>Some concepts<a class="indexterm" id="id873"/> in Samza are described in the following sections.</p><div class="section" title="Streams"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec88"/>Streams</h3></div></div></div><p>Samza <a class="indexterm" id="id874"/>processes streams of data—for example, website clickstreams, server logs, or any other event data. Messages can be added and read from a data stream. Multiple frameworks can access the same data stream and can partition the data based on the keys present in the message.</p><div class="mediaobject"><img alt="Streams" src="graphics/B05186_08_07.jpg"/></div></div><div class="section" title="Jobs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec89"/>Jobs</h3></div></div></div><p>A Samza job is<a class="indexterm" id="id875"/> the computation logic that reads data from input streams, applies some transformations to it, and outputs the resultant messages to a bunch of output streams.</p></div><div class="section" title="Partitions"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec90"/>Partitions</h3></div></div></div><p>Every stream<a class="indexterm" id="id876"/> is split into single or multiple partitions. Every partition is an ordered sequence of messages.</p><div class="mediaobject"><img alt="Partitions" src="graphics/B05186_08_08.jpg"/></div></div><div class="section" title="Tasks"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec91"/>Tasks</h3></div></div></div><p>A job is <a class="indexterm" id="id877"/>subdivided into multiple tasks for the parallelism of the computation. Every task reads data from a single partition for each input stream of the job.</p><div class="mediaobject"><img alt="Tasks" src="graphics/B05186_08_09.jpg"/></div></div><div class="section" title="Dataflow graphs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec92"/>Dataflow graphs</h3></div></div></div><p>Multiple jobs <a class="indexterm" id="id878"/>can be composed to develop a dataflow graph, in which the nodes are datastreams and the edges are the jobs.</p><div class="mediaobject"><img alt="Dataflow graphs" src="graphics/B05186_08_10.jpg"/></div></div></div><div class="section" title="Setting up Samza on Mesos"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec138"/>Setting up Samza on Mesos</h2></div></div></div><p>This topic <a class="indexterm" id="id879"/>covers how we can run Samza jobs on a Mesos cluster. We will package the Samza jobs in a tarball for the sake of simplicity. Samza also supports packaging it in a Docker image.</p><p>At the time of writing this book, Samza on Mesos is in its early stages and hasn't been tested in a production environment to the best of our knowledge. Let's follow the steps mentioned here to set up Samza on Mesos:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We first need to deploy the <code class="literal">samza-mesos</code> jar in the environment. For this, we can clone the repository and build it with the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/Banno/samza-mesos</strong></span>
<span class="strong"><strong>$ cd samza-mesos</strong></span>
<span class="strong"><strong>$ mvn clean install</strong></span>
</pre></div></li><li class="listitem">Once this is done, we can start importing the Maven dependency to our projects, as follows:<div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;eu.inn&lt;/groupId&gt;
  &lt;artifactId&gt;samza-mesos&lt;/artifactId&gt;
  &lt;version&gt;0.1.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;</pre></div></li></ol></div><div class="section" title="The deployment of Samza through Marathon"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl4sec12"/>The deployment of Samza through Marathon</h3></div></div></div><p>Samza jobs <a class="indexterm" id="id880"/>can be deployed through Marathon. Each Samza job is a Mesos framework, which creates one Mesos task for each of the Samza containers. It is easier to deploy the Samza jobs on Mesos through Marathon, as described here.</p><p>Samza jobs are usually deployed in a tarball, which should contain the following as the top-level directories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Bin</code>: This contains the standard Samza distributed shell scripts</li><li class="listitem" style="list-style-type: disc"><code class="literal">Config</code>: This should be with your job <code class="literal">.properties</code> file(s)</li><li class="listitem" style="list-style-type: disc"><code class="literal">Lib</code>: This contains all the <code class="literal">.jar</code> files</li></ul></div><p>Now, let's take a look at how we can submit a Samza job to Marathon to deploy it on Mesos. The JSON request for the same will look similar to the following:</p><div class="informalexample"><pre class="programlisting">{
  "id": "samza-jobs.my-job", /Job ID/
  "uris": [
    "hdfs://master-machine/my-job.tgz" /Job Resource/
  ],
  "cmd": "bin/run-job.sh --config-path=file://$PWD/config/my-job.properties --config=job.factory.class=eu.inn.samza.mesos.MesosJobFactory --config=mesos.master.connect=zk://zookeeper-machine:2181/mesos --config=mesos.package.path=hdfs://master-machine/my-job.tgz --config=mesos.executor.count=1", /Job Properties/
  "cpus": 0.1,
  "mem": 64, /Resources/
  "instances": 1,
  "env": {
    "JAVA_HEAP_OPTS": "-Xms64M -Xmx64M"
  }
}</pre></div><p>Note that here, <code class="literal">mesos.package.path</code> is the parameter pointing to the Samza tarball, which is kept in HDFS.</p><p>We can save the preceding JSON record to a file called <code class="literal">samza-job.json</code> and submit it to Marathon using the following <code class="literal">curl</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ curl -X POST -H "Content-Type: application/json" -d samza-job.json http://marathon-machine:8080/v2/apps</strong></span>
</pre></div></div><div class="section" title="An advanced configuration guide"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl4sec13"/>An advanced configuration guide</h3></div></div></div><p>The <a class="indexterm" id="id881"/>supported configuration properties are listed at <a class="ulink" href="https://github.com/Banno/samza-mesos">https://github.com/Banno/samza-mesos</a>.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec75"/>Summary</h1></div></div></div><p>This chapter introduced the reader to some important big data processing frameworks and covered topics such as the setup, configuration, and management of these frameworks on a distributed infrastructure using Mesos.</p><p>In the next chapter, we will discuss some of the important big data storage frameworks that are currently supported by Mesos (either in a beta or production-ready state), such as Cassandra, Elasticsearch, and Kafka, and understand how these can be set up and configured on Mesos.</p></div></body></html>