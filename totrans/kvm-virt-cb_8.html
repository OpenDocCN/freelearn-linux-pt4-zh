<html><head></head><body>
        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Kernel Tuning for KVM Performance</h1>
            

            <article class="calibre1">
                
<p class="calibre3">In this chapter, we are going to cover the following performance tuning recipes:</p>
<ul class="calibre16">
<li class="calibre17">Tuning the kernel for low I/O latency</li>
<li class="calibre17">Memory tuning for KVM guests</li>
<li class="calibre17">CPU performance options</li>
<li class="calibre17">NUMA tuning with libvirt</li>
<li class="calibre17">Tuning the kernel for network performance</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
            

            <article class="calibre1">
                
<p class="calibre3">In this chapter, we are going to explore various configuration options and tools that can help improve the performance of the host OS and the KVM instances running on it.</p>
<p class="calibre3">When running KVM virtual machines, it's important to understand that from the host perspective, they are regular processes. We can see that KVM guests are Linux processes by examining the process tree on the hypervisor:</p>
<pre class="calibre23">
<strong class="calibre4">root@kvm:~# virsh list<br class="calibre7"/> Id  Name State<br class="calibre7"/>----------------------------------------------------<br class="calibre7"/> 16  kvm  running<br class="calibre7"/><br class="calibre7"/>root@kvm:~# pgrep -lfa qemu</strong><br class="calibre7"/><strong class="calibre4">19913 /usr/bin/qemu-system-x86_64 -name kvm -S -machine pc-i440fx-trusty,accel=kvm,usb=off -m 1024 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 283c6653-9981-9396-efb4-fb864d87f769 -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-kvm/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/tmp/debian.img,format=raw,if=none,id=drive-ide0-0-0 -device ide-hd,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1 -netdev tap,fd=26,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:2f:df:93,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 0.0.0.0:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4 -msg timestamp=on</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<p class="calibre3">The virtual CPUs allocated to the KVM guests are Linux threads, managed by the host scheduler:</p>
<pre class="calibre23">
<strong class="calibre4">root@kvm:~# ps -eLf</strong><br class="calibre7"/><strong class="calibre4">UID PID PPID LWP C NLWP STIME TTY TIME CMD</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">libvirt+ 19913 1 19913 0 3 14:02 ? 00:00:00 /usr/bin/qemu-system-x86_64 -name kvm -S -machine pc-i440fx-trusty,accel=kvm,usb=off -m 1024 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 283c6653-9981-9396-efb4-fb864d87f769 -no-user-config -nodefaul</strong><br class="calibre7"/><strong class="calibre4">libvirt+ 19913 1 19914 0 3 14:02 ? 00:00:08 /usr/bin/qemu-system-x86_64 -name kvm -S -machine pc-i440fx-trusty,accel=kvm,usb=off -m 1024 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 283c6653-9981-9396-efb4-fb864d87f769 -no-user-config -nodefaul</strong><br class="calibre7"/><strong class="calibre4">libvirt+ 19913 1 19917 0 3 14:02 ? 00:00:00 /usr/bin/qemu-system-x86_64 -name kvm -S -machine pc-i440fx-trusty,accel=kvm,usb=off -m 1024 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 283c6653-9981-9396-efb4-fb864d87f769 -no-user-config -nodefaul</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<p class="calibre3">Depending on the type of I/O scheduler, the <kbd class="calibre13">libvirt</kbd> network driver, and memory configuration, the performance of the virtual machine can vary greatly. Before making any changes to the earlier-mentioned components, it is important to understand the type of work the guest OS will be performing. Tuning the host and guest OS for memory-intensive work will be different from I/O or CPU bound loads.</p>
<p class="calibre3">Because all KVM instances are just regular Linux processes, the QEMU driver can apply any of the following <strong class="calibre4">Control Group</strong> (<strong class="calibre4">cgroup</strong>) controllers: <kbd class="calibre13">cpuset</kbd>, <kbd class="calibre13">cpu</kbd>, <kbd class="calibre13">memory</kbd>, <kbd class="calibre13">blkio</kbd>, and device controllers. Using the cgroup controllers provides more granular control over the allowed CPU, memory, and I/O resources, as we are going to see in more detail in the following recipes.</p>
<p class="calibre3">Perhaps the most important point when tuning and optimizing any system is to establish the performance baseline prior to making any adjustments. Start by measuring the baseline performance of a subsystem, such as memory or I/O, make small incremental adjustments, then measure the impact of the changes again. Repeat as necessary until a desired effect is reached.</p>
<p class="calibre3">The recipes in this chapter are meant to give the reader a starting point for what can be tuned on the host and the virtual machines to improve performance, or account for the side effects of running different workloads on the same host/VM and the effects of multitenancy. All resources should be adjusted based on the type of workload, hardware setup, and other variables.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Tuning the kernel for low I/O latency</h1>
            

            <article class="calibre1">
                
<p class="calibre3">In this recipe, we are going to cover some of the disk performance optimization techniques by selecting an I/O scheduler and tuning the block I/O using Linux control groups, for the virtual guest and the host.</p>
<p class="calibre3">There are three I/O schedulers to choose from on the host OS and in the KVM instance:</p>
<ul class="calibre16">
<li class="calibre17"><kbd class="calibre13">noop</kbd>: This is one of the simplest kernel schedulers; it works by inserting all incoming I/O requests into a simple <strong class="calibre4">FIFO</strong> (<strong class="calibre4">First In</strong>, <strong class="calibre4">First Out</strong>) queue. This scheduler is useful when the host OS should not attempt to reorder I/O requests when multiple virtual machines are running.</li>
<li class="calibre17"><kbd class="calibre13">deadline</kbd>: This scheduler imposes a deadline on all I/O operations to prevent starvation of requests, giving priority to read requests, due to processes usually blocking on read operations.</li>
<li class="calibre17"><kbd class="calibre13">cfq</kbd>: The main goal of <strong class="calibre4">Completely Fair Queuing</strong> (<strong class="calibre4">CFQ</strong>) is to maximize the overall CPU utilization while allowing better interactive performance.</li>
</ul>
<p class="calibre3">Selecting the right I/O scheduler on the host and guests greatly depends on the workloads and the underlying hardware storage.</p>
<p class="calibre3">As a general rule, selecting the <kbd class="calibre13">noop</kbd> scheduler for the guest OS allows the host hypervisor to better optimize the I/O requests, because it <span>is aware of all requests coming from the virtual guests.</span> However, if the underlying storage for the KVM machines is iSCSI volumes or any other remote storage such as GlusterFS, using the deadline scheduler, might yield better results. </p>
<p class="calibre3">On most modern Linux kernels, the <kbd class="calibre13">deadline</kbd> scheduler is the default, and it might be sufficient for hosts running multiple KVM virtual machines. As with any system tuning, testing is required when changing the schedulers on the host and guest OS.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
            

            <article class="calibre1">
                
<p class="calibre3">For this recipe, we are going to need the following:</p>
<ul class="calibre16">
<li class="calibre17">An Ubuntu host, with libvirt and QEMU installed and configured</li>
<li class="calibre17">A running KVM virtual machine</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">To change the I/O scheduler on the host and the KVM instance and set an I/O weight, perform the following steps:</p>
<ol class="calibre18">
<li value="1" class="calibre17"><span>On the host OS, list the I/O scheduler currently in use, substituting the block device with whatever is appropriate for your system:</span></li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /sys/block/sda/queue/scheduler</strong><br class="calibre7"/><strong class="calibre4">noop deadline [cfq]</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">Change the I/O scheduler on demand and ensure it is in use by running:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# echo deadline &gt; /sys/block/sda/queue/scheduler</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# cat /sys/block/sda/queue/scheduler</strong><br class="calibre7"/><strong class="calibre4">noop [deadline] cfq</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">To make the change persistent across server restarts, add the following line to the GRUB default configuration and update:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# echo 'GRUB_CMDLINE_LINUX="elevator=deadline"' &gt;&gt; /etc/default/grub</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# tail -1 /etc/default/grub</strong><br class="calibre7"/><strong class="calibre4">GRUB_CMDLINE_LINUX="elevator=deadline"</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# update-grub2</strong><br class="calibre7"/><strong class="calibre4">Generating grub configuration file ...</strong><br class="calibre7"/><strong class="calibre4">Found linux image: /boot/vmlinuz-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">Found initrd image: /boot/initrd.img-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">done</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# cat /boot/grub/grub.cfg | grep elevator</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=deadline rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=deadline rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro recovery nomodeset elevator=deadline</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">For the KVM instance, set up the noop I/O scheduler persistently:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh console kvm1</strong><br class="calibre7"/><strong class="calibre4">Connected to domain kvm1</strong><br class="calibre7"/><strong class="calibre4">Escape character is ^]</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~# echo 'GRUB_CMDLINE_LINUX="elevator=noop"' &gt;&gt; /etc/default/grub</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~# tail -1 /etc/default/grub</strong><br class="calibre7"/><strong class="calibre4">GRUB_CMDLINE_LINUX="elevator=noop"</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~# update-grub2</strong><br class="calibre7"/><strong class="calibre4">Generating grub configuration file ...</strong><br class="calibre7"/><strong class="calibre4">Found linux image: /boot/vmlinuz-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">Found initrd image: /boot/initrd.img-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">done</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~# cat /boot/grub/grub.cfg | grep elevator</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=noop rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=noop rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro recovery nomodeset elevator=noop</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~#</strong>
</pre>
<ol start="5" class="calibre18">
<li value="5" class="calibre17">Set a weight of 100 for the KVM instance using the <kbd class="calibre13">blkio</kbd> cgroup controller:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh blkiotune --weight 100 kvm</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh blkiotune kvm</strong><br class="calibre7"/><strong class="calibre4">weight : 100</strong><br class="calibre7"/><strong class="calibre4">device_weight :</strong><br class="calibre7"/><strong class="calibre4">device_read_iops_sec:</strong><br class="calibre7"/><strong class="calibre4">device_write_iops_sec:</strong><br class="calibre7"/><strong class="calibre4">device_read_bytes_sec:</strong><br class="calibre7"/><strong class="calibre4">device_write_bytes_sec:</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="6" class="calibre18">
<li value="6" class="calibre17">Find the <kbd class="calibre13">cgroup</kbd> directory hierarchy on the host:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# mount | grep cgroup</strong><br class="calibre7"/><strong class="calibre4">none on /sys/fs/cgroup type tmpfs (rw)</strong><br class="calibre7"/><strong class="calibre4">systemd on /sys/fs/cgroup/systemd type cgroup (rw,noexec,nosuid,nodev,none,name=systemd)</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="7" class="calibre18">
<li value="7" class="calibre17">Ensure that the cgroup for the KVM instance contains the weight that we set up earlier on the <kbd class="calibre13">blkio</kbd> controller:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /sys/fs/cgroup/blkio/machine/kvm.libvirt-qemu/blkio.weight</strong><br class="calibre7"/><strong class="calibre4">100</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<div class="packt_infobox">For a detailed explanation on how Linux cgroups work, refer to the <em class="calibre29">Containerization with LXC</em> book from Packt publishing at <a href="https://www.packtpub.com/virtualization-and-cloud/containerization-lxc" class="calibre30">https://www.packtpub.com/virtualization-and-cloud/containerization-lxc</a>.</div>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">We can see what I/O scheduler the kernel is currently using by examining the <kbd class="calibre13">scheduler</kbd> file in the <kbd class="calibre13">/sys</kbd> virtual filesystem. In step 1, we see that it's the <kbd class="calibre13">cfq</kbd> scheduler. We then proceed to change the I/O scheduler on the running system in step 2. Please keep in mind that changing the scheduler on demand like that will not persist server restarts. In steps 3 and 4, we modify the GRUB configuration which will append the new scheduler information to the kernel boot instructions. Restarting the server or the virtual machine will now select the new I/O scheduler.</p>
<p class="calibre3">If running multiple virtual machines on the same host, it might be useful to give more I/O priority to some of them based on certain criteria, such as time of day and VM workload. In step 5, we use the <kbd class="calibre13">blkio</kbd> cgroup controller to set a weight for the KVM guest. Lower weight will give better I/O priority. In steps 6 and 7, we can see that the correct <kbd class="calibre13">cgroup</kbd> hierarchy has been created and the <kbd class="calibre13">blkio.weight</kbd> file contains the new weight we set with the <kbd class="calibre13">virsh</kbd> command.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Memory tuning for KVM guests</h1>
            

            <article class="calibre1">
                
<p class="calibre3">When it comes to memory tuning of KVM guests there are few options available, depending on the workload of the virtual machine. One such option is Linux HugePages.</p>
<p class="calibre3">Most Linux hosts by default address memory in 4 KB segments, named pages. However, the kernel is capable of using larger page sizes. Using HugePages (pages bigger than 4 KB) may improve performance by increasing the CPU cache hits against the transaction <strong class="calibre4">Lookaside Buffer</strong> (<strong class="calibre4">TLB</strong>). The TLB is a <span>memory cache that stores recent translations of virtual memory to physical addresses for quick retrieval.</span></p>
<p class="calibre3">In this recipe, we are going to enable and set HugePages on the hypervisor and the KVM guest, then examine the tuning options that the <kbd class="calibre13">virsh</kbd> command provides.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
            

            <article class="calibre1">
                
<p class="calibre3">For this recipe, we are going to need the following:</p>
<ul class="calibre16">
<li class="calibre17">An Ubuntu host, with libvirt and QEMU installed and configured</li>
<li class="calibre17">A running KVM virtual machine</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">To enable and set HugePages on the hypervisor and the KVM guest and use the <kbd class="calibre13">virsh</kbd> command to set various memory options, follow these steps:</p>
<ol class="calibre18">
<li value="1" class="calibre17"><span>Check the current HugePages settings on the host OS:</span></li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /proc/meminfo | grep -i huge</strong><br class="calibre7"/><strong class="calibre4">AnonHugePages: 509952 kB</strong><br class="calibre7"/><strong class="calibre4">HugePages_Total: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Free: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Rsvd: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Surp: 0</strong><br class="calibre7"/><strong class="calibre4">Hugepagesize: 2048 kB</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">Connect to the KVM guest and check the current HugePages settings:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm1:~# cat /proc/meminfo | grep -i huge</strong><br class="calibre7"/><strong class="calibre4">HugePages_Total: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Free: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Rsvd: 0</strong><br class="calibre7"/><strong class="calibre4">HugePages_Surp: 0</strong><br class="calibre7"/><strong class="calibre4">Hugepagesize: 2048 kB</strong><br class="calibre7"/><strong class="calibre4">root@kvm1:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">Increase the size of the pool of HugePages from 0 to 25000 on the hypervisor and verify the following:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl vm.nr_hugepages=25000</strong><br class="calibre7"/><strong class="calibre4">vm.nr_hugepages = 25000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# cat /proc/meminfo | grep -i huge<br class="calibre7"/>AnonHugePages: 446464 kB<br class="calibre7"/>HugePages_Total: 25000<br class="calibre7"/>HugePages_Free: 24484<br class="calibre7"/>HugePages_Rsvd: 0<br class="calibre7"/>HugePages_Surp: 0<br class="calibre7"/>Hugepagesize: 2048 kB<br class="calibre7"/>root@kvm:~# cat /proc/sys/vm/nr_hugepages</strong><br class="calibre7"/><strong class="calibre4">25000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">Check whether the hypervisor CPU supports 2 MB and 1 GB HugePages sizes:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /proc/cpuinfo | egrep -i "pse|pdpe1" | tail -1</strong><br class="calibre7"/><strong class="calibre4">flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="5" class="calibre18">
<li value="5" class="calibre17">Set 1 GB HugePages size by editing the default GRUB configuration and rebooting:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /etc/default/grub</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">GRUB_CMDLINE_LINUX_DEFAULT="rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet hugepagesz=1GB hugepages=1"</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# update-grub</strong><br class="calibre7"/><strong class="calibre4">Generating grub configuration file ...</strong><br class="calibre7"/><strong class="calibre4">Found linux image: /boot/vmlinuz-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">Found initrd image: /boot/initrd.img-3.13.0-107-generic</strong><br class="calibre7"/><strong class="calibre4">done</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# cat /boot/grub/grub.cfg | grep -i huge</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=deadline rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet hugepagesz=1GB hugepages=1</strong><br class="calibre7"/><strong class="calibre4"> linux /boot/vmlinuz-3.13.0-107-generic root=/dev/md126p1 ro elevator=deadline rd.fstab=no acpi=noirq noapic cgroup_enable=memory swapaccount=1 quiet hugepagesz=1GB hugepages=1</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# reboot</strong>
</pre>
<ol start="6" class="calibre18">
<li value="6" class="calibre17">Install the HugePages package:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# apt-get install hugepages</strong>
</pre>
<ol start="7" class="calibre18">
<li value="7" class="calibre17">Check the current HugePages size:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# hugeadm --pool-list</strong><br class="calibre7"/><strong class="calibre4"> Size      Minimum Current Maximum Default</strong><br class="calibre7"/><strong class="calibre4"> 2097152   25000   25000   25000   *</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="8" class="calibre18">
<li value="8" class="calibre17">Enable HugePages support for KVM:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sed -i 's/KVM_HUGEPAGES=0/KVM_HUGEPAGES=1/g' /etc/default/qemu-kvm</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# root@kvm:~# /etc/init.d/libvirt-bin restart<br class="calibre7"/>libvirt-bin stop/waiting<br class="calibre7"/>libvirt-bin start/running, process 16257<br class="calibre7"/>root@kvm:~#</strong>
</pre>
<ol start="9" class="calibre18">
<li value="9" class="calibre17">Mount the HugeTable virtual filesystem on the host OS:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# mkdir /hugepages</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# echo "hugetlbfs /hugepages hugetlbfs mode=1770,gid=2021 0 0" &gt;&gt; /etc/fstab</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# mount -a</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# mount | grep hugepages</strong><br class="calibre7"/><strong class="calibre4">hugetlbfs on /hugepages type hugetlbfs (rw,mode=1770,gid=2021)</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="10" class="calibre18">
<li value="10" class="calibre17">Edit the configuration for the KVM guest and enable HugePages:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh destroy kvm1</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 destroyed</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh edit kvm1</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4"> &lt;memoryBacking&gt;</strong><br class="calibre7"/><strong class="calibre4">   &lt;hugepages/&gt;</strong><br class="calibre7"/><strong class="calibre4"> &lt;/memoryBacking&gt;</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 XML configuration edited.</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh start kvm1</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 started</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<div class="packt_tip">If you see the following error when starting the KVM instance: <kbd class="calibre27">error: internal error: hugetlbfs filesystem is not mounted or disabled by administrator config</kbd>, make sure that the HugePages virtual filesystem was mounted successfully in step 9.<br class="calibre34"/>
<br class="calibre34"/>
If you see the following error when starting the KVM instance: <kbd class="calibre27">error: internal error: process exited while connecting to monitor: file_ram_alloc: can't mmap RAM pages: Cannot allocate memory</kbd>, you need to increase the HugePages pool in step 3.</div>
<ol start="11" class="calibre18">
<li value="11" class="calibre17">Update the memory hard limit for the KVM instance and verify, as follows:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh memtune kvm1</strong><br class="calibre7"/><strong class="calibre4">hard_limit : unlimited</strong><br class="calibre7"/><strong class="calibre4">soft_limit : unlimited</strong><br class="calibre7"/><strong class="calibre4">swap_hard_limit: unlimited</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh memtune kvm1 --hard-limit 2GB</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh memtune kvm1</strong><br class="calibre7"/><strong class="calibre4">hard_limit : 1953125</strong><br class="calibre7"/><strong class="calibre4">soft_limit : unlimited</strong><br class="calibre7"/><strong class="calibre4">swap_hard_limit: unlimited</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">Libvirt and KVM support and take advantage of HugePages. Please be aware that not every workload will benefit of having pages larger than the default. Instances running databases and memory bound KVM instances are good use cases. As always, before enabling this feature, measure the performance of your application inside the virtual machine to ensure that it will benefit from HugePages.</p>
<p class="calibre3">In this recipe, we enabled HugePages on the host and the guest OS and set a hard limit on the usable memory for the guest. Let's go through the steps in more detail.</p>
<p class="calibre3">In steps 1 and 2, we check the current state of HugePages. From the output, we can see that there's no HugePages pool currently allocated, indicated by the <kbd class="calibre13">HugePages_Total</kbd><em class="calibre22"> </em>field and the current size of the HugePages of 2 MB. </p>
<p class="calibre3">In step 3, we increase the HugePages pool size to 25000. The change is on demand and will not persist server reboot. To make it persistent, you can add it to the <kbd class="calibre13">/etc/sysctl.conf</kbd> file.</p>
<p class="calibre3">In order to use the HugePages feature, we need to ensure that the CPU of the host server has hardware support for it, as indicated by the <kbd class="calibre13">pse</kbd> and <kbd class="calibre13">pdpe1</kbd> flags, as shown in step 4.</p>
<p class="calibre3">In step 5, we configure the GRUB bootloader to start the kernel with HugePages support and a set size of 1 GB.</p>
<p class="calibre3">Although we can work directly with the files exposed by the <kbd class="calibre13">/proc</kbd> virtual filesystem, in step 6, we install the HugePages package, which provides a few useful userspace tools to list and manage various memory settings. We use the <kbd class="calibre13">hugeadm</kbd> command in step 7 to list the size of the HugePages pool.</p>
<p class="calibre3">To enable HugePages support for KVM, we update the <kbd class="calibre13">/etc/default/qemu-kvm</kbd> file in step 8, mount the virtual filesystem for it in step 9, and finally reconfigure the KVM virtual machine to use HugePages by adding the <kbd class="calibre13">&lt;hugepages/&gt;</kbd> stanza for the <kbd class="calibre13">&lt;memoryBacking&gt;</kbd> object.</p>
<p class="calibre3">Libvirt provides a convenient way to manage the amount of allocated memory for the KVM guests. In step 11, we set a hard limit of 2 GB for the <kbd class="calibre13">kvm1</kbd> virtual machine.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">CPU performance options</h1>
            

            <article class="calibre1">
                
<p class="calibre3">There are a few methods to control CPU allocation and the available CPU cycles for KVM machines-using cgroups and the libvirt-provided CPU pinning and affinity functions, we are going to explore in this recipe. CPU affinity is a scheduler property that connects a process to a given set of CPUs on the host OS.</p>
<p class="calibre3">When provisioning virtual machines with libvirt, the default behavior is to provision the guests on any available CPU cores. In some cases, <strong class="calibre4">Non-Uniform Memory Access</strong> (<strong class="calibre4">NUMA</strong>) is a good example of when we need to designate a core per KVM instance (as we are going to see in the next recipe), that it's better to assign the virtual machine to a specified CPU core. Since each KVM virtual machine is a kernel process (<kbd class="calibre13">qemu-system-x86_64</kbd> more specifically in our examples), we can do this using tools such as <kbd class="calibre13">taskset</kbd> or the <kbd class="calibre13">virsh</kbd> command. We can also use the cgroups CPU subsystem to manage CPU cycle allocation, which provides more granular control over CPU resource utilization per virtual machine.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
            

            <article class="calibre1">
                
<p class="calibre3">For this recipe, we are going to need the following:</p>
<ul class="calibre16">
<li class="calibre17">An Ubuntu host, with libvirt and QEMU installed and configured</li>
<li class="calibre17">A running KVM virtual machine</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">To pin a KVM virtual machine to a specific CPU and to change the CPU shares, perform the following:</p>
<ol class="calibre18">
<li value="1" class="calibre17">Obtain information about the available CPU cores on the hypervisor:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh nodeinfo</strong><br class="calibre7"/><strong class="calibre4">CPU model: x86_64</strong><br class="calibre7"/><strong class="calibre4">CPU(s): 40</strong><br class="calibre7"/><strong class="calibre4">CPU frequency: 2593 MHz</strong><br class="calibre7"/><strong class="calibre4">CPU socket(s): 1</strong><br class="calibre7"/><strong class="calibre4">Core(s) per socket: 10</strong><br class="calibre7"/><strong class="calibre4">Thread(s) per core: 2</strong><br class="calibre7"/><strong class="calibre4">NUMA cell(s): 2</strong><br class="calibre7"/><strong class="calibre4">Memory size: 131918328 KiB</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">Get information about the CPU allocation for the KVM guest:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh vcpuinfo kvm1</strong><br class="calibre7"/><strong class="calibre4">VCPU: 0</strong><br class="calibre7"/><strong class="calibre4">CPU: 2</strong><br class="calibre7"/><strong class="calibre4">State: running</strong><br class="calibre7"/><strong class="calibre4">CPU time: 9.1s</strong><br class="calibre7"/><strong class="calibre4">CPU Affinity: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">Pin the KVM instance CPU (<kbd class="calibre13">VCPU: 0</kbd>) to the first hypervisor CPU (<kbd class="calibre13">CPU: 0</kbd>) and display the result:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh vcpupin kvm1 0 0 --live</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh vcpuinfo kvm1</strong><br class="calibre7"/><strong class="calibre4">VCPU: 0</strong><br class="calibre7"/><strong class="calibre4">CPU: 0</strong><br class="calibre7"/><strong class="calibre4">State: running</strong><br class="calibre7"/><strong class="calibre4">CPU time: 9.3s</strong><br class="calibre7"/><strong class="calibre4">CPU Affinity: y---------------------------------------</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">List the share of runtime that is assigned to a KVM instance:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh schedinfo kvm1</strong><br class="calibre7"/><strong class="calibre4">Scheduler : posix</strong><br class="calibre7"/><strong class="calibre4">cpu_shares : 1024</strong><br class="calibre7"/><strong class="calibre4">vcpu_period : 100000</strong><br class="calibre7"/><strong class="calibre4">vcpu_quota : -1</strong><br class="calibre7"/><strong class="calibre4">emulator_period: 100000</strong><br class="calibre7"/><strong class="calibre4">emulator_quota : -1</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="5" class="calibre18">
<li value="5" class="calibre17">Modify the current CPU weight of a running virtual machine:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh schedinfo kvm cpu_shares=512</strong><br class="calibre7"/><strong class="calibre4">Scheduler : posix</strong><br class="calibre7"/><strong class="calibre4">cpu_shares : 512</strong><br class="calibre7"/><strong class="calibre4">vcpu_period : 100000</strong><br class="calibre7"/><strong class="calibre4">vcpu_quota : -1</strong><br class="calibre7"/><strong class="calibre4">emulator_period: 100000</strong><br class="calibre7"/><strong class="calibre4">emulator_quota : -1</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="6" class="calibre18">
<li value="6" class="calibre17">Check the CPU shares in the CPU cgroups subsystem:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# cat /sys/fs/cgroup/cpu/machine/kvm1.libvirt-qemu/cpu.shares</strong><br class="calibre7"/><strong class="calibre4">512</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="7" class="calibre18">
<li value="7" class="calibre17">Examine the updated XML instance definition:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh dumpxml kvm1</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4"> &lt;vcpu placement='static'&gt;1&lt;/vcpu&gt;</strong><br class="calibre7"/><strong class="calibre4"> &lt;cputune&gt;</strong><br class="calibre7"/><strong class="calibre4">   &lt;shares&gt;512&lt;/shares&gt;</strong><br class="calibre7"/><strong class="calibre4">   &lt;vcpupin vcpu='0' cpuset='0'/&gt;</strong><br class="calibre7"/><strong class="calibre4"> &lt;/cputune&gt;</strong><br class="calibre7"/><strong class="calibre4"> ...</strong><br class="calibre7"/><strong class="calibre4"> root@kvm:~#</strong>
</pre>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">We begin by gathering information about the CPU resources available on the hypervisor. From the output in step 1, we can see that the host OS has 40 CPUs on one socket.</p>
<p class="calibre3">In step 2, we collect information about the virtual machine CPU and its affinity with the host CPUs. In this example, the KVM guest has one virtual CPU, denoted by the <kbd class="calibre13">VCPU: 0</kbd> record and affinity to all 40 hypervisor processors, as indicated by the <kbd class="calibre13">CPU Affinity: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy</kbd> field. </p>
<p class="calibre3">In step 3, we pin/bind the virtual CPU to the first physical processor on the hypervisor. Note the change in the affinity output: <kbd class="calibre13">CPU Affinity: y---------------------------------------</kbd>. </p>
<p class="calibre3">From the output of the <kbd class="calibre13">virsh</kbd> command in step 4, we can observe that the CPU shares allocated to the KVM instance are set to 1024. This value is a ratio, meaning that if another guest has 512 shares, it will have twice fewer CPU runtime than that of an instance with 1024 shares. We reduce that value in step 5.</p>
<p class="calibre3">In steps 6 and 7, we confirm that the CPU shares were correctly set in the CPU cgroup subsystem on the host OS. As we mentioned earlier, CPU shares are configured using cgroups and can be adjusted directly or by the provided libvirt functionality, by means of the <kbd class="calibre13">virsh</kbd> command.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">NUMA tuning with libvirt</h1>
            

            <article class="calibre1">
                
<p class="calibre3">NUMA is a technology that allows the system memory to be divided into zones, also named nodes. The NUMA nodes are then allocated to particular CPUs or sockets. In contrast to the traditional monolithic memory approach, where each CPU/core can access all the memory regardless of its locality, usually resulting in larger latencies, NUMA bound processes can access memory that is local to the CPU they are being executed on. In most cases, this is much faster than the memory connected to the remote CPUs on the system. </p>
<p class="calibre3">Libvirt uses the <kbd class="calibre13">libnuma</kbd> library to enable NUMA functionality for virtual machines, as we can see here:</p>
<pre class="calibre23">
<strong class="calibre4">root@kvm:~# ldd /usr/sbin/libvirtd | grep numa</strong><br class="calibre7"/><strong class="calibre4"> libnuma.so.1 =&gt; /usr/lib/x86_64-linux-gnu/libnuma.so.1 (0x00007fd12d49e000)</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<p class="calibre3">Libvirt NUMA supports the following memory allocation policies to place virtual machines to NUMA nodes:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre4">strict</strong>: T<span>he placement will fail if the memory cannot be allocated on the target node</span></li>
<li class="calibre17"><strong class="calibre4">interleave</strong>: Memory pages are allocated in a round-robin fashion</li>
<li class="calibre17"><strong class="calibre4">preferred</strong>: This policy <span>allows the hypervisor to provide memory from other nodes in case there's not enough memory available from the specified nodes</span></li>
</ul>
<p class="calibre3">In this recipe, we are going to enable NUMA access for a KVM instance and explore its impact on the overall system performance.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
            

            <article class="calibre1">
                
<p class="calibre3">For this recipe, we are going to need the following:</p>
<ul class="calibre16">
<li class="calibre17">An Ubuntu host, with libvirt and QEMU installed and configured</li>
<li class="calibre17">A running KVM virtual machine</li>
<li class="calibre17">The <kbd class="calibre13">numastat</kbd> utility</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">To enable a KVM virtual machine to run on a given NUMA node and CPU using the strict<em class="calibre22"> </em>NUMA policy, perform the following steps:</p>
<ol class="calibre18">
<li value="1" class="calibre17"><span>Install the</span> <kbd class="calibre13">numactl</kbd> <span>package and check the hardware configuration of the hypervisor:</span></li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# apt-get install numactl</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# numactl --hardware</strong><br class="calibre7"/><strong class="calibre4">available: 2 nodes (0-1)</strong><br class="calibre7"/><strong class="calibre4">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 20 21 22 23 24 25 26 27 28 29</strong><br class="calibre7"/><strong class="calibre4">node 0 size: 64317 MB</strong><br class="calibre7"/><strong class="calibre4">node 0 free: 3173 MB</strong><br class="calibre7"/><strong class="calibre4">node 1 cpus: 10 11 12 13 14 15 16 17 18 19 30 31 32 33 34 35 36 37 38 39</strong><br class="calibre7"/><strong class="calibre4">node 1 size: 64509 MB</strong><br class="calibre7"/><strong class="calibre4">node 1 free: 31401 MB</strong><br class="calibre7"/><strong class="calibre4">node distances:</strong><br class="calibre7"/><strong class="calibre4">node 0 1</strong><br class="calibre7"/><strong class="calibre4"> 0: 10 21</strong><br class="calibre7"/><strong class="calibre4"> 1: 21 10</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">Display the current NUMA placement for the KVM guest:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# numastat -c kvm1<br class="calibre7"/><br class="calibre7"/>Per-node process memory usage (in MBs) for PID 22395 (qemu-system-x86)<br class="calibre7"/>            Node 0    Node 1    Total<br class="calibre7"/>            ------    ------    -----<br class="calibre7"/>Huge             0         0        0<br class="calibre7"/>Heap             1         1        2<br class="calibre7"/>Stack            2         2        4<br class="calibre7"/>Private         39        21       59<br class="calibre7"/>-------     ------    ------    -----<br class="calibre7"/>Total           42        23       65<br class="calibre7"/>root@kvm:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">Edit the XML instance definition, set the memory mode to strict, and select the second NUMA node (indexing starts from 0, so the second NUMA node is labeled as 1), then restart the guest:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh edit kvm1</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">&lt;vcpu placement='static' cpuset='10-11'&gt;2&lt;/vcpu&gt;</strong><br class="calibre7"/><strong class="calibre4">&lt;numatune&gt;</strong><br class="calibre7"/><strong class="calibre4"> &lt;memory mode='strict' nodeset='1'/&gt;</strong><br class="calibre7"/><strong class="calibre4">&lt;/numatune&gt;</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 XML configuration edited.</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh destroy kvm1</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 destroyed</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~# virsh start kvm1</strong><br class="calibre7"/><strong class="calibre4">Domain kvm1 started</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">Get the NUMA parameters for the KVM instance:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh numatune kvm1</strong><br class="calibre7"/><strong class="calibre4">numa_mode : strict</strong><br class="calibre7"/><strong class="calibre4">numa_nodeset : 1</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="5" class="calibre18">
<li value="5" class="calibre17">Print the current virtual CPU affinity:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# virsh vcpuinfo kvm1</strong><br class="calibre7"/><strong class="calibre4">VCPU: 0</strong><br class="calibre7"/><strong class="calibre4">CPU: 11</strong><br class="calibre7"/><strong class="calibre4">State: running</strong><br class="calibre7"/><strong class="calibre4">CPU time: 8.4s</strong><br class="calibre7"/><strong class="calibre4">CPU Affinity: ----------yy----------------------------</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">VCPU: 1</strong><br class="calibre7"/><strong class="calibre4">CPU: 10</strong><br class="calibre7"/><strong class="calibre4">State: running</strong><br class="calibre7"/><strong class="calibre4">CPU time: 0.3s</strong><br class="calibre7"/><strong class="calibre4">CPU Affinity: ----------yy----------------------------</strong><br class="calibre7"/><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="6" class="calibre18">
<li value="6" class="calibre17">Print the NUMA node placement for the KVM instance:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# numastat -c kvm1<br class="calibre7"/><br class="calibre7"/>Per-node process memory usage (in MBs) for PID 22395 (qemu-system-x86)<br class="calibre7"/>            Node 0    Node 1    Total<br class="calibre7"/>            ------    ------    -----<br class="calibre7"/>Huge             0         0        0<br class="calibre7"/>Heap             0         3        3<br class="calibre7"/>Stack            0         2        2<br class="calibre7"/>Private          0       174      174<br class="calibre7"/>-------     ------    ------    -----<br class="calibre7"/>Total            0       179      179<br class="calibre7"/>root@kvm:~#</strong>
</pre>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">We start by examining the NUMA setup on the host OS. From the output of the <kbd class="calibre13">numactl</kbd> command in step 1, we can observe that the hypervisor has two NUMA nodes: node 0 and node 1. Each node manages a list of CPUs. In this case, NUMA node 1 contains CPUs from 10 to 19 and from 30 to 39 and contains 64 GB of memory. This means that 64 GB of RAM is going to be local to those CPUs and access to the memory from those CPUs is going to be much faster than from CPUs that are part of node 0. To improve memory access latencies for a KVM guest, we need to pin the virtual CPUs assigned to the virtual machine to CPUs that are a part of the same NUMA node.</p>
<p class="calibre3">In step 2, we can see that the KVM instance uses memory from both NUMA nodes, which is not ideal. </p>
<p class="calibre3">In step 3, we edit the guest XML definition and pin the guest on the 10th and 11th CPUs, which are a part of the NUMA node 1, using the <kbd class="calibre13">cpuset='10-11'</kbd> parameter. We also specify the strict NUMA node and the second NUMA node with the <kbd class="calibre13">&lt;memory mode='strict' nodeset='1'/&gt;</kbd> parameter.</p>
<p class="calibre3">After restarting the instance, in step 4, we confirm that the KVM guest is now running using the strict NUMA mode on node 1. We also confirm that the CPU pinning is indeed what we specified in step 5. Note that the CPU affinity is flagged on the 10th and 11th elements of the CPU affinity element.</p>
<p class="calibre3">From the output in step 6, we can see that the KVM guest is now using memory only from the NUMA node 1 as desired. </p>
<p class="calibre3">If you run a memory intensive application before and after the NUMA adjustment and test, you will most likely see significant performance gains when accessing large amounts of memory inside the KVM guest, thanks to the CPU and memory locality that NUMA provides.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">There is more...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">In this recipe, we saw examples on how to manually assign a KVM process to a NUMA node by editing the XML definition of the guest. Some Linux distributions such as RHEL/CentOS 7 and Ubuntu 16.04 provide the <kbd class="calibre13">numad</kbd> (NUMA daemon) service, which aims at automatically balancing processes between NUMA nodes by monitoring the current memory topology:</p>
<ol class="calibre18">
<li value="1" class="calibre17">To install the service on Ubuntu 16.04, run:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# lsb_release -a</strong><br class="calibre7"/><strong class="calibre4">No LSB modules are available.</strong><br class="calibre7"/><strong class="calibre4">Distributor ID: Ubuntu</strong><br class="calibre7"/><strong class="calibre4">Description: Ubuntu 16.04.2 LTS</strong><br class="calibre7"/><strong class="calibre4">Release: 16.04</strong><br class="calibre7"/><strong class="calibre4">Codename: xenial</strong><br class="calibre7"/><strong class="calibre4">root@kvm2:~# apt install numad</strong><br class="calibre7"/><strong class="calibre4">...</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">To start the service, execute the following code:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# service numad start</strong><br class="calibre7"/><strong class="calibre4">root@kvm2:~# pgrep -lfa numad</strong><br class="calibre7"/><strong class="calibre4">12601 /usr/bin/numad -i 15</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">To manage a specific KVM guest with <kbd class="calibre13">numad</kbd>, pass the process ID of the KVM instance:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# numad -S 0 -p $(pidof qemu-system-x86_64)</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">The service will log any NUMA rebalancing attempts:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# tail /var/log/numad.log</strong><br class="calibre7"/><strong class="calibre4">Thu May 25 21:06:42 2017: Changing THP scan time in /sys/kernel/mm/transparent_hugepage/khugepaged/scan_sleep_millisecs from 10000 to 1000 ms.</strong><br class="calibre7"/><strong class="calibre4">Thu May 25 21:06:42 2017: Registering numad version 20150602 PID 12601</strong><br class="calibre7"/><strong class="calibre4">Thu May 25 21:09:25 2017: Adding PID 4601 to inclusion PID list</strong><br class="calibre7"/><strong class="calibre4">Thu May 25 21:09:25 2017: Scanning only explicit PID list processes</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<p class="calibre31">The <kbd class="calibre13">numad</kbd> service can be helpful on OpenStack compute nodes, where manual NUMA balancing may be too involving.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Tuning the kernel for network performance</h1>
            

            <article class="calibre1">
                
<p class="calibre3">Most modern Linux kernels ship sufficiently tuned for various network workloads. Some distributions provide predefined tuning services (a good example is <kbd class="calibre13">tuned</kbd> for Red Hat/CentOS), which include a set of profiles based on the server role.</p>
<p class="calibre3">Let's go over the steps taken during data transmission and reception, on a typical Linux host, before we delve into how to tune the hypervisor:</p>
<ol class="calibre18">
<li value="1" class="calibre17">The application first writes the data to a socket, which in turn is put in the transmit buffer.</li>
<li value="2" class="calibre17">The kernel encapsulates the data into a <strong class="calibre4">Protocol Data Unit</strong> (<strong class="calibre4">PDU</strong>).</li>
<li value="3" class="calibre17">The PDU is then moved onto the per-device transmit queue.</li>
<li value="4" class="calibre17">The <strong class="calibre4">Network Interface Cards</strong> (<strong class="calibre4">NIC</strong>) driver then pops the PDU from the transmit queue and copies it to the NIC.</li>
<li value="5" class="calibre17">The NIC sends the data and raises a hardware interrupt.</li>
<li value="6" class="calibre17">On the other end of the communication channel, the NIC receives the frame, copies it on the receive buffer, and raises hard interrupt.</li>
<li value="7" class="calibre17">The kernel in turn handles the interrupt and raises a soft interrupt to process the packet.</li>
<li value="8" class="calibre17">Finally, the kernel handles the soft interrupt and moves the packet up the TCP/IP stack for decapsulation, and puts it in a receive buffer for a process to read from. </li>
</ol>
<p class="calibre3">In this recipe, we are going to examine a few best practices for tuning the Linux kernel, usually resulting in better network performance, on multitenant KVM hosts.</p>
<div class="packt_infobox">Please make sure that you establish a baseline before making any configuration changes, by measuring the host performance first. Make small incremental changes, then measure the impact again.<br class="calibre34"/>
The examples in this recipe are not meant to be copied/pasted without prior understanding of the possible positive or negative impact they might make. Use the examples presented as a guide as to what can be tuned-the actual values must be carefully considered, based on the server type and the entire environment.</div>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">Getting ready</h1>
            

            <article class="calibre1">
                
<p class="calibre3">For this recipe, we are going to need the following:</p>
<ul class="calibre16">
<li class="calibre17">An Ubuntu host, with libvirt and QEMU installed and configured</li>
<li class="calibre17">A running KVM virtual machine</li>
</ul>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How to do it...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">To tune the kernel for better network performance, execute the following steps (for more information on what the kernel tunables are, read the <em class="calibre22">How it works...</em> section):</p>
<ol class="calibre18">
<li value="1" class="calibre17">Increase the max TCP send and receive socket buffer size:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.core.rmem_max<br class="calibre7"/>net.core.rmem_max = 212992<br class="calibre7"/>root@kvm:~# sysctl net.core.wmem_max<br class="calibre7"/>net.core.wmem_max = 212992<br class="calibre7"/>root@kvm:~# sysctl net.core.rmem_max=33554432<br class="calibre7"/></strong><strong class="calibre4">net.core.rmem_max = 33554432<br class="calibre7"/></strong><strong class="calibre4">root@kvm:~# sysctl net.core.wmem_max=33554432<br class="calibre7"/></strong><strong class="calibre4">net.core.wmem_max = 33554432<br class="calibre7"/></strong><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="2" class="calibre18">
<li value="2" class="calibre17">Increase the TCP buffer limits: min, default, and max number of bytes. Set max to 16 MB for 1 GE NIC, and 32 M or 54 M for 10 GE NIC:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_rmem</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_rmem = 4096 87380 6291456</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_wmem</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_wmem = 4096 16384 4194304</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_rmem="4096 87380 33554432"</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_rmem = 4096 87380 33554432</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_wmem="4096 65536 33554432"</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_wmem = 4096 65536 33554432</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="3" class="calibre18">
<li value="3" class="calibre17">Ensure that TCP window scaling is enabled:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_window_scaling</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_window_scaling = 1</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="4" class="calibre18">
<li value="4" class="calibre17">To help increase TCP throughput with 1 GB NICs or larger, increase the length of the transmit queue of the network interface. For paths with more than 50 ms RTT, a value of 5000-10000 is recommended:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# ifconfig eth0 txqueuelen 5000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="5" class="calibre18">
<li value="5" class="calibre17">Reduce the <kbd class="calibre13">tcp_fin_timeout</kbd> value:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_fin_timeout</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_fin_timeout = 60</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_fin_timeout=30</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_fin_timeout = 30</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="6" class="calibre18">
<li value="6" class="calibre17">Reduce the <kbd class="calibre13">tcp_keepalive_intvl</kbd> value:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_keepalive_intvl</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_keepalive_intvl = 75</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_keepalive_intvl=30</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_keepalive_intvl = 30</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="7" class="calibre18">
<li value="7" class="calibre17">Enable fast recycling of <kbd class="calibre13">TIME_WAIT</kbd> sockets. The default value is 0 (disabled):</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_tw_recycle</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_tw_recycle = 0</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_tw_recycle=1</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_tw_recycle = 1</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="8" class="calibre18">
<li value="8" class="calibre17">Enable the reusing of sockets in  the <kbd class="calibre13">TIME_WAIT</kbd> state for new connections. The default value is 0 (disabled):</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_tw_reuse</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_tw_reuse = 0</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_tw_reuse=1</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_tw_reuse = 1</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="9" class="calibre18">
<li value="9" class="calibre17">Starting with kernel version 2.6.13, Linux supports pluggable congestion control algorithms. The congestion control algorithm used is set using the <kbd class="calibre13">sysctl</kbd> variable <kbd class="calibre13">net.ipv4.tcp_congestion_control</kbd>, which is set to bic/cubic by default on Ubuntu. To get a list of congestion control algorithms that are available in your kernel (if you are running 2.6.20 or higher), run the following:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_available_congestion_control</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_available_congestion_control = cubic reno</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="10" class="calibre18">
<li value="10" class="calibre17">To enable more pluggable congestion control algorithms, load the kernel modules:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# modprobe tcp_htcp</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# modprobe tcp_bic</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# modprobe tcp_vegas</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# modprobe tcp_westwood</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_available_congestion_control</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_available_congestion_control = cubic reno htcp bic vegas westwood</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="11" class="calibre18">
<li value="11" class="calibre17">For long, fast paths, it is usually better to use cubic or htcp algorithms. Cubic is the default for a number of Linux distributions, but if it is not the default on your system, you can do the following:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_congestion_control</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_congestion_control = cubic</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="12" class="calibre18">
<li value="12" class="calibre17">If the hypervisor is overwhelmed with SYN connections, the following options might help in reducing the impact:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_max_syn_backlog</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_max_syn_backlog = 2048</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_max_syn_backlog=16384</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_max_syn_backlog = 16384</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_synack_retries</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_synack_retries = 5</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.ipv4.tcp_synack_retries=1</strong><br class="calibre7"/><strong class="calibre4">net.ipv4.tcp_synack_retries = 1</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="13" class="calibre18">
<li value="13" class="calibre17">Having a sufficient number of available file descriptors is quite important, since pretty much everything on Linux is a file. Each network connection uses a file descriptor/socket. To check your current max and available file descriptors, run the following code:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl fs.file-nr</strong><br class="calibre7"/><strong class="calibre4">fs.file-nr = 1280 0 13110746</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="14" class="calibre18">
<li value="14" class="calibre17">To increase the max file descriptors, execute the following:</li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl fs.file-max=10000000</strong><br class="calibre7"/><strong class="calibre4">fs.file-max = 10000000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl fs.file-nr</strong><br class="calibre7"/><strong class="calibre4">fs.file-nr = 1280 0 10000000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>
<ol start="15" class="calibre18">
<li value="15" class="calibre17"><span>If your hypervisor is using stateful iptable rules, the <kbd class="calibre13">nf_conntrack</kbd> kernel module might run out of memory for connection tracking and an error will be logged: <kbd class="calibre13">nf_conntrack: table full, dropping packet</kbd>. </span><span>In order to raise that limit and therefore allocate more memory, you need to calculate how much RAM each connection uses. You can get that information from the proc file <kbd class="calibre13">/proc/slabinfo</kbd>. </span><span>The <kbd class="calibre13">nf_conntrack</kbd> entry shows the active entries, how big each object is, and how many fit in a slab (each slab fits in one or more kernel page, usually 4 K if not using HugePages). Accounting for the overhead of the kernel page size, you can see from the <kbd class="calibre13">slabinfo</kbd> that each</span> <kbd class="calibre13">nf_conntrack</kbd> <span>object takes about 316 bytes (this will differ on different systems). So to track 1 M connections, you'll need to allocate roughly 316 MB of memory:</span></li>
</ol>
<pre class="calibre25">
<strong class="calibre4">root@kvm:~# sysctl net.netfilter.nf_conntrack_count</strong><br class="calibre7"/><strong class="calibre4">net.netfilter.nf_conntrack_count = 23</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl net.netfilter.nf_conntrack_max</strong><br class="calibre7"/><strong class="calibre4">net.netfilter.nf_conntrack_max = 65536</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# sysctl -w net.netfilter.nf_conntrack_max=1000000</strong><br class="calibre7"/><strong class="calibre4">net.netfilter.nf_conntrack_max = 1000000</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~# echo 250000 &gt; /sys/module/nf_conntrack/parameters/hashsize # hashsize = nf_conntrack_max / 4</strong><br class="calibre7"/><strong class="calibre4">root@kvm:~#</strong>
</pre>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    

        <section class="calibre1">

            <header class="calibre1">
                </header><h1 class="header-title" id="calibre_pb_0">How it works...</h1>
            

            <article class="calibre1">
                
<p class="calibre3">In step 1, we increase the maximum send and receive socket buffers. This will allocate more memory to the TCP stack, but on servers with a large amount of memory and many TCP connections, it will ensure that the buffer sizes will be sufficient. A g<span>ood starting point for selecting the default values is the <strong class="calibre4">Bandwidth Delay Product </strong>(<strong class="calibre4">BDP</strong>) based on a measured delay, for example, multiply the bandwidth of the link to the average round trip time to some host.</span></p>
<p class="calibre3">In step 2, we increase the <span>min, default, and max number of bytes used by TCP to regulate send buffer sizes. TCP dynamically adjusts the size of the send buffer from the default values.</span></p>
<p class="calibre3">In step 3, we make sure that window scaling is enabled. TCP window scaling automatically increases the receive window size.</p>
<div class="packt_infobox">For more information on window scaling, please refer to<a href="https://en.wikipedia.org/wiki/TCP_window_scale_option" class="calibre30"> https://en.wikipedia.org/wiki/TCP_window_scale_option</a>.</div>
<p class="calibre3">In step 5, we r<span>educe the</span> <kbd class="calibre13">tcp_fin_timeout</kbd> <span>value which specifies how many seconds to wait for a final FIN packet before the socket is forcibly closed. In steps 6 and 7, we reduce the number of seconds between TCP keep-alive probes and fast recycling of sockets in the <kbd class="calibre13">TIME_WAIT</kbd> state. </span></p>
<p class="calibre3">As a refresher, the following diagram shows the various TCP states a connection can be in:</p>
<div class="cdpaligncenter1"/>
<div class="cdpaligncenter1"><img class="image-border6" src="../images/00013.jpeg"/></div>
<div class="cdpaligncenter1"><span> TCP state diagram</span></div>
<p class="calibre3">In step 8, we enable the reuse of sockets in the <kbd class="calibre13">TIME_WAIT</kbd> state only for new connections. On hosts with large numbers of KVM instances, this might have a significant impact on how fast new connections can be established.</p>
<p class="calibre3">In steps 9 and 10, we enable various congestion control algorithms. The choice of congestion control algorithms is selected when the kernel is built. In step 11, we select the cubic algorithm, in which the window is a cubic function of time since the last congestion event, with the inflection point set to the window prior to that event. </p>
<div class="packt_infobox">For more information about network congestion-avoidance algorithms, please refer to <a href="https://en.wikipedia.org/wiki/TCP_congestion_control" class="calibre30">https://en.wikipedia.org/wiki/TCP_congestion_control</a>.</div>
<p class="calibre3">On systems experiencing an overwhelming amount of SYN requests, adjusting the maximum number of queued connection requests that have still not received an acknowledgement from the connecting client, using the <kbd class="calibre13">tcp_max_syn_backlog</kbd> and <kbd class="calibre13">tcp_synack_retries</kbd> options, might help. We do that in step 12.</p>
<p class="calibre3">In steps 13 and 14, we increase the maximum number of file descriptors on the system. This helps when a large number of network connections are present because each connection requires a file descriptor.</p>
<p class="calibre3">In the last step, we have the <kbd class="calibre13">nf_conntrack_max</kbd> option. This is useful if we are tracking connections on the hypervisor using the <kbd class="calibre13">nf_conntrack</kbd> kernel module.</p>


            </article>

            <footer class="calibre5">
                
            </footer>

        </section>
    </body></html>