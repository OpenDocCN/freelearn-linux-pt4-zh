- en: Chapter 2. Setting up the CoreOS Lab
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章：设置CoreOS实验室
- en: CoreOS can be deployed in Bare Metal, VMs, or a cloud provider such as Amazon
    AWS or Google GCE. In this chapter, we will cover how to set up the CoreOS development
    environment in Vagrant, Amazon AWS, Google GCE, and Bare Metal. This development
    environment will be used in all the chapters going forward.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS可以部署在裸机、虚拟机或云服务提供商如Amazon AWS或Google GCE上。在本章中，我们将介绍如何在Vagrant、Amazon
    AWS、Google GCE和裸机上设置CoreOS开发环境。这个开发环境将在接下来的所有章节中使用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Cloud-config for CoreOS
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS的Cloud-config
- en: CoreOS with Vagrant
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS与Vagrant
- en: CoreOS with Amazon AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS与Amazon AWS
- en: CoreOS with Google GCE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS与Google GCE
- en: The CoreOS installation on Bare Metal.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS在裸机上的安装。
- en: The basic debugging of the CoreOS cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS集群的基本调试
- en: 'Different CoreOS deployment options are covered here because of the following
    reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍了不同的CoreOS部署选项，原因如下：
- en: Vagrant with Virtualbox is useful for users who don't have a cloud account.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Vagrant和Virtualbox对于没有云账户的用户来说很有用。
- en: For some users, using a local machine might not be possible as VMs occupy a
    lot of resources, and using a cloud-based VM is the best choice in this case.
    As AWS and GCE are the most popular cloud providers, I chose these two.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些用户来说，使用本地机器可能不可行，因为虚拟机占用了大量资源，这时使用基于云的虚拟机是最佳选择。由于AWS和GCE是最流行的云提供商，因此我选择了这两者。
- en: Bare metal installation would be preferable for traditional in-house data centers.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于传统的内部数据中心，裸机安装是首选。
- en: In this book's examples, I have used one of the three approaches (Vagrant, AWS,
    and GCE) based on the simplicity of one of the approaches, better integration
    with one of the three approaches, or because of issues with a particular approach.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本书的示例中，我使用了三种方法中的一种（Vagrant、AWS和GCE），根据某种方法的简便性、与其中一种方法的更好集成，或者由于某种方法存在问题。
- en: Cloud-config
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config
- en: Cloud-config is a declarative configuration file format that is used by many
    Linux distributions to describe the initial server configuration. The cloud-init
    program takes care of parsing `cloud-config` during server initialization and
    configures the server appropriately. The `cloud-config` file provides you with
    a default configuration for the CoreOS node.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config是一种声明式配置文件格式，许多Linux发行版使用它来描述初始服务器配置。cloud-init程序负责在服务器初始化期间解析`cloud-config`并适当地配置服务器。`cloud-config`文件为CoreOS节点提供了默认配置。
- en: The CoreOS cloud-config file format
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS的cloud-config文件格式
- en: The `coreos-cloudinit` program takes care of the default configuration of the
    CoreOS node during bootup using the `cloud-config` file. The `cloud-config` file
    describes the configuration in the YAML format ([http://www.yaml.org/](http://www.yaml.org/)).
    CoreOS cloud-config follows the `cloud-config` specification with some CoreOS-specific
    options. The link, [https://coreos.com/os/docs/latest/cloud-config.html](https://coreos.com/os/docs/latest/cloud-config.html)
    covers the details of CoreOS `cloud-config`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`coreos-cloudinit`程序负责在启动时使用`cloud-config`文件进行CoreOS节点的默认配置。`cloud-config`文件使用YAML格式描述配置（[http://www.yaml.org/](http://www.yaml.org/)）。CoreOS
    cloud-config遵循`cloud-config`规范，并具有一些CoreOS特定的选项。链接，[https://coreos.com/os/docs/latest/cloud-config.html](https://coreos.com/os/docs/latest/cloud-config.html)，涵盖了CoreOS
    `cloud-config`的详细信息。'
- en: The main sections of cloud-config
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: cloud-config的主要部分
- en: 'The following are the main sections in the CoreOS `cloud-config` YAML file:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是CoreOS `cloud-config` YAML文件中的主要部分：
- en: 'CoreOS:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS：
- en: 'Etcd2: config parameters for etcd2'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd2：etcd2的配置参数
- en: 'Fleet: config parameters for Fleet'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fleet：Fleet的配置参数
- en: 'Flannel: config parameters for Flannel'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel：Flannel的配置参数
- en: 'Locksmith: config parameters for Locksmith'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locksmith：Locksmith的配置参数
- en: 'Update: config parameters for automatic updates'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新：自动更新的配置参数
- en: 'Units: Systemd units that need to be started'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元：需要启动的Systemd单元
- en: '`ssh_authorized_keys`: Public keys for the core user'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssh_authorized_keys`：Core用户的公钥'
- en: '`hostname`: Hostname for the CoreOS system'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hostname`：CoreOS系统的主机名'
- en: '`users`: Additional user account and group details'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users`：附加的用户帐户和组详细信息'
- en: '`write_files`: Creates files with specified user data'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_files`：使用指定的用户数据创建文件'
- en: '`manage_etc_hosts`: Specifies the contents of `/etc/hosts`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manage_etc_hosts`：指定`/etc/hosts`的内容'
- en: A sample CoreOS cloud-config
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS cloud-config的示例
- en: 'The following is a sample `cloud-config` file for a single node CoreOS cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个单节点CoreOS集群的`cloud-config`文件示例：
- en: '`#cloud-config coreos:   etcd2:     # Static cluster     name: etcdserver     initial-cluster-token: etcd-cluster-1
        initial-cluster: etcdserver=http://$private_ipv4:2380     initial-cluster-state: new
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4     metadata: "role=services"   flannel:
        interface: $public_ipv4   update:       reboot-strategy: "etcd-lock"   units:
        # To use etcd2, comment out the above service and uncomment these     # Note: this requires a release that contains etcd2
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: flanneld.service       drop-ins:         - name: 50-network-config.conf
              content: |             [Service]             ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''
          command: start     - name: docker-tcp.socket       command: start       enable: true
          content: |         [Unit]         Description=Docker Socket for the API
            [Socket]         ListenStream=2375         Service=docker.service         BindIPv6Only=both
            [Install]         WantedBy=sockets.target  write_files:   - path: "/etc/motd"
        permissions: "0644"     owner: "root"     content: |       --- My CoreOS Cluster ---`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     # 静态集群     name: etcdserver     initial-cluster-token: etcd-cluster-1
        initial-cluster: etcdserver=http://$private_ipv4:2380     initial-cluster-state: new
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # 监听官方端口和传统端口     # 如果您的应用程序不依赖于传统端口，则可以省略这些端口     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4     metadata: "role=services"   flannel:     interface: $public_ipv4
      update:       reboot-strategy: "etcd-lock"   units:     # 要使用etcd2，注释掉上述服务并取消注释这些
        # 注意：这需要包含etcd2的版本     - name: etcd2.service       command: start     - name: fleet.service
          command: start     - name: flanneld.service       drop-ins:         - name: 50-network-config.conf
              content: |             [Service]             ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''
          command: start     - name: docker-tcp.socket       command: start       enable: true
          content: |         [Unit]         Description=用于API的Docker Socket         [Socket]
            ListenStream=2375         Service=docker.service         BindIPv6Only=both
            [Install]         WantedBy=sockets.target  write_files:   - path: "/etc/motd"
        permissions: "0644"     owner: "root"     content: |       --- 我的CoreOS集群 ---'
- en: 'The following are some notes on the preceding `cloud-config`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于前面`cloud-config`的一些说明：
- en: 'The `etcd2` section specifies the configuration parameters for the `etcd2`
    service. In this case, we specify parameters needed to start `etcd` on the CoreOS
    node. The `public_ipv4` and `private_ipv4` environment variables are substituted
    with the CoreOS node''s IP address. As there is only one node, we use the static
    cluster definition approach rather than using a discovery token. Based on the
    specified parameters, the `20-cloudinit.conf` Drop-In Unit gets created in `/run/systemd/system/etcd2.service.d`
    with the following environment variables:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd2`部分指定了`etcd2`服务的配置参数。在这个例子中，我们指定了启动`etcd`所需的参数，并在CoreOS节点上进行配置。`public_ipv4`和`private_ipv4`环境变量将被替换为CoreOS节点的IP地址。由于只有一个节点，我们使用静态集群定义方式，而不是使用发现令牌。根据指定的参数，`20-cloudinit.conf`
    Drop-In单元将在`/run/systemd/system/etcd2.service.d`目录下创建，包含以下环境变量：'
- en: '`[Service] Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.17.8.101:2379"
    Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.17.8.101:2380" Environment="ETCD_INITIAL_CLUSTER=etcdserver=http://172.17.8.101:2380"
    Environment="ETCD_INITIAL_CLUSTER_STATE=new" Environment="ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster-1"
    Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001"
    Environment="ETCD_LISTEN_PEER_URLS=http://172.17.8.101:2380,http://172.17.8.101:7001"
    Environment="ETCD_NAME=etcdserver"`'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.17.8.101:2379"
    Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.17.8.101:2380" Environment="ETCD_INITIAL_CLUSTER=etcdserver=http://172.17.8.101:2380"
    Environment="ETCD_INITIAL_CLUSTER_STATE=new" Environment="ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster-1"
    Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001"
    Environment="ETCD_LISTEN_PEER_URLS=http://172.17.8.101:2380,http://172.17.8.101:7001"
    Environment="ETCD_NAME=etcdserver"`'
- en: 'The `fleet` section specifies the configuration parameters for the `fleet`
    service, including any metadata for the node. The `20-cloudinit.conf` Drop-In
    Unit gets created in `/run/systemd/system/fleet.service.d` with the following
    environment variables:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fleet` 部分指定 `fleet` 服务的配置参数，包括节点的任何元数据。`20-cloudinit.conf` Drop-In 单元将被创建在
    `/run/systemd/system/fleet.service.d`，并包含以下环境变量：'
- en: '`[Service] Environment="FLEET_METADATA=role=services" Environment="FLEET_PUBLIC_IP=172.17.8.101"`'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] Environment="FLEET_METADATA=role=services" Environment="FLEET_PUBLIC_IP=172.17.8.101"`'
- en: 'The update section specifies the update strategy for the CoreOS node. This
    gets updated in the node as `/etc/coreos/update.conf`:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新部分指定了 CoreOS 节点的更新策略。它将在节点中更新为 `/etc/coreos/update.conf`：
- en: '`GROUP=alpha REBOOT_STRATEGY=etcd-lock`'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`GROUP=alpha REBOOT_STRATEGY=etcd-lock`'
- en: 'The `units` section starts `etcd2`, `fleet`, and `flannel`. For `flannel`,
    we have a drop-in unit to update the subnet to be used for containers created
    with the Flannel network service. The `50-network-config.conf` Drop-in unit gets
    created in `/etc/systemd/system/flanneld.service.d`:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units` 部分启动 `etcd2`、`fleet` 和 `flannel`。对于 `flannel`，我们有一个 Drop-In 单元来更新用于创建容器的子网，该容器将与
    Flannel 网络服务一起使用。`50-network-config.conf` Drop-In 单元将被创建在 `/etc/systemd/system/flanneld.service.d`：'
- en: '`[Service] ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''`'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{
    "Network": "10.1.0.0/16" }''`'
- en: The `docker-tcp.socket` unit in the units section is a new `systemd` unit, and
    we specified the service content that allows for the docker daemon to be exposed
    through port `2375`. The unit will be created in `/etc/systemd/system`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元部分中的 `docker-tcp.socket` 是一个新的 `systemd` 单元，我们指定了允许通过端口 `2375` 暴露 docker 守护进程的服务内容。该单元将被创建在
    `/etc/systemd/system`。
- en: 'The `write_files` section can be used to create any static files. An example
    could be a hello text when a user logs in, which we can do with `/etc/motd`. The
    hello message would look as follows:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_files` 部分可用于创建任何静态文件。例如，当用户登录时显示问候文本，我们可以使用 `/etc/motd` 来实现。问候消息将如下所示：'
- en: '`Last login: Tue Sep 15 14:15:04 2015 from 10.0.2.2 --- My CoreOS Cluster ---
    core@core01 ~ $`'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Last login: Tue Sep 15 14:15:04 2015 from 10.0.2.2 --- My CoreOS Cluster ---
    core@core01 ~ $`'
- en: The cloud-config validator
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config 验证器
- en: Cloud-config uses the YAML syntax. YAML is a human-readable data serialization
    format and uses indents and spaces for alignment. It is better to validate the
    `cloud-config` YAML configuration files before using them. There are two options
    to validate the CoreOS `cloud-config`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config 使用 YAML 语法。YAML 是一种人类可读的数据序列化格式，并使用缩进和空格进行对齐。最好在使用 `cloud-config`
    YAML 配置文件之前进行验证。验证 CoreOS `cloud-config` 有两种方式。
- en: A hosted validator
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个托管的验证器
- en: Use this CoreOS-provided link ([https://coreos.com/validate/](https://coreos.com/validate/))
    to validate `cloud-config`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CoreOS 提供的链接 ([https://coreos.com/validate/](https://coreos.com/validate/))
    来验证 `cloud-config`。
- en: Here is an example of a valid and invalid `cloud-config` and the results using
    the validator.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个有效和无效的 `cloud-config` 示例，以及使用验证器的结果。
- en: Valid cloud-config
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的 cloud-config
- en: 'As we can see in the following screenshot, the validator says that the following
    `cloud-config` is valid:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示，验证器表示以下 `cloud-config` 是有效的：
- en: '![](img/00100.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpg)'
- en: Invalid cloud-config
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无效的 cloud-config
- en: 'Here, we can see that the validator has specified that `-` is missing in line
    14\. YAML uses spaces for the delimiting, so we need to make sure that the number
    of spaces is exact:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到验证器已指定第 14 行缺少 `-`。YAML 使用空格进行分隔，所以我们需要确保空格的数量完全正确：
- en: '![](img/00167.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.jpg)'
- en: The cloudinit validator
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: cloudinit 验证器
- en: 'We can use the `coreos-cloudinit --validate` option available in CoreOS to
    validate the cloud-config. Let''s look at the following sample `cloud-config`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 CoreOS 中可用的 `coreos-cloudinit --validate` 选项来验证 cloud-config。让我们看一下下面的示例
    `cloud-config`：
- en: '![](img/00109.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpg)'
- en: 'When we validate this, we get no errors, as shown in the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们验证时，我们没有得到错误，正如以下截图所示：
- en: '![](img/00112.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpg)'
- en: 'Now, let''s try the same `cloud-config` with errors. Here, we have `|` missing
    in the content line:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用带有错误的相同 `cloud-config`。这里，在内容行中缺少了 `|`：
- en: '![](img/00017.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpg)'
- en: 'We see the following errors when we validate:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们验证时，我们看到以下错误：
- en: '![](img/00120.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpg)'
- en: Executing cloud-config
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 cloud-config
- en: 'There are two `cloud-config` files that are run as part of the CoreOS bootup:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个 `cloud-config` 文件在 CoreOS 启动过程中作为一部分运行：
- en: System cloud-config
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统 cloud-config
- en: User cloud-config
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户 cloud-config
- en: System `cloud-config` is given by the provider (such as Vagrant or AWS) and
    is embedded as part of the CoreOS provider image. Different providers such as
    Vagrant, AWS, and GCE have their `cloud-config` present in `/usr/share/oem/cloud-config.yaml`.
    This `cloud-config` is responsible for setting up the provider-specific configurations,
    such as networking, SSH keys, mount options, and so on. The `coreos-cloudinit`
    program first executes system `cloud-config` and then user `cloud-config`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的 `cloud-config` 由提供商（如 Vagrant 或 AWS）提供，并作为 CoreOS 提供商镜像的一部分嵌入其中。不同的提供商（如
    Vagrant、AWS 和 GCE）在 `/usr/share/oem/cloud-config.yaml` 中有各自的 `cloud-config`。该
    `cloud-config` 负责设置提供商特定的配置项，如网络、SSH 密钥、挂载选项等。`coreos-cloudinit` 程序首先执行系统 `cloud-config`，然后执行用户
    `cloud-config`。
- en: Depending on the provider, user `cloud-config` can be supplied using either
    config-drive or an internal user data service. Config-drive is a universal way
    to provide `cloud-config` by mounting a read-only partition that contains `cloud-config`
    to the host machine. Rackspace uses config-drive to get user `cloud-config`, and
    AWS uses its internal user data service to fetch the user data and doesn't rely
    on config-drive. In the Vagrant scenario, Vagrantfile takes care of copying the
    `cloud-config` to the CoreOS VM.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供商的不同，可以使用 config-drive 或内部用户数据服务提供用户的 `cloud-config`。Config-drive 是通过挂载一个只读分区，提供包含
    `cloud-config` 的数据到主机机器上的通用方式。Rackspace 使用 config-drive 获取用户的 `cloud-config`，而
    AWS 则使用其内部的用户数据服务来获取用户数据，不依赖 config-drive。在 Vagrant 环境中，Vagrantfile 负责将 `cloud-config`
    复制到 CoreOS 虚拟机中。
- en: The CoreOS cluster with Vagrant
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Vagrant 搭建 CoreOS 集群
- en: 'Vagrant can be installed in Windows or Linux. The following is my development
    environment for the Vagrant CoreOS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant 可以在 Windows 或 Linux 上安装。以下是我用于 Vagrant CoreOS 的开发环境：
- en: 'Windows 7: I use mysysgit ([https://git-for-windows.github.io/](https://git-for-windows.github.io/))
    to get a Linux-like shell for Windows'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows 7：我使用 mysysgit ([https://git-for-windows.github.io/](https://git-for-windows.github.io/))
    为 Windows 获取类 Linux 的命令行。
- en: 'Vagrant 1.7.2: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vagrant 1.7.2: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html)'
- en: 'Virtualbox 4.3.28: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Virtualbox 4.3.28: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)'
- en: For a few of the examples in the book, I have used Vagrant to run CoreOS inside
    a Linux VM running on top of Windows laptop with Virtualbox.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的一些示例中，我使用 Vagrant 在 Windows 笔记本上的 Virtualbox 上运行 Linux 虚拟机中的 CoreOS。
- en: Steps to start the Vagrant environment
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Vagrant 环境的步骤
- en: 'Check out the coreos-vagrant code base:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 coreos-vagrant 的代码库：
- en: '`git clone https://github.com/coreos/coreos-vagrant.git`'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/coreos-vagrant.git`'
- en: 'Copy the sample `user-data` and `config.rb` files in the coreos-vagrant directory:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制 `user-data` 和 `config.rb` 的示例文件到 coreos-vagrant 目录中：
- en: '`cd coreos-vagrant``mv user-data.sample user-data``mv config.rb.sample config.rb`'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`cd coreos-vagrant``mv user-data.sample user-data``mv config.rb.sample config.rb`'
- en: Edit `Vagrantfile`, `user-data`, and `config.rb` based on your need.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要编辑 `Vagrantfile`、`user-data` 和 `config.rb`。
- en: 'Start the CoreOS cluster:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 CoreOS 集群：
- en: '`Vagrant up`'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Vagrant up`'
- en: 'SSH to the individual node:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SSH 到单独的节点：
- en: '`Vagrant ssh core-<id>`'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Vagrant ssh core-<id>`'
- en: Important files to be modified
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 需要修改的重要文件
- en: The following are important files to be modified along with commonly needed
    modifications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要修改的重要文件以及常见的修改项。
- en: Vagrantfile
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrantfile
- en: 'Vagrant sets up the VM environment based on the configuration defined in `Vagrantfile`.
    The following are certain relevant functionalities in the CoreOS context:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant 根据 `Vagrantfile` 中定义的配置来设置虚拟机环境。以下是在 CoreOS 环境中一些相关的功能：
- en: The version of CoreOS software to be used is specified using `update_channel`.
    The version can be specified as `stable`, `beta`, and `alpha`. More details on
    CoreOS software versions are covered in [Chapter 3](index_split_075.html#filepos216260),
    CoreOS Autoupdate.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的 CoreOS 软件版本通过 `update_channel` 来指定。版本可以指定为 `stable`（稳定版）、`beta`（测试版）和 `alpha`（开发版）。关于
    CoreOS 软件版本的更多细节，请参阅 [第 3 章](index_split_075.html#filepos216260)，CoreOS 自动更新。
- en: CPU and memory for the VM and ports to be exposed from the VM.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟机的 CPU 和内存以及需要暴露的端口。
- en: SSH key management.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSH 密钥管理。
- en: User-data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用户数据
- en: The `user-data` file is essentially the `cloud-config` file that specifies the
    discovery token, environment variables, and list of units to be started by default.
    Vagrant copies the `cloud-config` file to `/var/lib/coreos-vagrant/vagrantfile-user-data`
    inside the VM. The `coreos-cloudinit` reads `vagrantfile-user-data` on every boot
    and uses it to create the machine's user data file.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`user-data` 文件实际上是 `cloud-config` 文件，它指定了发现令牌、环境变量和默认启动的单元列表。Vagrant 会将 `cloud-config`
    文件复制到虚拟机内的 `/var/lib/coreos-vagrant/vagrantfile-user-data` 位置。`coreos-cloudinit`
    会在每次启动时读取 `vagrantfile-user-data`，并利用它来创建机器的用户数据文件。'
- en: Config.rb
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Config.rb
- en: The `config.rb` file specifies the count of CoreOS nodes. This file also provides
    you with an option to automatically generate a discovery token. Some options here
    overlap with the `Vagrantfile` like image version.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.rb` 文件指定了 CoreOS 节点的数量。该文件还提供了一个选项，可以自动生成发现令牌。这里的一些选项与 `Vagrantfile`
    中的设置重叠，比如镜像版本。'
- en: Vagrant – a three-node cluster with dynamic discovery
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个包含动态发现功能的三节点集群
- en: Here, we will create a three-node CoreOS cluster with `etcd2` and `fleet` running
    on each node and nodes discovering each other dynamically.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个三节点的 CoreOS 集群，每个节点上运行 `etcd2` 和 `fleet`，并且节点之间将动态发现彼此。
- en: Generating a discovery token
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生成发现令牌
- en: When we start a multinode CoreOS cluster, there needs to be a bootstrapping
    mechanism to discover the cluster members. For this, we generate a token specifying
    the number of initial nodes in the cluster as an argument. Each node needs to
    be started with this discovery token. Etcd will use the discovery token to put
    all the nodes with the same discovery token as part of the initial cluster. CoreOS
    runs the service to provide the discovery token from its central servers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动一个多节点的 CoreOS 集群时，需要有一个引导机制来发现集群成员。为此，我们生成一个令牌，并将集群中初始节点的数量作为参数传入。每个节点启动时都需要使用此发现令牌。Etcd
    会使用发现令牌将所有具有相同令牌的节点作为初始集群的一部分。CoreOS 运行该服务，从其中央服务器提供发现令牌。
- en: 'There are two approaches to generate a discovery token:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种生成发现令牌的方法：
- en: 'From the browser: `https://discovery.etcd.io/new?size=3`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从浏览器访问：`https://discovery.etcd.io/new?size=3`
- en: 'Using curl: `curl https://discovery.etcd.io/new?size=3`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 curl: `curl https://discovery.etcd.io/new?size=3`'
- en: 'The following is a curl example with a generated discovery token. This token
    needs to be copied to `user-data`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 curl 示例，包含一个生成的发现令牌。该令牌需要复制到 `user-data`：
- en: '![](img/00358.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00358.jpg)'
- en: Steps for cluster creation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 集群创建步骤
- en: 'The following is a `cloud-config` user data with the updated discovery token
    that we generated in the preceding section along with the necessary environment
    variables and service units. All three nodes will use this `cloud-config`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是包含我们在上一节中生成的更新发现令牌的 `cloud-config` 用户数据，以及必要的环境变量和服务单元。所有三个节点将使用此 `cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     #generate a new token for each unique cluster from https://discovery.etcd.io/new
        discovery: https://discovery.etcd.io/9a6b7af06c8a677b4e5f76ae9ce0da9c     # multi-region and multi-cloud deployments need to use $public_ipv4
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:
        # Note: this requires a release that contains etcd2     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     #为每个唯一集群从 https://discovery.etcd.io/new
    生成一个新的令牌     discovery: https://discovery.etcd.io/9a6b7af06c8a677b4e5f76ae9ce0da9c
        # 多区域和多云部署需要使用 $public_ipv4     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # 在官方端口和传统端口上监听
        # 如果你的应用程序不依赖于传统端口，可以省略它们     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     # 注意：这需要一个包含
    etcd2 的版本     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: We need to update `num_instances` to `3` in `config.rb` and then perform `vagrant
    up`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 `config.rb` 文件中将 `num_instances` 更新为 `3`，然后执行 `vagrant up`。
- en: To verify the basic cluster operation, we can check the following output, where
    we should see the cluster members.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证基本集群操作，我们可以检查以下输出，应该能够看到集群成员。
- en: 'The following `etcdctl` member output shows the three cluster members:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`etcdctl`成员输出，显示了三个集群成员：
- en: '![](img/00362.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00362.jpg)'
- en: 'The following fleet member output shows the three cluster members:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是fleet成员输出，显示了三个集群成员：
- en: '![](img/00364.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00364.jpg)'
- en: Vagrant – a three-node cluster with static discovery
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个具有静态发现的三节点集群
- en: Here, we will create a three-node CoreOS cluster and use a static approach to
    mention its cluster neighbors. In the dynamic discovery approach, we need to use
    a discovery token to discover the cluster members. Static discovery can be used
    for scenarios where access to the token server is not available to cluster members,
    and the cluster member IP addresses are known in advance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个三节点CoreOS集群，并使用静态方式指定其集群邻居。在动态发现方法中，我们需要使用发现令牌来发现集群成员。静态发现可以用于集群成员无法访问令牌服务器，并且集群成员的IP地址已知的场景。
- en: 'Perform the following steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: First, we need to create three separate instances of the CoreOS Vagrant environment
    by performing `git clone` separately for each node.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要通过分别对每个节点执行`git clone`来创建三个独立的CoreOS Vagrant实例。
- en: The `config.rb` file must be updated for each node with `num_instances` set
    to one.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须为每个节点更新`config.rb`文件，并将`num_instances`设置为1。
- en: Vagrantfile should be updated for each node so that IP addresses are statically
    assigned as `172.17.8.101` for `core-01`, `172.17.8.102` for `core-02`, and `172.17.8.103`
    for `core-03`. IP addresses should be updated based on your environment.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该更新每个节点的Vagrantfile，以便将IP地址静态分配为`172.17.8.101`（`core-01`）、`172.17.8.102`（`core-02`）和`172.17.8.103`（`core-03`）。IP地址应根据您的环境进行更新。
- en: 'The `cloud-config` user data for the first node is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-01     initial-advertise-peer-urls: http://172.17.8.101:2380
        listen-peer-urls: http://172.17.8.101:2380     listen-client-urls: http://172.17.8.101:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.101:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-01     initial-advertise-peer-urls: http://172.17.8.101:2380
        listen-peer-urls: http://172.17.8.101:2380     listen-client-urls: http://172.17.8.101:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.101:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
- en: 'The `cloud-config` user data for the second node is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-02     initial-advertise-peer-urls: http://172.17.8.102:2380
        listen-peer-urls: http://172.17.8.102:2380     listen-client-urls: http://172.17.8.102:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.102:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-02     initial-advertise-peer-urls: http://172.17.8.102:2380
        listen-peer-urls: http://172.17.8.102:2380     listen-client-urls: http://172.17.8.102:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.102:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
- en: 'The `cloud-config` user data for the third node is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-03     initial-advertise-peer-urls: http://172.17.8.103:2380
        listen-peer-urls: http://172.17.8.103:2380     listen-client-urls: http://172.17.8.103:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.103:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-03     initial-advertise-peer-urls:
    http://172.17.8.103:2380     listen-peer-urls: http://172.17.8.103:2380     listen-client-urls:
    http://172.17.8.103:2379,http://127.0.0.1:2379     advertise-client-urls: http://172.17.8.103:2379
        initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command:
    start     - name: fleet.service       command: start`'
- en: We need to perform `vagrant up` separately for each of the nodes. We should
    see the cluster member list updated in both the `etcdctl member list` and `fleetctl
    list-machines` outputs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分别为每个节点执行`vagrant up`命令。我们应该在`etcdctl member list`和`fleetctl list-machines`的输出中看到集群成员列表的更新。
- en: Vagrant – a production cluster with three master nodes and three worker nodes
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个包含三个主节点和三个工作节点的生产集群
- en: 'In [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview, we covered
    the CoreOS cluster architecture. A production cluster has one set of nodes (called
    master) to run critical services, and another set of nodes (called worker) to
    run application services. In this example, we create three master nodes running
    `etcd` and other critical services and another three worker nodes. Etcd in the
    worker nodes will proxy to the master nodes. Worker nodes will be used for user-created
    services while master nodes will be used for system services. This avoids resource
    contention. The following are the steps needed for this creation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](index_split_023.html#filepos77735)，CoreOS概述中，我们介绍了CoreOS集群架构。生产集群有一组节点（称为主节点）用于运行关键服务，另一组节点（称为工作节点）用于运行应用服务。在本示例中，我们创建了三个主节点来运行`etcd`和其他关键服务，以及另外三个工作节点。工作节点中的etcd将代理到主节点。工作节点将用于用户创建的服务，而主节点将用于系统服务。这避免了资源争用。以下是创建过程所需的步骤：
- en: Create a Vagrant three-node cluster for the master and a three-node cluster
    for the worker.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含三节点主集群和三节点工作集群的Vagrant集群。
- en: Update Vagrantfile to use non-conflicting IP address ranges between the master
    and worker nodes.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新Vagrantfile，确保主节点和工作节点之间的IP地址范围不冲突。
- en: Use the dynamic discovery token approach to create a token for the three-node
    clusters and update the `cloud-config` user data for both the master and worker
    nodes to the same token. We have specified the token size as `3` as worker nodes
    don't run `etcd`.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用动态发现令牌的方法为三节点集群创建令牌，并更新主节点和工作节点的`cloud-config`用户数据，确保令牌相同。我们已经将令牌大小指定为`3`，因为工作节点不运行`etcd`。
- en: 'The following is the user data for the master cluster:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是主节点集群的用户数据：
- en: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=master"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls:
    http://$private_ipv4:2380     # 在官方端口和遗留端口上监听     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=master"      public-ip: $public_ipv4   units:     - name:
    etcd2.service       command: start     - name: fleet.service       command: start`'
- en: 'The following is the user data for the worker cluster. The discovery token
    needs to be the same for the master and worker clusters:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是工作节点集群的用户数据。主节点和工作节点的发现令牌需要相同：
- en: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=worker"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # 在官方端口和传统端口上监听     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=worker"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
- en: The only difference between the master and worker user data is in the metadata
    used for fleet. In this example, we used `role` as `master` for the master cluster
    and `role` as `worker` for the worker cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点和工作节点用户数据之间的唯一区别是在 fleet 中使用的元数据。在这个示例中，我们为主节点集群使用 `role` 设置为 `master`，为工作节点集群使用
    `role` 设置为 `worker`。
- en: Let's look at the `etcdctl` member list and fleet machine list. The following
    output will be the same across all the nodes in the master and worker cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看 `etcdctl` 成员列表和 fleet 机器列表。以下输出在主节点和工作节点集群中是相同的。
- en: 'The `etcdctl` member output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl` 成员的输出如下：'
- en: '![](img/00306.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00306.jpg)'
- en: 'The fleet member output is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: fleet 成员的输出如下：
- en: '![](img/00370.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00370.jpg)'
- en: 'The following is the `journalctl –u etcd2.service` output on worker nodes that
    show worker nodes proxying to master nodes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `journalctl –u etcd2.service` 在工作节点上的输出，显示工作节点代理到主节点的情况：
- en: '![](img/00372.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00372.jpg)'
- en: A CoreOS cluster with AWS
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 的 CoreOS 集群
- en: 'Amazon AWS provides you with a public cloud service. CoreOS can be run on the
    VMs provided by AWS. The following are some prerequisites for this setup:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 AWS 为您提供公共云服务。CoreOS 可以在 AWS 提供的虚拟机（VM）上运行。以下是此设置的一些前提条件：
- en: You need an account in AWS. AWS provides you with a one-year trial account for
    free.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要一个 AWS 账户。AWS 提供一年免费的试用账户。
- en: Create and download a key pair. The key pair is needed to SSH to the nodes.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并下载密钥对。密钥对用于通过 SSH 连接到节点。
- en: The AWS interface can be accessed through the AWS console, which is a GUI interface,
    or by AWS CLI. AWS CLI ([http://aws.amazon.com/cli/](http://aws.amazon.com/cli/))
    can be installed in either Windows or Linux.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过 AWS 控制台（GUI 界面）或 AWS CLI 访问 AWS 接口。AWS CLI（[http://aws.amazon.com/cli/](http://aws.amazon.com/cli/)）可以在
    Windows 或 Linux 系统上安装。
- en: The following are two approaches of creating a CoreOS cluster with AWS.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 AWS 创建 CoreOS 集群的两种方法。
- en: AWS – a three-node cluster using Cloudformation
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AWS – 使用 Cloudformation 创建三节点集群
- en: 'Cloudformation is an AWS orchestration tool to manage a collection of AWS resources
    that include compute, storage, and networking. The link, [https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template](https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template),
    has the template file for the CoreOS cluster. The following are some of the key
    sections in the template:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudformation 是 AWS 的一个编排工具，用于管理包括计算、存储和网络在内的一组 AWS 资源。以下链接，[https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template](https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template)，提供了
    CoreOS 集群的模板文件。以下是模板中的一些关键部分：
- en: The AMI image ID to be used based on the region
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于区域的 AMI 镜像 ID
- en: The EC2 Instance type
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EC2 实例类型
- en: The security group configuration
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全组配置
- en: The CoreOS cluster size including the minimum and maximum size to autoscale
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS 集群的大小，包括自动扩展的最小和最大大小
- en: The initial cloud-config to be used
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用的初始 cloud-config
- en: 'For the following example, I modified the template to use `t2.micro` instead
    of `m3.medium` for the instance size. The following CLI can be used to create
    a three-node CoreOS cluster using `Cloudformation`. The discovery token in the
    below command needs to be updated with the generated token for your case:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下示例，我修改了模板，使用 `t2.micro` 代替 `m3.medium` 作为实例大小。以下 CLI 可用于使用 `Cloudformation`
    创建一个三节点的 CoreOS 集群。下面命令中的 discovery token 需要使用为您的情况生成的令牌来更新：
- en: '`aws cloudformation create-stack \     --stack-name coreos-test \     --template-body file://mycoreos-stable-hvm.template \
        --capabilities CAPABILITY_IAM \     --tags Key=Name,Value=CoreOS \     --parameters \      ParameterKey=DiscoveryURL,ParameterValue="https://discovery.etcd.io/925755234ab82c1ef7bcfbbacdd8c088" \
            ParameterKey=KeyPair,ParameterValue="keyname"`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws cloudformation create-stack \     --stack-name coreos-test \     --template-body file://mycoreos-stable-hvm.template \
        --capabilities CAPABILITY_IAM \     --tags Key=Name,Value=CoreOS \     --parameters \      ParameterKey=DiscoveryURL,ParameterValue="https://discovery.etcd.io/925755234ab82c1ef7bcfbbacdd8c088" \
            ParameterKey=KeyPair,ParameterValue="keyname"`'
- en: 'The following is the output of the successful stack using `aws cloudformation
    list-stacks`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 `aws cloudformation list-stacks` 成功创建堆栈后的输出：
- en: '![](img/00347.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00347.jpg)'
- en: After the preceding step, we can see that members are getting discovered successfully
    by both `etcd` and `fleet`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤完成后，我们可以看到成员已经成功地被 `etcd` 和 `fleet` 发现。
- en: AWS – a three-node cluster using AWS CLI
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: AWS – 使用 AWS CLI 创建一个三节点集群
- en: 'The following are some prerequisites to create a CoreOS cluster in AWS using
    AWS CLI:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 AWS CLI 在 AWS 中创建 CoreOS 集群的一些前提条件：
- en: Create a token for a three-node cluster from the discovery token service.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从发现令牌服务创建一个三节点集群的令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
    `2379` and `2380` are needed for the `etcd2` client-to-server and server-to-server
    communication.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露 `ssh`、`icmp`、`2379` 和 `2380` 端口。`2379` 和 `2380` 用于 `etcd2` 客户端与服务器以及服务器间的通信。
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html))
    based on your AWS Zone and update channel. The latest image IDs for different
    AWS Zones get automatically updated in this link.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此链接（[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)）根据你的
    AWS 区域和更新频道来确定 AMI 镜像 ID。不同 AWS 区域的最新镜像 ID 会自动更新在此链接中。
- en: 'The following CLI will create the three-node cluster:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 CLI 命令将创建一个三节点集群：
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos-test" --user-data file://cloud-config.yaml`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos-test" --user-data file://cloud-config.yaml`'
- en: Here, the `ami-85ada4b5` image ID is from the stable update channel. The `coreos-test`
    security group has the necessary ports that need to be exposed outside.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `ami-85ada4b5` 镜像 ID 来自稳定更新频道。`coreos-test` 安全组暴露了需要外部访问的必要端口。
- en: 'The following is the `cloud-config` that I used:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我使用的 `cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     # specify the initial size of your cluster with ?size=X
        discovery: https://discovery.etcd.io/47460367c9b15edffeb49de30cab9354     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     # 指定 集群的初始大小，通过 ?size=X     discovery: https://discovery.etcd.io/47460367c9b15edffeb49de30cab9354
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
- en: 'The following output shows the `etcd` member list and fleet member list with
    three nodes in the cluster:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了包含三节点的集群中的 `etcd` 成员列表和 `fleet` 成员列表：
- en: '![](img/00379.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00379.jpg)'
- en: The same example can be tried from the AWS Console, where we can specify the
    options from the GUI.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的示例可以通过 AWS 控制台进行尝试，在这里我们可以从 GUI 中指定选项。
- en: A CoreOS cluster with GCE
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: GCE 上的 CoreOS 集群
- en: 'Google''s GCE is another public cloud provider like Amazon AWS. CoreOS can
    be run on the VMs provided by GCE. The following are some prerequisites for this
    setup:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的 GCE 是另一家公共云服务提供商，类似于亚马逊 AWS。CoreOS 可以运行在 GCE 提供的虚拟机上。以下是此设置的一些前提条件：
- en: You need a GCE account. GCE provides you with a free trial account for 60 days.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要一个 GCE 账户。GCE 提供 60 天的免费试用账户。
- en: GCE resources can be accessed using gcloud SDK or GCE GUI Console. SDK can be
    downloaded from [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过 gcloud SDK 或 GCE GUI 控制台访问 GCE 资源。SDK 可以从 [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/)
    下载。
- en: A base project in GCE needs to be created under which all the resources reside.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在 GCE 中创建一个基础项目，所有资源都将托管在该项目下。
- en: A security token needs to be created, which is used for SSH access.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要创建一个安全令牌，用于 SSH 访问。
- en: GCE – a three-node cluster using GCE CLI
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: GCE – 使用 GCE CLI 创建一个三节点集群
- en: 'The following are some prerequisites to create a CoreOS cluster in GCE:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 GCE 中创建 CoreOS 集群的一些前提条件：
- en: Create a token for a three-node cluster from a discovery token service.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为一个三节点集群从发现令牌服务创建一个令牌。
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
    `2379` and `2380` are needed for the `etcd2` client-to-server and server-to-server
    communication.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个安全组，暴露端口 `ssh`、`icmp`、`2379` 和 `2380`。`2379` 和 `2380` 用于 `etcd2` 客户端到服务器以及服务器之间的通信。
- en: The link, [https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html),
    gets automatically updated with the latest GCE CoreOS releases from the stable,
    beta, and alpha channels. We need to pick the appropriate image that is needed.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接 [https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)
    会自动更新，包含来自稳定、测试和 alpha 通道的最新 GCE CoreOS 版本。我们需要选择所需的镜像。
- en: 'The following CLI can be used to create a three-node CoreOS GCE cluster from
    the stable release:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 CLI 可用于从稳定版本创建三节点 CoreOS GCE 集群：
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-stable-717-3-0-v20150710 --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=cloud-config.yaml`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-stable-717-3-0-v20150710 --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=cloud-config.yaml`'
- en: 'The following is the `cloud-config.yaml` file that''s used:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用的 `cloud-config.yaml` 文件：
- en: '`#cloud-config coreos:   etcd2:     # specify the initial size of your cluster with ?size=X
        discovery: https://discovery.etcd.io/46ad006905f767331a36bb2a4dbde3f5     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     # 指定 集群的初始大小，使用 ?size=X     discovery: https://discovery.etcd.io/46ad006905f767331a36bb2a4dbde3f5
        advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
- en: We can SSH to any of the nodes using `gcloud compute ssh <nodeid>`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `gcloud compute ssh <nodeid>` 通过 SSH 登录到任意节点。
- en: 'The following output shows you that the cluster is created successfully and
    members are seen from both `etcd` and `fleet`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示集群已成功创建，`etcd` 和 `fleet` 中的成员都已显示：
- en: '![](img/00382.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00382.jpg)'
- en: The CoreOS cluster can also be created using the GCE Console GUI interface.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 集群也可以通过 GCE 控制台 GUI 界面创建。
- en: CoreOS installation on Bare Metal
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸机上安装 CoreOS
- en: 'There are two approaches to install CoreOS on Bare Metal:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸机上安装 CoreOS 有两种方法：
- en: CoreOS ISO image
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS ISO 镜像
- en: PXE or IPXE boot
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PXE 或 IPXE 启动
- en: The steps below covers the approach to install CoreOS on Bare Metal using an
    ISO image.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤介绍了使用 ISO 镜像在裸机上安装 CoreOS 的方法。
- en: I installed using a CoreOS ISO image on the Virtualbox CD drive. The procedure
    should be the same if we burn the ISO image on CD and then install on Bare Metal.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过将 CoreOS ISO 镜像安装到 Virtualbox CD 驱动器上进行了安装。如果我们将 ISO 镜像烧录到 CD 上然后在裸机上安装，过程应该是相同的。
- en: 'The following is summary of the steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤总结：
- en: Download the required ISO image based on stable, beta, and alpha versions from
    [https://coreos.com/os/docs/latest/booting-with-iso.html](https://coreos.com/os/docs/latest/booting-with-iso.html).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://coreos.com/os/docs/latest/booting-with-iso.html](https://coreos.com/os/docs/latest/booting-with-iso.html)
    下载所需的稳定、测试和 alpha 版本的 ISO 镜像。
- en: Start a new Linux machine in Virtualbox with the required CPU, memory, and network
    settings, and mount the ISO image on the IDE drive.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Virtualbox 中启动一个新的 Linux 机器，配置所需的 CPU、内存和网络设置，并将 ISO 镜像挂载到 IDE 驱动器上。
- en: Create an SSH key to log in to the CoreOS node using `ssh-keygen`.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 SSH 密钥，使用 `ssh-keygen` 登录到 CoreOS 节点。
- en: Start the Linux machine and then use the CoreOS script to install CoreOS on
    the hard disk with the necessary `cloud-config`. The cloud-config used here is
    similar to cloud-config is used in previous sections, SSH key needs to be manually
    updated.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Linux 机器后，使用 CoreOS 脚本将 CoreOS 安装到硬盘上，并提供必要的 `cloud-config`。这里使用的 `cloud-config`
    与前面章节使用的类似，SSH 密钥需要手动更新。
- en: Remove the CD drive from Virtualbox and reboot. This will load the CoreOS image
    from the hard disk.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Virtualbox 中移除 CD 驱动器并重新启动，这样就会从硬盘加载 CoreOS 镜像。
- en: I have used the stable ISO image version 766.4.0.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的是稳定版本的 ISO 镜像 766.4.0。
- en: 'The following screenshot shows you the initial storage mounting on Virtualbox
    with the ISO image on the IDE drive:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了 Virtualbox 中初始存储挂载状态，ISO 镜像位于 IDE 驱动器上：
- en: '![](img/00386.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00386.jpg)'
- en: The easiest way to get `cloud-config` is by `wget`. When we boot from the CD,
    we cannot cut and paste as there is no Windows manager. The easiest way to get
    `cloud-config` to the node is by having cloud-config in a hosting location and
    fetch it using `wget`. The SSH key needs to be updated appropriately.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 `cloud-config` 的最简单方法是使用 `wget`。当我们从 CD 启动时，由于没有窗口管理器，无法进行复制粘贴。最简单的方式是将 `cloud-config`
    放置在一个托管位置，并使用 `wget` 从该位置获取到节点上。SSH 密钥需要适当更新。
- en: '`wget https://github.com/smakam/coreos/raw/master/single-node-cloudconfig.yml`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`wget https://github.com/smakam/coreos/raw/master/single-node-cloudconfig.yml`'
- en: 'The installation of CoreOS to the hard disk can be done using the CoreOS-provided
    script:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 将 CoreOS 安装到硬盘可以通过 CoreOS 提供的脚本完成：
- en: '`sudo coreos-install -d /dev/sda -C stable -c ~/cloud-config.yaml`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo coreos-install -d /dev/sda -C stable -c ~/cloud-config.yaml`'
- en: 'After successful installation, we can shut down the node and remove the IDE
    drive so that the bootup can happen from the hard disk. The following screenshot
    shows you the storage selection in Virtualbox to boot using the hard disk:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，我们可以关机并移除 IDE 驱动器，这样系统就会从硬盘启动。以下截图展示了在 Virtualbox 中使用硬盘启动的存储选择：
- en: '![](img/00420.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00420.jpg)'
- en: 'After the node is booted up, we can SSH to the node as we have already set
    up the SSH key. The following output shows you the CoreOS version on Bare Metal:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 节点启动后，我们可以通过 SSH 连接到节点，因为我们已经设置好了 SSH 密钥。以下输出显示了裸机上的 CoreOS 版本：
- en: '![](img/00393.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00393.jpg)'
- en: Basic debugging
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 基本调试
- en: The following are some basic debugging tools and approaches to debug issues
    in the CoreOS cluster.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些基本的调试工具和方法，用于调试 CoreOS 集群中的问题。
- en: journalctl
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: journalctl
- en: Systemd-Journal takes care of logging all the kernel and systemd services. Journal
    log files from all the services are stored in a centralized location in `/var/log/journal`.
    The logs are stored in the binary format, and this keeps it easy to manipulate
    to different formats.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Systemd-Journal 负责记录所有内核和 systemd 服务的日志。所有服务的日志文件都会存储在 `/var/log/journal` 中的集中位置。日志以二进制格式存储，这使得它易于转换成不同的格式。
- en: 'Here are some common examples that shows how to use Journalctl:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些常见的例子，展示了如何使用 Journalctl：
- en: '`Journalctl`: This lists the combined journal log from all the sources.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl`：这是列出所有来源的合并日志。'
- en: '`Journalctl –u etcd2.service`: This lists the logs from `etcd2.service`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl –u etcd2.service`：这是列出 `etcd2.service` 的日志。'
- en: '`Journalctl –u etcd2.service –f`: This lists the logs from `etcd2`. service
    like `tail –f` format.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl –u etcd2.service –f`：这是以类似 `tail –f` 格式列出 `etcd2.service` 的日志。'
- en: '`Journalctl –u etcd2.service –n 100`: This lists the logs of the last 100 lines.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl –u etcd2.service –n 100`：这是列出最后 100 行日志。'
- en: '`Journalctl –u etcd2.service –no-pager`: This lists the logs with no pagination,
    which is useful for search.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl –u etcd2.service –no-pager`：这是列出无分页的日志，适用于搜索。'
- en: '`Journalctl –p err –n 100`: This lists all 100 errors by filtering the logs.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Journalctl –p err –n 100`：这是通过筛选日志列出所有 100 个错误。'
- en: '`journalctl -u etcd2.service --since today`: This lists today''s logs of `etcd2.service`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`journalctl -u etcd2.service --since today`：这是列出今天的 `etcd2.service` 日志。'
- en: '`journalctl -u etcd2.service -o json-pretty`: This lists the logs of `etcd2.service`
    in JSON-formatted output.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`journalctl -u etcd2.service -o json-pretty`：这是以 JSON 格式列出 `etcd2.service`
    的日志。'
- en: systemctl
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: systemctl
- en: The `systemctl` utility can be used for basic monitoring and troubleshooting
    of the systemd units.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`systemctl` 工具可用于基本的系统d单元监控和故障排除。'
- en: 'The following example shows you the status of the `systemdunit docker.service`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了 `systemdunit docker.service` 的状态：
- en: '![](img/00395.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00395.jpg)'
- en: We can stop and restart services in case there are issues with a particular
    service.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个服务出现问题，我们可以停止并重新启动该服务。
- en: 'The following command will restart docker.service:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将重新启动 docker.service：
- en: '`sudo systemctl restart docker.service`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo systemctl restart docker.service`'
- en: 'When a service file is changed or environment variables are changed, we need
    to execute the following command to reload configuration before restarting the
    service for the changes to take effect:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务文件或环境变量发生更改时，我们需要执行以下命令来重新加载配置，以便更改生效，再重新启动服务：
- en: '`sudo systemctl daemon-reload`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`sudo systemctl daemon-reload`'
- en: 'The following command is useful to see the units that have failed:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令有助于查看哪些单元已失败：
- en: '`Systemctl --failed`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`Systemctl --failed`'
- en: Cloud-config
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config
- en: Earlier, we looked at how the `cloud-config` YAML file can be prevalidated.
    In case there are runtime errors, we can check it with `journalctl -b _EXE=/usr/bin/coreos-cloudinit`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们已经看过如何预验证 `cloud-config` YAML 文件。如果出现运行时错误，可以使用 `journalctl -b _EXE=/usr/bin/coreos-cloudinit`
    来检查。
- en: 'If we make changes to the `cloud-config` user data after the initial node setup,
    we can perform the following steps to activate the new configuration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在初始节点设置后对 `cloud-config` 用户数据进行了更改，可以执行以下步骤来激活新的配置：
- en: Perform `vagrant reload --provision` to get the new configuration.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行 `vagrant reload --provision` 以获取新的配置。
- en: The new `cloud-config` user data will be in `/var/lib/coreos-vagrant` as `vagrantfile-user-data`.
    Perform `sudo coreos-cloudinit --from-file vagrantfile-user-data` to update the
    new configuration.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的 `cloud-config` 用户数据将保存在 `/var/lib/coreos-vagrant`，文件名为 `vagrantfile-user-data`。执行
    `sudo coreos-cloudinit --from-file vagrantfile-user-data` 来更新新的配置。
- en: Logging from one CoreOS node to another
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个 CoreOS 节点登录到另一个 CoreOS 节点
- en: Sometimes, it is useful to SSH to other nodes from one of the CoreOS nodes in
    the cluster. The following set of commands can be used to forward the SSH agent
    that we can use to SSH from other nodes. More information on SSH agent forwarding
    can be found at [http://rabexc.org/posts/using-ssh-agent](http://rabexc.org/posts/using-ssh-agent).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，从集群中的某个 CoreOS 节点 SSH 登录到其他节点很有用。以下一组命令可用于转发 SSH 代理，我们可以使用它从其他节点进行 SSH 登录。有关
    SSH 代理转发的更多信息，请参见 [http://rabexc.org/posts/using-ssh-agent](http://rabexc.org/posts/using-ssh-agent)。
- en: '`` eval `ssh-agent` ```ssh-add <key> (Key is the private key)``ssh -i <key> core@<ip> -A (key is the private key)`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`` eval `ssh-agent` ```ssh-add <key>（密钥是私钥）``ssh -i <key> core@<ip> -A（密钥是私钥）`'
- en: 'After this, we can either SSH to the machine ID or a specific Fleet unit, as
    shown in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们可以通过 SSH 连接到机器 ID 或特定的 Fleet 单元，如下图所示：
- en: '![](img/00485.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00485.jpg)'
- en: Note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: SSH agent forwarding is not secure and should be used only to debug.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：SSH 代理转发不安全，应仅用于调试。
- en: Important files and directories
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 重要文件和目录
- en: 'Knowing these files and directories helps with debugging the issues:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些文件和目录有助于调试问题：
- en: Systemd unit file location - `/usr/lib64/systemd/system`.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: systemd 单元文件位置 - `/usr/lib64/systemd/system`。
- en: Network unit files `- /usr/lib64/systemd/network`.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络单元文件 `- /usr/lib64/systemd/network`。
- en: User-written unit files and drop-ins to change the default parameters `- /etc/systemd/system`.
    Drop-ins for specific configuration changes can be done using the configuration
    file under the specific service directory. For example, to modify the fleet configuration,
    create the `fleet.service.d` directory and put the configuration file in this
    directory.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户编写的单元文件和附加项用于更改默认参数 `- /etc/systemd/system`。特定配置更改的附加项可以在特定服务目录下使用配置文件进行。例如，要修改
    fleet 配置，可以创建 `fleet.service.d` 目录，并将配置文件放入该目录。
- en: User-written network unit files `- /etc/systemd/network`.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户编写的网络单元文件 `- /etc/systemd/network`。
- en: Runtime environment variables and drop-in configuration of individual components
    such as `etcd` and `fleet` `- /run/systemd/system/`.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时环境变量和各个组件的附加配置，如 `etcd` 和 `fleet`， `- /run/systemd/system/`。
- en: The vagrantfile user data containing the `cloud-config` user data used with
    Vagrant `- /var/lib/coreos-vagrant`.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含 `cloud-config` 用户数据的 vagrantfile 用户数据文件，使用 Vagrant `- /var/lib/coreos-vagrant`。
- en: The `systemd-journald` logs `- /var/log/journal`.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`systemd-journald` 会记录日志 `- /var/log/journal`。'
- en: '`cloud-config.yaml` associated with providers such as Vagrant, AWS, and `GCE-
    /usr/share/oem`. (CoreOS first executes this `cloud-config` and then executes
    the user-provided `cloud-config`.)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 Vagrant、AWS 和 `GCE` 等提供商相关的 `cloud-config.yaml` `- /usr/share/oem`。（CoreOS
    首先执行此 `cloud-config`，然后执行用户提供的 `cloud-config`。）
- en: Release channel and update strategy `- /etc/coreos/update.conf`.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布渠道和更新策略 `- /etc/coreos/update.conf`。
- en: The public and private IP address (`COREOS_PUBLIC_IPV4` and `COREOS_PRIVATE_IPV4`)
    `- /etc/environment`.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共和私有 IP 地址（`COREOS_PUBLIC_IPV4` 和 `COREOS_PRIVATE_IPV4`） `- /etc/environment`。
- en: The machine ID for the particular CoreOS node `- /etc/machine-id`.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定CoreOS节点的机器ID为`- /etc/machine-id`。
- en: The flannel network configuration `- /run/flannel/`.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: flannel网络配置`- /run/flannel/`。
- en: Common mistakes and possible solutions
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 常见错误及可能的解决方案
- en: For CoreOS on the cloud provider, there is a need to open up ports 2379 and
    2380 on the VM. 2379 is used for etcd client-to-server communication, and 2380
    is used for etcd server-to-server communication.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于云提供商上的CoreOS，需要在虚拟机上打开2379和2380端口。2379用于etcd客户端与服务器之间的通信，2380用于etcd服务器之间的通信。
- en: A discovery token needs to be generated every time for each cluster and cannot
    be shared. When a stale discovery token is shared, members will not be able to
    join the etcd cluster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次为每个集群生成一个发现令牌，且不可共享。当共享过期的发现令牌时，成员将无法加入etcd集群。
- en: Running multiple CoreOS clusters with Vagrant simultaneously can cause issues
    because of overlapping IP ranges. Care should be taken so that common parameters
    such as the IP address are not shared across clusters.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时运行多个CoreOS集群与Vagrant可能会导致问题，因为IP范围重叠。应特别注意，以避免在集群间共享常见参数（如IP地址）。
- en: Cloud-config YAML files need to be properly indented. It is better to use the
    cloud-config validator to check for issues.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云配置YAML文件需要正确缩进。最好使用云配置验证器来检查问题。
- en: When using discovery token, CoreOS node needs to have Internet access to access
    the token service.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用发现令牌时，CoreOS节点需要具备互联网访问权限以连接到令牌服务。
- en: When creating a discovery token, you need to use the size based on the count
    of members and all members need to be part of the bootstrap. If all members are
    not present, the cluster will not be formed. Members can be added or removed later.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建发现令牌时，需要根据成员数量选择大小，并且所有成员需要是引导的一部分。如果没有所有成员，集群将无法形成。以后可以添加或删除成员。
- en: Summary
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the basics of CoreOS cloud-config and how to set
    up the CoreOS development environment with Vagrant, Amazon AWS, Google GCE, and
    Bare Metal. We also covered some basic debugging steps for commonly encountered
    issues. As described in this chapter, it is easy to install CoreOS in the local
    data center or Cloud environments. It is better to try out deployment in a development
    cluster before moving to production environments. In the next chapter, we will
    cover how the CoreOS automatic update works.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了CoreOS云配置的基础知识，并展示了如何使用Vagrant、Amazon AWS、Google GCE和裸金属设置CoreOS开发环境。我们还介绍了一些常见问题的基本调试步骤。正如本章所描述的，安装CoreOS在本地数据中心或云环境中是很容易的。建议在进入生产环境之前，在开发集群中进行部署尝试。下一章我们将介绍CoreOS自动更新的工作原理。
- en: References
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Vagrant installation: [https://coreos.com/os/docs/latest/booting-on-vagrant.html](https://coreos.com/os/docs/latest/booting-on-vagrant.html)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vagrant安装：[https://coreos.com/os/docs/latest/booting-on-vagrant.html](https://coreos.com/os/docs/latest/booting-on-vagrant.html)
- en: 'AWS installation: [https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS安装：[https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)
- en: 'GCE installation: [https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCE安装：[https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)
- en: 'Bare Metal installation: [https://coreos.com/os/docs/latest/installing-to-disk.html](https://coreos.com/os/docs/latest/installing-to-disk.html)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裸金属安装：[https://coreos.com/os/docs/latest/installing-to-disk.html](https://coreos.com/os/docs/latest/installing-to-disk.html)
- en: 'CoreOS CloudInit: [https://github.com/coreos/coreos-cloudinit](https://github.com/coreos/coreos-cloudinit)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS CloudInit：[https://github.com/coreos/coreos-cloudinit](https://github.com/coreos/coreos-cloudinit)
- en: Further reading and tutorials
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读和教程
- en: 'Introduction to the cloud-config format: [https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting](https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云配置格式介绍：[https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting](https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting)
- en: 'CoreOS with AWS Cloudformation: [http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/](http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS与AWS CloudFormation：[http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/](http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/)
- en: 'The CoreOS bare-metal installation: [http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial](http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial)
    and [http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux](http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS 裸机安装：[http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial](http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial)
    和 [http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux](http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux)
- en: 'Using journalctl to view systemd logs: [https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 journalctl 查看 systemd 日志：[https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)
