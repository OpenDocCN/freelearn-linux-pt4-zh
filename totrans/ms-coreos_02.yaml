- en: Chapter 2. Setting up the CoreOS Lab
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS can be deployed in Bare Metal, VMs, or a cloud provider such as Amazon
    AWS or Google GCE. In this chapter, we will cover how to set up the CoreOS development
    environment in Vagrant, Amazon AWS, Google GCE, and Bare Metal. This development
    environment will be used in all the chapters going forward.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-config for CoreOS
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS with Vagrant
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS with Amazon AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS with Google GCE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CoreOS installation on Bare Metal.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basic debugging of the CoreOS cluster
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different CoreOS deployment options are covered here because of the following
    reasons:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Vagrant with Virtualbox is useful for users who don't have a cloud account.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some users, using a local machine might not be possible as VMs occupy a
    lot of resources, and using a cloud-based VM is the best choice in this case.
    As AWS and GCE are the most popular cloud providers, I chose these two.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bare metal installation would be preferable for traditional in-house data centers.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book's examples, I have used one of the three approaches (Vagrant, AWS,
    and GCE) based on the simplicity of one of the approaches, better integration
    with one of the three approaches, or because of issues with a particular approach.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-config
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-config is a declarative configuration file format that is used by many
    Linux distributions to describe the initial server configuration. The cloud-init
    program takes care of parsing `cloud-config` during server initialization and
    configures the server appropriately. The `cloud-config` file provides you with
    a default configuration for the CoreOS node.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The CoreOS cloud-config file format
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The `coreos-cloudinit` program takes care of the default configuration of the
    CoreOS node during bootup using the `cloud-config` file. The `cloud-config` file
    describes the configuration in the YAML format ([http://www.yaml.org/](http://www.yaml.org/)).
    CoreOS cloud-config follows the `cloud-config` specification with some CoreOS-specific
    options. The link, [https://coreos.com/os/docs/latest/cloud-config.html](https://coreos.com/os/docs/latest/cloud-config.html)
    covers the details of CoreOS `cloud-config`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: The main sections of cloud-config
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main sections in the CoreOS `cloud-config` YAML file:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'CoreOS:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Etcd2: config parameters for etcd2'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fleet: config parameters for Fleet'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flannel: config parameters for Flannel'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Locksmith: config parameters for Locksmith'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Update: config parameters for automatic updates'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Units: Systemd units that need to be started'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ssh_authorized_keys`: Public keys for the core user'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hostname`: Hostname for the CoreOS system'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users`: Additional user account and group details'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write_files`: Creates files with specified user data'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`manage_etc_hosts`: Specifies the contents of `/etc/hosts`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sample CoreOS cloud-config
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample `cloud-config` file for a single node CoreOS cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config coreos:   etcd2:     # Static cluster     name: etcdserver     initial-cluster-token: etcd-cluster-1
        initial-cluster: etcdserver=http://$private_ipv4:2380     initial-cluster-state: new
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4     metadata: "role=services"   flannel:
        interface: $public_ipv4   update:       reboot-strategy: "etcd-lock"   units:
        # To use etcd2, comment out the above service and uncomment these     # Note: this requires a release that contains etcd2
        - name: etcd2.service       command: start     - name: fleet.service       command: start
        - name: flanneld.service       drop-ins:         - name: 50-network-config.conf
              content: |             [Service]             ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''
          command: start     - name: docker-tcp.socket       command: start       enable: true
          content: |         [Unit]         Description=Docker Socket for the API
            [Socket]         ListenStream=2375         Service=docker.service         BindIPv6Only=both
            [Install]         WantedBy=sockets.target  write_files:   - path: "/etc/motd"
        permissions: "0644"     owner: "root"     content: |       --- My CoreOS Cluster ---`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     # 静态集群     name: etcdserver     initial-cluster-token: etcd-cluster-1
        initial-cluster: etcdserver=http://$private_ipv4:2380     initial-cluster-state: new
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # 监听官方端口和传统端口     # 如果您的应用程序不依赖于传统端口，则可以省略这些端口     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4     metadata: "role=services"   flannel:     interface: $public_ipv4
      update:       reboot-strategy: "etcd-lock"   units:     # 要使用etcd2，注释掉上述服务并取消注释这些
        # 注意：这需要包含etcd2的版本     - name: etcd2.service       command: start     - name: fleet.service
          command: start     - name: flanneld.service       drop-ins:         - name: 50-network-config.conf
              content: |             [Service]             ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''
          command: start     - name: docker-tcp.socket       command: start       enable: true
          content: |         [Unit]         Description=用于API的Docker Socket         [Socket]
            ListenStream=2375         Service=docker.service         BindIPv6Only=both
            [Install]         WantedBy=sockets.target  write_files:   - path: "/etc/motd"
        permissions: "0644"     owner: "root"     content: |       --- 我的CoreOS集群 ---'
- en: 'The following are some notes on the preceding `cloud-config`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于前面`cloud-config`的一些说明：
- en: 'The `etcd2` section specifies the configuration parameters for the `etcd2`
    service. In this case, we specify parameters needed to start `etcd` on the CoreOS
    node. The `public_ipv4` and `private_ipv4` environment variables are substituted
    with the CoreOS node''s IP address. As there is only one node, we use the static
    cluster definition approach rather than using a discovery token. Based on the
    specified parameters, the `20-cloudinit.conf` Drop-In Unit gets created in `/run/systemd/system/etcd2.service.d`
    with the following environment variables:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etcd2`部分指定了`etcd2`服务的配置参数。在这个例子中，我们指定了启动`etcd`所需的参数，并在CoreOS节点上进行配置。`public_ipv4`和`private_ipv4`环境变量将被替换为CoreOS节点的IP地址。由于只有一个节点，我们使用静态集群定义方式，而不是使用发现令牌。根据指定的参数，`20-cloudinit.conf`
    Drop-In单元将在`/run/systemd/system/etcd2.service.d`目录下创建，包含以下环境变量：'
- en: '`[Service] Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.17.8.101:2379"
    Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.17.8.101:2380" Environment="ETCD_INITIAL_CLUSTER=etcdserver=http://172.17.8.101:2380"
    Environment="ETCD_INITIAL_CLUSTER_STATE=new" Environment="ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster-1"
    Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001"
    Environment="ETCD_LISTEN_PEER_URLS=http://172.17.8.101:2380,http://172.17.8.101:7001"
    Environment="ETCD_NAME=etcdserver"`'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] Environment="ETCD_ADVERTISE_CLIENT_URLS=http://172.17.8.101:2379"
    Environment="ETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.17.8.101:2380" Environment="ETCD_INITIAL_CLUSTER=etcdserver=http://172.17.8.101:2380"
    Environment="ETCD_INITIAL_CLUSTER_STATE=new" Environment="ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster-1"
    Environment="ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001"
    Environment="ETCD_LISTEN_PEER_URLS=http://172.17.8.101:2380,http://172.17.8.101:7001"
    Environment="ETCD_NAME=etcdserver"`'
- en: 'The `fleet` section specifies the configuration parameters for the `fleet`
    service, including any metadata for the node. The `20-cloudinit.conf` Drop-In
    Unit gets created in `/run/systemd/system/fleet.service.d` with the following
    environment variables:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fleet` 部分指定 `fleet` 服务的配置参数，包括节点的任何元数据。`20-cloudinit.conf` Drop-In 单元将被创建在
    `/run/systemd/system/fleet.service.d`，并包含以下环境变量：'
- en: '`[Service] Environment="FLEET_METADATA=role=services" Environment="FLEET_PUBLIC_IP=172.17.8.101"`'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] Environment="FLEET_METADATA=role=services" Environment="FLEET_PUBLIC_IP=172.17.8.101"`'
- en: 'The update section specifies the update strategy for the CoreOS node. This
    gets updated in the node as `/etc/coreos/update.conf`:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新部分指定了 CoreOS 节点的更新策略。它将在节点中更新为 `/etc/coreos/update.conf`：
- en: '`GROUP=alpha REBOOT_STRATEGY=etcd-lock`'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`GROUP=alpha REBOOT_STRATEGY=etcd-lock`'
- en: 'The `units` section starts `etcd2`, `fleet`, and `flannel`. For `flannel`,
    we have a drop-in unit to update the subnet to be used for containers created
    with the Flannel network service. The `50-network-config.conf` Drop-in unit gets
    created in `/etc/systemd/system/flanneld.service.d`:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units` 部分启动 `etcd2`、`fleet` 和 `flannel`。对于 `flannel`，我们有一个 Drop-In 单元来更新用于创建容器的子网，该容器将与
    Flannel 网络服务一起使用。`50-network-config.conf` Drop-In 单元将被创建在 `/etc/systemd/system/flanneld.service.d`：'
- en: '`[Service] ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{ "Network": "10.1.0.0/16" }''`'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[Service] ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config ''{
    "Network": "10.1.0.0/16" }''`'
- en: The `docker-tcp.socket` unit in the units section is a new `systemd` unit, and
    we specified the service content that allows for the docker daemon to be exposed
    through port `2375`. The unit will be created in `/etc/systemd/system`.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元部分中的 `docker-tcp.socket` 是一个新的 `systemd` 单元，我们指定了允许通过端口 `2375` 暴露 docker 守护进程的服务内容。该单元将被创建在
    `/etc/systemd/system`。
- en: 'The `write_files` section can be used to create any static files. An example
    could be a hello text when a user logs in, which we can do with `/etc/motd`. The
    hello message would look as follows:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_files` 部分可用于创建任何静态文件。例如，当用户登录时显示问候文本，我们可以使用 `/etc/motd` 来实现。问候消息将如下所示：'
- en: '`Last login: Tue Sep 15 14:15:04 2015 from 10.0.2.2 --- My CoreOS Cluster ---
    core@core01 ~ $`'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Last login: Tue Sep 15 14:15:04 2015 from 10.0.2.2 --- My CoreOS Cluster ---
    core@core01 ~ $`'
- en: The cloud-config validator
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config 验证器
- en: Cloud-config uses the YAML syntax. YAML is a human-readable data serialization
    format and uses indents and spaces for alignment. It is better to validate the
    `cloud-config` YAML configuration files before using them. There are two options
    to validate the CoreOS `cloud-config`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud-config 使用 YAML 语法。YAML 是一种人类可读的数据序列化格式，并使用缩进和空格进行对齐。最好在使用 `cloud-config`
    YAML 配置文件之前进行验证。验证 CoreOS `cloud-config` 有两种方式。
- en: A hosted validator
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个托管的验证器
- en: Use this CoreOS-provided link ([https://coreos.com/validate/](https://coreos.com/validate/))
    to validate `cloud-config`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CoreOS 提供的链接 ([https://coreos.com/validate/](https://coreos.com/validate/))
    来验证 `cloud-config`。
- en: Here is an example of a valid and invalid `cloud-config` and the results using
    the validator.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个有效和无效的 `cloud-config` 示例，以及使用验证器的结果。
- en: Valid cloud-config
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的 cloud-config
- en: 'As we can see in the following screenshot, the validator says that the following
    `cloud-config` is valid:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示，验证器表示以下 `cloud-config` 是有效的：
- en: '![](img/00100.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.jpg)'
- en: Invalid cloud-config
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无效的 cloud-config
- en: 'Here, we can see that the validator has specified that `-` is missing in line
    14\. YAML uses spaces for the delimiting, so we need to make sure that the number
    of spaces is exact:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到验证器已指定第 14 行缺少 `-`。YAML 使用空格进行分隔，所以我们需要确保空格的数量完全正确：
- en: '![](img/00167.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00167.jpg)'
- en: The cloudinit validator
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: cloudinit 验证器
- en: 'We can use the `coreos-cloudinit --validate` option available in CoreOS to
    validate the cloud-config. Let''s look at the following sample `cloud-config`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 CoreOS 中可用的 `coreos-cloudinit --validate` 选项来验证 cloud-config。让我们看一下下面的示例
    `cloud-config`：
- en: '![](img/00109.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpg)'
- en: 'When we validate this, we get no errors, as shown in the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们验证时，我们没有得到错误，正如以下截图所示：
- en: '![](img/00112.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00112.jpg)'
- en: 'Now, let''s try the same `cloud-config` with errors. Here, we have `|` missing
    in the content line:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用带有错误的相同 `cloud-config`。这里，在内容行中缺少了 `|`：
- en: '![](img/00017.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpg)'
- en: 'We see the following errors when we validate:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们验证时，我们看到以下错误：
- en: '![](img/00120.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpg)'
- en: Executing cloud-config
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 cloud-config
- en: 'There are two `cloud-config` files that are run as part of the CoreOS bootup:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个 `cloud-config` 文件在 CoreOS 启动过程中作为一部分运行：
- en: System cloud-config
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统 cloud-config
- en: User cloud-config
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户 cloud-config
- en: System `cloud-config` is given by the provider (such as Vagrant or AWS) and
    is embedded as part of the CoreOS provider image. Different providers such as
    Vagrant, AWS, and GCE have their `cloud-config` present in `/usr/share/oem/cloud-config.yaml`.
    This `cloud-config` is responsible for setting up the provider-specific configurations,
    such as networking, SSH keys, mount options, and so on. The `coreos-cloudinit`
    program first executes system `cloud-config` and then user `cloud-config`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的 `cloud-config` 由提供商（如 Vagrant 或 AWS）提供，并作为 CoreOS 提供商镜像的一部分嵌入其中。不同的提供商（如
    Vagrant、AWS 和 GCE）在 `/usr/share/oem/cloud-config.yaml` 中有各自的 `cloud-config`。该
    `cloud-config` 负责设置提供商特定的配置项，如网络、SSH 密钥、挂载选项等。`coreos-cloudinit` 程序首先执行系统 `cloud-config`，然后执行用户
    `cloud-config`。
- en: Depending on the provider, user `cloud-config` can be supplied using either
    config-drive or an internal user data service. Config-drive is a universal way
    to provide `cloud-config` by mounting a read-only partition that contains `cloud-config`
    to the host machine. Rackspace uses config-drive to get user `cloud-config`, and
    AWS uses its internal user data service to fetch the user data and doesn't rely
    on config-drive. In the Vagrant scenario, Vagrantfile takes care of copying the
    `cloud-config` to the CoreOS VM.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据提供商的不同，可以使用 config-drive 或内部用户数据服务提供用户的 `cloud-config`。Config-drive 是通过挂载一个只读分区，提供包含
    `cloud-config` 的数据到主机机器上的通用方式。Rackspace 使用 config-drive 获取用户的 `cloud-config`，而
    AWS 则使用其内部的用户数据服务来获取用户数据，不依赖 config-drive。在 Vagrant 环境中，Vagrantfile 负责将 `cloud-config`
    复制到 CoreOS 虚拟机中。
- en: The CoreOS cluster with Vagrant
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Vagrant 搭建 CoreOS 集群
- en: 'Vagrant can be installed in Windows or Linux. The following is my development
    environment for the Vagrant CoreOS:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant 可以在 Windows 或 Linux 上安装。以下是我用于 Vagrant CoreOS 的开发环境：
- en: 'Windows 7: I use mysysgit ([https://git-for-windows.github.io/](https://git-for-windows.github.io/))
    to get a Linux-like shell for Windows'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows 7：我使用 mysysgit ([https://git-for-windows.github.io/](https://git-for-windows.github.io/))
    为 Windows 获取类 Linux 的命令行。
- en: 'Vagrant 1.7.2: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vagrant 1.7.2: [https://www.vagrantup.com/downloads.html](https://www.vagrantup.com/downloads.html)'
- en: 'Virtualbox 4.3.28: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Virtualbox 4.3.28: [https://www.virtualbox.org/wiki/Downloads](https://www.virtualbox.org/wiki/Downloads)'
- en: For a few of the examples in the book, I have used Vagrant to run CoreOS inside
    a Linux VM running on top of Windows laptop with Virtualbox.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中的一些示例中，我使用 Vagrant 在 Windows 笔记本上的 Virtualbox 上运行 Linux 虚拟机中的 CoreOS。
- en: Steps to start the Vagrant environment
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 Vagrant 环境的步骤
- en: 'Check out the coreos-vagrant code base:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 coreos-vagrant 的代码库：
- en: '`git clone https://github.com/coreos/coreos-vagrant.git`'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/coreos/coreos-vagrant.git`'
- en: 'Copy the sample `user-data` and `config.rb` files in the coreos-vagrant directory:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制 `user-data` 和 `config.rb` 的示例文件到 coreos-vagrant 目录中：
- en: '`cd coreos-vagrant``mv user-data.sample user-data``mv config.rb.sample config.rb`'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`cd coreos-vagrant``mv user-data.sample user-data``mv config.rb.sample config.rb`'
- en: Edit `Vagrantfile`, `user-data`, and `config.rb` based on your need.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要编辑 `Vagrantfile`、`user-data` 和 `config.rb`。
- en: 'Start the CoreOS cluster:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 CoreOS 集群：
- en: '`Vagrant up`'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Vagrant up`'
- en: 'SSH to the individual node:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SSH 到单独的节点：
- en: '`Vagrant ssh core-<id>`'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Vagrant ssh core-<id>`'
- en: Important files to be modified
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 需要修改的重要文件
- en: The following are important files to be modified along with commonly needed
    modifications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要修改的重要文件以及常见的修改项。
- en: Vagrantfile
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrantfile
- en: 'Vagrant sets up the VM environment based on the configuration defined in `Vagrantfile`.
    The following are certain relevant functionalities in the CoreOS context:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant 根据 `Vagrantfile` 中定义的配置来设置虚拟机环境。以下是在 CoreOS 环境中一些相关的功能：
- en: The version of CoreOS software to be used is specified using `update_channel`.
    The version can be specified as `stable`, `beta`, and `alpha`. More details on
    CoreOS software versions are covered in [Chapter 3](index_split_075.html#filepos216260),
    CoreOS Autoupdate.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的 CoreOS 软件版本通过 `update_channel` 来指定。版本可以指定为 `stable`（稳定版）、`beta`（测试版）和 `alpha`（开发版）。关于
    CoreOS 软件版本的更多细节，请参阅 [第 3 章](index_split_075.html#filepos216260)，CoreOS 自动更新。
- en: CPU and memory for the VM and ports to be exposed from the VM.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟机的 CPU 和内存以及需要暴露的端口。
- en: SSH key management.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSH 密钥管理。
- en: User-data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 用户数据
- en: The `user-data` file is essentially the `cloud-config` file that specifies the
    discovery token, environment variables, and list of units to be started by default.
    Vagrant copies the `cloud-config` file to `/var/lib/coreos-vagrant/vagrantfile-user-data`
    inside the VM. The `coreos-cloudinit` reads `vagrantfile-user-data` on every boot
    and uses it to create the machine's user data file.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`user-data` 文件实际上是 `cloud-config` 文件，它指定了发现令牌、环境变量和默认启动的单元列表。Vagrant 会将 `cloud-config`
    文件复制到虚拟机内的 `/var/lib/coreos-vagrant/vagrantfile-user-data` 位置。`coreos-cloudinit`
    会在每次启动时读取 `vagrantfile-user-data`，并利用它来创建机器的用户数据文件。'
- en: Config.rb
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Config.rb
- en: The `config.rb` file specifies the count of CoreOS nodes. This file also provides
    you with an option to automatically generate a discovery token. Some options here
    overlap with the `Vagrantfile` like image version.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.rb` 文件指定了 CoreOS 节点的数量。该文件还提供了一个选项，可以自动生成发现令牌。这里的一些选项与 `Vagrantfile`
    中的设置重叠，比如镜像版本。'
- en: Vagrant – a three-node cluster with dynamic discovery
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个包含动态发现功能的三节点集群
- en: Here, we will create a three-node CoreOS cluster with `etcd2` and `fleet` running
    on each node and nodes discovering each other dynamically.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个三节点的 CoreOS 集群，每个节点上运行 `etcd2` 和 `fleet`，并且节点之间将动态发现彼此。
- en: Generating a discovery token
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生成发现令牌
- en: When we start a multinode CoreOS cluster, there needs to be a bootstrapping
    mechanism to discover the cluster members. For this, we generate a token specifying
    the number of initial nodes in the cluster as an argument. Each node needs to
    be started with this discovery token. Etcd will use the discovery token to put
    all the nodes with the same discovery token as part of the initial cluster. CoreOS
    runs the service to provide the discovery token from its central servers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动一个多节点的 CoreOS 集群时，需要有一个引导机制来发现集群成员。为此，我们生成一个令牌，并将集群中初始节点的数量作为参数传入。每个节点启动时都需要使用此发现令牌。Etcd
    会使用发现令牌将所有具有相同令牌的节点作为初始集群的一部分。CoreOS 运行该服务，从其中央服务器提供发现令牌。
- en: 'There are two approaches to generate a discovery token:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种生成发现令牌的方法：
- en: 'From the browser: `https://discovery.etcd.io/new?size=3`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从浏览器访问：`https://discovery.etcd.io/new?size=3`
- en: 'Using curl: `curl https://discovery.etcd.io/new?size=3`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 curl: `curl https://discovery.etcd.io/new?size=3`'
- en: 'The following is a curl example with a generated discovery token. This token
    needs to be copied to `user-data`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 curl 示例，包含一个生成的发现令牌。该令牌需要复制到 `user-data`：
- en: '![](img/00358.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00358.jpg)'
- en: Steps for cluster creation
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 集群创建步骤
- en: 'The following is a `cloud-config` user data with the updated discovery token
    that we generated in the preceding section along with the necessary environment
    variables and service units. All three nodes will use this `cloud-config`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是包含我们在上一节中生成的更新发现令牌的 `cloud-config` 用户数据，以及必要的环境变量和服务单元。所有三个节点将使用此 `cloud-config`：
- en: '`#cloud-config coreos:   etcd2:     #generate a new token for each unique cluster from https://discovery.etcd.io/new
        discovery: https://discovery.etcd.io/9a6b7af06c8a677b4e5f76ae9ce0da9c     # multi-region and multi-cloud deployments need to use $public_ipv4
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     # legacy ports can be omitted if your application doesn''t depend on them
        listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001     listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
      fleet:     public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:
        # Note: this requires a release that contains etcd2     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     #为每个唯一集群从 https://discovery.etcd.io/new
    生成一个新的令牌     discovery: https://discovery.etcd.io/9a6b7af06c8a677b4e5f76ae9ce0da9c
        # 多区域和多云部署需要使用 $public_ipv4     advertise-client-urls: http://$public_ipv4:2379
        initial-advertise-peer-urls: http://$private_ipv4:2380     # 在官方端口和传统端口上监听
        # 如果你的应用程序不依赖于传统端口，可以省略它们     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
        public-ip: $public_ipv4   flannel:     interface: $public_ipv4   units:     # 注意：这需要一个包含
    etcd2 的版本     - name: etcd2.service       command: start     - name: fleet.service
          command: start`'
- en: We need to update `num_instances` to `3` in `config.rb` and then perform `vagrant
    up`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 `config.rb` 文件中将 `num_instances` 更新为 `3`，然后执行 `vagrant up`。
- en: To verify the basic cluster operation, we can check the following output, where
    we should see the cluster members.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证基本集群操作，我们可以检查以下输出，应该能够看到集群成员。
- en: 'The following `etcdctl` member output shows the three cluster members:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`etcdctl`成员输出，显示了三个集群成员：
- en: '![](img/00362.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00362.jpg)'
- en: 'The following fleet member output shows the three cluster members:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是fleet成员输出，显示了三个集群成员：
- en: '![](img/00364.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00364.jpg)'
- en: Vagrant – a three-node cluster with static discovery
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个具有静态发现的三节点集群
- en: Here, we will create a three-node CoreOS cluster and use a static approach to
    mention its cluster neighbors. In the dynamic discovery approach, we need to use
    a discovery token to discover the cluster members. Static discovery can be used
    for scenarios where access to the token server is not available to cluster members,
    and the cluster member IP addresses are known in advance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个三节点CoreOS集群，并使用静态方式指定其集群邻居。在动态发现方法中，我们需要使用发现令牌来发现集群成员。静态发现可以用于集群成员无法访问令牌服务器，并且集群成员的IP地址已知的场景。
- en: 'Perform the following steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: First, we need to create three separate instances of the CoreOS Vagrant environment
    by performing `git clone` separately for each node.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要通过分别对每个节点执行`git clone`来创建三个独立的CoreOS Vagrant实例。
- en: The `config.rb` file must be updated for each node with `num_instances` set
    to one.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须为每个节点更新`config.rb`文件，并将`num_instances`设置为1。
- en: Vagrantfile should be updated for each node so that IP addresses are statically
    assigned as `172.17.8.101` for `core-01`, `172.17.8.102` for `core-02`, and `172.17.8.103`
    for `core-03`. IP addresses should be updated based on your environment.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该更新每个节点的Vagrantfile，以便将IP地址静态分配为`172.17.8.101`（`core-01`）、`172.17.8.102`（`core-02`）和`172.17.8.103`（`core-03`）。IP地址应根据您的环境进行更新。
- en: 'The `cloud-config` user data for the first node is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-01     initial-advertise-peer-urls: http://172.17.8.101:2380
        listen-peer-urls: http://172.17.8.101:2380     listen-client-urls: http://172.17.8.101:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.101:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-01     initial-advertise-peer-urls: http://172.17.8.101:2380
        listen-peer-urls: http://172.17.8.101:2380     listen-client-urls: http://172.17.8.101:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.101:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
- en: 'The `cloud-config` user data for the second node is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-02     initial-advertise-peer-urls: http://172.17.8.102:2380
        listen-peer-urls: http://172.17.8.102:2380     listen-client-urls: http://172.17.8.102:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.102:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-02     initial-advertise-peer-urls: http://172.17.8.102:2380
        listen-peer-urls: http://172.17.8.102:2380     listen-client-urls: http://172.17.8.102:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.102:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
- en: 'The `cloud-config` user data for the third node is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个节点的`cloud-config`用户数据如下：
- en: '`#cloud-config coreos:   etcd2:     name: core-03     initial-advertise-peer-urls: http://172.17.8.103:2380
        listen-peer-urls: http://172.17.8.103:2380     listen-client-urls: http://172.17.8.103:2379,http://127.0.0.1:2379
        advertise-client-urls: http://172.17.8.103:2379     initial-cluster-token: etcd-cluster-1
        initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command: start
        - name: fleet.service       command: start`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     name: core-03     initial-advertise-peer-urls:
    http://172.17.8.103:2380     listen-peer-urls: http://172.17.8.103:2380     listen-client-urls:
    http://172.17.8.103:2379,http://127.0.0.1:2379     advertise-client-urls: http://172.17.8.103:2379
        initial-cluster-token: etcd-cluster-1     initial-cluster: core-01=http://172.17.8.101:2380,core-02=http://172.17.8.102:2380,core-03=http://172.17.8.103:2380
        initial-cluster-state: new   fleet:     public-ip: $public_ipv4   flannel:
        interface: $public_ipv4   units:     - name: etcd2.service       command:
    start     - name: fleet.service       command: start`'
- en: We need to perform `vagrant up` separately for each of the nodes. We should
    see the cluster member list updated in both the `etcdctl member list` and `fleetctl
    list-machines` outputs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分别为每个节点执行`vagrant up`命令。我们应该在`etcdctl member list`和`fleetctl list-machines`的输出中看到集群成员列表的更新。
- en: Vagrant – a production cluster with three master nodes and three worker nodes
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant – 一个包含三个主节点和三个工作节点的生产集群
- en: 'In [Chapter 1](index_split_023.html#filepos77735), CoreOS Overview, we covered
    the CoreOS cluster architecture. A production cluster has one set of nodes (called
    master) to run critical services, and another set of nodes (called worker) to
    run application services. In this example, we create three master nodes running
    `etcd` and other critical services and another three worker nodes. Etcd in the
    worker nodes will proxy to the master nodes. Worker nodes will be used for user-created
    services while master nodes will be used for system services. This avoids resource
    contention. The following are the steps needed for this creation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](index_split_023.html#filepos77735)，CoreOS概述中，我们介绍了CoreOS集群架构。生产集群有一组节点（称为主节点）用于运行关键服务，另一组节点（称为工作节点）用于运行应用服务。在本示例中，我们创建了三个主节点来运行`etcd`和其他关键服务，以及另外三个工作节点。工作节点中的etcd将代理到主节点。工作节点将用于用户创建的服务，而主节点将用于系统服务。这避免了资源争用。以下是创建过程所需的步骤：
- en: Create a Vagrant three-node cluster for the master and a three-node cluster
    for the worker.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个包含三节点主集群和三节点工作集群的Vagrant集群。
- en: Update Vagrantfile to use non-conflicting IP address ranges between the master
    and worker nodes.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新Vagrantfile，确保主节点和工作节点之间的IP地址范围不冲突。
- en: Use the dynamic discovery token approach to create a token for the three-node
    clusters and update the `cloud-config` user data for both the master and worker
    nodes to the same token. We have specified the token size as `3` as worker nodes
    don't run `etcd`.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用动态发现令牌的方法为三节点集群创建令牌，并更新主节点和工作节点的`cloud-config`用户数据，确保令牌相同。我们已经将令牌大小指定为`3`，因为工作节点不运行`etcd`。
- en: 'The following is the user data for the master cluster:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是主节点集群的用户数据：
- en: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=master"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls:
    http://$private_ipv4:2380     # 在官方端口和遗留端口上监听     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=master"      public-ip: $public_ipv4   units:     - name:
    etcd2.service       command: start     - name: fleet.service       command: start`'
- en: 'The following is the user data for the worker cluster. The discovery token
    needs to be the same for the master and worker clusters:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是工作节点集群的用户数据。主节点和工作节点的发现令牌需要相同：
- en: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # listen on both the official ports and the legacy ports     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=worker"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`#cloud-config coreos:   etcd2:     discovery: https://discovery.etcd.io/d49bac8527395e2a7346e694124c8222
        advertise-client-urls: http://$public_ipv4:2379     initial-advertise-peer-urls: http://$private_ipv4:2380
        # 在官方端口和传统端口上监听     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   fleet:
         metadata: "role=worker"      public-ip: $public_ipv4   units:     - name: etcd2.service
          command: start     - name: fleet.service       command: start`'
- en: The only difference between the master and worker user data is in the metadata
    used for fleet. In this example, we used `role` as `master` for the master cluster
    and `role` as `worker` for the worker cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点和工作节点用户数据之间的唯一区别是在 fleet 中使用的元数据。在这个示例中，我们为主节点集群使用 `role` 设置为 `master`，为工作节点集群使用
    `role` 设置为 `worker`。
- en: Let's look at the `etcdctl` member list and fleet machine list. The following
    output will be the same across all the nodes in the master and worker cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看 `etcdctl` 成员列表和 fleet 机器列表。以下输出在主节点和工作节点集群中是相同的。
- en: 'The `etcdctl` member output is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl` 成员的输出如下：'
- en: '![](img/00306.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00306.jpg)'
- en: 'The fleet member output is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: fleet 成员的输出如下：
- en: '![](img/00370.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00370.jpg)'
- en: 'The following is the `journalctl –u etcd2.service` output on worker nodes that
    show worker nodes proxying to master nodes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `journalctl –u etcd2.service` 在工作节点上的输出，显示工作节点代理到主节点的情况：
- en: '![](img/00372.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00372.jpg)'
- en: A CoreOS cluster with AWS
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AWS 的 CoreOS 集群
- en: 'Amazon AWS provides you with a public cloud service. CoreOS can be run on the
    VMs provided by AWS. The following are some prerequisites for this setup:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 AWS 为您提供公共云服务。CoreOS 可以在 AWS 提供的虚拟机（VM）上运行。以下是此设置的一些前提条件：
- en: You need an account in AWS. AWS provides you with a one-year trial account for
    free.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要一个 AWS 账户。AWS 提供一年免费的试用账户。
- en: Create and download a key pair. The key pair is needed to SSH to the nodes.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并下载密钥对。密钥对用于通过 SSH 连接到节点。
- en: The AWS interface can be accessed through the AWS console, which is a GUI interface,
    or by AWS CLI. AWS CLI ([http://aws.amazon.com/cli/](http://aws.amazon.com/cli/))
    can be installed in either Windows or Linux.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过 AWS 控制台（GUI 界面）或 AWS CLI 访问 AWS 接口。AWS CLI（[http://aws.amazon.com/cli/](http://aws.amazon.com/cli/)）可以在
    Windows 或 Linux 系统上安装。
- en: The following are two approaches of creating a CoreOS cluster with AWS.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 AWS 创建 CoreOS 集群的两种方法。
- en: AWS – a three-node cluster using Cloudformation
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AWS – 使用 Cloudformation 创建三节点集群
- en: 'Cloudformation is an AWS orchestration tool to manage a collection of AWS resources
    that include compute, storage, and networking. The link, [https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template](https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template),
    has the template file for the CoreOS cluster. The following are some of the key
    sections in the template:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudformation 是 AWS 的一个编排工具，用于管理包括计算、存储和网络在内的一组 AWS 资源。以下链接，[https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template](https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template)，提供了
    CoreOS 集群的模板文件。以下是模板中的一些关键部分：
- en: The AMI image ID to be used based on the region
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于区域的 AMI 镜像 ID
- en: The EC2 Instance type
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EC2 实例类型
- en: The security group configuration
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全组配置
- en: The CoreOS cluster size including the minimum and maximum size to autoscale
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreOS 集群的大小，包括自动扩展的最小和最大大小
- en: The initial cloud-config to be used
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用的初始 cloud-config
- en: 'For the following example, I modified the template to use `t2.micro` instead
    of `m3.medium` for the instance size. The following CLI can be used to create
    a three-node CoreOS cluster using `Cloudformation`. The discovery token in the
    below command needs to be updated with the generated token for your case:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下示例，我修改了模板，使用 `t2.micro` 代替 `m3.medium` 作为实例大小。以下 CLI 可用于使用 `Cloudformation`
    创建一个三节点的 CoreOS 集群。下面命令中的 discovery token 需要使用为您的情况生成的令牌来更新：
- en: '`aws cloudformation create-stack \     --stack-name coreos-test \     --template-body file://mycoreos-stable-hvm.template \
        --capabilities CAPABILITY_IAM \     --tags Key=Name,Value=CoreOS \     --parameters \      ParameterKey=DiscoveryURL,ParameterValue="https://discovery.etcd.io/925755234ab82c1ef7bcfbbacdd8c088" \
            ParameterKey=KeyPair,ParameterValue="keyname"`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the successful stack using `aws cloudformation
    list-stacks`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00347.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: After the preceding step, we can see that members are getting discovered successfully
    by both `etcd` and `fleet`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: AWS – a three-node cluster using AWS CLI
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some prerequisites to create a CoreOS cluster in AWS using
    AWS CLI:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Create a token for a three-node cluster from the discovery token service.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
    `2379` and `2380` are needed for the `etcd2` client-to-server and server-to-server
    communication.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the AMI image ID using this link ([https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html))
    based on your AWS Zone and update channel. The latest image IDs for different
    AWS Zones get automatically updated in this link.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following CLI will create the three-node cluster:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`aws ec2 run-instances --image-id ami-85ada4b5 --count 3 --instance-type t2.micro --key-name "yourkey" --security-groups "coreos-test" --user-data file://cloud-config.yaml`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Here, the `ami-85ada4b5` image ID is from the stable update channel. The `coreos-test`
    security group has the necessary ports that need to be exposed outside.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `cloud-config` that I used:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config coreos:   etcd2:     # specify the initial size of your cluster with ?size=X
        discovery: https://discovery.etcd.io/47460367c9b15edffeb49de30cab9354     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the `etcd` member list and fleet member list with
    three nodes in the cluster:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00379.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: The same example can be tried from the AWS Console, where we can specify the
    options from the GUI.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: A CoreOS cluster with GCE
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Google''s GCE is another public cloud provider like Amazon AWS. CoreOS can
    be run on the VMs provided by GCE. The following are some prerequisites for this
    setup:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: You need a GCE account. GCE provides you with a free trial account for 60 days.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCE resources can be accessed using gcloud SDK or GCE GUI Console. SDK can be
    downloaded from [https://cloud.google.com/sdk/](https://cloud.google.com/sdk/).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A base project in GCE needs to be created under which all the resources reside.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A security token needs to be created, which is used for SSH access.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCE – a three-node cluster using GCE CLI
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some prerequisites to create a CoreOS cluster in GCE:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Create a token for a three-node cluster from a discovery token service.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a security group exposing the ports `ssh`, `icmp`, `2379`, and `2380`.
    `2379` and `2380` are needed for the `etcd2` client-to-server and server-to-server
    communication.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link, [https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html),
    gets automatically updated with the latest GCE CoreOS releases from the stable,
    beta, and alpha channels. We need to pick the appropriate image that is needed.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following CLI can be used to create a three-node CoreOS GCE cluster from
    the stable release:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`gcloud compute instances create core1 core2 core3 --image https://www.googleapis.com/compute/v1/projects/coreos-cloud/global/images/coreos-stable-717-3-0-v20150710 --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=cloud-config.yaml`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `cloud-config.yaml` file that''s used:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '`#cloud-config coreos:   etcd2:     # specify the initial size of your cluster with ?size=X
        discovery: https://discovery.etcd.io/46ad006905f767331a36bb2a4dbde3f5     advertise-client-urls: http://$private_ipv4:2379,http://$private_ipv4:4001
        initial-advertise-peer-urls: http://$private_ipv4:2380     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
        listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001   units:
        - name: etcd2.service       command: start     - name: fleet.service       command: start`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: We can SSH to any of the nodes using `gcloud compute ssh <nodeid>`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows you that the cluster is created successfully and
    members are seen from both `etcd` and `fleet`:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00382.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: The CoreOS cluster can also be created using the GCE Console GUI interface.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS installation on Bare Metal
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches to install CoreOS on Bare Metal:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS ISO image
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PXE or IPXE boot
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps below covers the approach to install CoreOS on Bare Metal using an
    ISO image.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: I installed using a CoreOS ISO image on the Virtualbox CD drive. The procedure
    should be the same if we burn the ISO image on CD and then install on Bare Metal.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is summary of the steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Download the required ISO image based on stable, beta, and alpha versions from
    [https://coreos.com/os/docs/latest/booting-with-iso.html](https://coreos.com/os/docs/latest/booting-with-iso.html).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new Linux machine in Virtualbox with the required CPU, memory, and network
    settings, and mount the ISO image on the IDE drive.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an SSH key to log in to the CoreOS node using `ssh-keygen`.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start the Linux machine and then use the CoreOS script to install CoreOS on
    the hard disk with the necessary `cloud-config`. The cloud-config used here is
    similar to cloud-config is used in previous sections, SSH key needs to be manually
    updated.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the CD drive from Virtualbox and reboot. This will load the CoreOS image
    from the hard disk.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have used the stable ISO image version 766.4.0.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows you the initial storage mounting on Virtualbox
    with the ISO image on the IDE drive:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00386.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: The easiest way to get `cloud-config` is by `wget`. When we boot from the CD,
    we cannot cut and paste as there is no Windows manager. The easiest way to get
    `cloud-config` to the node is by having cloud-config in a hosting location and
    fetch it using `wget`. The SSH key needs to be updated appropriately.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '`wget https://github.com/smakam/coreos/raw/master/single-node-cloudconfig.yml`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation of CoreOS to the hard disk can be done using the CoreOS-provided
    script:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo coreos-install -d /dev/sda -C stable -c ~/cloud-config.yaml`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'After successful installation, we can shut down the node and remove the IDE
    drive so that the bootup can happen from the hard disk. The following screenshot
    shows you the storage selection in Virtualbox to boot using the hard disk:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00420.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'After the node is booted up, we can SSH to the node as we have already set
    up the SSH key. The following output shows you the CoreOS version on Bare Metal:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00393.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Basic debugging
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The following are some basic debugging tools and approaches to debug issues
    in the CoreOS cluster.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: journalctl
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Systemd-Journal takes care of logging all the kernel and systemd services. Journal
    log files from all the services are stored in a centralized location in `/var/log/journal`.
    The logs are stored in the binary format, and this keeps it easy to manipulate
    to different formats.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common examples that shows how to use Journalctl:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '`Journalctl`: This lists the combined journal log from all the sources.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Journalctl –u etcd2.service`: This lists the logs from `etcd2.service`.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Journalctl –u etcd2.service –f`: This lists the logs from `etcd2`. service
    like `tail –f` format.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Journalctl –u etcd2.service –n 100`: This lists the logs of the last 100 lines.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Journalctl –u etcd2.service –no-pager`: This lists the logs with no pagination,
    which is useful for search.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Journalctl –p err –n 100`: This lists all 100 errors by filtering the logs.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`journalctl -u etcd2.service --since today`: This lists today''s logs of `etcd2.service`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`journalctl -u etcd2.service -o json-pretty`: This lists the logs of `etcd2.service`
    in JSON-formatted output.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: systemctl
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The `systemctl` utility can be used for basic monitoring and troubleshooting
    of the systemd units.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows you the status of the `systemdunit docker.service`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00395.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: We can stop and restart services in case there are issues with a particular
    service.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will restart docker.service:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo systemctl restart docker.service`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'When a service file is changed or environment variables are changed, we need
    to execute the following command to reload configuration before restarting the
    service for the changes to take effect:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo systemctl daemon-reload`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command is useful to see the units that have failed:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '`Systemctl --failed`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-config
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we looked at how the `cloud-config` YAML file can be prevalidated.
    In case there are runtime errors, we can check it with `journalctl -b _EXE=/usr/bin/coreos-cloudinit`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'If we make changes to the `cloud-config` user data after the initial node setup,
    we can perform the following steps to activate the new configuration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Perform `vagrant reload --provision` to get the new configuration.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new `cloud-config` user data will be in `/var/lib/coreos-vagrant` as `vagrantfile-user-data`.
    Perform `sudo coreos-cloudinit --from-file vagrantfile-user-data` to update the
    new configuration.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging from one CoreOS node to another
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is useful to SSH to other nodes from one of the CoreOS nodes in
    the cluster. The following set of commands can be used to forward the SSH agent
    that we can use to SSH from other nodes. More information on SSH agent forwarding
    can be found at [http://rabexc.org/posts/using-ssh-agent](http://rabexc.org/posts/using-ssh-agent).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '`` eval `ssh-agent` ```ssh-add <key> (Key is the private key)``ssh -i <key> core@<ip> -A (key is the private key)`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we can either SSH to the machine ID or a specific Fleet unit, as
    shown in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00485.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: SSH agent forwarding is not secure and should be used only to debug.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Important files and directories
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing these files and directories helps with debugging the issues:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Systemd unit file location - `/usr/lib64/systemd/system`.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network unit files `- /usr/lib64/systemd/network`.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-written unit files and drop-ins to change the default parameters `- /etc/systemd/system`.
    Drop-ins for specific configuration changes can be done using the configuration
    file under the specific service directory. For example, to modify the fleet configuration,
    create the `fleet.service.d` directory and put the configuration file in this
    directory.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-written network unit files `- /etc/systemd/network`.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runtime environment variables and drop-in configuration of individual components
    such as `etcd` and `fleet` `- /run/systemd/system/`.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vagrantfile user data containing the `cloud-config` user data used with
    Vagrant `- /var/lib/coreos-vagrant`.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `systemd-journald` logs `- /var/log/journal`.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud-config.yaml` associated with providers such as Vagrant, AWS, and `GCE-
    /usr/share/oem`. (CoreOS first executes this `cloud-config` and then executes
    the user-provided `cloud-config`.)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Release channel and update strategy `- /etc/coreos/update.conf`.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The public and private IP address (`COREOS_PUBLIC_IPV4` and `COREOS_PRIVATE_IPV4`)
    `- /etc/environment`.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The machine ID for the particular CoreOS node `- /etc/machine-id`.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The flannel network configuration `- /run/flannel/`.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common mistakes and possible solutions
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: For CoreOS on the cloud provider, there is a need to open up ports 2379 and
    2380 on the VM. 2379 is used for etcd client-to-server communication, and 2380
    is used for etcd server-to-server communication.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discovery token needs to be generated every time for each cluster and cannot
    be shared. When a stale discovery token is shared, members will not be able to
    join the etcd cluster.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running multiple CoreOS clusters with Vagrant simultaneously can cause issues
    because of overlapping IP ranges. Care should be taken so that common parameters
    such as the IP address are not shared across clusters.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-config YAML files need to be properly indented. It is better to use the
    cloud-config validator to check for issues.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using discovery token, CoreOS node needs to have Internet access to access
    the token service.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When creating a discovery token, you need to use the size based on the count
    of members and all members need to be part of the bootstrap. If all members are
    not present, the cluster will not be formed. Members can be added or removed later.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of CoreOS cloud-config and how to set
    up the CoreOS development environment with Vagrant, Amazon AWS, Google GCE, and
    Bare Metal. We also covered some basic debugging steps for commonly encountered
    issues. As described in this chapter, it is easy to install CoreOS in the local
    data center or Cloud environments. It is better to try out deployment in a development
    cluster before moving to production environments. In the next chapter, we will
    cover how the CoreOS automatic update works.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Vagrant installation: [https://coreos.com/os/docs/latest/booting-on-vagrant.html](https://coreos.com/os/docs/latest/booting-on-vagrant.html)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS installation: [https://coreos.com/os/docs/latest/booting-on-ec2.html](https://coreos.com/os/docs/latest/booting-on-ec2.html)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GCE installation: [https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html](https://coreos.com/os/docs/latest/booting-on-google-compute-engine.html)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bare Metal installation: [https://coreos.com/os/docs/latest/installing-to-disk.html](https://coreos.com/os/docs/latest/installing-to-disk.html)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS CloudInit: [https://github.com/coreos/coreos-cloudinit](https://github.com/coreos/coreos-cloudinit)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading and tutorials
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction to the cloud-config format: [https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting](https://www.digitalocean.com/community/tutorials/an-introduction-to-cloud-config-scripting)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoreOS with AWS Cloudformation: [http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/](http://blog.michaelhamrah.com/2015/03/managing-coreos-clusters-on-aws-with-cloudformation/)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CoreOS bare-metal installation: [http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial](http://stevieholdway.tumblr.com/post/90167512059/coreos-bare-metal-iso-install-tutorial)
    and [http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux](http://linuxconfig.org/how-to-perform-a-bare-metal-installation-of-coreos-linux)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using journalctl to view systemd logs: [https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
