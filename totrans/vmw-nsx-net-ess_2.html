<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0; NSX Architecture"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2.  NSX Architecture </h1></div></div></div><p>In this chapter, we will have a high-level discussion on NSX architecture. To properly install and configure NSX, we should understand the core components that are involved in the NSX solution. By the end of this chapter, we will have a good understanding of the following aspects:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Network planes</li><li class="listitem" style="list-style-type: disc">NSX core components</li><li class="listitem" style="list-style-type: disc">Identifying controller roles</li><li class="listitem" style="list-style-type: disc">The controller cluster</li><li class="listitem" style="list-style-type: disc">VXLAN architecture</li></ul></div><div class="section" title="Introducing network planes"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Introducing network planes</h1></div></div></div><p>In a traditional switch/router, routing and packet forwarding is ideally done on the same device. What does this mean? Let's take a classic example of configuring a router. We might configure SSH for managing the router and later configure routing protocols to exchange the routes with its neighbors. All these common tasks are done specifically on the same hardware device. So, in a nutshell, each and every router will take a forwarding decision based on the configuration of routers. The power of software-defined networking is decoupling the forwarding and control plane functionality to a centralized device called a controller and the end result is the controller maintaining the forwarding information and taking decisions rather than going via hop by hop in the traditional way. As shown in the following figure, the three functional planes of a network are the management plane, the control plane, and the data plane:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" alt="Introducing network planes"/></div><p>
</p><p>The three functional planes of a network are explained as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Management plane</strong></span>: The management plane is a straightforward concept: a slice of software through which we will make changes and configure network devices, and protocols such as SSH and SNMP are used to access and monitor them.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Control plane</strong></span>: The classic example for the control plane is learning routes and making decisions based on routing algorithms. However, control plane functionality is not limited to learning routes. The control plane also helps in pairing with vendor-specific devices, and secures control plane access such as SSH and Telnet.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data plane</strong></span>: Data plane traffic is traditionally performed in dedicated hardware devices by consuming a little bit of compute resources. Primarily, the data plane is focused on data forwarding tasks.</li></ul></div><p>I know most of us will be wondering why we are discussing network planes here. Network planes are DNA in the NSX world and we have all three planes which make the network virtualization layer.</p></div></div>
<div class="section" title="NSX vSphere components"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>NSX vSphere components</h1></div></div></div><p>NSX uses the management plane, control plane, and data plane models. The components are represented diagrammatically in the following diagram:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" alt="NSX vSphere components"/></div><p>
</p><div class="section" title="The management plane"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec5"/>The management plane</h2></div></div></div><p>The management plane contains the NSX Manager and vCenter Server. It is important to know that each NSX Manager should be registered with only one vCenter Server. The NSX Manager provides a management UI and API for NSX. We will be discussing NSX Manager and vCenter Server integration during NSX Manager installation and configuration modules. Once the integration is done, NSX Manager can be managed from a vSphere web client, which acts as a single pane of glass for configuring and securing the vSphere infrastructure. Immediate benefit is network administrators no longer need to switch between multiple management consoles. All network services can be configured and monitored from a single interface.</p></div><div class="section" title="The control plane"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec6"/>The control plane</h2></div></div></div><p>The control plane primarily consists of NSX Controllers and the control VM, which allows us to perform distributed routing. The control plane also allows multicast-free VXLAN networks, which was a limitation in earlier vCloud networking and security versions. Controllers maintain ARP, VTEP (VXLAN tunnel endpoint), and MAC table. The NSX logical router control virtual machine and VMware NSX Controller are virtual machines that are deployed by VMware NSX Manager. The <span class="strong"><strong>User World Agent</strong></span> (<span class="strong"><strong>UWA</strong></span>) is composed of the ntcpad and vsfwd daemons on the ESXi host. Communication related to NSX between the NSX Manager instance or the NSX Controller instances and the ESXi host happen through the UWA. NSX Controller clusters are deployed in ODD number fashion and the maximum number of supported controllers is three. Since every controller in a control cluster is active at the same time, it ensures that the control plane is intact even if there is a controller failure. Controllers talk to each other to be in sync through a secured SSL channel. Controllers use a slicing technology to divide the workload among other controllers. Have a look at the following figure, which is a three-node controller cluster, in which slicing technology is dividing the workload across the controllers:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_02_03.jpg" alt="The control plane"/></div><p>
</p><p>It is important to understand that there are two types of applications running on each of these controllers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VXLAN</strong></span>: Enables extension of a Layer-2 IP subnet anywhere in the fabric, irrespective of the physical network design.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Logical router</strong></span>: Routing between IP subnets can be done in a logical space without traffic touching the physical router. This routing is performed directly in the hypervisor kernel with minimal CPU/memory overhead. This functionality provides an optimal data path for routing traffic within the virtual infrastructure.</li></ul></div><p>The functionality of these applications is to learn and populate controller tables, and also distribute learned routes to underlying ESXi hosts. Lastly, the control plane and data plane configuration will be intact even during the failure of a management plane-this is the real power of software-defined networking.</p><div class="section" title="Three-node controller clusters"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec0"/>Three-node controller clusters</h3></div></div></div><p>In a large-scale distributed system with <span class="emphasis"><em>n</em></span> number of servers, it is extremely difficult to ensure that one specific server can perform a write operation to a database or that only one server is the master that processes all writes. The fundamental problem is we do not have a simple way through which process execution can be done. How do we resolve this? All we need is to promote one server as master, and have some consensus with other servers. Paxos is a distributed consensus protocol published in 1989. The algorithm also ensures we have a leader election whenever there is a server failure. Paxos distinguishes the roles of proposer, acceptor, and learner, where a process (server/node) can play one or more roles simultaneously. The following are a few vendors who are using the Paxos algorithm extensively for the same reason:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">VMware NSX Controller uses a Paxos-based algorithm within an NSX Controller cluster</li><li class="listitem" style="list-style-type: disc">Amazon Web Services uses the Paxos algorithm extensively to power its platform</li><li class="listitem" style="list-style-type: disc">Nutanix implements the Paxos algorithm to ensure strict consistency is maintained in cassandara (for storing cluster metadata)</li><li class="listitem" style="list-style-type: disc">Apache Mesos uses the Paxos algorithm for its replicated log coordination</li><li class="listitem" style="list-style-type: disc">Google uses the Paxos algorithm for providing the Chubby lock service for loosely coupled distributed systems</li><li class="listitem" style="list-style-type: disc">The Windows fabric used by many of the Azure services makes use of the Paxos algorithm for replication between nodes in a cluster</li></ul></div><p>NSX Controllers are deployed in a three-node clustered fashion to ensure we are getting the highest level of resiliency since the controllers are running a fault-tolerant, distributed consensus algorithm called <span class="strong"><strong>Paxos</strong></span>.</p><div class="section" title="Controller roles"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec0"/>Controller roles</h4></div></div></div><p>The NSX Controller provides the control plane functions for routing and logical switching functions. Each controller node is running a set of roles that defines the type of task the controller node can run. There are total of five roles running in a controller node; they are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">API</li><li class="listitem" style="list-style-type: disc">Persistence server</li><li class="listitem" style="list-style-type: disc">Logical manager</li><li class="listitem" style="list-style-type: disc">Switch manager</li><li class="listitem" style="list-style-type: disc">Directory server</li></ul></div><p>While each of these roles needs a different master, it is important to understand that the leader is the responsible controller for allocating the tasks to other controllers.</p><p>The following figure depicts the responsibilities of various roles:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_02_04-1024x783.jpg" alt="Controller roles"/></div><p>
</p><p>As we know, three node controllers form a control cluster. We will have a look at the role election per controller. Each role has a master controller node and only if a master controller node for a given role fails there would be a cluster-wide election for a new master role. This is one of the prime reasons a three-node cluster is a must in an enterprise environment, to avoid any split-brain situation which might eventually end up with data inconsistencies and the whole purpose of the control plane would be defeated. In the following figure, we have a three-node control cluster running and each controller is running a master role:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 1</strong></span>: Directory server master role running</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 2</strong></span>: Persistence server master role running</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Controller 3</strong></span>: API and switch manager master role running</li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" alt="Controller roles"/></div><p>
</p></div></div></div><div class="section" title="The data plane"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec7"/>The data plane</h2></div></div></div><p>NSX logical switches, ESXi hypervisor, distributed switches, and NSX edge devices are all data plane components. Once the management plane is up and running, we can deploy control plane and data plane software and components. Behind the scenes, these three <span class="strong"><strong>VMware Installation Bundles</strong></span> (<span class="strong"><strong>VIB</strong></span>) get pushed to the underlying ESXi hypervisor:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">VXLAN VIB</li><li class="listitem">Distributed routing VIB</li><li class="listitem">Distributed firewall VIB</li></ol></div><p>Up to now, we have discussed the management, control, and data plane components in the NSX world; in the upcoming modules, we will have a closer look at the installation part and design specification for each layer.</p></div></div>
<div class="section" title="Overlay networks"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Overlay networks</h1></div></div></div><p>A virtual network which is built on top of a physical network is called an overlay network. Does that sound familiar by any chance? Most enterprise environments would have used VPN technology for securing private or public networks, which is an IP-over-IP overlay technology. However, it is very important to understand that not all overlay networks are built on top of IP networks. The real question is, why do we require an overlay network ? As we can see from the following figure, we have two data centers and each of these data centers is following a spine-leaf topology. Flexible workload placement and virtual machine mobility, along with strong multitenancy for each tenant, are common asks in a virtualized infrastructure:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_02_06.jpg" alt="Overlay networks"/></div><p>
</p><p>VXLAN, MPLS, NVGRE, VPN, and OTV are some of the classic examples of network-based overlays. Let's go back to the roots of server virtualization. Server virtualization virtualizes a physical server and allows multiple virtual machines to run on top of it with its own compute and storage resources. Now, each virtual machine might be having one or more than one MAC address, which eventually increases the MAC table in the network. Added to that, VM mobility forces the broadcast domains to grow. The traditional way of segmenting a network would be with the help of VLAN. As per the 802.1q standard, a VLAN tag is a 12-bit space providing a maximum of 4,096 VLANS. This is not a feasible solution in current multi tenant cloud environments wherein multiple machines reside on the same server and network isolation is required and workloads keep spiking, which demands a few VLANs to be provisioned for future growth and VLAN sprawl continues to happen based on workload mobility. Overlay networks alleviate this problem by providing Layer 2 connectivity independent of physical networks. We all know new technologies solve a lot of problems; however, there will always be challenges associated with them. Guess how difficult it will be for a network administrator to troubleshoot an overlay network! Trust me, when the mapping between the overlay network and physical network is crystal clear, it is extremely easy to perform troubleshooting. An NSX-VXLAN-based overlay network is a host-based overlay network which uses a UDP-based VXLAN encapsulation.</p></div>
<div class="section" title="The VLAN packet"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>The VLAN packet</h1></div></div></div><p>Before trying to understand VXLAN, let's go back to the fundamentals of a VLAN packet. How does  tagging work in a VLAN packet? It's very simple concept: 4 bytes are inserted into the Ethernet header field (IEEE), which are a combination of a 2-byte <span class="strong"><strong>Tag Protocol Identifier</strong></span> (<span class="strong"><strong>TPID</strong></span>) and 2 bytes of <span class="strong"><strong>Tag Control Information</strong></span> (<span class="strong"><strong>TCI</strong></span>). The priority field is a 3-bit field that allows information priority to be encoded in the overall frame, 0 being the lowest priority and 8 the highest value. CFI is typically a bit used for compatibility between Ethernet and token ring networks and if the value is 0, those are Ethernet switches. Last but not the least, we have the VLAN field - VID:</p><p>
</p><div class="mediaobject"><img src="graphics/B03244_02_07.jpg" alt="The VLAN packet"/></div><p>
</p><p>The act of creating a VLAN on a switch involves defining a set of switch ports, and end devices get connected to these ports. They all become part of that VLAN domain which eventually stops a broadcast not to be forwarded to another set of VLANs. I know whatever we have discussed so far is something which we will have heard in every networking class. This is just a repetition to ensure we never forget it. Now we can move on and discuss VXLAN.</p></div>
<div class="section" title="A VXLAN overview"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>A VXLAN overview</h1></div></div></div><p>VXLAN is a technology developed by vendors such as Arista, VMware, Cisco, and Broadcom. Each of these VXLAN networks is called a logical switch (virtual wires in vCloud network security solution) and they are identified by a 24-bit segment-ID. In this way, customers can create up to 16 million VXLAN networks. <span class="strong"><strong>Virtual Tunnel End Points</strong></span> (<span class="strong"><strong>VTEPs</strong></span>) are the endpoints that encapsulate and de-encapsulate the VXLAN frames. Let's understand a few key terminologies in VXLAN; and we will discuss VXLAN frames after that:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VXLAN VIB</strong></span>: VXLAN VIB or VMkernel modules are pushed to an underlying hypervisor during ESXi host preparation from NSX Manager.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Vmknic adapter</strong></span>: Virtual adapter is responsible for sending ARP, DHCP, and multicast join messages. Yes, there would be an IP assigned (static/dynamic) to vmknic from the VTEP IP pool, which is one of the prerequisites for VXLAN configuration. NSX supports multiple VXLAN vmknics per host for uplink load balancing features.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VXLAN port group</strong></span>: VXLAN port group is preconfigured during host preparation and it includes components and features such as NIC teaming policy, VLAN, and other NIC details.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VTEP proxy</strong></span>: VTEP proxy is a VTEP that forwards VXLAN traffic to its local segment from another VTEP in a remote segment. NSX uses three modes of VXLAN: unicast, multicast, and hybrid. In unicast mode, VXLAN VTEP proxy is called UTEP and in hybrid mode, it is called MTEP.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VNI</strong></span>: <span class="strong"><strong>VXLAN Network Identifier</strong></span> (<span class="strong"><strong>VNI</strong></span>) or segment ID is similar to VLAN-bit field; however, VNI is a 24-bit address that gets added to a VXLAN frame. This is one of the most critical elements in VXLAN frames, since it uniquely identifies the VXLAN network just as a VLAN ID identifies a VLAN network. VNI numbers start with 5000.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transport zones</strong></span>: Transport zones are basically a cluster or a group of clusters which define a VXLAN boundary or domain. Transport zones can be local or universal for multi VC deployment based on NSX design.</li></ul></div><div class="section" title="The VXLAN frame"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec8"/>The VXLAN frame</h2></div></div></div><p>The following are the main components of the VXLAN frame along with the figure:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outer Ethernet header (L2 header)</strong></span>: The destination MAC in the outer Ethernet header can be a next hop router MAC or destination VTEP MAC addresses and the source outer MAC address will be the source VTEP MAC address.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outer IP header (L3 header)</strong></span>: Respective VTEP source and destination IP will be populated in the outer IP header.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outer UDP header (L4 header)</strong></span>: The outer UDP header is a combination of source port and destination port. IANA has assigned the value <code class="literal">4789</code> for UDP; however, the VMware NSX default UDP port is <code class="literal">8472.</code> So it is important to allow port <code class="literal">8472</code> in physical/virtual firewall devices for VXLAN traffic.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>VXLAN header</strong></span>: This is an 8-byte field which will have the VXLAN flag value, segment-ID, and reserved field:</li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/image_02_008.jpg" alt="The VXLAN frame"/></div><p>
</p><p>Since we have discussed the VXLAN packet format, let's move on and check the inner Ethernet frame.</p></div><div class="section" title="The inner Ethernet frame"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec9"/>The inner Ethernet frame</h2></div></div></div><p>The following are the main components of the inner Ethernet frame along with the figure:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Frame Check Sequence (FCS)</strong></span>: FCS is a field at the end of the frame which is used to store the <span class="strong"><strong>cyclic redundancy check</strong></span> (<span class="strong"><strong>CRC</strong></span>) information. CRC is an algorithm which will run each time a frame is built, based on the data in the frame. When a receiving host receives the frame and runs the CRC, the answer should be the same. If not, the frame is discarded.</li><li class="listitem"><span class="strong"><strong>Ethernet payload</strong></span>: The length of a frame is very important, whether it is maximum or minimum frame size. The Ethernet payload field is a length field delimiting the length of the packet.</li><li class="listitem"><span class="strong"><strong>Inner source MAC address</strong></span>: The inner source MAC address will be the MAC address of the virtual machine which is connected to the VXLAN network.</li><li class="listitem"><span class="strong"><strong>Outer destination MAC address</strong></span>: The outer destination MAC address will be the MAC address of the destination virtual machine. Pause for a moment: what if a virtual machine doesn't know the destination virtual machine MAC? There is no rocket science here. All it does is a traditional broadcast, <code class="literal">ff:ff:ff:ff:ff:ff</code> (destination MAC), which is the broadcast address, and addresses every network adapter in the network and later this complete L2 frame will get encapsulated with the VXLAN frame before leaving the hypervisor host.</li></ol></div></div><div class="section" title="The life of a VXLAN packet"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>The life of a VXLAN packet</h2></div></div></div><p>If we need to understand how a sequence of packets from a source machine reaches the destination in a unicast, multicast, or broadcast domain, all we do is a simple packet walk. In the following example, if <span class="strong"><strong>VM-A</strong></span> is communicating with <span class="strong"><strong>VM-B</strong></span> for the first time in a VXLAN domain, how is the encapsulation and de-encapsulation process happening?</p><p>The following diagram shows you how:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_009.jpg" alt="The life of a VXLAN packet"/></div><p>
</p><p>Let us now go through the steps one by one:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">VM A-192.168.1.1, MAC- A on ESXi host A generates a broadcast frame (Layer 2 broadcast frame)</li><li class="listitem">VTEP A on host A encapsulates the broadcast frame into UDP header with the destination IP as multicast/unicast IP based on VXLAN replication modes (Layer 2 header gets encapsulated with VXLAN header)<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note4"/>Note</h3><p>We will certainly have a discussion on VXLAN replication modes in <a class="link" href="ch04.html" title="Chapter 4. NSX Virtual Networks and Logical Router">Chapter 4</a>, <span class="emphasis"><em>NSX Virtual Networks and Logical Routing</em></span>.</p></div></div></li><li class="listitem">The physical network delivers the packet to the host, because it was part of either the multicast group or unicast IP that we have defined during VXLAN replication modes.</li><li class="listitem">VTEP B on ESXi host B will look at the VXLAN header (24-bit) and if matches with the VTEP table entries, it removes the VXLAN encapsulation header and delivers the Layer 2 packet to the virtual machine.</li></ol></div><p>The preceding four steps mentioned are a simple packet walk on how virtual machines communicate in a VXLAN network. However, I haven't explained about ARP suppression and VTEP table learning because I want to explain that during NSX virtual networks and logical router.</p><p>Think for a minute and check in which scenario there won't be any encapsulation and de-encapsulation even though virtual machines are connected to a VXLAN network. No brainteasers here. If both the virtual machines are residing on same ESXi host and same VXLAN network, all it does is traditional Layer 2 learning and there is no encapsulation or de-encapsulation. These are very important points to note since it would ease a lot of troubleshooting issues when virtual machine to virtual machine communication is not happening due to VXLAN/physical network issues. I'm sure we have done such troubleshooting in vSphere environments.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Summary</h1></div></div></div><p>We began the chapter with a brief introduction of NSX core components and looked at the management, control, and data plane components. We then discussed the NSX Manager and the NSX Controller clusters, which was followed by a VXLAN architecture overview discussion where we looked at the VLAN and VXLAN packet followed by a simple packet walk. Now we are familiar with the core components and their functionality.</p><p>In the next chapter, we will discuss NSX Manager installation and configuration.</p></div></body></html>