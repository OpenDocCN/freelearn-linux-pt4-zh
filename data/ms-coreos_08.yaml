- en: Chapter 8. Container Orchestration
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章 容器编排
- en: As Containers became the basis of modern application development and deployment,
    it is necessary to deploy hundreds or thousands of Containers to a single data
    center cluster or data center clusters. The cluster could be an on-premises cluster
    or in a cloud. It is necessary to have a good Container orchestration system to
    deploy and manage Containers at scale.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着容器成为现代应用开发和部署的基础，部署数百个或数千个容器到单一数据中心集群或数据中心集群变得必要。集群可以是本地集群或云端集群。为了在大规模下部署和管理容器，必须有一个良好的容器编排系统。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: The basics of modern application deployment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代应用部署的基础
- en: Container orchestration with Kubernetes, Docker Swarm, and Mesos and their core
    concepts, installation, and deployment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes、Docker Swarm 和 Mesos 进行容器编排及其核心概念、安装和部署
- en: Comparison of popular orchestration solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的编排解决方案比较
- en: Application definition with Docker Compose
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker Compose 进行应用定义
- en: Packaged Container Orchestration solutions—the AWS container service, Google
    container engine, and CoreOS Tectonic
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打包的容器编排解决方案——AWS 容器服务、Google 容器引擎和 CoreOS Tectonic
- en: Modern application deployment
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现代应用部署
- en: We covered the basics of Microservices in [Chapter 1](index_split_023.html#filepos77735),
    CoreOS Overview. In cloud-based application development, infrastructure is treated
    as cattle rather than pet ([http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story](http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story)).
    What this means is that the infrastructure is commonly a commodity hardware that
    can easily go bad and high availability needs to be handled at either the application
    layer or application Orchestration layer. High availability can be taken care
    of by having a combination of the load balancer and Orchestration system that
    monitors the health of services taking necessary actions such as respawning the
    service if it dies. Containers have the nice property of isolation and packaging
    that allows independent teams to develop individual components as Containers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](index_split_023.html#filepos77735)中介绍了微服务的基础，CoreOS 概述。在基于云的应用开发中，基础设施被视为“牲畜”而非“宠物”([http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story](http://www.slideshare.net/randybias/pets-vs-cattle-the-elastic-cloud-story))。这意味着基础设施通常是商品化的硬件，可能会出现故障，高可用性需要在应用层或应用编排层进行处理。高可用性可以通过负载均衡器和编排系统的组合来实现，这些系统监控服务的健康状况，采取必要的行动，例如在服务崩溃时重新启动服务。容器具有隔离和封装的优良特性，使得独立的团队能够作为容器开发各自的组件。
- en: 'Companies can adopt a pay-as-you-grow model where they can scale their Containers
    as they grow. It is necessary to manage hundreds or thousands of Containers at
    scale. To do this efficiently, we need a Container Orchestration system. The following
    are some characteristics of a Container Orchestration system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 企业可以采用按需增长模式，根据需要扩展容器。在大规模管理数百或数千个容器时，必须使用容器编排系统。以下是容器编排系统的一些特点：
- en: It treats disparate infrastructure hardware as a collection and represents it
    as one single resource to the application
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将不同的基础设施硬件视为一个集合，并将其表示为应用程序的单一资源
- en: It schedules Containers based on user constraints and uses the infrastructure
    in the most efficient manner
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它根据用户约束调度容器，并以最有效的方式利用基础设施
- en: It scales out containers dynamically
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它动态地扩展容器
- en: It maintains high availability of services
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它保持服务的高可用性
- en: 'There is a close relation between the application definition and Container
    Orchestration. The application definition is typically a manifest file describing
    the Containers that are part of the application and the services that the Container
    exposes. Container Orchestration is done based on the application definition.
    The Container Orchestrator operates on resources that could be a VM or bare metal.
    Typically, the nodes where Containers run are installed with Container-optimized
    OSes, such as CoreOS, DCOS, and Atomic. The following image shows you the relationship
    between the application definition, Container Orchestration, and Container-optimized
    nodes along with some examples of solutions in each category:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序定义与容器编排之间有着密切的关系。应用程序定义通常是一个清单文件，描述了应用程序中的容器以及容器暴露的服务。容器编排是基于应用程序定义进行的。容器编排器在资源上运行，这些资源可以是虚拟机或裸机。通常，容器运行的节点安装有容器优化的操作系统，例如
    CoreOS、DCOS 和 Atomic。以下图像展示了应用程序定义、容器编排和容器优化节点之间的关系，并给出了每个类别中一些解决方案的示例：
- en: '![](img/00458.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00458.jpg)'
- en: Container Orchestration
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排
- en: A basic requirement of Container orchestration is to efficiently deploy M containers
    into N compute resources.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排的基本要求是高效地将 M 个容器部署到 N 个计算资源中。
- en: 'The following are some problems that a Container Orchestration system should
    solve:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是容器编排系统应该解决的一些问题：
- en: It should schedule containers efficiently, giving enough control to the user
    to tweak scheduling parameters based on their need
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该高效地调度容器，并为用户提供足够的控制，以便根据需要调整调度参数
- en: It should provide Container networking across the cluster
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该提供跨集群的容器网络
- en: Services should be able to discover each other dynamically
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务应该能够动态发现彼此
- en: Orchestration system should be able to handle service failures
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排系统应该能够处理服务故障
- en: We will cover Kubernetes, Docker Swarm, and Mesos in the following sections.
    Fleet is used internally by CoreOS for Container orchestration. Fleet has very
    minimal capabilities and works well for the deployment of critical system services
    in CoreOS. For very small deployments, Fleet can be used for Container orchestration,
    if necessary.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中介绍 Kubernetes、Docker Swarm 和 Mesos。Fleet 是 CoreOS 内部用于容器编排的工具。Fleet
    功能非常简约，适合在 CoreOS 中部署关键系统服务。对于非常小的部署，如果需要，也可以使用 Fleet 进行容器编排。
- en: Kubernetes
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is an open source platform for Container Orchestration. This was
    initially started by Google and now multiple vendors are working together in this
    open source project. Google has used Containers to develop and deploy applications
    in their internal data center and they had a system called Borg ([http://research.google.com/pubs/pub43438.html](http://research.google.com/pubs/pub43438.html))
    for cluster management. Kubernetes uses a lot of the concepts from Borg combined
    with modern technologies available now. Kubernetes is lightweight, works across
    almost all environments, and has a lot of industry traction currently.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个开源的容器编排平台。最初由谷歌发起，现在多个供应商正在共同参与这个开源项目。谷歌一直使用容器来开发和部署应用程序，且在其内部数据中心有一个叫
    Borg的系统（[http://research.google.com/pubs/pub43438.html](http://research.google.com/pubs/pub43438.html)）用于集群管理。Kubernetes
    借鉴了 Borg 的许多概念，并结合了现有的现代技术。Kubernetes 轻量级，能够在几乎所有环境中运行，目前在行业中有着广泛的应用。
- en: Concepts of Kubernetes
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的概念
- en: Kubernetes has some unique concepts, and it will be good to understand them
    before diving deep into the architecture of Kubernetes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有一些独特的概念，深入了解这些概念会对理解 Kubernetes 的架构非常有帮助。
- en: Pods
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Pods
- en: Pods are a set of Containers that are scheduled together in a single node and
    need to work closely with each other. All containers in a Pod share the IPC namespace,
    network namespace, UTS namespace, and PID namespace. By sharing the IPC namespace,
    Containers can use IPC mechanisms to talk to each other.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 是一组在同一个节点上调度并需要紧密协作的容器。Pod 中的所有容器共享 IPC 命名空间、网络命名空间、UTS 命名空间和 PID 命名空间。通过共享
    IPC 命名空间，容器可以使用 IPC 机制相互通信。
- en: 'By sharing the network namespace, Containers can use sockets to talk to each
    other, and all Containers in a Pod share a single IP address. By sharing the UTS
    namespace, volumes can be mounted to a Pod and all Containers can see these volumes.
    The following are some common application deployment patterns with Pods:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过共享网络命名空间，容器可以使用套接字相互通信，并且Pod中的所有容器共享一个IP地址。通过共享UTS命名空间，卷可以挂载到Pod上，所有容器都可以访问这些卷。以下是一些常见的应用部署模式与Pods结合使用的例子：
- en: 'Sidecar pattern: An example is an application container and logging container
    or application synchronizer container such as a Git synchronizer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sidecar模式：一个例子是应用容器和日志容器，或者像Git同步器这样的应用同步容器。
- en: 'Ambassador pattern: In this pattern, the application container and proxy container
    work together. When the application container changes, external services can still
    talk to the proxy container as before. An example is a redis application container
    with the redis proxy.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ambassador模式：在这种模式下，应用容器和代理容器一起工作。当应用容器发生变化时，外部服务仍然可以像以前一样与代理容器通信。一个例子是一个带有redis代理的redis应用容器。
- en: 'Adapter pattern: In this pattern, there is an application container and adapter
    container that adapts to different environments. An example is a logging container
    that works as an adapter and changes with different cloud providers but the interface
    to the adapter container remains the same.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adapter模式：在这种模式下，存在一个应用容器和适配器容器，适配不同的环境。一个例子是一个日志容器，它作为适配器，随着不同云服务提供商的变化而变化，但适配器容器的接口保持不变。
- en: The smallest unit in Kubernetes is a Pod and Kubernetes takes care of scheduling
    the Pods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中最小的单元是Pod，Kubernetes负责调度Pods。
- en: 'The following is a Pod definition example with the NGINX Container and Git
    helper container:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个包含NGINX容器和Git助手容器的Pod定义示例：
- en: '`apiVersion: v1 kind: Pod metadata:   name: www spec:   containers:   - name: nginx
        image: nginx   - name: git-monitor     image: kubernetes/git-monitor     env:
        - name: GIT_REPO       value: http://github.com/some/repo.git`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Pod metadata:   name: www spec:   containers:   - name: nginx
        image: nginx   - name: git-monitor     image: kubernetes/git-monitor     env:
        - name: GIT_REPO       value: http://github.com/some/repo.git`'
- en: Networking
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 网络
- en: Kubernetes has the one IP per Pod approach. This approach was taken to avoid
    the pains associated with NAT to access Container services when Containers shared
    the host IP address. All Containers in a pod share the same IP address. Pods across
    nodes can talk to each other using different techniques such as cloud-based routing
    from cloud providers, Flannel, Weave, Calico, and others. The end goal is to have
    Networking as a plugin within Kubernetes and the user can choose the plugin based
    on their needs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes采用每个Pod一个IP的方式。这种方式避免了容器共享主机IP地址时，使用NAT访问容器服务的痛苦。Pod中的所有容器共享相同的IP地址。跨节点的Pods可以使用不同的技术进行通信，例如云提供商的云路由、Flannel、Weave、Calico等。最终目标是将网络作为Kubernetes中的插件，用户可以根据需求选择插件。
- en: Services
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 服务
- en: Services are an abstraction that Kubernetes provides to logically combine Pods
    that provide similar functionality. Typically, Labels are used as selectors to
    create services from Pods. As Pods are ephemeral, Kubernetes creates a service
    object with its own IP address that always remains permanent. Kubernetes takes
    care of load balancing for multiple pods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是Kubernetes提供的一种抽象，用于逻辑上组合提供相似功能的Pods。通常，标签（Labels）作为选择器，用来从Pods创建服务。由于Pods是短暂的，Kubernetes会创建一个服务对象，并为其分配一个永远保持不变的IP地址。Kubernetes负责多个Pod的负载均衡。
- en: 'The following is an example service:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例服务：
- en: '`{     "kind": "Service",     "apiVersion": "v1",     "metadata": {         "name": "my-service"
        },     "spec": {         "selector": {             "app": "MyApp"         },
            "ports": [             {                 "protocol": "TCP",                 "port": 80,
                    "targetPort": 9376             }         ]     } }`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`{     "kind": "Service",     "apiVersion": "v1",     "metadata": {         "name": "my-service"
        },     "spec": {         "selector": {             "app": "MyApp"         },
            "ports": [             {                 "protocol": "TCP",                 "port": 80,
                    "targetPort": 9376             }         ]     } }`'
- en: In the preceding example, we created a `my-service` service that groups all
    pods with a `Myapp` label. Any request to the `my-service` service's IP address
    and port number `80` will be load balanced to all the pods with the `Myapp` label
    and redirected to port number `9376`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们创建了一个`my-service`服务，将所有带有`Myapp`标签的Pod组合在一起。任何访问`my-service`服务的IP地址和端口号`80`的请求都会被负载均衡到所有带有`Myapp`标签的Pods，并被重定向到端口号`9376`。
- en: Services need to be discovered internally or externally based on the type of
    service. An example of internal discovery is a web service needing to talk to
    a database service. An example of external discovery is a web service that gets
    exposed to the outside world.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据服务类型，需要在内部或外部发现服务。内部发现的示例是Web服务需要与数据库服务通信。外部发现的示例是将Web服务暴露给外部世界。
- en: 'For internal service discovery, Kubernetes provides two options:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内部服务发现，Kubernetes提供两种选项：
- en: 'Environment variable: When a new Pod is created, environment variables from
    older services can be imported. This allows services to talk to each other. This
    approach enforces ordering in service creation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境变量：创建新Pod时，可以从旧服务导入环境变量。这允许服务之间进行通信。此方法强制执行服务创建中的顺序。
- en: 'DNS: Every service registers to the DNS service; using this, new services can
    find and talk to other services. Kubernetes provides the `kube-dns` service for
    this.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS：每个服务都注册到DNS服务；通过此服务，新服务可以找到并与其他服务通信。Kubernetes为此提供了`kube-dns`服务。
- en: 'For external service discovery, Kubernetes provides two options:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于外部服务发现，Kubernetes提供两种选项：
- en: 'NodePort: In this method, Kubernetes exposes the service through special ports
    (30000-32767) of the node IP address.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NodePort：通过此方法，Kubernetes通过节点IP地址的特殊端口（30000-32767）公开服务。
- en: 'Loadbalancer: In this method, Kubernetes interacts with the cloud provider
    to create a load balancer that redirects the traffic to the Pods. This approach
    is currently available with GCE.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器：通过此方法，Kubernetes与云提供商交互以创建负载均衡器，将流量重定向到Pod。目前此方法在GCE上可用。
- en: Kubernetes architecture
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: 'The following diagram shows you the different software components of the Kubernetes
    architecture and how they interact with each other:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Kubernetes架构的不同软件组件及其相互交互方式：
- en: '![](img/00460.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00460.jpg)'
- en: 'The following are some notes on the Kubernetes architecture:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于Kubernetes架构的一些注释：
- en: The master node hosts the Kubernetes control services. Slave nodes run the pods
    and are managed by master nodes. There can be multiple master nodes for redundancy
    purposes and to scale master services.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点托管Kubernetes控制服务。从节点运行Pod并由主节点管理。可以有多个主节点以实现冗余和扩展主服务。
- en: Master nodes run the critical services such as the Scheduler, Replication controller,
    and API server. Slave nodes run the critical services such as Kubelet and Kube-proxy.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点运行关键服务，例如调度器、复制控制器和API服务器。从节点运行关键服务，例如Kubelet和Kube-proxy。
- en: User interaction with Kubernetes is through Kubectl, which uses standard Kubernetes-exposed
    APIs.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户与Kubernetes的交互通过Kubectl进行，它使用标准的Kubernetes公开的API。
- en: The Kubernetes scheduler takes care of scheduling the pods in the nodes based
    on the constraints specified in the Pod manifest.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes调度器负责根据Pod清单中指定的约束在节点上调度Pod。
- en: The replication controller is necessary to maintain high availability of Pods
    and create multiple instances of pods as specified in the replication controller
    manifest.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制控制器是维护Pod的高可用性和根据复制控制器清单创建多个Pod实例所必需的。
- en: The API server in the master node talks to Kubelet of each slave node to provision
    the pods.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点中的API服务器与每个从节点的Kubelet进行通信，以部署Pod。
- en: Kube-proxy takes care of service redirection and load balancing the traffic
    to the Pods.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube-proxy负责服务重定向和负载均衡流量到Pod。
- en: Etcd is used as a shared data repository for all nodes to communicate with each
    other.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Etcd用作所有节点之间通信的共享数据存储库。
- en: DNS is used for service discovery.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS用于服务发现。
- en: Kubernetes installation
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes安装
- en: Kubernetes can be installed on baremetal, VM, or in cloud providers such as
    AWS, GCE, and Azure. We can decide on the choice of the host OS on any of these
    systems. In this chapter, all the examples will use CoreOS as the host OS. As
    Kubernetes consists of multiple components such as the API server, scheduler,
    replication controller, kubectl, and kubeproxy spread between master and slave
    nodes, the manual installation of the individual components would be complicated.
    There are scripts provided by Kubernetes and its users that automate some of the
    node setup and software installation. The latest stable version of Kubernetes
    as of October 2015 is 1.0.7 and all examples in this chapter are based on the
    1.0.7 version.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以安装在裸金属、虚拟机或云提供商（如AWS、GCE和Azure）上。我们可以根据这些系统中的任何操作系统来决定主机操作系统的选择。在本章中，所有示例都将使用CoreOS作为主机操作系统。由于Kubernetes由多个组件组成，如API服务器、调度器、复制控制器、kubectl和kubeproxy，这些组件分布在主节点和从节点之间，手动安装这些单独的组件会非常复杂。Kubernetes及其用户提供了自动化某些节点设置和软件安装的脚本。截至2015年10月，Kubernetes的最新稳定版本是1.0.7，本章中的所有示例均基于1.0.7版本。
- en: Non-Coreos Kubernetes installation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 非CoreOS Kubernetes安装
- en: 'For non-Coreos-based Kubernetes installation, the procedure is straightforward:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非CoreOS基础的Kubernetes安装，过程很简单：
- en: Find the Kubernetes release necessary from [https://github.com/kubernetes/kubernetes/releases](https://github.com/kubernetes/kubernetes/releases).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/kubernetes/kubernetes/releases](https://github.com/kubernetes/kubernetes/releases)找到所需的Kubernetes发布版本。
- en: Download `kubernetes.tar.gz` for the appropriate version and unzip them.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载适当版本的`kubernetes.tar.gz`并解压。
- en: Set `KUBERNETES_PROVIDER` as one of these (AWS, GCE, Vagrant, and so on)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`KUBERNETES_PROVIDER`设置为以下之一（AWS、GCE、Vagrant等）
- en: Change the cluster size and any other configuration parameter in the `cluster`
    directory.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cluster`目录中更改集群大小和任何其他配置参数。
- en: Run `cluster/kube-up.sh`.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`cluster/kube-up.sh`。
- en: Kubectl installation
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Kubectl安装
- en: Kubectl is the CLI client to interact with Kubernetes. Kubectl is not installed
    by default. Kubectl can be installed in either the client machine or the kubernetes
    master node.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Kubectl是与Kubernetes交互的CLI客户端。Kubectl默认未安装。Kubectl可以安装在客户端机器或Kubernetes主节点上。
- en: 'The following command can be used to install kubectl. It is needed to match
    kubectl version with Kubernetes version:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装kubectl。需要确保kubectl版本与Kubernetes版本匹配：
- en: '`ARCH=linux; wget https://storage.googleapis.com/kubernetes-release/release/v1.0.7/bin/$ARCH/amd64/kubectl`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`ARCH=linux; wget https://storage.googleapis.com/kubernetes-release/release/v1.0.7/bin/$ARCH/amd64/kubectl`'
- en: 'If Kubectl is installed in the client machine, we can use the following command
    to proxy the request to the Kubernetes master node:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在客户端机器上已安装Kubectl，我们可以使用以下命令将请求代理到Kubernetes主节点：
- en: '`ssh -f -nNT -L 8080:127.0.0.1:8080 core@<control-external-ip>`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh -f -nNT -L 8080:127.0.0.1:8080 core@<control-external-ip>`'
- en: Vagrant installation
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Vagrant安装
- en: I used the procedure at [https://github.com/pires/kubernetes-vagrant-coreos-cluster](https://github.com/pires/kubernetes-vagrant-coreos-cluster)
    to create a Kubernetes cluster running on the Vagrant environment with CoreOS.
    I initially tried this in Windows. As I faced the issue mentioned in [https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158](https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158),
    I moved to the Vagrant environment running on Ubuntu Linux.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[https://github.com/pires/kubernetes-vagrant-coreos-cluster](https://github.com/pires/kubernetes-vagrant-coreos-cluster)中的过程，在Vagrant环境中使用CoreOS创建了一个Kubernetes集群。我最初在Windows上尝试过此操作。由于我遇到了[https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158](https://github.com/pires/kubernetes-vagrant-coreos-cluster/issues/158)中提到的问题，我转向了在Ubuntu
    Linux上运行的Vagrant环境。
- en: 'The following are the commands:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是命令：
- en: '`Git clone https://github.com/pires/kubernetes-vagrant-coreos-cluster.git``Cd coreos-container-platform-as-a-service/vagrant``Vagrant up`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Git clone https://github.com/pires/kubernetes-vagrant-coreos-cluster.git``Cd
    coreos-container-platform-as-a-service/vagrant``Vagrant up`'
- en: 'The following output shows the two running Kubernetes nodes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了两个运行中的Kubernetes节点：
- en: '![](img/00462.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00462.jpg)'
- en: GCE installation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GCE安装
- en: I used the procedure at [https://github.com/rimusz/coreos-multi-node-k8s-gce](https://github.com/rimusz/coreos-multi-node-k8s-gce)
    to create a Kubernetes cluster running in GCE with CoreOS.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[https://github.com/rimusz/coreos-multi-node-k8s-gce](https://github.com/rimusz/coreos-multi-node-k8s-gce)中的过程，在GCE中使用CoreOS创建了一个Kubernetes集群。
- en: 'The following are the commands:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是命令：
- en: '`git clone https://github.com/rimusz/coreos-multi-node-k8s-gce``cd coreos-multi-node-k8s-gce`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone https://github.com/rimusz/coreos-multi-node-k8s-gce``cd coreos-multi-node-k8s-gce`'
- en: In the `settings` file, change `project`, `zone`, `node count`, and any other
    necessary changes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `settings` 文件中，修改 `project`、`zone`、`node count` 以及其他必要的配置。
- en: 'Run the following three scripts in the same order:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相同顺序运行以下三个脚本：
- en: '`1-bootstrap_cluster.sh``2-get_k8s_fleet_etcd.sh``3-install_k8s_fleet_units.sh`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`1-bootstrap_cluster.sh``2-get_k8s_fleet_etcd.sh``3-install_k8s_fleet_units.sh`'
- en: 'The following output shows the cluster composed of three nodes:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了由三个节点组成的集群：
- en: '![](img/00464.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00464.jpg)'
- en: 'The following output shows the Kubernetes client and server versions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了 Kubernetes 客户端和服务端版本：
- en: '![](img/00465.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00465.jpg)'
- en: 'The following output shows the Kubernetes services running in master and slave
    nodes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了运行在主节点和从节点上的 Kubernetes 服务：
- en: '![](img/00468.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00468.jpg)'
- en: The script used in this example uses Fleet to orchestrate Kubernetes services.
    As we can see in the preceding image, the API server, controller, and scheduler
    run in the master node and kubelet and proxy run in the slave nodes. There are
    three copies of kubelet and kube-proxy, one each for every slave node.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中使用的脚本通过 Fleet 协调 Kubernetes 服务。如前面图示所示，API 服务器、控制器和调度器运行在主节点上，而 kubelet
    和代理运行在从节点上。每个从节点上都有三个副本的 kubelet 和 kube-proxy。
- en: AWS installation
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 安装
- en: I used the procedure at [https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html)
    to create the Kubernetes CoreOS cluster running on AWS.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 [https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html)
    中的流程在 AWS 上创建了运行 CoreOS 的 Kubernetes 集群。
- en: 'The first step is to install the kube-aws tool:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部是安装 kube-aws 工具：
- en: '`Git clone https://github.com/coreos/coreos-kubernetes/releases/download/v0.1.0/kube-aws-linux-amd64.tar.gz`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`Git clone https://github.com/coreos/coreos-kubernetes/releases/download/v0.1.0/kube-aws-linux-amd64.tar.gz`'
- en: Unzip and copy kube-aws to an executable path. Make sure that `~/.aws/credentials`
    is updated with your credentials.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解压并将 kube-aws 复制到可执行路径。确保 `~/.aws/credentials` 已更新为你的凭证。
- en: 'Create a default `cluster.yaml` file:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个默认的 `cluster.yaml` 文件：
- en: '`curl --silent --location https://raw.githubusercontent.com/coreos/coreos-kubernetes/master/multi-node/aws/cluster.yaml.example > cluster.yaml`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl --silent --location https://raw.githubusercontent.com/coreos/coreos-kubernetes/master/multi-node/aws/cluster.yaml.example
    > cluster.yaml`'
- en: Modify `cluster.yaml` with your `keyname`, `region`, and `externaldnsname`;
    `externaldnsname` matters for external access only.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 修改 `cluster.yaml` 文件，填写你的 `keyname`、`region` 和 `externaldnsname`；`externaldnsname`
    只对外部访问重要。
- en: 'To deploy the cluster, we can perform the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署集群，我们可以执行以下操作：
- en: '`Kube-aws up`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kube-aws up`'
- en: 'The following output shows the two nodes that are part of the Kubernetes cluster:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了属于 Kubernetes 集群的两个节点：
- en: '![](img/00470.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00470.jpg)'
- en: An example of a Kubernetes application
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 应用示例
- en: 'The following diagram illustrates the guestbook example that we will use to
    illustrate the different Kubernetes concepts discussed in the previous sections.
    This example is based on the reference at [http://kubernetes.io/v1.1/examples/guestbook/README.html](http://kubernetes.io/v1.1/examples/guestbook/README.html):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了我们将用来说明前面章节中讨论的不同 Kubernetes 概念的留言本示例。此示例基于 [http://kubernetes.io/v1.1/examples/guestbook/README.html](http://kubernetes.io/v1.1/examples/guestbook/README.html)
    的参考：
- en: '![](img/00472.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00472.jpg)'
- en: 'The following are some notes on this guestbook application:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于这个留言本应用的一些说明：
- en: This application uses the php frontend with the redis master and slave backend
    to store the guestbook database
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该应用使用 php 前端与 redis 主从后端存储留言本数据库。
- en: Frontend RC creates three instances of the `kubernetes/example-guestbook-php-redis`
    container
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端 RC 创建了三个 `kubernetes/example-guestbook-php-redis` 容器实例。
- en: Redis-master RC creates one instance of the redis container
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis-master RC 创建了一个 redis 容器实例。
- en: Redis-slave RC creates two instances of the `kubernetes/redis-slave` container
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis-slave RC 创建了两个 `kubernetes/redis-slave` 容器实例。
- en: For this example, I used the cluster created in the previous section with Kubernetes
    running on AWS with CoreOS. There is one master node and two slave nodes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我使用了上一节中创建的集群，该集群在 AWS 上运行 Kubernetes，使用 CoreOS。集群中有一个主节点和两个从节点。
- en: 'Let''s look at the nodes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些节点：
- en: '![](img/00474.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00474.jpg)'
- en: 'In this example, the Kubernetes cluster uses flannel to communicate across
    pods. The following output shows the flannel subnet allocated to each node in
    the cluster:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，Kubernetes 集群使用 flannel 跨 pods 进行通信。以下输出展示了分配给集群中每个节点的 flannel 子网：
- en: '![](img/00477.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00477.jpg)'
- en: 'The following are the commands necessary to start the application:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是启动应用程序所需的命令：
- en: '`kubectl create -f redis-master-controller.yaml``kubectl create --validate=false -f redis-master-service.yaml``kubectl create -f redis-slave-controller.yaml``kubectl create --validate=false -f redis-slave-service.yaml``kubectl create -f frontend-controller.yaml``kubectl create --validate=false -f frontend-service.yaml`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl create -f redis-master-controller.yaml``kubectl create --validate=false
    -f redis-master-service.yaml``kubectl create -f redis-slave-controller.yaml``kubectl
    create --validate=false -f redis-slave-service.yaml``kubectl create -f frontend-controller.yaml``kubectl
    create --validate=false -f frontend-service.yaml`'
- en: 'Let''s look at the list of pods:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 Pod 列表：
- en: '![](img/00478.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00478.jpg)'
- en: The preceding output shows three instances of the php frontend, one instance
    of the redis master, and two instances of the redis slave.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了三个 php 前端实例，一个 Redis 主节点实例和两个 Redis 从节点实例。
- en: 'Let''s look at the list of RC:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 RC 列表：
- en: '![](img/00480.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00480.jpg)'
- en: The preceding output shows the replication count per pod. Frontend has three
    replicas, `redis-master` has one replica, and `redis-slave` has two replicas,
    as we requested.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了每个 Pod 的副本数。前端有三个副本，`redis-master`有一个副本，`redis-slave`有两个副本，正如我们所要求的那样。
- en: 'Let''s look at the list of services:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下服务列表：
- en: '![](img/00481.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00481.jpg)'
- en: In the preceding output, we can see the three services comprising the guestbook
    application.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们可以看到组成留言簿应用程序的三个服务。
- en: 'For internal service discovery, this example uses `kube-dns`. The following
    output shows the `kube-dns` RC running:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内部服务发现，本示例使用了 `kube-dns`。以下输出显示了正在运行的 `kube-dns` RC：
- en: '![](img/00484.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00484.jpg)'
- en: 'For external service discovery, I modified the example to use the `NodePort`
    mechanism, where one of the internal ports gets exposed. The following is the
    new `frontend-service.yaml` file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于外部服务发现，我修改了示例以使用 `NodePort` 机制，其中一个内部端口被暴露。以下是新的 `frontend-service.yaml` 文件：
- en: '`apiVersion: v1 kind: Service metadata:   name: frontend   labels:     name: frontend
    spec:   # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.   type: NodePort   ports:
        # the port that this service should serve on   - port: 80   selector:     name: frontend`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: frontend   labels:     name:
    frontend spec:   # 如果你的集群支持它，取消注释以下内容以自动创建   # 外部负载均衡 IP 用于前端服务。   type: NodePort   ports:     #
    服务应该监听的端口   - port: 80   selector:     name: frontend`'
- en: 'The following is the output when we start the frontend service with the `NodePort`
    type. The output shows that the service is exposed using port `30193`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在启动 `NodePort` 类型的前端服务时的输出。输出显示该服务通过端口 `30193` 被暴露：
- en: '![](img/00486.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00486.jpg)'
- en: 'Once we expose port `30193` using the AWS firewall, we can access the guestbook
    application as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们通过 AWS 防火墙暴露端口 `30193`，就可以按以下方式访问留言簿应用程序：
- en: '![](img/00488.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00488.jpg)'
- en: 'Let''s look at the application containers in `Node1`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `Node1` 中的应用程序容器：
- en: '![](img/00489.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00489.jpg)'
- en: 'Let''s look at the application containers in `Node2`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `Node2` 中的应用程序容器：
- en: '![](img/00492.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00492.jpg)'
- en: The preceding output accounts for three instances of frontend, one instance
    of redis master, and two instances of redis slave.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出包含了三个前端实例，一个 Redis 主节点实例和两个 Redis 从节点实例。
- en: 'To illustrate how the replication controller maintains the pod replication
    count, I went and stopped the guestbook frontend Docker Container in one of the
    nodes, as shown in the following image:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明复制控制器如何维持 Pod 的副本数，我在一个节点中停止了留言簿前端 Docker 容器，如下图所示：
- en: '![](img/00493.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00493.jpg)'
- en: 'Kubernetes RC detects that the Pod is not running and restarts the Pod. This
    can be seen in the restart count for one of the guestbook pods, as shown in the
    following image:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes RC 检测到 Pod 未运行并重新启动了 Pod。这可以通过查看其中一个留言簿 Pod 的重启次数来看到，如下图所示：
- en: '![](img/00495.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00495.jpg)'
- en: 'To do some basic debugging, we can log in to the pods or containers themselves.
    The following example shows you how we can get inside the Pod:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行一些基本的调试，我们可以登录到 Pod 或容器中。以下示例显示了如何进入 Pod：
- en: '![](img/00496.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00496.jpg)'
- en: The preceding output shows the IP address in the guestbook pod, which agrees
    with the flannel subnet allocated to that node, as shown in the Flannel output
    in the beginning of this example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了在留言簿 Pod 中的 IP 地址，这与分配给该节点的 Flannel 子网一致，正如本示例开头的 Flannel 输出所示。
- en: 'Another command that''s useful for the debugging is `kubectl logs` as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个对调试有用的命令是 `kubectl logs`，如下所示：
- en: '![](img/00476.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00476.jpg)'
- en: Kubernetes with Rkt
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 与 Rkt
- en: By default, Kubernetes works with Container runtime Docker. The architecture
    of Kubernetes allows other Container runtime such as Rkt to work with Kubernetes.
    There is active work going on ([https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt](https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt))
    to integrate Kubernetes with Rkt and CoreOS.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes 使用容器运行时 Docker。Kubernetes 的架构允许其他容器运行时，如 Rkt，与 Kubernetes 一起工作。目前正在进行积极的工作（[https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt](https://github.com/kubernetes/kubernetes/tree/master/docs/getting-started-guides/rkt)）以将
    Kubernetes 与 Rkt 和 CoreOS 集成。
- en: Kubernetes 1.1 update
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 1.1 更新
- en: Kubernetes released 1.1 version ([http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html](http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html))
    in November 2015\. Significant additions in 1.1 are increased performance, auto-scaling,
    and job objects for the batching tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 于 2015 年 11 月发布了 1.1 版本（[http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html](http://blog.kubernetes.io/2015/11/Kubernetes-1-1-Performance-upgrades-improved-tooling-and-a-growing-community.html)）。1.1
    版本的重要新增功能包括性能提升、自动扩展和用于批处理任务的作业对象。
- en: Docker Swarm
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: 'Swarm is Docker''s native Orchestration solution. The following are some properties
    of Docker Swarm:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Swarm 是 Docker 的原生编排解决方案。以下是 Docker Swarm 的一些特点：
- en: Rather than managing individual Docker nodes, the cluster can be managed as
    a single entity.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其单独管理每个 Docker 节点，不如将整个集群作为一个整体来管理。
- en: Swarm has a built-in scheduler that decides the placement of Containers in the
    cluster. Swarm uses user-specific constraints and affinities ([https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/))
    to decide the Container placement. Constraints could be CPU and memory, and affinity
    are parameters to group related Containers together. Swarm also has the provision
    to take its scheduler out and work with other schedulers such as Kubernetes.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 内置调度器，负责决定容器在集群中的位置。Swarm 使用用户特定的约束条件和亲和性（[https://docs.docker.com/swarm/scheduler/filter/](https://docs.docker.com/swarm/scheduler/filter/)）来决定容器的放置。约束条件可以是
    CPU 和内存，而亲和性是将相关容器分组在一起的参数。Swarm 还提供将其调度器移除，并与其他调度器（如 Kubernetes）协作的选项。
- en: 'The following image shows the Docker Swarm architecture:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 Docker Swarm 架构：
- en: '![](img/00188.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00188.jpg)'
- en: 'The following are some notes on the Docker Swarm architecture:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于 Docker Swarm 架构的一些说明：
- en: The Swarm Master takes care of scheduling Docker Containers based on the scheduling
    algorithm, constraints, and affinities. Supported algorithms are spread, `binpack`,
    and `random`. The default algorithm is spread. Multiple Swarm masters can be run
    in parallel to provide high availability. The Spread scheduling is used to distribute
    workloads evenly. The binpack scheduling is used to utilize each node fully before
    scheduling on another node.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 主节点负责根据调度算法、约束条件和亲和性调度 Docker 容器。支持的算法有 spread、`binpack` 和 `random`。默认算法是
    spread。可以并行运行多个 Swarm 主节点，以提供高可用性。Spread 调度算法用于均匀分配工作负载。Binpack 调度算法则是在将任务调度到其他节点之前，先充分利用每个节点。
- en: The Swarm Agent runs in each node and communicates to the Swarm Master.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swarm 代理在每个节点上运行，并与 Swarm 主节点通信。
- en: There are different approaches available for Swarm worker nodes to discover
    the Swarm Master. Discovery is necessary because the Swarm Master and agents run
    on different nodes and Swarm agents are not started by the Swarm Master. It is
    necessary for Swarm agents and the Swarm Master to discover each other in order
    to understand that they are part of the same cluster. Available discovery mechanisms
    are Docker hub, Etcd, Consul, and others.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多种方法可以让 Swarm 工作节点发现 Swarm 主节点。发现是必要的，因为 Swarm 主节点和代理程序运行在不同的节点上，并且 Swarm 代理不是由
    Swarm 主节点启动的。Swarm 代理和 Swarm 主节点需要相互发现，才能了解它们属于同一个集群。可用的发现机制包括 Docker hub、Etcd、Consul
    等。
- en: Docker Swarm integrates with the Docker machine to ease the creation of Docker
    nodes. Docker Swarm integrates with Docker compose for multicontainer application
    orchestration.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Swarm 与 Docker machine 集成，简化了 Docker 节点的创建。Docker Swarm 与 Docker compose
    集成，用于多容器应用的编排。
- en: With the Docker 1.9 release, Docker Swarm integrates with multi-host Docker
    networking that allows Containers scheduled across hosts to talk to each other.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着Docker 1.9版本的发布，Docker Swarm与多主机Docker网络集成，使得跨主机调度的容器能够相互通信。
- en: The Docker Swarm installation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm安装
- en: 'A prerequisite for this example is to install Docker 1.8.1 and Docker-machine
    0.5.0\. I used the procedure at [https://docs.docker.com/swarm/install-w-machine/](https://docs.docker.com/swarm/install-w-machine/)
    to create a single Docker Swarm master with two Docker Swarm agent nodes. The
    following are the steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的前提是安装Docker 1.8.1和Docker-machine 0.5.0。我使用[https://docs.docker.com/swarm/install-w-machine/](https://docs.docker.com/swarm/install-w-machine/)中的步骤创建了一个带有两个Docker
    Swarm代理节点的单个Docker Swarm主节点。以下是步骤：
- en: Create a discovery token.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建发现令牌。
- en: Create a Swarm master node with the created discovery token.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用创建的发现令牌创建Swarm主节点。
- en: Create two Swarm agent nodes with the created discovery token.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用创建的发现令牌创建两个Swarm代理节点。
- en: 'By setting the environment variable to `swarm-master`, as shown in the following
    command we can control the Docker swarm cluster using regular Docker commands:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将环境变量设置为`swarm-master`，如以下命令所示，我们可以使用常规的Docker命令控制Docker Swarm集群：
- en: '`eval $(docker-machine env --swarm swarm-master)`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`eval $(docker-machine env --swarm swarm-master)`'
- en: 'Let''s look at the `docker info` output on the Swarm cluster:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下Swarm集群中`docker info`的输出：
- en: '![](img/00056.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00056.jpg)'
- en: The preceding output shows that three nodes (one master and two agents) are
    in the cluster and that four containers are running in the cluster. The `swarm-master`
    node has two containers and the `swarm-agent` node has one container each. These
    containers are used to manage the Swarm service. Application containers are scheduled
    only in Swarm agent nodes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示集群中有三个节点（一个主节点和两个代理节点），并且集群中正在运行四个容器。`swarm-master`节点有两个容器，而`swarm-agent`节点每个有一个容器。这些容器用于管理Swarm服务。应用容器只会在Swarm代理节点中调度。
- en: 'Let''s look at the individual containers in the master node. This shows the
    master and agent services running:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下主节点中的单个容器。这显示了主节点和代理服务正在运行：
- en: '![](img/00191.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00191.jpg)'
- en: 'Let''s look at the container running in the agent node. This shows the `swarm
    agent` running:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一下运行在代理节点中的容器。这显示了正在运行的`swarm agent`：
- en: '![](img/00193.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00193.jpg)'
- en: An example of Docker Swarm
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm示例
- en: 'To illustrate the Docker Swarm Container orchestration, let''s start four `nginx`
    containers:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示Docker Swarm容器编排，我们先启动四个`nginx`容器：
- en: '`docker run -d --name nginx1 nginx``docker run -d --name nginx2 nginx``docker run -d --name nginx3 nginx``docker run -d --name nginx4 nginx`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -d --name nginx1 nginx``docker run -d --name nginx2 nginx``docker
    run -d --name nginx3 nginx``docker run -d --name nginx4 nginx`'
- en: 'From the following output, we can see that the four containers are spread equally
    between `swarm-agent-00` and `swarm-agent-01`. The default `spread` scheduling
    strategy has been used here:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们可以看到四个容器均匀分布在`swarm-agent-00`和`swarm-agent-01`之间。这里使用了默认的`spread`调度策略：
- en: '![](img/00195.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00195.jpg)'
- en: 'The following output shows the overall Container count across the cluster that
    includes the master and two agent nodes. The container count eight includes Swarm
    service containers as well as nginx application containers:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了包括主节点和两个代理节点在内的整个集群的容器总数。总计八个容器，包括Swarm服务容器和nginx应用容器：
- en: '![](img/00252.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00252.jpg)'
- en: Mesos
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos
- en: Apache Mesos is an open source clustering software. Mesosphere's DCOS is the
    commercial version of Apache Mesos. Mesos combines the Clustering OS and Cluster
    manager. Clustering OS is responsible for representing resources from multiple
    disparate computers in one single resource over which applications can be scheduled.
    The cluster manager is responsible for scheduling the jobs in the cluster. The
    same cluster can be used for different workloads such as Hadoop and Spark. There
    is a two-level scheduling within Mesos. The first-level scheduling does resource
    allocation among frameworks and the framework takes care of scheduling the jobs
    within that particular framework. Each framework is an application category such
    as Hadoop, Spark, and others. For general purpose applications, the best framework
    available is Marathon. Marathon is a distributed INIT and HA system to schedule
    containers. The Chronos framework is like a Cron job and this framework is suitable
    to run shorter workloads that need to be run periodically. The Aurora framework
    provides you with a much more fine-grained control for complex jobs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Mesos 是一个开源的集群软件。Mesosphere 的 DCOS 是 Apache Mesos 的商业版。Mesos 结合了集群操作系统（Clustering
    OS）和集群管理器（Cluster Manager）。集群操作系统负责将来自多个不同计算机的资源表示为一个统一的资源，以便可以在其上调度应用程序。集群管理器负责在集群中调度任务。相同的集群可以用于不同的工作负载，如
    Hadoop 和 Spark。在 Mesos 中有两级调度。第一层调度负责在框架之间进行资源分配，框架则负责在该框架内调度任务。每个框架是一个应用程序类别，如
    Hadoop、Spark 等。对于通用应用程序，最好的框架是 Marathon。Marathon 是一个分布式 INIT 和高可用（HA）系统，用于调度容器。Chronos
    框架类似于 Cron 作业，适合运行需要定期执行的较短工作负载。Aurora 框架则为复杂任务提供了更加细粒度的控制。
- en: 'The following image shows you the different layers in the Mesos architecture:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了 Mesos 架构中的不同层次：
- en: '![](img/00198.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00198.jpg)'
- en: Comparing Kubernetes, Docker Swarm, and Mesos
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 Kubernetes、Docker Swarm 和 Mesos
- en: 'Even though all these solutions (Kubernetes, Docker Swarm, and Mesos) do application
    Orchestration, there are many differences in their approach and use cases. I have
    tried to summarize the differences based on their latest available release. All
    these Orchestration solutions are under active development, so the feature set
    can vary going forward. This table is updated as of October 2015:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些解决方案（Kubernetes、Docker Swarm 和 Mesos）都用于应用程序编排，但它们在方法和使用场景上存在很多差异。我尝试基于它们的最新版本总结这些差异。所有这些编排解决方案都在积极开发中，因此功能集可能会发生变化。此表格已更新至
    2015 年 10 月：
- en: '| Feature | Kubernetes | Docker Swarm | Mesos |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 特性 | Kubernetes | Docker Swarm | Mesos |'
- en: '| Deployment unit | Pods | Container | Process or Container |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 部署单元 | Pods | 容器 | 进程或容器 |'
- en: '| Container runtime | Docker and Rkt. | Docker. | Docker; there is some discussion
    ongoing on Mesos with Rkt. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 容器运行时 | Docker 和 Rkt。 | Docker。 | Docker；Mesos 正在讨论与 Rkt 的集成。 |'
- en: '| Networking | Each container has an IP address, and can use external network
    plugins. | Initially, this did Port forwarding with a common agent IP address.
    With Docker 1.9, it uses Overlay networking and per container IP address. It can
    use external network plugins. | Initially, this did Port forwarding with a common
    agent IP address. Currently, it works on per Container IP. integration with Calico.
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 每个容器都有一个 IP 地址，并且可以使用外部网络插件。 | 最初，通过一个公共代理 IP 地址进行端口转发。使用 Docker 1.9
    后，采用了 Overlay 网络和每个容器的 IP 地址，并且可以使用外部网络插件。 | 最初，通过一个公共代理 IP 地址进行端口转发。目前，使用每个容器的
    IP 地址，并与 Calico 集成。 |'
- en: '| Workloads | Homogenous workload. With namespaces, multiple virtual clusters
    can be created. | Homogenous workload. | Multiple frameworks such as Marathon,
    Aurora, and Hadoop can be run in parallel. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 工作负载 | 同质工作负载。通过命名空间，可以创建多个虚拟集群。 | 同质工作负载。 | 可以并行运行多个框架，如 Marathon、Aurora
    和 Hadoop。 |'
- en: '| Service discovery | This can use either environment variable-based discovery
    or Kube-dns for dynamic discovery. | Static with modification of `/etc/hosts`.
    A DNS approach is planned in the future. | DNS-based approach to discover services
    dynamically. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 服务发现 | 可以使用基于环境变量的发现或 Kube-dns 进行动态发现。 | 静态，修改 `/etc/hosts`。未来计划使用 DNS 方法。
    | 基于 DNS 的方法，用于动态发现服务。 |'
- en: '| High availability | With a replication controller, services are highly available.
    Service scaling can be done easily. | Service high availability is not yet implemented.
    | Frameworks take care of service high availability. For example, Marathon has
    the `Init.d` system to run containers. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 高可用性 | 使用复制控制器，服务具有高度可用性。服务扩展可以轻松完成。 | 服务的高可用性尚未实现。 | 框架处理服务的高可用性。例如，Marathon具有`Init.d`系统来运行容器。
    |'
- en: '| Maturity | Relatively new. The first production release was done a few months
    before. | Relatively new. The first production release was done a few months before.
    | Pretty stable and used in big production environments. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 成熟度 | 相对较新。首次生产版本发布于几个月前。 | 相对较新。首次生产版本发布于几个月前。 | 稳定性较好，广泛应用于大型生产环境中。 |'
- en: '| Complexity | Easy. | Easy. | This is a little difficult to set up. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 复杂性 | 简单。 | 简单。 | 设置稍微有些困难。 |'
- en: '| Use case | This is more suitable for homogenous workloads. | Presenting Docker
    frontend makes it attractive for Docker users not needing to learn any new management
    interface. | Suitable for heterogeneous workloads. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 使用场景 | 更适合同质化工作负载。 | 提供Docker前端界面，使得Docker用户无需学习新的管理界面也能轻松使用。 | 适合异质化工作负载。
    |'
- en: Kubernetes can be run as a framework on top of Mesos. In this case, Mesos does
    the first-level scheduling for Kubernetes and Kubernetes schedules and manages
    applications scheduled. This project ([https://github.com/mesosphere/kubernetes-mesos](https://github.com/mesosphere/kubernetes-mesos))
    is dedicated to running Kubernetes on top of Mesos.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以作为框架在Mesos之上运行。在这种情况下，Mesos为Kubernetes提供一级调度，而Kubernetes则调度和管理已调度的应用程序。这个项目（[https://github.com/mesosphere/kubernetes-mesos](https://github.com/mesosphere/kubernetes-mesos)）致力于在Mesos之上运行Kubernetes。
- en: There is work ongoing to integrate Docker Swarm with Kubernetes so that Kubernetes
    can be used as a scheduler and process manager for the cluster while users can
    still use the Docker interface to manage containers using Docker Swarm.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 目前正在进行的工作是将Docker Swarm与Kubernetes集成，以便Kubernetes可以作为集群的调度器和过程管理器运行，同时用户仍然可以使用Docker
    Swarm的Docker界面来管理容器。
- en: Application definition
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序定义
- en: When an application is composed of multiple Containers, it is useful to represent
    each Container property along with its dependencies in a single JSON or YAML file
    so that the application can be instantiated as a whole rather than instantiating
    each Container of the application separately. The application definition file
    takes care of defining the multicontainer application. Docker-compose defines
    both the application file and runtime to instantiate containers based on the application
    file.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个应用程序由多个容器组成时，使用单一的JSON或YAML文件表示每个容器的属性及其依赖关系非常有用，这样可以整体实例化应用程序，而不是单独实例化应用程序中的每个容器。应用程序定义文件负责定义多容器应用程序。Docker-compose定义了应用程序文件和运行时，根据应用程序文件实例化容器。
- en: Docker-compose
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Docker-compose
- en: Docker-compose provides you with an application definition format, and when
    we run the tool, Docker-compose takes care of parsing the application definition
    file and instantiating the Containers taking care of all the dependencies.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Docker-compose为你提供了应用程序定义格式，当我们运行该工具时，Docker-compose负责解析应用程序定义文件并实例化容器，处理所有依赖关系。
- en: 'Docker-compose has the following advantages and use cases:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Docker-compose具有以下优点和使用场景：
- en: It gives a simple approach to specify an application's manifest that contains
    multiple containers along with their constraints and affinities
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一种简单的方法来指定包含多个容器及其约束和亲和性的应用程序清单
- en: It integrates well with Dockerfile, Docker Swarm, and multihost networking
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与Dockerfile、Docker Swarm和多主机网络集成良好
- en: The same compose file can be adapted to different environments using environment
    variables
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的compose文件可以通过环境变量适应不同的环境
- en: A single-node application
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点应用程序
- en: The following example shows you how to build a multicontainer WordPress application
    with a WordPress and MySQL container.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何构建一个包含WordPress和MySQL容器的多容器WordPress应用程序。
- en: 'The following is the `docker-compose.yml` file defining the Containers and
    their properties:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是定义容器及其属性的`docker-compose.yml`文件：
- en: '`wordpress:   image: wordpress   ports:    - "8080:80"   environment:     WORDPRESS_DB_HOST: "composeword_mysql_1:3306"
        WORDPRESS_DB_PASSWORD: mysql mysql:   image: mysql   environment:     MYSQL_ROOT_PASSWORD: mysql`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordpress:   image: wordpress   ports:    - "8080:80"   environment:     WORDPRESS_DB_HOST: "composeword_mysql_1:3306"
        WORDPRESS_DB_PASSWORD: mysql mysql:   image: mysql   environment:     MYSQL_ROOT_PASSWORD: mysql`'
- en: 'The following command shows you how to start the application using `docker-compose`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令展示了如何使用 `docker-compose` 启动应用：
- en: '`docker-compose –p composeword –f docker-compose.yml up -d`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-compose –p composeword –f docker-compose.yml up -d`'
- en: 'The following is the output of the preceding command:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面命令的输出：
- en: '![](img/00200.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00200.jpg)'
- en: Containers are prefixed with a keyword specified in the `-p` option. In the
    preceding example, we have used `composeword_mysql_1` as the hostname, and the
    IP address is derived dynamically from the container using this and updated in
    `/etc/hosts`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 容器名称以 `-p` 选项中指定的关键字为前缀。在上面的示例中，我们使用了 `composeword_mysql_1` 作为主机名，且 IP 地址是通过此容器动态生成并更新到
    `/etc/hosts` 中。
- en: 'The following output shows the running containers of the wordpress application:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 Wordpress 应用运行的容器：
- en: '![](img/00202.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00202.jpg)'
- en: 'The following output is the `/etc/hosts` output in the wordpress container;
    the one which shows that the IP address of the MySQL container is dynamically
    updated:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出是 Wordpress 容器中 `/etc/hosts` 的输出，显示 MySQL 容器的 IP 地址已被动态更新：
- en: '![](img/00220.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00220.jpg)'
- en: A multinode application
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一个多节点应用
- en: I used the example at [https://docs.docker.com/engine/userguide/networking/get-started-overlay/](https://docs.docker.com/engine/userguide/networking/get-started-overlay/)
    to create a web application spanning multiple nodes using `docker-compose`. In
    this case, `docker-compose` is integrated with Docker Swarm and Docker multihost
    networking.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 [https://docs.docker.com/engine/userguide/networking/get-started-overlay/](https://docs.docker.com/engine/userguide/networking/get-started-overlay/)
    中的示例，通过 `docker-compose` 创建了一个跨多个节点的 Web 应用。在这个例子中，`docker-compose` 与 Docker Swarm
    和 Docker 多主机网络集成。
- en: The prerequisite for this example is to have a working Docker Swarm cluster
    and Docker version 1.9+.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的前提是需要有一个正常工作的 Docker Swarm 集群以及 Docker 版本 1.9+。
- en: 'The following command creates the multihost counter application. This application
    has a web container as the frontend and a mongo container as the backend. These
    commands have to be executed against the Swarm cluster:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令创建了多主机计数器应用。该应用有一个 Web 容器作为前端，一个 Mongo 容器作为后端。必须在 Swarm 集群中执行这些命令：
- en: '`docker-compose –p counter –x-networking up -d`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker-compose –p counter –x-networking up -d`'
- en: 'The following is the output of the preceding command:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面命令的输出：
- en: '![](img/00204.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00204.jpg)'
- en: 'The following output shows the overlay network `counter` created as part of
    this application:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了作为本应用一部分创建的叠加网络 `counter`：
- en: '![](img/00206.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00206.jpg)'
- en: 'The following output shows the running Containers in the Swarm cluster:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 Swarm 集群中运行的容器：
- en: '![](img/00207.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00207.jpg)'
- en: 'The following output shows the Swarm cluster information. There are in total
    five containers—three of them are Swarm service containers and two of them are
    the preceding application containers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了 Swarm 集群信息，共有五个容器——其中三个是 Swarm 服务容器，两个是前述应用容器：
- en: '![](img/00209.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00209.jpg)'
- en: 'The following output shows the working web application:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了工作中的 Web 应用：
- en: '![](img/00211.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00211.jpg)'
- en: Packaged Container Orchestration solutions
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 打包的容器编排解决方案
- en: 'There are many components necessary for the deployment of a distributed microservice
    application at scale. The following are some of the important components:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个大规模分布式微服务应用需要许多组件，以下是一些重要的组件：
- en: An infrastructure cluster
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基础设施集群
- en: A Container-optimized OS
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个容器优化操作系统
- en: A Container orchestrator with a built-in scheduler, service discovery, and networking
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个内置调度器、服务发现和网络功能的容器编排工具
- en: Storage integration
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储集成
- en: Multitenant capability with authentication
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多租户能力并具备认证功能
- en: An API at all layers to ease management
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以简化管理的 API 层次结构
- en: Cloud providers such as Amazon and Google already have an ecosystem to manage
    VMs, and their approach has been to integrate Containers and Container orchestration
    into their IaaS offering so that Containers play well with their other tools.
    The AWS Container service and Google Container engine fall in this category. The
    focus of CoreOS has been to develop a secure Container-optimized OS and open source
    tools for distributed application development. CoreOS realized that integrating
    their offering with Kubernetes would give their customers an integrated solution,
    and Tectonic provides this integrated solution.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Amazon 和 Google 这样的云服务提供商已经有了管理虚拟机的生态系统，他们的做法是将容器和容器编排集成到他们的 IaaS 产品中，以便容器能够与他们的其他工具良好配合。AWS
    容器服务和 Google 容器引擎属于这一类。CoreOS 的重点是开发一个安全的容器优化操作系统，并为分布式应用程序开发提供开源工具。CoreOS 意识到，将他们的产品与
    Kubernetes 集成将为客户提供一个集成的解决方案，Tectonic 提供了这个集成解决方案。
- en: There are a few other projects such as OpenStack Magnum ([https://github.com/openstack/magnum](https://github.com/openstack/magnum))
    and Cisco's Mantl ([https://mantl.io/](https://mantl.io/)) that falls under this
    category of managed Container Orchestration. We have not covered these in this
    chapter.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他项目，如 OpenStack Magnum（[https://github.com/openstack/magnum](https://github.com/openstack/magnum)）和
    Cisco 的 Mantl（[https://mantl.io/](https://mantl.io/)），也属于此类托管容器编排服务。我们在本章没有涉及这些内容。
- en: The AWS Container service
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 容器服务
- en: 'The AWS EC2 Container Service (ECS) is a Container Orchestration service from
    AWS. The following are some of the key characteristics of this service:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EC2 容器服务（ECS）是 AWS 提供的一种容器编排服务。以下是该服务的一些关键特性：
- en: ECS creates and manages the node cluster where containers are launched. The
    user needs to specify only the cluster size.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ECS 创建并管理容器启动的节点集群。用户只需指定集群的规模。
- en: Container health is monitored by container agents running on the node. The Container
    agent communicates to the master node that makes all service-related decisions.
    This allows for high availability of Containers.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器健康状况通过运行在节点上的容器代理进行监控。容器代理与主节点通信，主节点负责做出所有与服务相关的决策。这确保了容器的高可用性。
- en: ECS takes care of scheduling the containers across the cluster. A scheduler
    API is implemented as a plugin and this allows integration with other schedulers
    such as Marathon and Kubernetes.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ECS 负责在集群中调度容器。调度器 API 实现为插件，这使得与其他调度器（如 Marathon 和 Kubernetes）的集成成为可能。
- en: ECS integrates well with other AWS services such as Cloudformation, ELB, logging,
    Volume management, and others.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ECS 与其他 AWS 服务如 Cloudformation、ELB、日志记录、Volume 管理等良好集成。
- en: Installing ECS and an example
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 ECS 及示例
- en: ECS can be controlled from the AWS console or using the AWS CLI or ECS CLI.
    For the following example, I have used the ECS CLI, which can be installed using
    the procedure in this link ([http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ECS 可以通过 AWS 控制台、AWS CLI 或 ECS CLI 进行控制。以下示例中，我使用了 ECS CLI，ECS CLI 可以通过此链接的过程安装（[http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html)）。
- en: I used the following example ([http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html))
    to create a WordPress application with two containers (WordPress and MySQL) using
    the compose YML file.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了以下示例（[http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_tutorial.html)）来创建一个包含两个容器（WordPress
    和 MySQL）的 WordPress 应用程序，使用 compose YML 文件。
- en: 'The following are the steps:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤：
- en: Create an ECS cluster.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 ECS 集群。
- en: Deploy the application as a service over the cluster.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将应用程序作为服务部署到集群上。
- en: The cluster size or service size can be dynamically changed later based on the
    requirements.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群规模或服务规模可以根据需求动态调整。
- en: 'The following command shows the running containers of the WordPress application:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示 WordPress 应用程序的运行容器：
- en: '`ecs-cli ps`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`ecs-cli ps`'
- en: 'The following is the output of the preceding command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述命令的输出：
- en: '![](img/00213.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00213.jpg)'
- en: 'We can scale the application using the `ecs-cli` command. The following command
    scales each container to two:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `ecs-cli` 命令来扩展应用程序。以下命令将每个容器的规模设置为二：
- en: '`ecs-cli compose --file hello-world.yaml scale 2`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`ecs-cli compose --file hello-world.yaml scale 2`'
- en: 'The following output shows the running containers at this point. As we can
    see, containers have scaled to two:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了此时正在运行的容器。正如我们所看到的，容器已经扩展到两个：
- en: '![](img/00214.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00214.jpg)'
- en: Note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: The MySQL container is scaled typically using a single master and multiple
    slaves.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：MySQL容器通常使用单一主节点和多个从节点进行扩展。
- en: 'We can also log in to each AWS node and look at the running containers. The
    following output shows three containers in one of the AWS nodes. Two of them are
    application containers, and the third one is the ECS agent container that does
    container monitoring and talks to the master node:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以登录到每个AWS节点并查看正在运行的容器。以下输出显示了其中一个AWS节点中的三个容器。两个是应用程序容器，第三个是ECS代理容器，它负责容器监控并与主节点通信：
- en: '![](img/00216.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00216.jpg)'
- en: Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: To log in to each node, we need to use `ec2-user` as the username along
    with the private key used while creating the cluster.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要登录每个节点，我们需要使用`ec2-user`作为用户名，并使用创建集群时使用的私钥。
- en: To demonstrate HA, I tried stopping containers or nodes. Containers got rescheduled
    because the Container agent monitors containers in each node.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示高可用性，我尝试停止容器或节点。由于容器代理监控每个节点中的容器，容器会被重新调度。
- en: Google Container Engine
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Google容器引擎
- en: 'Google Container Engine is the cluster manager and container orchestration
    solution from Google that is built on top of Kubernetes. The following are the
    differences or benefits that we get from GCE compared to running a container cluster
    using Kubernetes as specified in the Kubernetes installation section:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Google容器引擎是Google提供的集群管理和容器编排解决方案，构建在Kubernetes之上。与使用Kubernetes运行容器集群相比，GCE有以下差异或优势：
- en: A node cluster is created automatically by Google Container engine. The user
    needs to specify only the cluster size and the CPU and memory requirement.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点集群由Google容器引擎自动创建。用户只需指定集群大小以及CPU和内存需求。
- en: Kubernetes is composed of multiple individual services such as an API server,
    scheduler, and agents that need to be installed for the Kubernetes system to work.
    Google Container engine takes care of creating the Kubernetes master with appropriate
    services and installing other Kubernetes services in agent nodes.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes由多个独立的服务组成，如API服务器、调度器和代理等，这些服务需要安装才能使Kubernetes系统正常工作。Google容器引擎负责创建带有适当服务的Kubernetes主节点，并在代理节点中安装其他Kubernetes服务。
- en: Google Container engine integrates well with other Google services such as VPC
    networking, Logging, autoscaling, load balancing, and so on.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google容器引擎与其他Google服务（如VPC网络、日志记录、自动扩展、负载均衡等）集成得很好。
- en: The Docker hub, Google container registry, or on-premise registry can be used
    to store Container images.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用Docker Hub、Google容器注册表或本地注册表来存储容器镜像。
- en: Installing GCE and an example
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 安装GCE和示例
- en: The procedure at [https://cloud.google.com/container-engine/docs/before-you-begin](https://cloud.google.com/container-engine/docs/before-you-begin)
    can be used to install the gcloud container components and kubectl. Containers
    can also be managed using the GCE dashboard.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/container-engine/docs/before-you-begin](https://cloud.google.com/container-engine/docs/before-you-begin)中的步骤可用于安装gcloud容器组件和kubectl。容器也可以使用GCE控制台进行管理。'
- en: I used the procedure at [https://cloud.google.com/container-engine/docs/tutorials/guestbook](https://cloud.google.com/container-engine/docs/tutorials/guestbook)
    to create a guestbook application containing three services. This application
    is the same as the one used in the An example of Kubernetes application section
    specified earlier.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用[https://cloud.google.com/container-engine/docs/tutorials/guestbook](https://cloud.google.com/container-engine/docs/tutorials/guestbook)中的步骤创建了一个包含三个服务的留言簿应用程序。这个应用程序与之前指定的Kubernetes应用程序示例中的应用程序相同。
- en: 'The following are the steps:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是步骤：
- en: Create a node cluster with the required cluster size. This will automatically
    create a Kubernetes master and appropriate agent services will be installed in
    the nodes.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有所需集群大小的节点集群。这将自动创建一个Kubernetes主节点，并在节点上安装适当的代理服务。
- en: Deploy the application using a replication controller and service files.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用复制控制器和服务文件部署应用程序。
- en: The cluster can be dynamically resized later based on the need.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群可以根据需求动态调整大小。
- en: 'The following is the cluster that I created. There are four nodes in the cluster
    as specified by `NUM_NODES`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我创建的集群。该集群中有四个节点，数量由`NUM_NODES`指定：
- en: '![](img/00217.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00217.jpg)'
- en: 'The following command shows the running services that consist of frontend,
    redis-master, and redis-slave. The Kubernetes service is also running in the master
    node:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令显示了正在运行的服务，其中包括 frontend、redis-master 和 redis-slave。Kubernetes 服务也在主节点上运行：
- en: '`Kubectl get services`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kubectl get services`'
- en: 'The following is the output of the preceding command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一个命令的输出：
- en: '![](img/00342.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00342.jpg)'
- en: 'As the frontend service is integrated with the GCE load balancer, there is
    also an external IP address. Using the external IP address, guestbook service
    can be accessed. The following command shows the list of endpoints associated
    with the load balancer:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前端服务已与 GCE 负载均衡器集成，因此还有一个外部 IP 地址。通过外部 IP 地址可以访问 guestbook 服务。以下命令显示了与负载均衡器关联的端点列表：
- en: '`Kubectl describe services frontend`'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kubectl describe services frontend`'
- en: 'The following is the output of the preceding command:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前一个命令的输出：
- en: '![](img/00222.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00222.jpg)'
- en: 'To resize the cluster, we need to first find the instance group associated
    with the cluster and resize it. The following command shows the instance group
    associated with the guestbook:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 要调整集群大小，首先需要找到与集群关联的实例组并对其进行调整。以下命令显示了与 guestbook 关联的实例组：
- en: '![](img/00224.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00224.jpg)'
- en: 'Using the instance group, we can resize the cluster as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实例组，我们可以按如下方式调整集群大小：
- en: '![](img/00225.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00225.jpg)'
- en: The initial set of outputs that show the cluster size as four were done after
    the resizing of the cluster.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 显示集群大小为四的初始输出是在调整集群大小后生成的。
- en: 'We can log in to the individual nodes and see the containers launched in the
    node using regular Docker commands. In the following output, we see one instance
    of `redis-slave` and one instance of front end running in this node. Other Containers
    are Kubernetes infrastructure containers:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以登录到各个节点，并使用常规的 Docker 命令查看节点中启动的容器。在以下输出中，我们看到一个 `redis-slave` 实例和一个前端实例在该节点中运行。其他容器为
    Kubernetes 基础设施容器：
- en: '![](img/00227.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00227.jpg)'
- en: CoreOS Tectonic
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS Tectonic
- en: Tectonic is the commercial offering from CoreOS where they have integrated CoreOS
    and the open source components of CoreOS (Etcd, Fleet, Flannel, Rkt, and Dex)
    along with Kubernetes. With Tectonic, CoreOS is integrating their other commercial
    offerings such as CoreUpdate, Quay repository, and Enterprise CoreOS into Tectonic.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Tectonic 是 CoreOS 的商业产品，集成了 CoreOS 和 CoreOS 的开源组件（Etcd、Fleet、Flannel、Rkt 和 Dex）以及
    Kubernetes。通过 Tectonic，CoreOS 将其其他商业产品如 CoreUpdate、Quay 仓库和 Enterprise CoreOS
    集成到 Tectonic 中。
- en: The plan is to expose the Kubernetes API as it is in Tectonic. Development in
    CoreOS open source projects will continue as it is, and the latest software will
    be updated to Tectonic.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 计划将 Kubernetes API 作为其在 Tectonic 中的原始形式公开。CoreOS 开源项目的开发将继续进行，并且最新的软件会更新到 Tectonic
    中。
- en: 'The following diagram illustrates the different components of Tectonic:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Tectonic 的不同组件：
- en: '![](img/00228.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00228.jpg)'
- en: 'Tectonic provides you with Distributed Trusted Computing (DTM), where security
    is provided at all layers including hardware and software. The following are some
    unique differentiators:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Tectonic 为你提供分布式可信计算（DTM），在所有层级（包括硬件和软件）提供安全性。以下是一些独特的区别点：
- en: At the firmware level, the customer key can be embedded, and this allows customers
    to verify all the software running in the system.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在固件级别，可以嵌入客户密钥，这允许客户验证系统中运行的所有软件。
- en: Secure keys embedded in the firmware can verify the bootloader as well as CoreOS.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入固件中的安全密钥可以验证引导加载程序以及 CoreOS。
- en: Containers such as Rkt can be verified with their image signature.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如 Rkt 这样的容器可以通过其镜像签名进行验证。
- en: Logs can be made tamper-proof using the TPM hardware module embedded in the
    CPU motherboard.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用嵌入在 CPU 主板中的 TPM 硬件模块，使日志防篡改。
- en: Summary
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the importance of Container Orchestration along
    with the internals of popular container orchestration solutions, such as Kubernetes,
    Docker Swarm, and Mesos. There are many companies offering integrated Container
    orchestration solutions, and we covered a few popular ones such as the AWS Container
    service, Google Container Engine, and CoreOS Tectonic. For all the technologies
    covered in this chapter, installation and examples have been provided so that
    you can try them out. Customers have a choice of picking between integrated Container
    Orchestration solutions and manually integrating the Orchestration solution in
    their infrastructure. The factors affecting the choice would be flexibility, integration
    with in-house solutions, and cost. In the next chapter, we will cover OpenStack
    integration with Containers and CoreOS.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讲解了容器编排的重要性，并深入介绍了流行的容器编排解决方案，如 Kubernetes、Docker Swarm 和 Mesos。许多公司提供集成的容器编排解决方案，我们也介绍了一些流行的解决方案，例如
    AWS 容器服务、Google 容器引擎和 CoreOS Tectonic。对于本章介绍的所有技术，均提供了安装和示例，供您尝试使用。客户可以选择使用集成的容器编排解决方案，或者手动将编排解决方案集成到他们的基础设施中。影响选择的因素包括灵活性、与内部解决方案的集成以及成本。下一章我们将介绍
    OpenStack 与容器和 CoreOS 的集成。
- en: References
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'The Kubernetes page: [http://kubernetes.io/](http://kubernetes.io/)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 页面: [http://kubernetes.io/](http://kubernetes.io/)'
- en: 'Mesos: [http://mesos.apache.org/](http://mesos.apache.org/) and [https://mesosphere.com/](https://mesosphere.com/)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mesos: [http://mesos.apache.org/](http://mesos.apache.org/) 和 [https://mesosphere.com/](https://mesosphere.com/)'
- en: 'Docker Swarm: [https://docs.docker.com/swarm/](https://docs.docker.com/swarm/)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Swarm: [https://docs.docker.com/swarm/](https://docs.docker.com/swarm/)'
- en: 'Kubernetes on CoreOS: [https://coreos.com/kubernetes/docs/latest/](https://coreos.com/kubernetes/docs/latest/)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes on CoreOS: [https://coreos.com/kubernetes/docs/latest/](https://coreos.com/kubernetes/docs/latest/)'
- en: 'Google Container Engine: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Google 容器引擎: [https://cloud.google.com/container-engine/](https://cloud.google.com/container-engine/)'
- en: 'AWS ECS: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AWS ECS: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
- en: 'Docker Compose: [https://docs.docker.com/compose](https://docs.docker.com/compose)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker Compose: [https://docs.docker.com/compose](https://docs.docker.com/compose)'
- en: 'Docker machine: [https://docs.docker.com/machine/](https://docs.docker.com/machine/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker machine: [https://docs.docker.com/machine/](https://docs.docker.com/machine/)'
- en: 'Tectonic: [https://tectonic.com](https://tectonic.com)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tectonic: [https://tectonic.com](https://tectonic.com)'
- en: 'Tectonic Distributed Trusted Computing: [https://tectonic.com/blog/announcing-distributed-trusted-computing/](https://tectonic.com/blog/announcing-distributed-trusted-computing/)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tectonic 分布式可信计算: [https://tectonic.com/blog/announcing-distributed-trusted-computing/](https://tectonic.com/blog/announcing-distributed-trusted-computing/)'
- en: Further reading and tutorials
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步阅读和教程
- en: 'Container Orchestration with Kubernetes and CoreOS: [https://www.youtube.com/watch?v=tA8XNVPZM2w](https://www.youtube.com/watch?v=tA8XNVPZM2w)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 Kubernetes 和 CoreOS 进行容器编排: [https://www.youtube.com/watch?v=tA8XNVPZM2w](https://www.youtube.com/watch?v=tA8XNVPZM2w)'
- en: 'Comparing Orchestration solutions: [http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html](http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html),
    [http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1](http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1),
    and [https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca](https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '比较编排解决方案: [http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html](http://radar.oreilly.com/2015/10/swarm-v-fleet-v-kubernetes-v-mesos.html),
    [http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1](http://www.slideshare.net/giganati/orchestration-tool-roundup-kubernetes-vs-docker-vs-heat-vs-terra-form-vs-tosca-1),
    和 [https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca](https://www.openstack.org/summit/vancouver-2015/summit-videos/presentation/orchestration-tool-roundup-kubernetes-vs-heat-vs-fleet-vs-maestrong-vs-tosca)'
- en: 'Mesosphere introduction: [https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mesosphere 介绍: [https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere](https://www.digitalocean.com/community/tutorials/an-introduction-to-mesosphere)'
- en: 'Docker and AWS ECS: 0[https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga](https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Docker 和 AWS ECS: 0[https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga](https://medium.com/aws-activate-startup-blog/cluster-based-architectures-using-docker-and-amazon-ec2-container-service-f74fa86254bf#.afp7kixga)'
