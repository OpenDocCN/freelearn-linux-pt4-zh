- en: Chapter 3. NSX Manager Installation and Configuration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章：NSX Manager 安装与配置
- en: 'This chapter explains the list of prerequisites and installation steps for
    NSX Manager installation. The following are the key points that we will discuss:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 NSX Manager 安装的前提条件和安装步骤。以下是我们将讨论的关键点：
- en: NSX Manager requirements
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Manager 要求
- en: NSX Manager installation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Manager 安装
- en: NSX Manager design considerations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Manager 设计考虑
- en: Controller requirements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器要求
- en: NSX Controller deployments design considerations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 控制器部署设计考虑
- en: NSX data plane installation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 数据平面安装
- en: NSX Manager requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX Manager 要求
- en: VMware NSX Manager is a preconfigured virtual appliance which we can download
    from the VMware website just like any other VMware software. This preconfigured
    virtual machine comes with 16 GB of memory, 4 VCPUs, and 60 GB of storage space.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: VMware NSX Manager 是一个预配置的虚拟设备，我们可以像下载任何其他 VMware 软件一样，从 VMware 网站上下载。这个预配置的虚拟机配备了
    16 GB 内存、4 个 VCPU 和 60 GB 存储空间。
- en: 'Let''s have a quick look at the prerequisites for NSX Manager 6.2 installation:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看 NSX Manager 6.2 安装的前提条件：
- en: VMware vCenter server 6.0
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware vCenter Server 6.0
- en: VMware ESXi 6.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware ESXi 6.0
- en: Host clusters prepared with NSX 6.2
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备好 NSX 6.2 的主机集群
- en: vSphere distributed switch which is supported with the respective version of
    host and virtual center
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的 vSphere 分布式交换机，适用于相应版本的主机和虚拟中心
- en: Ensure we have shared data stores through which we can leverage vSphere HA/DRS
    features to prevent downtime for NSX Manager VM
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们有共享数据存储，可以利用 vSphere HA/DRS 功能来防止 NSX Manager 虚拟机的停机时间
- en: Confirm whether we are using a dual stack or IPV4/IPV6 only networks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认我们是否使用双栈或仅 IPV4/IPV6 网络
- en: Collect the Gateway, DNS, Syslog, and NTP server configuration
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集网关、DNS、Syslog 和 NTP 服务器配置
- en: All port requirements are updated in the VMware public knowledge base article
    at [https://kb.vmware.com/kb/2079386](https://kb.vmware.com/kb/2079386).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有端口要求已在 VMware 公共知识库文章 [https://kb.vmware.com/kb/2079386](https://kb.vmware.com/kb/2079386)
    中更新。
- en: 'After downloading the NSX Manager OVA, we will follow the four-step process
    for installation and configuration of the manager software as listed in the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 NSX Manager OVA 后，我们将按照以下四个步骤进行管理软件的安装和配置：
- en: Deploy the NSX Manager OVA file
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署 NSX Manager OVA 文件
- en: Log in to NSX Manager
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 NSX Manager
- en: Establish the NSX Manager and vCenter Server connection
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立 NSX Manager 和 vCenter Server 的连接
- en: Configure backup options
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置备份选项
- en: With that said, let's get started with the installation process. From my experience,
    I can confidently comment that deploying any appliances is a very easy task. However,
    when things start getting weird, we are forced to go back and review the deployment
    process and we find that the majority of the issues are something that we totally
    missed during the installation process wherein we just clicked the **Next**, and
    **Finish** buttons.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们开始安装过程。根据我的经验，我可以自信地说，部署任何虚拟设备都是一项非常简单的任务。然而，当事情变得奇怪时，我们不得不回头查看部署过程，结果发现大多数问题都是我们在安装过程中完全忽视的——我们只是点击了
    **下一步** 和 **完成** 按钮。
- en: NSX Manager installation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX Manager 安装
- en: 'For installing NSX Manager, perform the following steps:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 NSX Manager，请执行以下步骤：
- en: Open vCenter via VMware web client.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 VMware Web 客户端打开 vCenter。
- en: Select **VMs and Templates**, right-click your data center, and select **Deploy
    OVF Template**.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **虚拟机和模板**，右键点击数据中心，选择 **部署 OVF 模板**。
- en: Paste the VMware download URL or click **Browse** to select the file on your
    computer.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粘贴 VMware 下载 URL 或点击 **浏览** 选择计算机上的文件。
- en: Click in the checkbox **Accept extra configuration options**.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击复选框 **接受额外配置选项**。
- en: This allows you to set IPv4 and IPv6 addresses, default gateway, DNS, NTP, and
    SSH properties during the installation. If we do not set these configurations
    during deployment, we can always set them after the deployment.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这允许你在安装过程中设置 IPv4 和 IPv6 地址、默认网关、DNS、NTP 和 SSH 属性。如果在部署过程中未设置这些配置，我们可以在部署后随时进行设置。
- en: Accept the VMware license agreements.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受 VMware 许可协议。
- en: Edit the NSX Manager name (if required). Select the location for the deployed
    NSX Manager.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 NSX Manager 名称（如有需要）。选择部署 NSX Manager 的位置。
- en: This name will appear in the vCenter Server inventory.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该名称将出现在 vCenter Server 清单中。
- en: The folder you select will be used to apply permissions to the NSX Manager.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你选择的文件夹将用于为 NSX Manager 应用权限。
- en: Select a host or cluster on which to deploy the NSX Manager appliance. I would
    prefer selecting a cluster and letting DRS decide the best host for placing the
    appliance.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个主机或集群来部署NSX Manager设备。我更倾向于选择一个集群，然后让DRS决定将设备放置在哪个主机上。
- en: Change the virtual disk format to **Thick Provision**, and select the destination
    data store for the virtual machine configuration files and the virtual disks.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将虚拟磁盘格式更改为**厚预配**，并为虚拟机配置文件和虚拟磁盘选择目标数据存储。
- en: Select the management port group for NSX Manager.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择NSX Manager的管理端口组。
- en: Finally, review the screen after all the options are configured and we can see
    that this is an IPV4 only deployment. So stay focused and review the screen once
    more and if any corrections are required, we should go back to previous steps
    and correct it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在配置完所有选项后，检查屏幕，可以看到这是一个仅支持IPV4的部署。所以要集中注意力，再次检查屏幕，如果需要做任何修改，我们应该回到前一步进行修正。
- en: Now that we have successfully deployed NSX Manager, it is worth double-checking
    all the configurations are intact and the manager appliance can ping DNS, NTP,
    Gateway, ESXi hosts, and VC. This is a very important step as any communication
    issues between manager and these components will have a direct impact on the functionality/deployment
    of VMware NSX features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功部署NSX Manager，值得再次确认所有配置是否完好无损，并且管理设备能够与DNS、NTP、网关、ESXi主机和VC进行通信。这是一个非常重要的步骤，因为管理设备与这些组件之间的任何通信问题都会直接影响VMware
    NSX功能的正常运行/部署。
- en: 'The following figure shows NSX Manager OVF details:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了NSX Manager OVF的详细信息：
- en: '![NSX Manager installation](img/image_03_001.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![NSX Manager 安装](img/image_03_001.jpg)'
- en: Let's make a note of the key configuration details from the preceding image
    and I will explain the design decisions for choosing it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先记录下前面图片中的关键配置细节，然后我会解释选择这些配置的设计决策。
- en: Understanding the key configuration details
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解关键配置细节
- en: As I said earlier, there are a few key design factors which we need to take
    care of while deploying NSX Manager appliances. Let's talk more about them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所说，在部署NSX Manager设备时，我们需要注意几个关键的设计因素。接下来我们详细讨论一下。
- en: Target - Management and Edge cluster
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标 - 管理和边缘集群
- en: 'The management and edge cluster is a dedicated cluster in vSphere and it is
    always recommended to have a unique cluster for deploying all management components,
    which will ease any upgrade activity on the cluster without impacting the compute
    cluster, which is explicitly used for deploying end user virtual machines. In
    this example, we have a preconfigured vSphere cluster. And we leverage this cluster
    for deploying the following components:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 管理和边缘集群是vSphere中的专用集群，建议始终为所有管理组件部署一个独立的集群，这样可以在不影响计算集群的情况下轻松进行集群升级，计算集群是专门用于部署最终用户虚拟机的。在这个示例中，我们有一个预配置的vSphere集群。我们将利用这个集群来部署以下组件：
- en: NSX Manager
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX Manager
- en: NSX Controller
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 控制器
- en: Vcenter Server
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vcenter Server
- en: Any other third-party or VMware management software
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何其他第三方或VMware管理软件
- en: But remember that NSX can be integrated with a wide range of VMware products,
    such as Horizon View, SRM, vCloud Director, VRA, VIO, and VCO. So, based on the
    type of product integration and data center design, situations might demand having
    multiple management clusters for isolation purposes. For example, if we have two
    vCenter Servers in the primary site, one for vSphere with NSX integration and
    a second vCenter Server for SRM integration, it is okay to create two separate
    management clusters. Again, this is a design choice, whether we want all our eggs
    in one basket or we are okay to place them in unique baskets (cluster, rack).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，NSX可以与多种VMware产品集成，例如Horizon View、SRM、vCloud Director、VRA、VIO和VCO。因此，根据产品集成类型和数据中心设计，某些情况可能要求有多个管理集群用于隔离。例如，如果在主站点有两个vCenter
    Server，一个用于与NSX集成的vSphere，另一个用于与SRM集成的vCenter Server，那么创建两个独立的管理集群是可以的。再次强调，这是一种设计选择，取决于我们是否愿意将所有资源集中在一个集群中，或者是否愿意将它们分布在不同的集群（机架）中。
- en: Network mapping
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络映射
- en: 'We have connected NSX Manager to a vSphere distributed switch port group called
    **Mgmt_VDS_MGT**. These are preconfigured port groups which will be ideally configured
    with a VLAN port group. Either we can have a separate distributed switch for the
    management and edge cluster or we can have one single distributed switch which
    spans across multiple clusters. The preferred method of deployment would be having
    a unique distributed switch, as that would remain a VMotion boundary for management
    virtual machines. Yes, we don''t want a **Distributed Resource Scheduler** (**DRS**)
    or manual migration movement for those virtual machines to any other compute cluster,
    as this would defeat the purpose of having a unique cluster for management and
    compute machines. Added to that, based on the physical network design, let''s
    assume, as shown in the following screenshot, that we have a top-of-rack switch
    for the management and edge cluster which is running on a single rack. Any network-level
    changes in TOR switches have no impact on the compute cluster. Are we planning
    for LACP, static EtherChannel, or virtual PortChannel like configurations? It
    is important to note that the LACP configuration should be consistent across all
    port groups which are from the same distributed switch. The following diagram
    depicts a typical NSX enterprise vSphere design with separate compute and management
    and edge clusters:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将NSX Manager连接到一个名为**Mgmt_VDS_MGT**的vSphere分布式交换机端口组。这些是预先配置的端口组，理想情况下会配置一个VLAN端口组。我们可以为管理和边缘集群创建一个单独的分布式交换机，或者可以使用一个跨多个集群的单一分布式交换机。首选的部署方法是使用独特的分布式交换机，因为这将成为管理虚拟机的VMotion边界。是的，我们不希望为这些虚拟机进行**分布式资源调度器**（**DRS**）或手动迁移，以便将其移动到任何其他计算集群，因为这样会违背为管理和计算机器创建独立集群的目的。此外，基于物理网络设计，假设如以下截图所示，我们为管理和边缘集群配置了一个位于单个机架上的顶层交换机。TOR交换机中的任何网络级更改都不会影响计算集群。我们是计划使用LACP、静态EtherChannel还是类似虚拟PortChannel的配置？需要注意的是，LACP配置应该在来自同一分布式交换机的所有端口组中保持一致。下图展示了一个典型的NSX企业vSphere设计，具有单独的计算、管理和边缘集群：
- en: '![Network mapping](img/image_03_002.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![网络映射](img/image_03_002.jpg)'
- en: Compute and edge cluster with top of rack design
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 带顶层交换机设计的计算和边缘集群
- en: 'After confirming all our initial configurations are intact, we can log in to
    NSX Manager via a supported web browser, as shown in the following screenshot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在确认所有初始配置完好无损后，我们可以通过支持的Web浏览器登录到NSX Manager，如下方截图所示：
- en: '![Network mapping](img/image_03_003.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![网络映射](img/image_03_003.jpg)'
- en: NSX Manager initial screen
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NSX Manager初始屏幕
- en: 'From the preceding image, we can see the NSX Manager initial page and for me
    there is a significant difference between vCloud networking security and the NSX
    Manager initial page: all the options are well arranged with a straightforward
    explanation. By default, we can log in to NSX Manager using the following credentials. The
    default username and password for NSX Manager is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图片中，我们可以看到NSX Manager的初始页面，我认为vCloud网络安全和NSX Manager初始页面之间有显著的区别：所有选项都排列得井井有条，并有简明的解释。默认情况下，我们可以使用以下凭据登录到NSX
    Manager。NSX Manager的默认用户名和密码如下：
- en: 'User: `admin`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户名：`admin`
- en: 'Password: `default`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密码：`default`
- en: For security-hardening purposes, we can always change the password and create
    multiple users for management access. We can also see from the preceding image
    that on the right-hand side top corner, we have the initial configuration for
    NSX Manager along with the version of NSX, which is 6.2, and the build number.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 出于安全加固的目的，我们可以随时更改密码并创建多个用户进行管理访问。从前面的图片中，我们可以看到在右上角有NSX Manager的初始配置，以及NSX的版本号6.2和构建号。
- en: NSX Manager virtual appliance management
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NSX Manager虚拟设备管理
- en: 'Let''s discuss different options that are listed in the preceding GUI. All
    these are key NSX appliance configuration and integration options:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论在前面的GUI中列出的不同选项。所有这些都是NSX设备配置和集成的关键选项：
- en: '**View summary**: The view summary page gives us the complete configuration
    summary of NSX manager. This includes Virtual Appliance DNS, IP, version, and
    uptime/current time, along with common components and management services as shown
    in the following screenshot:![NSX Manager virtual appliance management](img/image_03_004.jpg)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查看摘要**：查看摘要页面为我们提供了NSX Manager的完整配置摘要。包括虚拟设备DNS、IP、版本、运行时间/当前时间，以及常见的组件和管理服务，如下方截图所示：![NSX
    Manager虚拟设备管理](img/image_03_004.jpg)'
- en: NSX Manager summary
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NSX Manager摘要
- en: One look at the preceding screenshot and we decipher that vPostgres, RabbitMQ,
    and management services should be up and running. **RabbitMQ** Server is a process
    which is hosted on NSX Manager and they interact with the **Firewall daemon (vsfwd**),
    which is running on the ESXi host via the message bus. We will discuss vsfwd in
    more detail in Chapter 6, *Configuring and Managing NSX Network Services*; however,
    as of now we have to understand the importance of this service and how they communicate
    in the NSX world. Postgres is the NSX Manager database which comes along with
    the appliance. Hence, it is very important to note that any editing or table changes
    in the database through any methods will have a direct impact and it is highly
    recommended to perform such practices with the help of the VMware support team.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 看一下前面的屏幕截图，我们可以解读出 vPostgres、RabbitMQ 和管理服务应该正在运行。**RabbitMQ** 服务器是一个托管在 NSX
    管理器上的进程，它们通过消息总线与运行在 ESXi 主机上的**防火墙守护进程 (vsfwd)** 进行交互。我们将在第六章 *配置和管理 NSX 网络服务*
    中更详细地讨论 vsfwd；不过，目前我们需要了解这个服务的重要性以及它们在 NSX 环境中的通信方式。Postgres 是 NSX 管理器数据库，它随设备一起提供。因此，需要特别注意通过任何方法编辑或更改数据库中的表将直接影响系统，强烈建议在
    VMware 支持团队的帮助下执行此类操作。
- en: '**Manage appliance settings**: Manage settings will be extremely helpful to
    make any configuration changes against NSX Manager. For example, there is a new
    Syslog Server configured and we would like to leverage a new Syslog Server to
    be configured with the current NSX Manager. We can easily go ahead and update
    the changes by editing the **Syslog Server** tab. In earlier versions of NSX,
    SSL was disabled by default. However, starting from the NSX 6.1 release, SSL is
    enabled by default. So let''s take a look at the following screenshot, which shows
    the **General** settings of NSX Manager:![NSX Manager virtual appliance management](img/B03244_03_05-1024x586.jpg)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理设备设置**：管理设置对于对 NSX 管理器进行任何配置更改将极为有用。例如，如果配置了一个新的 Syslog 服务器，并且我们希望将新的 Syslog
    服务器与当前的 NSX 管理器一起配置，我们可以通过编辑**Syslog 服务器**标签轻松更新这些更改。在早期版本的 NSX 中，默认情况下禁用了 SSL。但是，从
    NSX 6.1 版本开始，SSL 默认启用。所以让我们看一下以下屏幕截图，它显示了 NSX 管理器的**常规**设置：![NSX 管理器虚拟设备管理](img/B03244_03_05-1024x586.jpg)'
- en: Time Settings, Syslog Server and Locale settings of NSX Manager
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置 NSX 管理器的时间设置、Syslog 服务器和区域设置
- en: '**Manage vCenter registration**: NSX Manager and vCenter Server have a one-to-one
    relationship. With cross VCenter Server NSX installation, this is one of the most
    confused topics as we believe multiple VC is supported with a single NSX instance,
    which is totally wrong. NSX 6.2 allows you to manage multiple vCenter NSX environments
    from a single primary NSX Manager. Even in a Cross vCenter Server NSX installation,
    the NSX Manager to vCenter Server relationship is still one-to-one. More about
    cross VC installation will be discussed in [Chapter 7](ch07.html "Chapter 7. NSX
    Cross vCenter"), *NSX Cross vCenter*, NSX-vCross-vCenter feature and design decisions.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理 vCenter 注册**：NSX 管理器和 vCenter 服务器之间是一对一的关系。在跨 vCenter 服务器的 NSX 安装中，这是最容易混淆的主题之一，因为我们认为多个
    vCenter 可以通过一个 NSX 实例来支持，这是完全错误的。NSX 6.2 允许你通过单个主 NSX 管理器来管理多个 vCenter NSX 环境。即使在跨
    vCenter 服务器的 NSX 安装中，NSX 管理器与 vCenter 服务器之间的关系仍然是一对一的。关于跨 vCenter 安装的更多内容将会在[第七章](ch07.html
    "第7章. NSX 跨 vCenter")中讨论，*NSX 跨 vCenter*，NSX-v 跨 vCenter 功能和设计决策。'
- en: Register vCenter Server with NSX Manager
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 vCenter 服务器与 NSX 管理器注册
- en: 'We will quickly go ahead and register NSX Manager with vCenter Server by following
    steps. The procedure for NSX Manager registration:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速进行将 NSX 管理器与 vCenter 服务器注册的操作，按照以下步骤进行。NSX 管理器注册的流程：
- en: Firstly, log in to the NSX Manager virtual appliance.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，登录到 NSX 管理器虚拟设备。
- en: Under **Appliance Management**, click **Manage Appliance Settings**.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**设备管理**下，点击**管理设备设置**。
- en: We need to type the IP address of the vCenter Server.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要输入 vCenter 服务器的 IP 地址。
- en: Type the vCenter Server username and password.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 vCenter 服务器的用户名和密码。
- en: Click `OK`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `OK`。
- en: Wait for few seconds for successful connection to the vCenter Server.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待几秒钟以确保成功连接到 vCenter 服务器。
- en: '![Register vCenter Server with NSX Manager](img/B03244_03_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![将 vCenter 服务器与 NSX 管理器注册](img/B03244_03_06.jpg)'
- en: Register SSO with NSX Manager
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 SSO 与 NSX 管理器注册
- en: For **single sign-on** (**SSO**) service integration with NSX, we need to configure
    the lookup service, which will improve the security of user authentication for
    vCenter users and enables NSX to authenticate from identity services such as AD,
    NIS, and LDA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **单点登录**（**SSO**）服务与 NSX 的集成，我们需要配置查找服务，这将增强 vCenter 用户身份验证的安全性，并使得 NSX 能够从身份服务（如
    AD、NIS 和 LDA）进行身份验证。
- en: 'Procedure for LookUp service registration:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查找服务注册过程：
- en: Log in to the NSX Manager virtual appliance.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到 NSX Manager 虚拟设备。
- en: Under **Appliance Management**, click **Manage Settings**.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **设备管理** 下，点击 **管理设置**。
- en: Click **NSX Management Service**.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **NSX 管理服务**。
- en: Click **Edit** next to **Lookup Service**.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **编辑**，位于 **查找服务** 旁边。
- en: Type the name or IP address of the host that has the lookup service.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入具有查找服务的主机的名称或 IP 地址。
- en: Change the port number if required. The default port is **7444**.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有需要，更改端口号。默认端口为 **7444**。
- en: The lookup service URL is displayed based on the specified host and port. Type
    the vCenter administrator user name and password.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找服务的 URL 会根据指定的主机和端口显示。输入 vCenter 管理员用户名和密码。
- en: This enables NSX Manager to register itself with the Security Token Service
    server.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使得 NSX Manager 能够将自身注册到安全令牌服务服务器。
- en: Click **OK**.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **确定**。
- en: Confirm that the lookup service status is connected.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认查找服务状态已连接。
- en: 'The following figure shows successful registration of the lookup service and
    vCenter Server registration for NSX Manager:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了查找服务成功注册以及 vCenter 服务器为 NSX Manager 注册的情况：
- en: '![Register SSO with NSX Manager](img/B03244_03_07-1024x628.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NSX Manager 注册 SSO](img/B03244_03_07-1024x628.jpg)'
- en: 'It''s time to explore other NSX Manager settings, such as Tech Support Logs,
    upgrading, and restoring the management appliance:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候探索其他 NSX Manager 设置，例如技术支持日志、升级和恢复管理设备：
- en: '**Download Tech Support Log**: For diagnostic purposes, we can go ahead and
    download the NSX Manager logs by clicking the **download** button under **NSX
    Manager virtual appliance management**.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载技术支持日志**：出于诊断目的，我们可以通过点击 **下载** 按钮下载 NSX Manager 日志，该按钮位于 **NSX Manager
    虚拟设备管理** 下。'
- en: '**Backup and Restore**: NSX Manager data, including system configuration, events,
    and audit log tables (stored in the Postgres DB), can be backed up at any time
    by performing an on-demand backup from the NSX Manager GUI. It is also possible
    to schedule periodic backups to be performed (hourly, daily or weekly).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份与恢复**：NSX Manager 数据，包括系统配置、事件和审计日志表（存储在 Postgres 数据库中），可以随时通过在 NSX Manager
    图形界面执行按需备份来进行备份。还可以安排定期备份（每小时、每天或每周）。'
- en: '**Upgrade**: Based on the version of NSX Manager or vCloud network security
    solution, we will be in need of upgrading the management plane. Once we download
    the upgrade bundle, we need to upload the same to the **NSX Manager-Upgrade-Upload
    New Bundle** tab and click on **Upgrade**. Please note, upgrading NSX Managers
    won''t upgrade control plane or data plane components.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级**：根据 NSX Manager 或 vCloud 网络安全解决方案的版本，我们可能需要升级管理平面。下载升级包后，我们需要将其上传到 **NSX
    Manager-Upgrade-Upload New Bundle** 标签，然后点击 **升级**。请注意，升级 NSX Manager 不会升级控制平面或数据平面组件。'
- en: 'Keep in mind that we are using an administrator role account to register the
    vCenter Server with NSX. Also note that on ASCII characters in the password will
    create synchronization issues with NSX Manager. A successful registration of NSX
    Manager with vCenter Server will let us manage NSX Manager via VMware web client
    and we will see **Networking & Security** solution in the web client inventory
    as shown in the following screenshot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们使用管理员角色账户将 vCenter 服务器注册到 NSX。另外，请注意，密码中的 ASCII 字符会导致与 NSX Manager 的同步问题。成功将
    NSX Manager 注册到 vCenter 服务器后，我们可以通过 VMware Web 客户端管理 NSX Manager，并且在 Web 客户端的库存中会看到
    **网络与安全** 解决方案，如下图所示：
- en: '![Register SSO with NSX Manager](img/B03244_03_08.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![使用 NSX Manager 注册 SSO](img/B03244_03_08.jpg)'
- en: Lastly, let's go ahead and license NSX Manager.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们为 NSX Manager 分配许可证。
- en: 'Log in to the vCenter Server with the vSphere web client and perform the following
    steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 vSphere Web 客户端登录 vCenter 服务器，并执行以下步骤：
- en: In the middle pane, click the **Solutions** tab.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在中间窗格中，点击 **解决方案** 标签。
- en: Select the **NSX** for vSphere solution.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **NSX** for vSphere 解决方案。
- en: Click the **Assign License Key** link.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **分配许可证密钥** 链接。
- en: In the **Assign License Key** panel, select **Assign a new license key** from
    the drop-down menu.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **分配许可证密钥** 面板中，从下拉菜单中选择 **分配新许可证密钥**。
- en: In the **License key** textbox, enter or paste your NSX for vSphere license
    key.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**许可证密钥**文本框中，输入或粘贴您的 NSX for vSphere 许可证密钥。
- en: Click **OK**.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确定**。
- en: The VMware NSX for multi-hypervisor license key may also be used to license
    VMware NSX for vSphere.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: VMware NSX 的多虚拟化平台许可证密钥也可用于授权 VMware NSX for vSphere。
- en: NSX Manager deployment consideration
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 管理器部署注意事项
- en: Due to the critical role NSX plays in a vSphere environment, it is extremely
    important to know and implement NSX management, control plane and data plane features.
    NSX management is NSX Manager, which provides a single point for configuring all
    NSX features and in addition we can leverage REST-API calls for deployment, configuration,
    and other tasks. So let's talk about communication channels from the management
    plane to other components.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NSX 在 vSphere 环境中扮演着关键角色，因此了解和实施 NSX 管理、控制平面和数据平面功能至关重要。NSX 管理由 NSX 管理器提供，它为配置所有
    NSX 功能提供了一个单一的入口点。此外，我们还可以利用 REST-API 调用进行部署、配置及其他任务。那么，接下来我们来谈谈从管理平面到其他组件的通信渠道。
- en: The communication path
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通信路径
- en: 'The following list shows the communication path between NSX Manager and various
    components:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了 NSX 管理器与各个组件之间的通信路径：
- en: '**NSX Manager to VCenter Server**: Communication between manager and VC is
    via vSphere API'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX 管理器到 vCenter Server**：管理器与 vCenter 之间的通信通过 vSphere API 实现。'
- en: '**NSX Manager to Controllers**: Communication between manager and controllers
    is via HTTPs'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX 管理器到控制器**：管理器与控制器之间的通信通过 HTTPS 实现。'
- en: '**NSX Manager to ESXi hosts**: Communication between manager and underlying
    ESXi hosts would be via message bus.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX 管理器到 ESXi 主机**：管理器与底层 ESXi 主机之间的通信通过消息总线进行。'
- en: Network and port requirements
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络和端口要求
- en: NSX Manager virtual machines being part of the management plane, typically the
    NSX Manager and vCenter are placed on a single management network (vSphere PortGroup).
    I know most of the architects would be wondering having isolated networks (different
    subnets) for NSX Manager and vCenter Server will remain supported? The answer
    is *yes*, they can reside in different networks and also in different VLANs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为管理平面一部分的 NSX 管理器虚拟机，通常 NSX 管理器和 vCenter 会放置在同一个管理网络（vSphere PortGroup）中。我知道大多数架构师会好奇，是否可以将
    NSX 管理器和 vCenter Server 放置在隔离的网络（不同子网）中？答案是*可以*，它们可以位于不同的网络中，甚至不同的 VLAN 中。
- en: 'NSX for vSphere protocol and port requirements are updated as shown in the
    following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示，NSX for vSphere 的协议和端口要求已更新：
- en: '![Network and port requirements](img/B03244_03_09.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![网络和端口要求](img/B03244_03_09.jpg)'
- en: Ensure that all these ports and protocols are allowed in the network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 确保网络中允许所有这些端口和协议。
- en: User roles and permissions
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户角色和权限
- en: 'Firstly, NSX roles and permissions are totally different from vCenter Server
    roles and permissions. Hence, it is important to secure user access. Using **Role
    Based Access Control** (**RBAC**), we can secure a user access. Adding to that,
    we can also leverage vCenter SSO identity source users and groups once after properly
    configuring a lookup service with NSX Manager. NSX provides scope to restrict
    the area that a user can access in the NSX system:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，NSX 角色和权限与 vCenter Server 角色和权限完全不同。因此，确保用户访问安全是非常重要的。通过**基于角色的访问控制**（**RBAC**），我们可以保障用户访问的安全。除此之外，在正确配置
    NSX 管理器的查找服务后，我们还可以利用 vCenter SSO 身份源的用户和组。NSX 提供了范围限制功能，用于限制用户可以访问的 NSX 系统区域：
- en: '**Global**: The user has access to all areas of NSX'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局**：用户可以访问 NSX 的所有区域。'
- en: '**Limited access**: The user has access to only the NSX areas defined in the
    user profile'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制访问**：用户仅能访问用户配置文件中定义的 NSX 区域。'
- en: 'Various user roles are given as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 各种用户角色如下：
- en: '**Enterprise Administrator:** NSX operations and security'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业管理员**：NSX 操作和安全管理。'
- en: '**NSX Administrator**: NSX operations only, for example, install virtual appliances,
    configure port groups, and so on'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX 管理员**：仅限于 NSX 操作，例如安装虚拟设备、配置端口组等。'
- en: '**Security Administrator**: Read and write access to NSX security area, such
    as defining data security policies, creating port groups, and creating reports
    for NSX modules, and has read-only access to other areas'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全管理员**：对 NSX 安全区域具有读写访问权限，例如定义数据安全策略、创建端口组和创建 NSX 模块的报告，并对其他区域具有只读访问权限。'
- en: '**Auditor**: Has read-only access to all areas'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审计员**：对所有区域拥有只读访问权限。'
- en: A user cannot be defined without a role. After a role is assigned to users,
    the role can be changed. All these roles are extremely important when giving the
    permissions to NSX users and ensure we are giving limited access to non-enterprise
    administrator accounts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在没有角色的情况下无法定义。为用户分配角色后，可以更改该角色。这些角色在为 NSX 用户授予权限时极其重要，确保我们只为非企业管理员帐户提供有限的访问权限。
- en: Controller requirements
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制器要求
- en: 'Now that we are clear about manager deployment and design decisions, let''s
    discuss controller requirements. NSX Controllers are also deployed as virtual
    appliances with default compute resources per controllers. Since we have already
    registered NSX Manager with virtual center and ensured that we have ports and
    protocols opened, let me re-emphasize once again why controllers are required:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经明确了管理器部署和设计决策，让我们讨论一下控制器的要求。NSX 控制器也作为虚拟设备进行部署，每个控制器都有默认的计算资源。由于我们已经将
    NSX 管理器与虚拟中心注册，并确保我们已打开端口和协议，我再次强调为什么需要控制器：
- en: VXLAN unicast and hybrid mode replication
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN 单播和混合模式复制
- en: Distributed logical routing
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式逻辑路由
- en: Since NSX Controllers are virtual machines with control plane intelligence,
    from a network requirement perspective, they need to have an IP address. However,
    we don't stick with the traditional method of manual or DHCP discovery processes
    for IP assignments. Prior to controller deployment, let's configure IP pools.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 NSX 控制器是具有控制平面智能的虚拟机，从网络需求的角度来看，它们需要有一个 IP 地址。然而，我们并不依赖传统的手动或 DHCP 自动发现过程来分配
    IP 地址。在部署控制器之前，让我们先配置 IP 池。
- en: The procedure for controller IP pool creation
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制器 IP 池创建过程
- en: 'IP pools are used for assigning IP addresses to controllers and **VXLAN Tunnel
    Endpoints** (**VTEP**). I certainly love this feature, which is a less manual
    process. All we need is to create an IP pool and the controller will pick an IP
    from the pool while it is getting created; also, it will release an IP during
    the deletion time:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: IP 池用于为控制器和**VXLAN 隧道端点**（**VTEP**）分配 IP 地址。我特别喜欢这个功能，它减少了手动操作的过程。我们只需要创建一个
    IP 池，控制器在创建时会从池中选择一个 IP 地址；而且，在删除时，它也会释放一个 IP 地址：
- en: From **vCenter Server Web Client**, navigate to **Networking Security** and
    under **Manage** select **Grouping Objects** as shown in the following screenshot:![The
    procedure for controller IP pool creation](img/B03244_03_10-1024x280.jpg)
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**vCenter Server Web 客户端**中，导航到**网络安全**，在**管理**下选择**分组对象**，如下图所示：![控制器 IP 池创建过程](img/B03244_03_10-1024x280.jpg)
- en: Click the **+ **icon and we need to configure a static IP pool so that individual
    controllers can pick one IP from this pool as shown in the following screenshot:![The
    procedure for controller IP pool creation](img/B03244_03_11.jpg)
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**+**图标，我们需要配置一个静态 IP 池，以便各个控制器能够从这个池中选择一个 IP 地址，如下图所示：![控制器 IP 池创建过程](img/B03244_03_11.jpg)
- en: Now that we have an IP pool ready, we can switch back to the NSX home installation
    page and click on the **+** sign under **NSX Controller Nodes** as shown in the
    following screenshot:![The procedure for controller IP pool creation](img/B03244_03_12-1024x624.jpg)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好 IP 池，可以返回到 NSX 主页面，并点击**NSX 控制器节点**下的**+**号，如下图所示：![控制器 IP 池创建过程](img/B03244_03_12-1024x624.jpg)
- en: Select and update respectively **vCenter Datacenter**, **Cluster/Resource Pool**,
    shared **Datastore** location, vSphere PortGroup for connectivity, and lastly
    the name of **IP pool** which we created in step 2\. After accomplishing the task,
    you'll see the following screenshot:![The procedure for controller IP pool creation](img/image_03_013.jpg)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别选择并更新**vCenter 数据中心**、**集群/资源池**、共享的**数据存储**位置、vSphere 网络组（PortGroup）以确保连通性，最后选择我们在步骤
    2 中创建的**IP 池**名称。完成此任务后，您将看到以下截图：![控制器 IP 池创建过程](img/image_03_013.jpg)
- en: After the first controller is deployed, we can deploy an additional two more
    controllers by following the same steps since the three node control cluster is
    mandatory.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个控制器部署完成后，我们可以通过相同的步骤再部署两个控制器，因为三节点控制集群是强制性的。
- en: Successful deployment of controllers will have a **Normal** status and a green
    check mark as shown in the following screenshot:![The procedure for controller
    IP pool creation](img/image_03_014.jpg)
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成功部署的控制器将显示**正常**状态，并且会有一个绿色的勾选标记，如下图所示：![控制器 IP 池创建过程](img/image_03_014.jpg)
- en: 'Let''s make a note of all three controller IPs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记录下所有三个控制器的 IP 地址：
- en: '**Controller 1**: **192.168.110.201**'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器 1**：**192.168.110.201**'
- en: '**Controller 2**: **192.168.110.202**'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器 2**：**192.168.110.202**'
- en: '**Controller 3**: **192.168.110.203**'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器 3**：**192.168.110.203**'
- en: 'Even though we have controllers deployed and the status is green, it is important
    to check the control cluster connections and their status from the command line,
    which would give granular-level details. SSH to all three controllers and issue
    the following command:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经部署了控制器并且状态为绿色，仍然需要从命令行检查控制集群连接及其状态，这将提供更为详细的信息。SSH 到所有三个控制器并发出以下命令：
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The controller types and their status are explained as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器类型及其状态解释如下：
- en: '`Join status`: Verify the controller node is reporting join complete'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`加入状态`：验证控制器节点是否报告加入完成'
- en: '`Majority status`: Verify the controller is connected to the cluster majority'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`大多数状态`：验证控制器是否已连接到集群的大多数节点'
- en: '`Cluster ID`: All the controller nodes in a cluster should have the same cluster
    ID'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`集群 ID`：集群中的所有控制器节点应具有相同的集群 ID'
- en: 'Remember the controller roles that we discussed in [Chapter 2](ch02.html "Chapter 2. 
    NSX Architecture"), *NSX Architecture*? Yes, those are the five roles which are
    populated here, as shown in the following screenshot - `api_provider`, `persistence_server`,
    `switch_manager`, `logical_manager`, and `directory_server`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在 [第 2 章](ch02.html "第 2 章. NSX 架构") 讨论的控制器角色吗？*NSX 架构*？是的，这些就是五个角色，它们在这里填充，如下面的截图所示：`api_provider`、`persistence_server`、`switch_manager`、`logical_manager`
    和 `directory_server`：
- en: '![The procedure for controller IP pool creation](img/image_03_015.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![控制器 IP 池创建过程](img/image_03_015.jpg)'
- en: Okay, this is all really great, but I know we have a few questions left. Let's
    have a look at those questions one by one.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这一切都很棒，但我知道我们还有一些问题。让我们逐一看一下这些问题。
- en: How do we check which controller is master for those roles?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何检查哪些控制器是这些角色的主控？
- en: 'The following command will display that output for us:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将为我们显示该输出：
- en: '[PRE1]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s do a random check on one of those controllers, in this example, `Controller
    2`, which is `192.168.110.202`. As we can see from the following screenshot, except
    for the persistence server, all the roles are master in controller `192.168.110.202`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对其中一个控制器进行随机检查，在这个例子中是 `控制器 2`，其 IP 地址为 `192.168.110.202`。从下面的截图可以看到，除了持久化服务器，所有角色在控制器
    `192.168.110.202` 上都是主控：
- en: '![The procedure for controller IP pool creation](img/image_03_016.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![控制器 IP 池创建过程](img/image_03_016.jpg)'
- en: SSH session NSX Controller
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SSH 会话 NSX 控制器
- en: 'The controller cluster majority leader listens on port `2878` and will have
    a `Y` in the `listening` column. To check that, let''s issue the following command
    and check on the same controller `192.168.110.202` that we have checked during
    step 2:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器集群的大多数领导者监听端口 `2878`，并在 `listening` 列中显示 `Y`。要检查这一点，我们在步骤 2 中检查的同一控制器 `192.168.110.202`
    上发出以下命令：
- en: '[PRE2]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We got the following output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![The procedure for controller IP pool creation](img/image_03_017.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![控制器 IP 池创建过程](img/image_03_017.jpg)'
- en: Does that look bit weird?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来有点奇怪吗？
- en: We know that the controller cluster majority leader listens on port `2878` and
    would have a `**-**` in the `listening` column for persistence server for controller
    `192.168.110.202` . No rocket science here; as per the `Show Control-Cluster roles`
    output which we have tested in step 7, except for the persistence server role,
    for the rest of the roles, controller cluster 2 was master, hence `Show Control-Cluster
    Connections` is reporting `-` for the persistence server. I hope we now have some
    basic understanding of controller roles. We will discuss a few troubleshooting
    scenarios in [Chapter 8](ch08.html "Chapter 8.  NSX Troubleshooting"), *NSX Troubleshooting*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道控制器集群的大多数领导者监听端口 `2878`，并且对于控制器 `192.168.110.202` 的持久化服务器，在 `listening`
    列中会显示 `**-**`。这里没有什么难度；根据我们在第 7 步中测试的 `Show Control-Cluster roles` 输出，除持久化服务器角色外，控制器集群
    2 的其他所有角色都是主控，因此 `Show Control-Cluster Connections` 报告持久化服务器的状态为 `-`。我希望我们现在对控制器角色有了基本的了解。我们将在
    [第 8 章](ch08.html "第 8 章. NSX 故障排除")，*NSX 故障排除* 中讨论一些故障排除场景。
- en: NSX Controller design consideration
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX控制器设计考虑因素
- en: NSX Controller virtual machines are the DNA of the control plane, hence it is
    important to take decisions on where to install and connect the controller. Lastly,
    we don't want the controller to get exposed to users who are leveraging NSX features;
    basically, no control plane attack.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: NSX 控制器虚拟机是控制平面的核心，因此决定安装和连接控制器的位置非常重要。最后，我们不希望控制器暴露给利用 NSX 特性的用户；基本上，不允许控制平面受到攻击。
- en: Communication path
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通信路径
- en: 'It''s good to know the communication protocol used between NSX Manager, controllers
    and NSX Edges:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 了解 NSX Manager、控制器和 NSX Edge 之间使用的通信协议是很有帮助的：
- en: Communication between controller and NSX Manager - HTTPS
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器和 NSX Manager 之间的通信 - HTTPS
- en: Communication between Edge and controller - HTTPS
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Edge 和控制器之间的通信 - HTTPS
- en: Network and port requirements
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络和端口要求
- en: 'Ensure that the port requirements mentioned in the following screenshot are
    met for controller communication:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 确保满足以下截图中提到的端口要求，以支持控制器之间的通信：
- en: '![Network and port requirements](img/image_03_018.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![网络和端口要求](img/image_03_018.jpg)'
- en: Controller deployment consideration
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制器部署注意事项
- en: 'I know, I keep telling you this: the real power of NSX is all about controllers.
    How we deploy our controllers, what best practices are implemented, all makes
    a vital difference in NSX design. You know by now, because of overlay networks,
    there will be a whole bunch of design best practices that we might need to do
    in both the physical and virtual worlds. But here, we will discuss controller
    deployment considerations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，我一直在告诉你：NSX 的真正强大之处就在于控制器。我们如何部署控制器、实施哪些最佳实践，都在 NSX 设计中起着至关重要的作用。你现在应该明白，由于覆盖网络，我们可能需要在物理和虚拟环境中实施大量设计最佳实践。但在这里，我们将讨论控制器部署的注意事项：
- en: First and foremost, NSX Controllers should be deployed in the same vCenter where
    NSX Manager is registered. The only exception would be while leveraging cross-VC
    NSX design, which we will discuss in [Chapter 7](ch07.html "Chapter 7. NSX Cross
    vCenter"), *NSX Cross vCenter*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，NSX 控制器应该部署在与 NSX Manager 注册的同一个 vCenter 中。唯一的例外是采用跨 vCenter 的 NSX 设计，我们将在[第
    7 章](ch07.html "第 7 章. 跨 vCenter NSX")中讨论 *跨 vCenter 的 NSX*。
- en: While deploying the controllers, don't make any other configuration changes
    or deploy any other NSX features. The rule is the same even if we are deleting
    a controller and deploying a new one during break fix scenarios.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署控制器时，不要进行其他配置更改，也不要部署其他 NSX 功能。即使是在故障修复场景中删除一个控制器并部署新控制器时，这个规则依然适用。
- en: Use a separate vSphere Edge cluster for controller deployments. For small-scale
    deployments, it is okay to deploy controllers in a management cluster.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于控制器部署，建议使用单独的 vSphere Edge 集群。对于小规模部署，将控制器部署到管理集群中也是可以的。
- en: NSX Controllers should be reachable with all ESXi host vmkernel networks and
    NSX Manager management networks.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 控制器应该能够与所有 ESXi 主机的 vmkernel 网络和 NSX Manager 管理网络进行通信。
- en: The controller being a VM, most of the enterprise environments will have the
    vSphere **Distributed Resource Schedular** (**DRS**) feature and it will consider
    it as a normal virtual machine and will migrate or place it on same ESXi host
    based on the placement algorithm. To ensure controllers are not getting deployed
    or migrated to the same host and if there is a host failure this will have a direct
    impact on controllers and the entire control plane will be down. To avoid such
    situations, we will have to leverage vSphere anti affinity rules to avoid deploying
    more than one controller on the same ESXi host. Adding to that, I would highly
    recommend starting with more than three host clusters and later scale accordingly.
    This way, we can easily place controllers on separate ESXi hosts and scale accordingly.
    Don't get my message wrong, we are deploying three controllers on three different
    hosts and not leaving the rest of the host as a spare one. In any environment,
    we will be doing maintenance activity, sometimes as a part of a software upgrade
    or maybe adding or removing hardware devices from the server. For such scenarios,
    when we take a downtime for one of the hosts, we are still left with more than
    three ESXi hosts and controllers would be placed on them based on the anti affinity
    rules.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于控制器是虚拟机，大多数企业环境将启用 vSphere **分布式资源调度器**（**DRS**）功能，它会将控制器视为普通虚拟机，并根据部署算法将其迁移或放置在同一
    ESXi 主机上。为了确保控制器不会被部署或迁移到同一主机上，并避免主机故障时直接影响控制器及整个控制平面，我们需要利用 vSphere 的反亲和性规则，避免在同一
    ESXi 主机上部署多个控制器。此外，我强烈建议从多个主机集群（至少三个）开始，并根据需要进行扩展。这样，我们就能轻松将控制器部署到不同的 ESXi 主机上并扩展规模。不要误解我的意思，我们是在三个不同的主机上部署三个控制器，而不是将剩余的主机作为备用主机。在任何环境中，我们都需要进行维护操作，有时是软件升级的一部分，或者是从服务器中添加或移除硬件设备。在这种情况下，当我们对某台主机进行停机维护时，其他主机依然存在，因此控制器会根据反亲和性规则部署到这些主机上。
- en: All the hosts in the cluster should have automatic VM startup/shutdown enabled.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的所有主机应启用自动 VM 启动/关闭功能。
- en: Note
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The first host where the controllers are deployed will have automatic VM startup/shutdown
    enabled by default.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 部署控制器的第一个主机默认启用自动 VM 启动/关闭。
- en: That summarizes controller deployment and key design aspects. With that, let's
    move to data plane preparation. Time to memorize what we did so far? Yes, let's
    do it. We have deployed NSX Manager (management plane) and registered the solution
    with vCenter Server. Later, we deployed NSX Controllers (control plane) and, finally,
    we are in the last phase of installation, which is the data plane.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以上总结了控制器部署和关键设计方面。现在，我们来谈谈数据平面准备。是时候回顾我们目前为止所做的工作了吗？是的，让我们来回顾一下。我们已部署了 NSX 管理器（管理平面）并将解决方案注册到
    vCenter Server。接着，我们部署了 NSX 控制器（控制平面），最后，我们进入了安装的最后阶段——数据平面。
- en: The NSX data plane
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 数据平面
- en: The NSX data plane consists of **vSphere Distributed Switch** (**VDS**), kernel
    modules, User World Agents, NSX Edge, and Distributed routing/firewall and bridging
    modules. Firstly, let's discuss preparing ESXi clusters for NSX.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: NSX 数据平面包括**vSphere 分布式交换机**（**VDS**）、内核模块、用户世界代理、NSX 边缘设备、分布式路由/防火墙及桥接模块。首先，让我们讨论如何为
    NSX 准备 ESXi 集群。
- en: 'What is host preparation? All it does is install the kernel modules and builds
    a management and control plane domain. The kernel modules that we refer to here
    are as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是主机准备？它的作用就是安装内核模块，并构建一个管理和控制平面域。我们在这里提到的内核模块如下：
- en: Distributed routing
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式路由
- en: Distributed firewall
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式防火墙
- en: VXLAN bridging
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VXLAN 桥接
- en: In a multi-cluster environment, we need to perform this installation per cluster
    level. Based on the vSphere design, we can always introduce new hosts to the cluster
    and preparation is automated for newly added hosts, which makes life easier for
    architects.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在多集群环境中，我们需要在每个集群层级执行此安装。根据 vSphere 设计，我们始终可以将新主机添加到集群中，且对于新添加的主机，准备工作会自动完成，这使得架构师的工作更加轻松。
- en: The host preparation procedure
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主机准备过程
- en: 'Let''s discuss how an ESXi host is prepared to push the kernel modules:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何准备 ESXi 主机以推送内核模块：
- en: In vCenter, navigate to **Home** | **Networking & Security** | **Installation**
    and select the **Host Preparation** tab.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 vCenter 中，导航到**主页**|**网络与安全**|**安装**，并选择**主机准备**标签。
- en: Select the respective cluster and click the gear icon and click **Install**
    as shown in the following screenshot:![The host preparation procedure](img/image_03_019.jpg)
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择相应的集群，点击齿轮图标并点击**安装**，如下图所示：![主机准备过程](img/image_03_019.jpg)
- en: Monitor the installation. Repeat the same steps for other clusters.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控安装过程。对其他集群重复相同步骤。
- en: 'Now, one common question from all vSphere folks: do we need to reboot the host
    since VIBS got installed? The answer is a **BIG FAT NO**! Only after uninstallation
    scenarios do we need a host reboot. We humans tend to forget things easily, don''t
    we? No problem, there would be a message populated near our ESXi host icon notifying
    *Reboot Required*.'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，所有 vSphere 用户常有的一个问题是：安装了 VIBS 后，我们是否需要重启主机？答案是**绝对不需要**！只有在卸载场景下，我们才需要重启主机。我们人类总是容易忘记一些事情，不是吗？没问题，我们的
    ESXi 主机图标旁会弹出提示，提醒*需要重启*。
- en: For each vSphere cluster, we will go ahead and configure ****VXLAN**** networking
    prerequisites.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 vSphere 集群，我们将继续配置****VXLAN**** 网络先决条件。
- en: Firstly, we will configure a **static pool** for the **VTEP IP** assignment,
    which is similar to the controller **IP pool** configuration that we did earlier.
    The **DHCP** pool assignment is also possible; however, in this case, I'm showcasing
    the IP assignment with static pools.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将为**VTEP IP** 分配配置一个**静态池**，这类似于我们之前配置的控制器**IP 池**。当然，也可以使用**DHCP**池分配，但在这里，我展示的是通过静态池分配
    IP 的方式。
- en: 'From NSX Manager, navigate to manage **IP pools** and click on the **+** sign.
    Update the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从 NSX 管理器中，导航到管理**IP 池**并点击**+**号。更新以下内容：
- en: VTEP **Name**
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VTEP **名称**
- en: '**Gateway** address'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网关** 地址'
- en: '**Prefix Length**'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前缀长度**'
- en: '**Static IP Pool** for VTEP IP assignments.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 VTEP IP 分配配置**静态 IP 池**。
- en: 'Have a look at the following screenshot for reference:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下图：
- en: '![The host preparation procedure](img/image_03_020.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![主机准备过程](img/image_03_020.jpg)'
- en: 'Select one of the vSphere clusters and click the configure link provided in
    the VXLAN column:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个 vSphere 集群，并点击 VXLAN 列中的配置链接：
- en: Select the vSphere distributed switch.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 vSphere 分布式交换机。
- en: Update the **VLAN** number. Enter `0` if you're not using a VLAN, which will
    pass along untagged traffic.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新**VLAN**编号。如果不使用VLAN，请输入`0`，这将通过未标记的流量传递。
- en: Ensure **MTU** is `**1600**` (VXLAN overhead)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保**MTU**为`**1600**`（VXLAN开销）
- en: For VMKnic IP addressing, we need to make use of the earlier VTEP IP pool that
    we configured.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于VMKnic IP地址配置，我们需要利用先前配置的VTEP IP池。
- en: '**VMKNic Teaming Policy** method is used for bonding the vmnics (physical NICs)
    for use with the VTEP port group. I have selected for fail over. The other options
    are Static EtherChannel, LACP (Active), LACP (Passive), Load Balance by Source
    ID, Load Balance by Source MAC, and Enhanced LACP. The following screenshot is
    updated with a list of supported VXLAN NIC teaming policies:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**VMKNic聚合策略**方法用于将vmnic（物理NIC）绑定到VTEP端口组。我选择了故障转移。其他选项包括静态EtherChannel、LACP（主动）、LACP（被动）、按源ID负载平衡、按源MAC负载平衡以及增强型LACP。以下截图更新了支持的VXLAN
    NIC聚合策略列表：'
- en: '![The host preparation procedure](img/image_03_021.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![主机准备过程](img/image_03_021.jpg)'
- en: VTEP NIC teaming design is extremely critical in NSX environments. Most customers
    would go with single VTEP configuration primarily because of the simplicity in
    the design. However, if we have more than 10G VXLAN traffic, LACP or Static EtherChannel
    would be the preferred load balancing policy.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: VTEP NIC聚合设计在NSX环境中至关重要。大多数客户会选择单一VTEP配置，主要是因为设计的简单性。然而，如果我们的VXLAN流量超过10G，LACP或静态EtherChannel将是首选的负载均衡策略。
- en: VTEP Value is the number of VTEPs per host. Is there any specific reason why
    we would go for multi VTEP configuration? Well, if we have more than one physical
    link that we would like to use for VXLAN traffic and the upstream switches do
    not support LACP the use of multiple VTEPs allows us to balance the traffic between
    physical links.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: VTEP值是每个主机的VTEP数量。为什么我们要选择多VTEP配置？如果我们有多个物理链路希望用于VXLAN流量，并且上游交换机不支持LACP，使用多个VTEP可以使我们在物理链路之间平衡流量。
- en: 'The following screenshot depicts the preceding step:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了前面的步骤：
- en: '![The host preparation procedure](img/image_03_022.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![主机准备过程](img/image_03_022.jpg)'
- en: Based on the requirement, we can repeat the same step for other clusters as
    well, and each such configuration will create a VXLAN distributed port group in
    vSphere.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需求，我们也可以对其他集群重复相同的步骤，每个这样的配置将在vSphere中创建一个VXLAN分布式端口组。
- en: 'Successful installation of VXLAN modules will show as **Configured** as highlighted
    in the following screenshot:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN模块的成功安装将显示为**已配置**，如下图所示：
- en: '![The host preparation procedure](img/image_03_023.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![主机准备过程](img/image_03_023.jpg)'
- en: 'I know most us will have a few design-related queries on VXLAN and how it works.
    Stay focused: we are going in the right direction and will discuss that in upcoming
    chapters.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道我们大多数人对于VXLAN及其工作原理会有一些设计相关的疑问。保持专注：我们正在朝着正确的方向前进，并将在后续章节中讨论这一点。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We started this chapter with an introduction to NSX Manager requirements and
    we covered all design aspects of the management plane. Later, we discussed NSX
    Controller requirements and key design decisions. Finally, we moved to data plane
    installation. In the next chapter, we will discuss managing and deploying NSX
    logical networks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们介绍了NSX Manager的要求，并覆盖了管理平面的所有设计方面。随后，我们讨论了NSX Controller的要求和关键设计决策。最后，我们进入了数据平面的安装。在下一章中，我们将讨论如何管理和部署NSX逻辑网络。
