- en: Chapter 4. NSX Virtual Networks and Logical Router
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. NSX 虚拟网络与逻辑路由器
- en: 'Scalability and flexibility are impressive features of Network Virtualization.
    On any IP network, any Switches/Routers, any physical network design NSX works
    flawlessly. With the emerging interest in overlay networks, there are different
    encapsulation technologies currently in the market. VXLAN, NVGRE, LISP are few
    in that list. In this chapter we will discuss on logical networks and how NSX
    simplifies datacenter routing and switching. Following topics will be the key
    points:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和灵活性是网络虚拟化的显著特点。无论是在任何 IP 网络、任何交换机/路由器上，还是任何物理网络设计中，NSX 都能完美工作。随着对覆盖网络（Overlay
    Network）的兴趣日益增长，目前市场上有多种封装技术。VXLAN、NVGRE、LISP 等是其中的一部分。在本章中，我们将讨论逻辑网络以及 NSX 如何简化数据中心的路由和交换。以下是关键点：
- en: NSX logical switches
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 逻辑交换机
- en: NSX virtual network creation - multicast, unicast, and hybrid replication modes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 虚拟网络创建 - 组播、单播和混合复制模式
- en: NSX virtual network best practices and deployment considerations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 虚拟网络最佳实践与部署考虑事项
- en: NSX logical router
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 逻辑路由器
- en: NSX logical routing and bridging best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX 逻辑路由与桥接最佳实践
- en: NSX logical switches
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NSX 逻辑交换机
- en: Using VMware NSX virtual networks, we can create a logical network on top of
    any IP network. We have already discussed VXLAN fundamentals and the host installation
    process in the previous chapters. Now that we have a good understanding of the
    basics, it's time to move on with virtual network creation. Before we begin exploring,
    it is important to understand that each logical network is a separate broadcast
    domain.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 VMware NSX 虚拟网络，我们可以在任何 IP 网络之上创建逻辑网络。我们已经在前几章中讨论了 VXLAN 基础知识和主机安装过程。现在我们对基础知识有了较好的理解，是时候继续进行虚拟网络的创建了。在开始之前，理解每个逻辑网络都是一个独立的广播域非常重要。
- en: Logical network prerequisites
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑网络先决条件
- en: 'Firstly, let''s have a look at the prerequisites for logical network creation:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看看逻辑网络创建的先决条件：
- en: Host preparation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机准备
- en: Segment ID (VNI) pool
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段 ID (VNI) 池
- en: Global transport zone
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局传输区域
- en: Host preparation
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主机准备
- en: We have already discussed in detail how the underlying ESXi host is prepared
    from NSX Manager in [Chapter 3](ch03.html "Chapter 3. NSX Manager Installation
    and Configuration") , *NSX Manager Installation and Configuration*. Here, it is
    time to recollect that knowledge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [第3章](ch03.html "第3章. NSX 管理器安装与配置") *NSX 管理器安装与配置* 中详细讨论了如何通过 NSX 管理器准备底层
    ESXi 主机。现在是回顾这些知识的时候了。
- en: The hypervisor kernel modules enable ESXi hosts to support VXLAN, the logical
    switch, the distributed router, and the distributed firewall.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 超级管理程序内核模块使 ESXi 主机能够支持 VXLAN、逻辑交换机、分布式路由器和分布式防火墙。
- en: Segment ID (VNI) pool
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 段 ID (VNI) 池
- en: As we know, the VXLAN network identifier is a 24-bit address that gets added
    to the VXLAN frame, which allows us to isolate each VXLAN network from another
    VXLAN network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，VXLAN 网络标识符是一个 24 位地址，它会被添加到 VXLAN 帧中，从而实现将每个 VXLAN 网络与其他 VXLAN 网络隔离。
- en: Steps to configure the VNI pool
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置 VNI 池的步骤
- en: 'Here are the steps to configure the VNI pool:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 VNI 池的步骤如下：
- en: On the **Logical Network Preparation** tab, click the **Segment ID** button.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **逻辑网络准备** 选项卡中，点击 **段 ID** 按钮。
- en: Click **Edit** to open the **Edit Segment IDs and Multicast Address Allocation** dialog
    box and configure the given options.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **编辑** 打开 **编辑段 ID 和组播地址分配** 对话框，并配置给定的选项。
- en: 'Click **OK** as shown in the following screenshot:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **确定** 如下图所示：
- en: '![Steps to configure the VNI pool](img/image_04_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![配置 VNI 池的步骤](img/image_04_001.jpg)'
- en: In this example, we are not using multicast networks; rather, we want to leverage
    unicast networks. One of the classic examples of multicast VXLAN networks would
    be when a customer has existing VXLAN networks (created when he was using vCloud
    network security) and the management software was upgraded to NSX. For such a
    scenario, people stick with multicast mode VXLAN networks in NSX. Also, please
    note that we don't even need a controller in that case. Do not use 239.0.0.0/24
    or 239.128.0.0/24 as the multicast address range, because these networks are used
    for local subnet control, meaning that the physical switches flood all traffic
    that uses these addresses. The complete list is documented at [https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01](https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01)
    .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们并没有使用多播网络，而是希望利用单播网络。一个经典的多播 VXLAN 网络示例是，当客户已经有现有的 VXLAN 网络（在使用 vCloud
    网络安全时创建）且管理软件升级到 NSX 后，很多人会在 NSX 中继续使用多播模式 VXLAN 网络。请注意，在这种情况下，我们甚至不需要控制器。不要使用
    239.0.0.0/24 或 239.128.0.0/24 作为多播地址范围，因为这些网络用于本地子网控制，这意味着物理交换机会泛洪所有使用这些地址的流量。完整的列表已在
    [https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01](https://tools.ietf.org/html/draft-ietf-mboned-ipv4-mcast-unusable-01)
    中文档化。
- en: Transport zone
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传输区
- en: 'A transport zone is a boundary for a VNI. All clusters in the same transport
    zone share the same VNI. A transport zone can contain multiple clusters and a
    cluster can be part of multiple transport zones or, in other words, a host can
    be part of multiple transport zones. In the following screenshot, we have three
    clusters: **Cluster A**, **Cluster B**, and **Cluster C**. **Cluster A** and **Cluster
    B** are part of **Transport Zone A** and **Cluster C** is part of **Transport
    Zone B**.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传输区是 VNI 的边界。同一传输区中的所有集群共享相同的 VNI。一个传输区可以包含多个集群，一个集群也可以属于多个传输区，换句话说，一个主机可以属于多个传输区。在下图中，我们有三个集群：**集群
    A**、**集群 B** 和 **集群 C**。**集群 A** 和 **集群 B** 属于 **传输区 A**，**集群 C** 属于 **传输区 B**。
- en: '![Transport zone](img/image_04_003.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![传输区](img/image_04_003.jpg)'
- en: Configuring a global transport zone
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置全局传输区
- en: 'The following steps will help you to configure the transport zone:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助您配置传输区：
- en: On the **Logical Network Preparation** tab, click **Transport Zones**.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **逻辑网络准备** 标签页中，点击 **传输区**。
- en: 'Click the green plus sign to open the **New Transport Zone** dialog box and
    configure the following options and click **OK**:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击绿色加号打开 **新建传输区** 对话框，配置以下选项后点击 **确定**：
- en: Enter the transport zone name in the **Name** text box.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **名称** 文本框中输入传输区名称。
- en: In **Control Plane Mode**, select the **Unicast**, **Multicast**, or **Hybrid**
    button.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **控制平面模式** 下，选择 **单播**、**多播** 或 **混合模式** 按钮。
- en: In **Select clusters to add**, select the check box for each of the vSphere
    clusters listed. Also, have a look at the distributed switch selection highlighted
    in red. We are following one of the NSX-DVS design best practices. Both the compute
    clusters are running on **Compute_VDS** and the management cluster is on **Mgmt_Edge_VDS**.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **选择要添加的集群** 中，勾选列出的每个 vSphere 集群的复选框。同时，查看红色高亮的分布式交换机选择。我们遵循 NSX-DVS 设计最佳实践之一。两个计算集群运行在
    **Compute_VDS** 上，管理集群运行在 **Mgmt_Edge_VDS** 上。
- en: 'Once it is updated, verify that the transport zone appears in the transport
    zone list, with a control plane mode set to unicast, multicast, or hybrid based
    on the preceding selection. Refer to the following figure:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新后，验证传输区是否出现在传输区列表中，并根据前述选择设置控制平面模式为单播、多播或混合模式。参见下图：
- en: '![Configuring a global transport zone](img/image_04_004.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![配置全局传输区](img/image_04_004.jpg)'
- en: 'We have now completed all the prerequisites for NSX logical networks. In this
    example, we have the following virtual machines already created in vSphere without
    any network connectivity. We will go ahead and create four logical networks and
    will connect to virtual machines and later we will test connectivity:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了 NSX 逻辑网络的所有先决条件。在这个示例中，我们已经在 vSphere 中创建了以下虚拟机，但尚未建立网络连接。我们将继续创建四个逻辑网络，并将其连接到虚拟机，稍后进行连通性测试：
- en: '**Two web servers**: web-sv-01a and web-sv-02a'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两个 Web 服务器**：web-sv-01a 和 web-sv-02a'
- en: '**One DB server**: DB-sv-01a'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个数据库服务器**：DB-sv-01a'
- en: '**Application server**: app-sv-01a'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用服务器**：app-sv-01a'
- en: Creating logical switches
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建逻辑交换机
- en: 'In an old-fashioned vSphere environment, ideally, virtual machines will be
    connected to a preconfigured vSphere PortGroup with or without VLAN tagging. But
    now we are in the NSX world and let''s leverage an NSX logical switch for virtual
    machine connectivity. In the left navigation pane, select **Logical Switches**
    and in the center pane, click the green plus sign to open the **New Logical Switch**
    dialog box. Perform the following actions to configure the logical switch:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的vSphere环境中，理想情况下，虚拟机将连接到预配置的vSphere PortGroup，无论是否使用VLAN标记。但现在我们处于NSX世界，并且让我们利用NSX逻辑交换机来连接虚拟机。在左侧导航窗格中，选择**Logical
    Switches**，在中心窗格中点击绿色的加号打开**New Logical Switch**对话框。按照以下步骤配置逻辑交换机：
- en: Enter `App-Tier` in the **Name** text box.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Name**文本框中输入`App-Tier`。
- en: Verify that the **Transport Zone** selection is **Transport**.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证**Transport Zone**选择项是否为**Transport**。
- en: Verify that the **Control Plane Mode** selection is **Unicast**.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证**Control Plane Mode**选择项是否为**Unicast**。
- en: 'Click **OK**:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确定**：
- en: '![Creating logical switches](img/image_04_005.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![创建逻辑交换机](img/image_04_005.jpg)'
- en: 'The following are the two options shown on the **New Logical Switch** screen:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在**New Logical Switch**屏幕上显示的两个选项：
- en: '****Enable IP Discovery**** : Another great feature, which minimizes ARP flooding
    in a VXLAN network. When a virtual machine sends an ARP packet, the switch security
    module - which is nothing but a dvfilter module attached to VNIC - will query
    the NSX Controller to know if they have the MAC entry for that destination IP.
    Everyone knows that we have two chances in that case:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****启用IP发现****：另一个伟大的功能，能够最小化VXLAN网络中的ARP泛洪。当虚拟机发送ARP包时，交换机安全模块——即附加到VNIC的dvfilter模块——将查询NSX控制器，看看是否有目标IP的MAC条目。每个人都知道在这种情况下我们有两个选择：'
- en: The controller has the MAC entry
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器有MAC条目
- en: The controller doesn't have the MAC entry
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器没有MAC条目
- en: In the first condition, since the controller has the MAC entry, it will respond
    with the MAC address and, that way, ARP traffic is reduced. In the second condition,
    the controller responds with no MAC reply and the ARP will flood in the normal
    way.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一种情况下，由于控制器有MAC条目，它会回应MAC地址，从而减少ARP流量。在第二种情况下，控制器没有MAC回应，ARP流量将以常规方式泛洪。
- en: '****Enable MAC Learning**** : When we enable MAC learning a VLAN/MAC pair table
    is maintained on each vNIC which is used by dvfilter data. This table will be
    intact whenever VM migrates from one host to another host with the help of dvfilter
    data.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****启用MAC学习****：启用MAC学习时，会在每个vNIC上维护一个VLAN/MAC对表，该表由dvfilter数据使用。无论虚拟机从一个主机迁移到另一个主机，借助dvfilter数据，该表将保持不变。'
- en: 'Wait for the update to complete and confirm app-network appears with the status
    set to **Normal**. Repeat Steps 1-4 and create three more logical switches, and
    name them **Web-Tier**, **DB-Tier**, and **Transit** networks. The successful
    creation of logical switches will show us the same results when populated under
    logical switches, as shown in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 等待更新完成并确认app-network显示状态为**正常**。重复步骤1-4并创建三个逻辑交换机，分别命名为**Web-Tier**、**DB-Tier**和**Transit**网络。成功创建逻辑交换机后，当它们在逻辑交换机下显示时，将呈现以下截图所示的相同结果：
- en: '![Creating logical switches](img/image_04_007.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![创建逻辑交换机](img/image_04_007.jpg)'
- en: 'With the preceding steps, we see four port groups created in the vSphere networking
    option with their respective VNI-ID:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过之前的步骤，我们看到在vSphere网络选项中创建了四个端口组，它们的VNI-ID如下：
- en: '**5000**: **Transit** network'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5000**：**Transit**网络'
- en: '**5001**: **Web_Tier** network'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5001**：**Web_Tier**网络'
- en: '**5002**: **App_Tier** network'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5002**：**App_Tier**网络'
- en: '**5003**: **DB_Tier** network'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5003**：**DB_Tier**网络'
- en: '![Creating logical switches](img/B03244_04_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![创建逻辑交换机](img/B03244_04_06.jpg)'
- en: Understanding replication modes
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解复制模式
- en: 'Let''s discuss replication modes in more detail and later we will connect the
    logical switches to virtual machines. With the addition of an NSX Controller,
    the requirement for multicast protocol support on physical networks is completely
    removed for VXLAN. There are three replication modes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论复制模式，稍后我们将把逻辑交换机连接到虚拟机。随着NSX控制器的加入，VXLAN对物理网络的多播协议支持要求完全消除。复制模式有三种：
- en: '****Multicast**** : When multicast replication mode is chosen for a given logical
    switch, NSX relies on the Layer 2 and Layer 3 multicast capability of the data
    center physical network to ensure VXLAN encapsulated multi-destination traffic
    is sent to all the VTEPs. This mode is recommended only when you are upgrading
    from older VXLAN deployments (vCloud network security). It requires PIM/IGMP on
    a physical network.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****组播****：当为给定的逻辑交换机选择组播复制模式时，NSX依赖于数据中心物理网络的第2层和第3层组播功能，以确保VXLAN封装的多目标流量能够发送到所有VTEP。这种模式仅在从旧版本的VXLAN部署（如vCloud网络安全）升级时推荐使用。它需要物理网络上的PIM/IGMP支持。'
- en: '****Unicast**** : The control plane is handled by the NSX Controller. All unicast
    traffic leverages head end replication. No multicast IP address or special network
    configuration is required. In unicast mode, the ESXi hosts in the NSX domain are
    divided into separate VTEP segments based on the IP subnet their VTEP interfaces
    belong to. There will be a UTEP selection for each segment to play the role of
    **Unicast Tunnel End Point** (**UTEP**). The UTEP is responsible for replicating
    multi-destination traffic received from the ESXi hypervisor hosting the VM sourcing
    the traffic and belonging to a different VTEP segment from all the ESXi hosts
    part of its segment.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****单播****：控制平面由NSX控制器处理。所有单播流量使用头端复制。无需组播IP地址或特殊的网络配置。在单播模式下，NSX域中的ESXi主机根据其VTEP接口所属的IP子网被划分到不同的VTEP段中。每个段都会选择一个UTEP作为**单播隧道端点**（**UTEP**）。UTEP负责复制从承载源流量的虚拟机的ESXi主机接收到的多目标流量，这些流量属于不同VTEP段，并且是该段内所有ESXi主机的一部分。'
- en: '****Hybrid**** : The optimized unicast mode. Offloads local traffic replication
    to a physical network (L2 multicast). This requires IGMP snooping on the first-hop
    switch, but does not require PIM. The first-hop switch handles traffic replication
    for the subnet.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '****混合模式****：优化的单播模式。将本地流量复制卸载到物理网络（L2组播）。这需要在第一跳交换机上启用IGMP嗅探，但不需要PIM。第一跳交换机负责子网的流量复制。'
- en: 'We will consider the following screenshot as a network topology and will explain
    all three modes of replication, which will give us a precise picture of how replication
    works:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把下面的截图视为网络拓扑，并解释所有三种复制模式，这将帮助我们精确了解复制是如何工作的：
- en: '![Understanding replication modes](img/image_04_009.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![理解复制模式](img/image_04_009.jpg)'
- en: 'Firstly, let''s understand the configuration. The preceding screenshot shows
    us the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解一下配置。前面的截图显示了以下内容：
- en: There are two transport zones in this set-up, ****Transport Zone A**** and ****Transport
    Zone B****
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个设置中，有两个传输区，分别是****传输区A****和****传输区B****。
- en: A ****Distributed Virtual Switch** (**DVS**)** is part of both the transport
    zones
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个****分布式虚拟交换机**（**DVS**）**是两个传输区的组成部分。
- en: All the virtual machines are connected to one common VXLAN network - ****VXLAN
    5001****
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有虚拟机都连接到一个公共的VXLAN网络 —— ****VXLAN 5001****。
- en: We will walk through all the modes one by one and also discuss their design
    decisions in the following sections.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一讨论所有模式，并在接下来的章节中讨论它们的设计决策。
- en: Unicast mode packet walk
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单播模式数据包行走
- en: 'Let''s discuss a unicast mode VXLAN packet walk:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下单播模式VXLAN数据包行走：
- en: '**VM-A** generated **Broadcast, Unknown Unicast, Multicast** (**BUM**) traffic,
    which is typically Layer 2 traffic.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VM-A** 生成了**广播、未知单播、组播**（**BUM**）流量，通常是第2层流量。'
- en: '**ESXi-A** will do a local VTEP lookup and will learn that the packet needs
    to be locally replicated (same subnet), in this case **ESXi-B**.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ESXi-A** 将执行本地VTEP查找，并学习到数据包需要在本地进行复制（同一子网），在这种情况下是 **ESXi-B**。'
- en: 'In addition to that, the packet will also get remotely replicated. Since we
    have four hosts in a remote subnet (ESXi E, F, G, and H), to which host will it
    send the packet? This is one key differentiator between multicast mode VXLAN and
    unicast. In unicast mode, the packet will be sent to a proxy module called **UTEP**
    with a replicate-locally bit set. Why is it like that? The answer is very simple:
    since there is a replicate-locally bit set, UTEP will replicate the packet locally
    to one of the ESXi hosts that are part of same subnet. In this example, we have
    two subnets; based on the network topology, the process will be same for every
    subnet.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外，数据包还会被远程复制。由于我们有四台主机位于远程子网（ESXi E、F、G 和 H），它会将数据包发送到哪一台主机？这是组播模式 VXLAN
    和单播模式之间的一个关键区别。在单播模式下，数据包将被发送到一个名为 **UTEP** 的代理模块，设置了本地复制位。为什么会这样？答案很简单：由于设置了本地复制位，UTEP
    将数据包在本地复制到同一子网中的某一台 ESXi 主机。在这个例子中，我们有两个子网；根据网络拓扑，处理过程在每个子网中是相同的。
- en: Design decisions for unicast mode VXLAN
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单播模式 VXLAN 的设计决策
- en: 'There is not much to be discussed about unicast mode VXLAN design. However,
    it is important to know the following points:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 关于单播模式 VXLAN 设计，没有太多需要讨论的内容。然而，了解以下几点很重要：
- en: We could simply stick with traditional IP network design and just ensure that
    the MTU is increased to 1,600.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以简单地沿用传统的 IP 网络设计，只需确保将 MTU 增加到 1,600。
- en: Let's go back and read the VXLAN packet walk in unicast mode; what does it do
    in a nutshell? It replicates the packet locally and sends one copy to the remote
    subnet and again replicates it locally. Who does that replication? The ESXi host
    does all this intelligent work and of course, based on how big the environment
    is or how often we have **BUM** traffic, it will create a slight overhead on the
    hypervisor. So I would suggest unicast mode as the best way to start using VXLAN;
    however, it is not a great candidate for large environments.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们回过头来看一下单播模式下的 VXLAN 数据包处理流程，简而言之，它做了什么？它会在本地复制数据包，然后将一份发送到远程子网，再次在本地复制。谁来执行这个复制工作？ESXi
    主机完成了所有这些智能化操作，当然，基于环境的大小或 **BUM** 流量的频率，它会给虚拟化管理程序带来一些轻微的开销。所以我建议单播模式作为开始使用 VXLAN
    的最佳方式；然而，它并不是大型环境的理想选择。
- en: In environments where customers have multicast limitation, unicast mode VXLAN
    is best.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在客户有组播限制的环境中，单播模式 VXLAN 是最佳选择。
- en: Multicast mode packet walk
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组播模式数据包处理
- en: 'Whenever I used to explain about multicast VXLAN networks, it made me recollect
    the VMware **vCloud Networking and Security** (**vCNS**) solution days. Multicast
    mode VXLAN was the starting stage of VXLAN implementation in both virtualized
    vSphere environments and cloud environments running on vCloud director software.
    The solution was very powerful; however, physical network prerequisites were one
    of the difficult factors for all architects because it really defeats the purpose
    of saying NSX can be run on any IP network. The bitter truth is that IP networking
    demands some requirements for the technology to work flawlessly. With that said,
    let''s start with a packet walk:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我解释组播 VXLAN 网络时，总让我想起 VMware **vCloud Networking and Security**（**vCNS**）解决方案的时代。组播模式
    VXLAN 是 VXLAN 实现在虚拟化 vSphere 环境和运行 vCloud director 软件的云环境中的初始阶段。这个解决方案非常强大；然而，物理网络的前提条件是所有架构师面临的困难之一，因为它确实违背了
    NSX 可以在任何 IP 网络上运行的说法。残酷的现实是，IP 网络确实要求一些条件才能使技术无缝工作。话虽如此，让我们从数据包处理开始：
- en: '**VM-A** generates **BUM** traffic.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VM-A** 生成 **BUM** 流量。'
- en: '**The ESXi A** host encapsulates the packet with a VXLAN header (5001). Time
    to start guessing who it will send the packet to. Will it simply be broadcast?
    The Layer 2 frame is a broadcast frame encapsulated with a VXLAN header; however,
    the host would be sending it to one of the multicast groups. How will we ensure
    the multicast reaches only **ESXi B**, **E**, **F**, **G**, and **H** since we
    have a virtual machine running on the same VXLAN network? This is where a physical
    network requirement is a must. We need IGMP snoop for that; if not, that would
    be treated as an unknown multicast packet.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ESXi A** 主机将数据包封装在一个 VXLAN 头部（5001）中。现在开始猜测它会将数据包发送到哪里。它会仅仅广播吗？二层帧是一个广播帧，并带有
    VXLAN 头部；然而，主机会将其发送到某个组播组。我们如何确保组播只到达**ESXi B**、**E**、**F**、**G** 和 **H**，因为我们有一个虚拟机正在同一个
    VXLAN 网络上运行？这就是为什么物理网络要求必须的原因。我们需要 IGMP snoop；如果没有，它将被视为一个未知的组播数据包。'
- en: The **router** will perform an L3 multicast and will send it to a Layer 2 switch
    and the switch will again check the multicast group and will send it to the right
    host. Eventually, the VM running on the destination host will receive the packet
    after getting de-encapsulated by the VTEP.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路由器** 将执行 L3 多播并将其发送到二层交换机，交换机会再次检查多播组，并将其发送到正确的主机。最终，目标主机上的虚拟机将在 VTEP 解封装后接收到该数据包。'
- en: Design decisions for multicast mode VXLAN
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多播模式 VXLAN 的设计决策
- en: 'As I mentioned, we certainly need to take care of physical network prerequirements
    in multicast mode VXLAN:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所提到的，我们确实需要在多播模式的 VXLAN 网络中处理物理网络的前提要求：
- en: '**IGMP snoop** and **IP multicasting** are required in the switch and router
    throughout the network.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IGMP snoop** 和 **IP 多播** 在整个网络中的交换机和路由器上是必需的。'
- en: Ideally, one VXLAN segment to one multicast group is the recommended way to
    provide optimal multicast forwarding, which also demands an increase in multicast
    groups if we have large segments. Something which I have seen in cloud environments,
    wherein VXLAN networks are created on-the-fly and the cloud provider ensures enough
    multicast IP is available for 1:1 mapping; that way, a packet forwarded to one
    tenant won't be seen by other tenants.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想情况下，一个 VXLAN 段对应一个多播组是提供最佳多播转发的推荐方式，这也要求如果我们有较大的段，则需要增加多播组。这是我在云环境中看到的，VXLAN
    网络是动态创建的，云提供商会确保为 1:1 映射提供足够的多播 IP；这样，转发给一个租户的数据包就不会被其他租户看到。
- en: Hybrid mode packet walk
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合模式数据包转发
- en: 'Hybrid mode VXLAN is recommended for most large environments, primarily because
    of the simplicity and limited configuration changes that are demanded in the network.
    Let''s have a look at a hybrid mode VXLAN packet walk:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模式 VXLAN 推荐用于大多数大型环境，主要是因为其简便性和对网络中配置更改的限制要求。让我们来看一下混合模式 VXLAN 数据包的转发过程：
- en: '**Virtual Machine A** generates **BUM** traffic.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**虚拟机 A** 生成 **BUM** 流量。'
- en: '**ESXi A** host encapsulates the **L2 header** with **VXLAN header 5001** and
    will send it to a physical switch.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ESXi A** 主机将 **L2 头** 封装为 **VXLAN 头 5001**，并将其发送到物理交换机。'
- en: In this case, an encapsulated L2 header will be send it to a multicast group
    which is defined on the physical switch. I hope it makes more sense now. The physical
    switch will deliver the packet to destination ESXi host part of that multicast
    group, in this case, ESXi B, C, D. In addition to that, ESXi A will send a `Locally_Replicate_BIT`
    set packet to a remote subnet. This packet will be received by a proxy module
    called **Multicast Tunnel End Point** (**MTEP**). Again, it is a straightforward
    answer, since there is a replicate locally bit set, MTEP (ESXi host) will replicate
    the packet locally to one of the ESXi host that are part of same subnet.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，一个封装的 L2 头将被发送到物理交换机上定义的多播组。我希望现在更加清晰了。物理交换机会将数据包传递到该多播组中的目标 ESXi 主机，在这个例子中是
    ESXi B、C、D。除此之外，ESXi A 会将一个设置了 `Locally_Replicate_BIT` 的数据包发送到远程子网。这个数据包将由一个称为
    **Multicast Tunnel End Point**（**MTEP**）的代理模块接收。同样，这是一个直接的答案，因为设置了本地复制位，MTEP（ESXi
    主机）将把数据包本地复制到同一子网中其他 ESXi 主机。
- en: MTEP will again send the packet to a physical switch and the physical switch
    will deliver the packet to all the host part of same multicast group.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MTEP 将再次将数据包发送到物理交换机，物理交换机会将数据包传递到同一多播组中所有的主机。
- en: Design decisions for hybrid mode VXLAN
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合模式 VXLAN 的设计决策
- en: 'Hybrid mode VXLAN being one of most widely used replication modes, I believe
    all of us will be interested to know the key design decisions:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模式 VXLAN 作为最广泛使用的复制模式之一，我相信我们都会对一些关键的设计决策感兴趣：
- en: '**IGMP snoop** is required to be configured on a physical switch throughout
    the VXLAN network.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IGMP snoop** 需要在整个 VXLAN 网络中的物理交换机上进行配置。'
- en: IP multicast is not required in the physical router throughout the network.
    I'm not sure if I'm safe enough to say that, because replication modes can be
    selected per logical switch, which means we can deploy a logical switch in unicast,
    multicast, or hybrid mode. What if we are deploying logical switch A in multicast
    and logical switch B in hybrid mode in the same VXLAN domain? It demands IP multicasting
    in physical networking. But again, we don't need IP multicasting explicitly for
    hybrid mode VXLAN networking.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个网络中的物理路由器不需要 IP 多播。我不确定我是否可以说得足够安全，因为可以根据每个逻辑交换机选择复制模式，这意味着我们可以在单播、多播或混合模式下部署逻辑交换机。如果我们在同一个
    VXLAN 域中将逻辑交换机 A 部署为多播模式，逻辑交换机 B 部署为混合模式，那么这就需要物理网络中启用 IP 多播。但再次强调，对于混合模式的 VXLAN
    网络，我们并不显式地需要 IP 多播。
- en: It is strongly recommended to define an **IGMP Querier** for each VLAN to ensure
    successful L2 multicast delivery and avoid non-deterministic behavior. In order
    for IGMP, and thus IGMP snooping, to function, a multicast router must exist on
    the network and generate IGMP queries. The tables created for snooping (holding
    the member ports for each multicast group) are associated with the querier. I
    believe we have a strong foundation in VXLAN and its replication modes; let's
    move on to the connectivity of logical switches and virtual machines.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议为每个VLAN定义一个**IGMP Querier**，以确保成功的L2多播传输并避免非确定性行为。为了让IGMP和IGMP嗅探功能正常工作，网络上必须存在一个多播路由器并生成IGMP查询。为嗅探（持有每个多播组成员端口的表格）创建的表格与查询器相关联。我相信我们已经在VXLAN及其复制模式方面打下了坚实的基础；接下来让我们继续讨论逻辑交换机和虚拟机的连接。
- en: Connecting virtual machines to logical switches
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连接虚拟机到逻辑交换机
- en: 'Since we have already created logical switches, let''s go ahead and connect
    logical switches to the following virtual machines:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经创建了逻辑交换机，接下来让我们将逻辑交换机连接到以下虚拟机：
- en: '**Two web servers**: web-sv-01a and web-sv-02a'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两台Web服务器**：web-sv-01a和web-sv-02a'
- en: '**One DB server**: DB-sv-01a'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一台数据库服务器**：DB-sv-01a'
- en: '**Application server**: app-sv-01a'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用服务器**：app-sv-01a'
- en: 'Let us see how to connect the logical switches:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何连接逻辑交换机：
- en: Click the **vSphere Web Client** home icon.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**vSphere Web Client**主页图标。
- en: On the **vSphere Web Client** home tab, click **Inventories** | **Networking
    & Security**.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**vSphere Web Client**主页标签页中，点击**库存** | **网络与安全**。
- en: In the left navigation pane, select **Logical Switches**. In the center pane,
    select the **App-Tier** logical switch.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航窗格中，选择**逻辑交换机**。在中间窗格中，选择**应用层**逻辑交换机。
- en: Click the **Add Virtual Machines** icon, or select **Add VM** from the **Actions**
    drop-down menu.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加虚拟机**图标，或者从**操作**下拉菜单中选择**添加虚拟机**。
- en: In the **App-Tier**,add virtual machines dialog box.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**应用层**中，点击虚拟机对话框。
- en: In the filter list, select the **app-01a** check boxes.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在筛选器列表中，勾选**app-01a**复选框。
- en: Click **Next**. In the **Select VNICs** list, select the **Network Adapter 1
    (VM Network)** check box.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**。在**选择VNICs**列表中，勾选**网络适配器1（虚拟机网络）**复选框。
- en: Click **Next** as shown in the following screenshot:![Connecting virtual machines
    to logical switches](img/B03244_04_08-1024x516.jpg)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**，如以下截图所示：![连接虚拟机到逻辑交换机](img/B03244_04_08-1024x516.jpg)
- en: Click **Finish**.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**完成**。
- en: Repeat Steps 1-7 and connect both the web servers (web-sv-01a, web-sv-02a) and
    DB server.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-7，连接两个Web服务器（web-sv-01a，web-sv-02a）和数据库服务器。
- en: Testing connectivity
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试连接性
- en: 'As we know that our three-tier application, web, app, and DB, is connected
    to logical switches, let''s do some basic testing to confirm their connectivity:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道三层应用程序，web、app和DB，已连接到逻辑交换机，让我们做一些基本测试以确认它们的连接性：
- en: Firstly, go ahead and power on those machines.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，启动这些虚拟机。
- en: Click the **vSphere Web Client** home icon.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**vSphere Web Client**主页图标。
- en: On the **vSphere Web Client** home tab, click the **Inventories** | **VMs and
    Templates** icon.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**vSphere Web Client**主页标签页中，点击**库存** | **虚拟机和模板**图标。
- en: 'Expand the VMs and templates inventory tree and power on each of the following
    virtual machines found in the discovered virtual machine folder:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开虚拟机和模板库存树，并启动在已发现的虚拟机文件夹中的以下虚拟机：
- en: web-sv-01a
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: web-sv-01a
- en: web-sv-02a
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: web-sv-02a
- en: app-sv-01a
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: app-sv-01a
- en: db-sv-01a
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: db-sv-01a
- en: '![Testing connectivity](img/image_04_011.jpg)'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![测试连接性](img/image_04_011.jpg)'
- en: To power on a virtual machine, select the virtual machine in the inventory,
    then select **Power On** from the **Actions** drop-down menu.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动虚拟机，请在库存中选择虚拟机，然后从**操作**下拉菜单中选择**开机**。
- en: 'Once the machines are powered on, we will go ahead and record their IP address:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦虚拟机启动，我们将记录它们的IP地址：
- en: '`web-01a`: `172.16.10.11`'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`web-01a`：`172.16.10.11`'
- en: '`web-02a`: `172.16.10.12`'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`web-02a`：`172.16.10.12`'
- en: '`app-01a`: `172.16.20.11`'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`app-01a`：`172.16.20.11`'
- en: '`DB-01a`: `172.16.30.11`'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DB-01a`：`172.16.30.11`'
- en: 'We now do a simple ping test between `web-01a` and `app-01a` as shown in the
    following screenshot:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对`web-01a`和`app-01a`进行简单的ping测试，如以下截图所示：
- en: '![Testing connectivity](img/image_04_012.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![测试连接性](img/image_04_012.jpg)'
- en: Why do we have 100% packet loss when we ping `web-01a (172.16.10.11)` and `app-01a(172.16.20.11)`?
    Will a logical switch perform a Layer 3 routing? Definitely not. The traditional
    way of performing routing for such networks would be through a physical router,
    which is nothing but going all the way out of the rack, gets it routed to the
    right destination. Let's not do that legacy routing in this case, we would leverage
    NSX logical router capability.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么当我们 ping `web-01a (172.16.10.11)` 和 `app-01a(172.16.20.11)` 时会出现 100% 丢包？逻辑交换机会执行第三层路由吗？当然不会。传统的路由方式是通过物理路由器来执行，这意味着必须走出机架，将数据路由到正确的目的地。我们在此不使用传统的路由方式，而是利用
    NSX 逻辑路由器功能。
- en: The Distributed Logical Router
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式逻辑路由器
- en: 'The whole purpose of routing is to process the packets between two different
    IP networks. Let''s discuss the fundamentals of routing before getting into logical
    routers. Every router will build a routing table, which will have information
    about **destination network**, **next hop router**, **metrics, and administrative
    distance**. There are two methods of building a routing table:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 路由的主要目的是在两个不同的 IP 网络之间处理数据包。在深入了解逻辑路由器之前，让我们先讨论一下路由的基础知识。每个路由器都会构建一个路由表，其中包含
    **目标网络**、**下一跳路由器**、**度量值和管理距离** 等信息。构建路由表有两种方法：
- en: '**Static routing**: Static routing is manually created and updated by a network
    administrator. Based on the network topology, we will be in need of configuring
    a static route on each and every router for end-to-end network connectivity. Even
    though this gives full control over the routing, it would be an extremely tedious
    job to configure routes on a large network.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态路由**：静态路由是由网络管理员手动创建和更新的。根据网络拓扑，我们需要在每个路由器上配置静态路由，以实现端到端的网络连接。尽管这种方式可以完全控制路由，但在大型网络中配置路由将是一项非常繁琐的工作。'
- en: '**Dynamic routing**: Dynamic routing is created and updated by a routing protocol
    running on a router; **Routing Information Protocol** (**RIP**) and **Open Shortest
    Path First** (**OSPF**) are some examples. Dynamic routing protocols are intelligent
    enough to choose a better path whenever there is a change in routing infrastructure.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态路由**：动态路由是通过运行在路由器上的路由协议创建和更新的；**路由信息协议** (**RIP**) 和 **开放最短路径优先** (**OSPF**)
    就是其中的一些例子。动态路由协议足够智能，能够在路由基础设施发生变化时选择更好的路径。'
- en: The VMware NSX Distributed Logical Router supports static routes, OSPF, ISIS,
    and BGP routing protocols. It is important to know that dynamic routing protocols
    are supported only on external interface (uplink) of the **Distributed Logical
    Router** (**DLR**). The DLR allows an ESXi hypervisor to locally do routing intelligence,
    through which we can optimize East-West data plane traffic.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: VMware NSX 分布式逻辑路由器支持静态路由、OSPF、ISIS 和 BGP 路由协议。需要注意的是，动态路由协议仅在 **分布式逻辑路由器**
    (**DLR**) 的外部接口（上行链路）上受支持。DLR 允许 ESXi 超级管理程序在本地执行路由智能，从而优化东西向数据平面流量。
- en: Deploying a Distributed Logical Router
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署分布式逻辑路由器
- en: A DLR is a virtual appliance which has the control plane intelligence and it
    relies on NSX Controllers to push the routing updates to the ESXi kernel modules.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DLR 是一种虚拟设备，具有控制平面智能，并依赖于 NSX 控制器将路由更新推送到 ESXi 内核模块。
- en: Procedure for deploying a logical router
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署逻辑路由器的步骤
- en: 'Let''s walk through the step-by-step configuration of a Distributed Logical
    Router:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步配置分布式逻辑路由器：
- en: In the vSphere web client, navigate to **Home** | **Networking & Security**
    | **NSX Edges**.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 vSphere Web 客户端中，导航到 **主页** | **网络与安全** | **NSX 边缘**。
- en: Note
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: Select the appropriate NSX Manager on which to make your changes. If you are
    creating a universal logical router, you must select the primary NSX Manager.
    We will be discussing about Primary/Secondary NSX manager concepts in  [Chapter
    7](ch07.html "Chapter 7. NSX Cross vCenter") , *NSX Cross vCenter*.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择适当的 NSX 管理器，以便进行更改。如果您正在创建一个通用逻辑路由器，则必须选择主要的 NSX 管理器。我们将在 [第 7 章](ch07.html
    "第7章. NSX 跨 vCenter") 中讨论主要/次要 NSX 管理器的概念，*NSX 跨 vCenter*。
- en: Select the type of router you wish to add; in this case, we would add **logical
    router**.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您希望添加的路由器类型；在本例中，我们将添加 **逻辑路由器**。
- en: Select **Logical (Distributed) Router** to add a logical router local to the
    selected NSX Manager.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **逻辑（分布式）路由器** 以将逻辑路由器添加到选定的 NSX 管理器本地。
- en: Note
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: Since we haven't discussed the cross-vCenter NSX environment, we won't leverage
    a universal logical distributed router in this chapter.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们尚未讨论跨vCenter的NSX环境，本章不会使用通用逻辑分布式路由器（DLR）。
- en: Type a name for the device. This name appears in your **vCenter inventory**.
    The name should be unique across all logical routers within a single tenant. Optionally,
    you can also enter a hostname. This name appears in the CLI. If you do not specify
    the hostname, the edge ID, which gets created automatically, is displayed in the
    CLI.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为设备输入一个名称。此名称将出现在您的**vCenter库存**中。该名称在单个租户的所有逻辑路由器中应唯一。您还可以选择输入主机名。该名称将显示在CLI中。如果没有指定主机名，自动创建的边缘ID将显示在CLI中。
- en: The **Deploy Edge Appliance** option is selected by default. An edge appliance
    (also called a logical router virtual appliance) is required for dynamic routing
    and the logical router appliance's firewall, which applies to logical router pings,
    SSH access, and dynamic routing traffic. You can deselect the **Deploy Edge Appliance**
    option if you require only static routes, and do not want to deploy an edge appliance.
    You cannot add an edge appliance to the logical router after the logical router
    has been created.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下选中了**部署边缘设备**选项。边缘设备（也称为逻辑路由器虚拟设备）用于动态路由和逻辑路由器设备的防火墙，适用于逻辑路由器的ping、SSH访问和动态路由流量。如果您只需要静态路由，并且不想部署边缘设备，您可以取消选中**部署边缘设备**选项。逻辑路由器创建后，无法再添加边缘设备。
- en: 'The **Enable High Availability** option is not selected by default. Select
    the **Enable High Availability** check box to enable and configure high availability.
    High Availability is required if you are planning to do dynamic routing. I want
    everyone to think from a cloud provider perspective: if your tenant is requesting
    the High Availability feature, how do you satisfy that requirement? NSX Edge replicates
    the configuration of the primary appliance for the standby appliance and ensures
    that the two HA NSX Edge virtual machines are not on the same ESXi host even after
    you use DRS and vMotion.  Two virtual machines are deployed on vCenter in the
    same resource pool and data store as the appliance you configured. Local link
    IPs are assigned to HA virtual machines in the NSX Edge HA so that they can communicate
    with each other. But remember that instead of one control VM, we have two control
    VMs running now so definitely it will consume twice the compute resource.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，**启用高可用性**选项没有选中。选中**启用高可用性**复选框以启用并配置高可用性。如果您计划进行动态路由，则需要高可用性。我希望大家从云提供商的角度考虑：如果您的租户请求高可用性功能，您如何满足这个要求？NSX
    Edge会为备用设备复制主设备的配置，并确保即使使用DRS和vMotion，两个高可用性的NSX Edge虚拟机也不会部署在同一ESXi主机上。两个虚拟机被部署在vCenter中，与您配置的设备位于同一资源池和数据存储中。NSX
    Edge HA为高可用性虚拟机分配本地链接IP，以便它们能够相互通信。但请记住，我们现在有两个控制VM，而不是一个控制VM，因此肯定会消耗两倍的计算资源。
- en: 'The following screenshot shows NSX DLR-VM deployment:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图展示了NSX DLR-VM的部署：
- en: '![Procedure for deploying a logical router](img/image_04_013.jpg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/image_04_013.jpg)'
- en: 'Type and retype a password for the logical router. The password must be 12-255
    characters and must contain the following:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为逻辑路由器输入并重新输入密码。密码必须为12-255个字符，并且必须包含以下内容：
- en: At least one uppercase letter
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个大写字母
- en: At least one lowercase letter
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个小写字母
- en: At least one number
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个数字
- en: At least one special character
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一个特殊字符
- en: Enable **SSH** and set the log level (optional). By default, **SSH** is disabled.
    If you do not enable SSH, you can still access the logical router by opening the
    virtual appliance console. Enabling SSH here causes the SSH process to run on
    the logical router virtual appliance, but you will also need to adjust the logical
    router firewall configuration manually to allow SSH access to the logical router's
    protocol address. The protocol address is configured when you configure dynamic
    routing on the logical router. By default, the log level is emergency.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用**SSH**并设置日志级别（可选）。默认情况下，**SSH**是禁用的。如果您没有启用SSH，仍然可以通过打开虚拟设备控制台访问逻辑路由器。在这里启用SSH会使SSH进程在逻辑路由器虚拟设备上运行，但您还需要手动调整逻辑路由器的防火墙配置，以允许SSH访问逻辑路由器的协议地址。协议地址是在您为逻辑路由器配置动态路由时配置的。默认情况下，日志级别为紧急。
- en: Note
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: On logical routers, only IPv4 addressing is supported.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在逻辑路由器上，仅支持IPv4地址。
- en: 'Configure the interfaces. Under Configure interfaces, add four **logical interfaces**
    (**LIFs**) to the logical router:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置接口。在配置接口下，向逻辑路由器添加四个**逻辑接口**（**LIFs**）：
- en: Uplink connected to **Transit-Network-01** logical switch with an IP of 192.168.10.2/29
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上行接口连接到**Transit-Network-01** 逻辑交换机，IP地址为192.168.10.2/29
- en: Internal connected to **Web-Tier-01 Logical Switch** with IP 172.16.10.1/24
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部接口连接到**Web-Tier-01 逻辑交换机**，IP地址为172.16.10.1/24
- en: Internal connected to **App-Tier-01 Logical Switch** with IP 172.16.20.1/24
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部接口连接到**App-Tier-01 逻辑交换机**，IP地址为172.16.20.1/24
- en: Internal connected to **DB-Tier-01 Logical Switch** with IP 172.16.30.1/24
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部接口连接到**DB-Tier-01 逻辑交换机**，IP地址为172.16.30.1/24
- en: 'The following screenshot depicts the **Add Interface** screen:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图显示了**添加接口**的界面：
- en: '![Procedure for deploying a logical router](img/image_04_014.jpg)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/image_04_014.jpg)'
- en: 'Configure interfaces of this NSX Edge: Internal interfaces are for connections
    to logical switches that allow VM-to-VM (East-West) communication. Internal interfaces
    are created on the logical router virtual appliance and we call them LIF. Uplink
    interfaces are for North-South communication. A logical router uplink interface
    can be connected to an NSX Edge services gateway, third-party router VM, or a
    VLAN-backed dvPortgroup to make the logical router connection to a physical router
    directly. You must have at least one uplink interface for dynamic routing to work.
    Uplink interfaces are created as vNICs on the logical router virtual appliance.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置此NSX Edge的接口：内部接口用于连接逻辑交换机，允许虚拟机之间的通信（东西向）。内部接口是在逻辑路由器虚拟设备上创建的，我们称之为LIF。上行接口用于南北向通信。逻辑路由器的上行接口可以连接到NSX
    Edge服务网关、第三方路由器虚拟机，或连接到VLAN支持的dvPortgroup，从而直接将逻辑路由器与物理路由器连接。为了使动态路由正常工作，必须至少有一个上行接口。上行接口作为vNIC在逻辑路由器虚拟设备上创建。
- en: Note
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We can add, remove, and modify interfaces after a logical router is deployed.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部署逻辑路由器后，我们可以添加、删除和修改接口。
- en: 'The following screenshot depicts the DLR configuration that we have performed
    so far:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了我们迄今为止执行的DLR配置：
- en: '![Procedure for deploying a logical router](img/image_04_015.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/image_04_015.jpg)'
- en: Now that we have successfully deployed a DLR and configured with logical interfaces,
    we would expect the DLR to perform basic routing functionality for web, app, and
    DB machines to communicate with each other, which was not possible earlier.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功部署了一个DLR并配置了逻辑接口，我们希望DLR能够执行基本的路由功能，使Web、应用和数据库服务器可以互相通信，这在之前是无法实现的。
- en: 'The following screenshot depicts the three-tier application architecture without
    routing:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了没有路由的三层应用架构：
- en: '![Procedure for deploying a logical router](img/image_04_016.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/image_04_016.jpg)'
- en: 'Let''s go ahead and perform a quick ping test between `web-01a (172.16.10.11)`
    and `app (172.16.20.11)`. As we can see from the following screenshot, web servers
    and application servers are able to communicate each other since we have a Distributer
    Logical Router, which does the routing in this case. The first ping result is
    before adding the Distributed Logical Router:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一次快速的ping测试，测试`web-01a (172.16.10.11)`和`app (172.16.20.11)`之间的连通性。如下面的截图所示，Web服务器和应用服务器能够互相通信，因为我们有一个分布式逻辑路由器，在此案例中负责路由。第一次ping测试结果是在添加分布式逻辑路由器之前：
- en: '![Procedure for deploying a logical router](img/image_04_017.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/image_04_017.jpg)'
- en: So far, we have discussed the **Distributed Logical Router** (**DLR**), which
    allows ESXi hypervisor to locally do routing intelligence through which we can
    optimize East-West data plane traffic. But I know we are very keen to view the
    DLR routing table in an ESXi host. Let's focus on the following screenshot to
    know the network topology.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了**分布式逻辑路由器**（**DLR**），它使ESXi虚拟化平台能够在本地执行路由智能，从而优化东西向的数据平面流量。但我知道我们很期待查看ESXi主机中的DLR路由表。让我们聚焦于下面的截图，了解网络拓扑。
- en: 'The following screenshot depicts the three-tier application architecture with
    DLR connection:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了连接了DLR的三层应用架构：
- en: '![Procedure for deploying a logical router](img/B03244_04_16.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的过程](img/B03244_04_16.jpg)'
- en: 'The following questions might come to our mind:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会有以下问题：
- en: How many networks do we have?
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有多少个网络？
- en: 172.16.10.0/24
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 172.16.10.0/24
- en: 172.16.20.0/24
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 172.16.20.0/24
- en: 172.16.30.0/24
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 172.16.30.0/24
- en: 192.168.10.0/29
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 192.168.10.0/29
- en: Are the networks directly connected to the router?
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络是否直接连接到路由器？
- en: Yes, they are connected to the router.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的，它们已连接到路由器。
- en: 'We will go ahead and SSH to one of the ESXi hosts to check the logical router
    instance, MAC, ARP, and routing tables, which will certainly give granular-level
    details:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过SSH登录到其中一台ESXi主机，检查逻辑路由器实例、MAC地址、ARP和路由表，这将为我们提供更详细的信息：
- en: '[PRE0]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding command will display the logical router instance as shown in
    the following screenshot. You can see the following parameters:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将显示逻辑路由器实例，如下图所示。你可以看到以下参数：
- en: '`VDR Name` is `default+edge-19`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VDR 名称`为`default+edge-19`'
- en: '`Number of Lifs` is `4`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LIF 数量`为`4`'
- en: Remember we connected four logical networks to the distributed router? Hence
    the count is `4`
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记得我们将四个逻辑网络连接到了分布式路由器吗？因此数量是`4`
- en: '`Number of Routes` is `4`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`路由数`为`4`'
- en: 'Since we have connected four logical networks, the router is aware of those
    directly connected networks:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已连接四个逻辑网络，因此路由器能够识别这些直接连接的网络：
- en: '![Procedure for deploying a logical router](img/B03244_04_17.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的流程](img/B03244_04_17.jpg)'
- en: 'The following command will verify the network routes discovered by DLR:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将验证由DLR发现的网络路由：
- en: '[PRE1]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE2]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The logical router routing table is pushed by the NSX Controller to the ESXi
    host and it will be consistent across all the ESXi hosts. You will see the following
    output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑路由器的路由表由NSX控制器推送到ESXi主机，并且在所有ESXi主机之间保持一致。你将看到以下输出：
- en: '![Procedure for deploying a logical router](img/B03244_04_18.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的流程](img/B03244_04_18.jpg)'
- en: 'Now log in to the controller CLI to view the logical router state information:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在登录到控制器CLI查看逻辑路由器状态信息：
- en: '[PRE3]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will see the following output:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '![Procedure for deploying a logical router](img/B03244_04_19.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的流程](img/B03244_04_19.jpg)'
- en: 'The other command is:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个命令是：
- en: '[PRE4]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'All four logical switches (VXLAN 5000, 5001, 5002, and 5003, which we have
    connected to the logical router) are displaying in the following output with their
    respective interface IP, which would be the default gateway for web, app, and
    DB machines. Again, the idea here is to showcase the power of NSX CLI commands,
    which give granular-level information and are extremely useful when troubleshooting:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们连接到逻辑路由器的四个逻辑交换机（VXLAN 5000、5001、5002和5003）都显示在以下输出中，并带有它们各自的接口IP，这些IP将作为Web、应用和数据库机器的默认网关。再次强调，这里的目的是展示NSX
    CLI命令的强大功能，它们提供了细粒度的信息，在故障排除时非常有用：
- en: '![Procedure for deploying a logical router](img/B03244_04_20.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![部署逻辑路由器的流程](img/B03244_04_20.jpg)'
- en: Understanding logical interfaces
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解逻辑接口
- en: 'I''m pretty sure we now have a good understanding of how distributed routing
    works in the NSX environment. Again, NSX DLR is not limited between VXLAN networks;
    we can certainly leverage the routing functionality between VXLAN and VLAN networks.
    Let''s discuss logical interfaces in more detail:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信我们现在对NSX环境中分布式路由的工作原理有了充分的理解。再次强调，NSX DLR不仅限于VXLAN网络；我们完全可以利用VXLAN和VLAN网络之间的路由功能。接下来让我们更详细地讨论逻辑接口：
- en: If the Distributed Logical Router connects to a vSphere distributed switch port
    group, the interface is called a **VLAN LIF**. VLAN LIFs make use of **Designated
    Instance** (**DI**) for resolving ARP queries. The NSX Controller randomly selects
    one of the ESXi hosts as the designated instance to ease the ARP traffic so that
    any ARP traffic for that subnet will be handled by one of the ESXi hosts and every
    other ESXi host is also aware of where DI is running.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分布式逻辑路由器连接到vSphere分布式交换机端口组，则该接口称为**VLAN LIF**。VLAN LIF使用**指定实例**（**DI**）来解析ARP查询。NSX控制器会随机选择一个ESXi主机作为指定实例，以便处理ARP流量，这样该子网的任何ARP流量都会由其中一台ESXi主机处理，其他所有ESXi主机也会知道DI的运行位置。
- en: If the Distributed Logical Router connects to a logical switch, the interface
    is called a **VXLAN LIF**.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分布式逻辑路由器连接到逻辑交换机，则该接口称为**VXLAN LIF**。
- en: A LIF can either be an uplink or internal.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个LIF可以是上行链路接口或内部接口。
- en: Multiple LIFs can be configured on one Distributed Logical Router instance.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在一个分布式逻辑路由器实例上配置多个LIF。
- en: An ARP table is maintained for each LIF.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个LIF都有一个ARP表。
- en: Each LIF has assigned an IP address representing the default IP gateway for
    the logical network it connects to and a vMAC address. The IP address is unique
    for each LIF, whereas the same virtual MAC is assigned to all the defined LIFs.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个LIF都分配了一个IP地址，表示它连接的逻辑网络的默认IP网关，并且分配了一个vMAC地址。每个LIF的IP地址是唯一的，而所有定义的LIF都分配了相同的虚拟MAC地址。
- en: We can configure up to 999 interfaces, with a maximum of eight uplinks.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最多可以配置 999 个接口，最多支持 8 个上行链路。
- en: 'The routing table can be populated in multiple ways:'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由表可以通过多种方式填充：
- en: Directly connected
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接连接
- en: Static routes
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态路由
- en: OSPF
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: OSPF
- en: BGP
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: BGP
- en: Route redistribution
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由重分发
- en: We will discuss dynamic routing and route redistribution during [Chapter 5](ch05.html
    "Chapter 5.  NSX Edge Services") , *NSX Edge Services*, which will give us a clear
    view on how tenants access public networks (North-South connectivity).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [第 5 章](ch05.html "第 5 章. NSX Edge 服务") 中讨论动态路由和路由重分发，*NSX Edge 服务*，这将清楚地展示租户如何访问公共网络（北南连接）。
- en: Logical router deployment considerations
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑路由器部署注意事项
- en: '**Distributed Logical Router** (**DLR**) deployment is highly critical in NSX
    environments. Let''s check a few critical decision factors:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**分布式逻辑路由器** (**DLR**) 的部署在 NSX 环境中至关重要。让我们检查一些关键的决策因素：'
- en: Ensure that controllers are up-and-running before deploying a logical router.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署逻辑路由器之前，确保控制器已启动并正常运行。
- en: Don't deploy a logical router during controller deployment. This is not limited
    to DLR deployments; it applies for all NSX features.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在控制器部署期间不要部署逻辑路由器。这不仅限于 DLR 部署，适用于所有 NSX 功能。
- en: If a logical router is to be connected to VLAN dvPortgroups, ensure that all
    hypervisor hosts with a logical router appliance installed can reach each other
    on UDP port 6999 for logical router VLAN-based ARP proxy to work.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果逻辑路由器要连接到 VLAN dvPortgroups，请确保安装了逻辑路由器设备的所有虚拟化主机能够通过 UDP 端口 6999 互相访问，以便逻辑路由器
    VLAN 基于 ARP 代理功能能正常工作。
- en: Logical router interfaces should not be created on two different distributed
    port groups (dvPortgroups) with the same VLAN ID if the two networks are in the
    same vSphere distributed switch.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个网络位于同一 vSphere 分布式交换机上，则逻辑路由器接口不应创建在两个不同的分布式端口组（dvPortgroups）上，并且它们的 VLAN
    ID 应相同。
- en: Starting with VMware NSX for vSphere 6.2, the L2 bridging feature can now participate
    in distributed logical routing. The VXLAN network to which the bridge instance
    is connected will be used to connect the routing instance and the bridge instance.
    This was unsupported in earlier releases.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 VMware NSX for vSphere 6.2 开始，L2 桥接功能现在可以参与分布式逻辑路由。桥接实例连接的 VXLAN 网络将用于连接路由实例和桥接实例。这个功能在早期版本中不支持。
- en: DLR interfaces don't support trunking; however, each DLR interface can be connected
    to NSX Edge sub-interfaces which support trunking. But we are limited with leveraging
    IP-Sec, L2-VPN, BGP (dynamic routing), DHCP, and DNAT features on sub-interfaces.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLR 接口不支持中继；但是，每个 DLR 接口可以连接到支持中继的 NSX Edge 子接口。不过，我们在子接口上使用 IP-Sec、L2-VPN、BGP（动态路由）、DHCP
    和 DNAT 功能时有所限制。
- en: 1,000 logical interfaces are supported on DLR.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLR 支持 1,000 个逻辑接口。
- en: DLR doesn't support **virtual routing and forwarding** (**VRF**). For true network
    multitenancy, we need to deploy a unique DLR which can be connected to the same
    or different NSX Edges.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLR 不支持 **虚拟路由与转发** (**VRF**)。要实现真正的网络多租户功能，我们需要部署一个独特的 DLR，它可以连接到相同或不同的 NSX
    Edge。
- en: '**Equal Cost Multi Path** (**ECMP**) is supported in DLR; however, state full
    firewalls are not supported, primarily because of asymmetric routing behavior.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等价成本多路径** (**ECMP**) 在 DLR 中受支持；然而，由于存在非对称路由行为，状态全防火墙不受支持。'
- en: Note
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We will discuss ECMP and asymmetric routing in the next chapter.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在下一章讨论 ECMP 和非对称路由。
- en: The DLR control VM should not deployed in the compute cluster. If the host fails,
    both the data plane and control plane will be impacted at the same time if the
    control VM is also residing on same ESXi host. So the right place to deploy the
    DLR control VM will be either on the management cluster or if we have a separate
    vSphere Edge cluster, that is the best option.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLR 控制虚拟机不应部署在计算集群中。如果主机失败，数据平面和控制平面将同时受到影响，特别是当控制虚拟机也位于相同的 ESXi 主机上时。因此，部署
    DLR 控制虚拟机的正确位置应为管理集群，或者如果我们有一个单独的 vSphere Edge 集群，那将是最佳选择。
- en: Layer 2 bridges
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层 2 桥接
- en: 'Logical network to physical network access might be required due to multiple
    reasons in a NSX environment:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NSX 环境中，由于多种原因，可能需要将逻辑网络连接到物理网络：
- en: During **Physical to Virtual** (**P2V**) migrations where changing IP addresses
    is not an option
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **物理到虚拟** (**P2V**) 迁移过程中，如果不更改 IP 地址
- en: Extending virtual services in the logical switch to external devices
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将虚拟服务扩展到逻辑交换机外部设备
- en: Extending physical network services to virtual machines in logical switches
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将物理网络服务扩展到逻辑交换机中的虚拟机
- en: Accessing existing physical network and security resources
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问现有的物理网络和安全资源
- en: 'Since Layer 2 bridging is a NSX Edge Distributed Logical Router functionality,
    the L2 bridge runs on the same host on which the edge logical router control virtual
    machine is running. Bridging is entirely done at kernel level, as it was for Distributed
    Logical Routing. A special dvPort type called a **sink port** is used to steer
    packets to the bridge. In the following screenshot, we have a VXLAN environment
    wherein virtual machines in VXLAN network 5006 need to communicate with a physical
    site, which is in VLAN-100:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Layer 2 桥接是 NSX Edge 分布式逻辑路由器功能的一部分，L2 桥接运行在与边缘逻辑路由器控制虚拟机相同的主机上。桥接完全在内核级别完成，就像分布式逻辑路由一样。使用了一种特殊的
    dvPort 类型，称为**接收端口**，用于将数据包引导到桥接中。在以下截图中，我们有一个 VXLAN 环境，其中 VXLAN 网络 5006 中的虚拟机需要与
    VLAN-100 中的物理站点进行通信：
- en: '![Layer 2 bridges](img/B03244_04_21-1024x617.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![Layer 2 桥接](img/B03244_04_21-1024x617.jpg)'
- en: NSX Layer 2 bridging
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: NSX Layer 2 桥接
- en: Deploying an L2 bridge
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 L2 桥接
- en: 'Let''s have a look at the deployment of a Layer 2 bridge:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Layer 2 桥接的部署：
- en: Log in to the vSphere Web Client.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录 vSphere Web 客户端。
- en: Click **Networking & Security** and then click **NSX Edges**.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**网络与安全**，然后点击**NSX 边缘**。
- en: Double-click a logical router.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击逻辑路由器。
- en: Click **Manage** and then click **Bridging**.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**管理**，然后点击**桥接**。
- en: Click the add icon.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击添加图标。
- en: Type a name for the bridge.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入桥接的名称。
- en: Select the logical switch that you want to create a bridge for.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要为其创建桥接的逻辑交换机。
- en: Select the distributed virtual port group to which you want to bridge the logical
    switch.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要将逻辑交换机桥接到的分布式虚拟端口组。
- en: Click **OK**.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确定**。
- en: 'In the following example, we are bridging logical switch **bRANCH** with **Mgmt_Edge_VDS**
    port group which is VLAN enabled:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将逻辑交换机**bRANCH**与启用 VLAN 的**Mgmt_Edge_VDS**端口组进行桥接：
- en: '![Deploying an L2 bridge](img/B03244_04_22.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![部署 L2 桥接](img/B03244_04_22.jpg)'
- en: Design considerations for the L2 bridge
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 桥接的设计考虑
- en: 'Like any other NSX components, L2 bridging is an equally important design decision
    factor. The following are the key points:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他 NSX 组件一样，L2 桥接是一个同样重要的设计决策因素。以下是关键要点：
- en: Bridging VLAN-ID 0 is not supported.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持桥接 VLAN-ID 0。
- en: Multiple bridges are supported per logical router; however, we cannot have more
    than one bridge instance active per VXLAN-VLAN pair.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个逻辑路由器支持多个桥接实例；然而，我们不能在每个 VXLAN-VLAN 配对中拥有多个活动的桥接实例。
- en: Bridging cannot be used for VLAN-VLAN connection.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桥接不能用于 VLAN-VLAN 连接。
- en: Bridging is not a data center interconnect technology.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桥接不是数据中心互连技术。
- en: Starting from NSX 6.2, DLR interfaces can be connected to a VXLAN network that
    is bridged with a VLAN network. Earlier versions don't have this feature.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 NSX 6.2 开始，DLR 接口可以连接到与 VLAN 网络桥接的 VXLAN 网络。早期版本没有此功能。
- en: Don't mix a DLR and a next hop router (NSX Edge) on the same host; host failure
    will have a direct impact on both the devices.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要将 DLR 和下跳路由器（NSX Edge）混合部署在同一主机上；主机故障会对这两个设备产生直接影响。
- en: Even though Layer 2 bridging is a great feature, remember that all ARP resolutions
    are done explicitly by a bridge instance module running on the same host wherein
    we have deployed the logical router. For the same reason, running too many bridge
    instances and that too if they all are on the same host in addition to that if
    we have UTEPs and MTEPs running on the same host there would be certainly a performance
    impact. So try to run bridge instances on separate hosts as much as we can or,
    in another words, distribute logical router deployment across the management cluster.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管 Layer 2 桥接是一个很好的功能，但请记住，所有的 ARP 解析都是由运行在与我们部署逻辑路由器相同主机上的桥接实例模块显式完成的。出于同样的原因，运行过多的桥接实例，尤其是它们都在同一主机上，再加上如果
    UTEP 和 MTEP 也都在同一主机上运行，肯定会对性能产生影响。因此，尽量将桥接实例分布在不同的主机上，或者换句话说，跨管理集群分布逻辑路由器的部署。
- en: Summary
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: We started this chapter with an introduction to the NSX Distributed Logical
    Router and discussed VXLAN replication modes and a few packet walks. Later, we
    covered a few key design decisions while deploying a DLR. We also discussed the
    Layer 2 bridging feature of DLRs and we moved on to important design decisions
    that need to be noted while leveraging bridging functionality.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们介绍了 NSX 分布式逻辑路由器，并讨论了 VXLAN 复制模式和一些数据包流程。接着，我们介绍了在部署 DLR 时的一些关键设计决策。我们还讨论了
    DLR 的 Layer 2 桥接功能，并继续讲解了在利用桥接功能时需要注意的重要设计决策。
- en: There are exciting times ahead as we discuss more and more features and their
    functionalities.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在前方有令人激动的时刻，我们将讨论越来越多的功能及其作用。
- en: In the next chapter, we will discuss NSX Edge routing and we will establish
    connectivity with the DLR with a dynamic routing protocol.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论 NSX Edge 路由，并通过动态路由协议与 DLR 建立连接。
