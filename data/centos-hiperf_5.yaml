- en: Chapter 5. Monitoring the Cluster Health
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 监控集群健康状态
- en: In [Chapter 2](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 2. Installing Cluster Services and Configuring Network Components"),
    *Installing Cluster Services and Configuring Network Components*, we mentioned
    that becoming familiar with PCS and its myriad options would be helpful along
    the path that might lead us to the installation of a full operational high availability
    cluster. Although during the previous chapters we confirmed how true that statement
    was, here we will make further use of PCS to monitor the performance and availability
    of our cluster in order to identify and prevent possible bottlenecks and troubleshoot
    any issue that may arise.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0 "第2章.
    安装集群服务并配置网络组件")，*安装集群服务并配置网络组件*中，我们提到，熟悉PCS及其众多选项将有助于我们朝着安装一个完全可操作的高可用性集群的目标迈进。尽管在前面的章节中我们已经确认了这一说法的准确性，但在这里，我们将进一步利用PCS来监控集群的性能和可用性，以便识别并防止可能的瓶颈，并解决可能出现的任何问题。
- en: Cluster services and performance
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群服务和性能
- en: Although every system administrator must be well acquainted with the widely
    used Linux commands, such as `top` and `ps`, to quickly report a snapshot of running
    daemons and other processes in each node, you must also learn to rely on the new
    utilities provided by CentOS 7 to start our node monitoring, which we have introduced
    in previous chapters. But even more importantly, we will also use PCS-based commands
    to gain further insight into our cluster and its resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个系统管理员必须熟悉广泛使用的Linux命令，如`top`和`ps`，以便快速报告每个节点中运行的守护进程和其他进程的快照，但您还必须学会依赖CentOS
    7提供的新工具，来启动我们的节点监控，这些工具我们在之前的章节中已经介绍过。但更重要的是，我们还将使用基于PCS的命令来深入了解我们的集群及其资源。
- en: Monitoring the node status
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控节点状态
- en: As you can guess, perhaps the first thing that you always need to check is the
    status of each node—whether they are online or offline. Otherwise, there is little
    point in proceeding with further availability and performance analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所猜测的那样，也许您首先需要检查的就是每个节点的状态——它们是在线还是离线。否则，继续进行进一步的可用性和性能分析就没有意义了。
- en: If you have a network management system (such as **Zabbix** or **Nagios**) server,
    you can easily monitor the status of your cluster members and receive alerts when
    they are unreachable. If not, you must come up with a supplementary solution of
    your own (which may not be as effective or errorproof) that you can use to detect
    when a node has gone offline.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个网络管理系统（例如**Zabbix**或**Nagios**）服务器，您可以轻松监控集群成员的状态，并在它们无法访问时收到警报。如果没有，您必须想出一个补充解决方案（可能没有那么有效或没有那么防错），用来检测节点何时离线。
- en: One such solution is a simple bash script (we will name it `pingreport.sh`,
    save it inside `/root/scripts`, and make it executable with `chmod +x /root/scripts/pingreport.sh`)
    which will periodically ping your nodes from another host and report via an e-mail
    to the system administrator if one of them is offline in order for you to take
    appropriate action. The following shell script does just that for nodes with IP
    addresses `192.168.0.2` and `192.168.0.3` (you can add as many nodes in the `NODES`
    variable, which will be used in the following for loop, but remember to separate
    them with a blank space). If both nodes are pingable, the report will be empty
    and no e-mails will be sent.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是一个简单的bash脚本（我们将其命名为`pingreport.sh`，并将其保存在`/root/scripts`中，通过`chmod +x
    /root/scripts/pingreport.sh`使其可执行），它将定期从另一台主机ping您的节点，并通过电子邮件向系统管理员报告，如果其中一个节点处于离线状态，以便您采取适当的措施。以下shell脚本正是这样做的，它针对IP地址为`192.168.0.2`和`192.168.0.3`的节点（您可以在`NODES`变量中添加更多节点，这些节点将用于以下的for循环，但记得用空格分隔它们）。如果两个节点都能ping通，则报告将为空，且不会发送任何电子邮件。
- en: 'In order to take advantage of the following script, you will need to have an
    e-mail solution in place in order to send out alerts. In this case, we use the
    mail tool called mailx, which is available after installing a package (`yum install
    mailx`):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用以下脚本，您需要准备好电子邮件解决方案，以便发送警报。在这种情况下，我们使用名为mailx的邮件工具，该工具在安装包（`yum install
    mailx`）后可用：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Even though the preceding script is enough to determine whether a node is pingable
    or not, you can tweak that script as you like, and then add it to cron in order
    for it to run automatically on the desired frequency. For example, the following
    cron job will execute the script every five minutes, regardless of the day:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的脚本足以判断一个节点是否可以 ping 通，你可以根据需要调整该脚本，然后将其添加到 cron 中，以便按期望的频率自动运行。例如，以下的 cron
    作业将每五分钟执行一次该脚本，无论是哪个日子：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to run the script manually, you can do so as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想手动运行脚本，可以按以下方式进行：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following example indicates that both `192.168.0.2` and `192.168.0.3` were
    not pingable when the script was last run. Note that for simplicity, the script
    was executed from `node01`, a cluster member; however, under normal circumstances,
    you will want to use a separate host for this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例表明，在上次运行脚本时，`192.168.0.2` 和 `192.168.0.3` 都无法 ping 通。请注意，为了简便起见，脚本从 `node01`（集群成员）执行；然而，在正常情况下，你会希望使用一个独立的主机来执行该操作：
- en: '![Monitoring the node status](img/00053.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![监控节点状态](img/00053.jpeg)'
- en: We will resume working with the script later in this chapter and extend its
    functionalities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后继续使用该脚本，并扩展其功能。
- en: 'Now, it is time to dig a little deeper and view the status of the nodes configured
    in `corosync`/`pacemaker` with the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候深入查看 `corosync`/`pacemaker` 中配置节点的状态了，使用以下命令：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding command, a vertical bar is used to indicate mutually exclusive
    arguments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，使用竖线来表示互斥的参数。
- en: 'In the following screenshot, you can see how `pcs status nodes both` returns
    the status of both `pacemaker` and `corosync` on both nodes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，你可以看到`pcs status nodes both`如何返回两个节点上`pacemaker`和`corosync`的状态：
- en: '![Monitoring the node status](img/00054.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![监控节点状态](img/00054.jpeg)'
- en: Note
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Although you can check the cluster's overall status with `pcs status`, as we
    have mentioned earlier, `pcs status nodes both` will give you the fine-grained
    node status information. You can stop one (or both) of the services on either
    node and run this same command to verify. This is equivalent to using `systemctl
    is-active pacemaker | corosync` on each node.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以使用`pcs status`检查集群的整体状态，但正如我们之前提到的，`pcs status nodes both`会提供更加精细的节点状态信息。你可以停止其中一个（或两个）节点上的服务，然后运行相同的命令来验证。这相当于在每个节点上使用`systemctl
    is-active pacemaker | corosync`。
- en: Monitoring the resources
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控资源
- en: As we have explained in the previous chapters, a cluster resource is a highly
    available service that is made available through at least one of the nodes. Among
    the resources that we configured up until this point, we can mention the virtual
    IP, the replicated storage device, the web server, and the database server. You
    can refer to [Chapter 4](part0030_split_000.html#SJGS1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 4. Real-world Implementations of Clustering"), *Real-world Implementations
    of Clustering*, where we added constraints that indicated how (in what order)
    and where (in which node) the cluster resources should be started.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所解释的，集群资源是通过至少一个节点提供的高可用服务。在我们到目前为止配置的资源中，可以提到虚拟 IP、复制存储设备、Web 服务器和数据库服务器。你可以参考[第
    4 章](part0030_split_000.html#SJGS1-1d2c6d19b9d242db82da724021b51ea0 "第 4 章：集群的实际应用")，*集群的实际应用*，我们在其中添加了约束，指示了集群资源应该如何（按什么顺序）以及在哪个节点上启动。
- en: Either `pcs status` or `pcs resource show`, the preferred alternative, will
    list the names and status of all currently configured resources.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是`pcs status`还是`pcs resource show`，都将列出当前所有配置资源的名称和状态。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you specify a resource using its ID (that is, `pcs resource show virtual_ip`),
    you will see the options for the configured resource. On the other hand, if `--full`
    is specified (`pcs resource show --full`), all configured resource options will
    be displayed instead.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过其 ID 指定一个资源（即`pcs resource show virtual_ip`），你将看到该资源的配置选项。另一方面，如果指定了`--full`（`pcs
    resource show --full`），则会显示所有配置的资源选项。
- en: 'If a resource is started on the wrong node (for example, if it depends on a
    service that is currently active on another node), you will get an informative
    message when you attempt to use it. For example, the following screenshot shows
    that `dbserver` is started on `node02`, whereas its associated underlying storage
    device (`db_fs`) has been started on `node01`. You will recall from earlier chapters
    that this is part of the output of `pcs status`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果资源在错误的节点上启动（例如，如果它依赖于当前在另一个节点上活动的服务），在尝试使用它时，你会收到一条信息性消息。例如，以下截图显示`dbserver`在`node02`上启动，而其相关的底层存储设备（`db_fs`）已在`node01`上启动。你会从之前的章节中回忆到，这是`pcs
    status`输出的一部分：
- en: '![Monitoring the resources](img/00055.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00055.jpeg)'
- en: 'For this reason, if you attempt to log on to the database server using the
    virtual IP address (which is the common link to the cluster resources), you will
    get the error message indicated in the following screenshot telling you that you
    can''t connect to the MariaDB instance:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你尝试使用虚拟IP地址登录到数据库服务器（这是集群资源的公共链接），你将得到以下截图中所示的错误消息，告诉你无法连接到MariaDB实例：
- en: '![Monitoring the resources](img/00056.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00056.jpeg)'
- en: 'Let''s see what happens (as shown in the next screenshot) when we move the
    `dbserver` resource to `node01` and enable it manually so that it starts right
    away. The following constraint is intended to cause `dbserver` to prefer `node01`
    so that it always runs on `node01` whenever such a node is available:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们将`dbserver`资源移动到`node01`并手动启用它，以便它立即启动时会发生什么（如下一个截图所示）。以下约束旨在使`dbserver`优先选择`node01`，这样每当有可用节点时，它始终在`node01`上运行：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Monitoring the resources](img/00057.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00057.jpeg)'
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you need to remove a constraint, find out its id with `pcs constraint --full`
    and locate the associated resource. Then, delete it with `pcs constraint remove
    constraint_id`, where `constraint_id` is the identification as returned by the
    first command. You can also manually remove resources from one node to another
    with `pcs resource move <resource_id> <node_name>`, but be aware that the current
    constraints may or may not allow you to successfully complete the operation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要删除约束，可以使用`pcs constraint --full`查找其ID，并定位相关资源。然后，使用`pcs constraint remove
    constraint_id`删除它，其中`constraint_id`是第一个命令返回的标识符。你也可以使用`pcs resource move <resource_id>
    <node_name>`手动将资源从一个节点移到另一个节点，但请注意，当前的约束可能会允许或不允许你成功完成此操作。
- en: 'Now we can access the database server resource as expected, as shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按预期访问数据库服务器资源，如下所示：
- en: '![Monitoring the resources](img/00058.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00058.jpeg)'
- en: 'Once in a while, you may encounter some errors during or after a failover procedure
    or during boot—you name it. These messages are visible in the output of `pcs status`,
    as in the excerpt shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔，在故障转移过程或启动过程中，你可能会遇到一些错误——你可以自己判断。这些消息会出现在`pcs status`的输出中，如下文摘所示：
- en: '![Monitoring the resources](img/00059.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00059.jpeg)'
- en: 'Before we proceed further, perhaps you will ask yourself: What if I want to
    save all available information about cluster problems to properly analyze and
    troubleshoot offline? If you are expecting PCS to have a tool to help you with
    that, you are right. Put a date and time following the `--from` and `--to` options
    and replace `dest` with a filename (a specific example is provided in the following
    command as well):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，也许你会问自己：如果我想保存关于集群问题的所有可用信息，以便进行离线分析和故障排除该怎么办？如果你期待PCS提供一个工具来帮助你，你是对的。在`--from`和`--to`选项后输入日期和时间，并将`dest`替换为文件名（在以下命令中也提供了具体示例）：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create a tarball containing every piece of information that is needed
    when reporting cluster problems. If `--from` and `--to` are not used, the report
    will include the data of the last 24 hours.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含报告集群问题所需的所有信息的tar包。如果没有使用`--from`和`--to`选项，则报告将包括过去24小时的数据。
- en: In the screenshot that will follow, we have omitted the `--from` and `--to`
    flags for brevity, and we can see yet another reason why setting up key-based
    authentication via `ssh` during [Chapter 1](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 1. Cluster Basics and Installation on CentOS 7"), *Cluster Basics and
    Installation on CentOS 7* was not a mere suggestion—you have to report cluster
    information from both nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的截图中，我们省略了`--from`和`--to`标志以简洁起见，我们还可以看到设置通过`ssh`进行基于密钥的认证在[第1章](part0014_split_000.html#DB7S1-1d2c6d19b9d242db82da724021b51ea0
    "第1章：CentOS 7上的集群基础和安装")中并非仅仅是一个建议——您必须报告来自两个节点的集群信息。
- en: 'In our case, we will execute the following command to obtain a tarball named
    `YYYY-MM-DD-report.tar.gz` in the current working directory. Note that the date
    part in the filename is for identification purposes only:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将执行以下命令，在当前工作目录中获取一个名为`YYYY-MM-DD-report.tar.gz`的tar包。请注意，文件名中的日期部分仅用于标识：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Monitoring the resources](img/00060.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00060.jpeg)'
- en: 'Once the tarball with the report files has been created, you can untar and
    examine it. You will notice that it contains the files and directories seen in
    the following image. Before proceeding further, you may want to take a look at
    some of them, shown as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦包含报告文件的tar包创建完成，您可以解压并检查它。您会注意到它包含了以下图像中看到的文件和目录。在进一步操作之前，您可能希望查看其中的一些文件，如下所示：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Monitoring the resources](img/00061.jpeg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00061.jpeg)'
- en: 'Now, of course you want to purge records of the past failed actions that have
    been resolved. For this reason, PCS allows you to instruct the cluster to forget
    the operation history of a resource (or all of them), reset the fail count, and
    redetect the current states:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您现在可能想清除过去已解决的失败操作记录。为此，PCS允许您指示集群忘记资源（或所有资源）的操作历史，重置故障计数，并重新检测当前状态：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that if `resource_id` is not specified, then all resources/STONITH devices
    will be cleaned up.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果未指定`resource_id`，则所有资源/STONITH设备将被清理。
- en: 'Finally, while we are still talking about monitoring cluster resources, we
    might as well ask ourselves: Is there a way we can backup the current cluster
    configuration files and restore them later if needed, and can we easily go back
    to a previous configuration? The answer to both questions is yes—let''s see how.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们还在讨论监控集群资源时，不妨问自己：是否有一种方法可以备份当前的集群配置文件，并在需要时恢复它们，是否可以轻松回到之前的配置？这两个问题的答案是肯定的——让我们看看怎么做。
- en: 'In order to back up the cluster configuration files, you will use the following
    command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了备份集群配置文件，您将使用以下命令：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, `<filename>` is a file identification of your choice to which PCS will
    append the `tar.bz2` extension after creating the tarball.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<filename>`是您选择的文件标识，PCS会在创建tar包后为其追加`tar.bz2`扩展名。
- en: 'Consider the following example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the tarball backup with the contents shown in the following
    screenshot For our convenience, let us create a subdirectory named `cluster_config`
    inside our current working directory. We will use this newly created subdirectory
    to extract the contents of the report tarball:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致创建一个tar包备份，内容如以下截图所示。为了方便起见，我们将在当前工作目录中创建一个名为`cluster_config`的子目录。我们将使用这个新创建的子目录来解压报告tar包的内容：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Monitoring the resources](img/00062.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00062.jpeg)'
- en: Note
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you have followed the installation process step by step, as outlined in this
    book, bzip2 will most likely not be available. You will need to install it with
    `yum update && yum install bzip2` in order to untar the cluster configuration
    tarball.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您按照本书中概述的步骤逐步进行安装，bzip2可能未安装。您需要使用`yum update && yum install bzip2`来安装它，以便解压集群配置tar包。
- en: 'Restoring the configuration is just as easy (you will need to stop the node
    and then start it again after the restoration process is completed), use the following
    command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复配置同样简单（您需要停止节点并在恢复过程完成后重新启动它），请使用以下命令：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This command will restore the backed-up cluster configuration files on all nodes
    using the backup as source. If you only need to restore the files on the current
    node, use the `--local flag`. Note that filename must be the `.tar.bz2` file (not
    the extracted files).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将使用备份文件作为源，在所有节点上恢复集群配置文件。如果只需要恢复当前节点的文件，请使用`--local flag`。请注意，文件名必须是`.tar.bz2`文件（而不是解压后的文件）。
- en: 'You can also go back to a certain point in time, as far as cluster configuration
    is concerned, using `pcs config checkpoint` with its associated options. With
    no options, `pcs config checkpoint` will list all available configuration checkpoints,
    as shown here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过 `pcs config checkpoint` 及其相关选项，回到集群配置的某个时间点。如果不加任何选项，`pcs config checkpoint`
    会列出所有可用的配置检查点，如下所示：
- en: '![Monitoring the resources](img/00063.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00063.jpeg)'
- en: 'The `pcs config checkpoint view <checkpoint_number>` command displays to standard
    output the specified configuration checkpoint details, as shown in the next screenshot.
    Consider the following example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`pcs config checkpoint view <checkpoint_number>` 命令将指定的配置检查点详细信息显示到标准输出中，如下图所示。请参考以下示例：'
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Monitoring the resources](img/00064.jpeg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![监控资源](img/00064.jpeg)'
- en: The `pcs config checkpoint restore <checkpoint_number>` command restores cluster
    configuration to a specified checkpoint, which is why it's a great idea to check
    the details of the desired checkpoint before restoring.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`pcs config checkpoint restore <checkpoint_number>` 命令将集群配置恢复到指定的检查点，这也是在恢复之前检查所需检查点详细信息的一个好主意。'
- en: When a resource refuses to start
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当资源拒绝启动
- en: Under normal circumstances, cluster resources will be managed automatically
    without much intervention from the system administrator. However, there will be
    times when something may prevent a resource from starting properly, and it will
    be necessary to take immediate action.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，集群资源将自动管理，系统管理员不需要太多干预。然而，有时可能会出现某些问题，导致资源无法正常启动，这时就需要采取即时的行动。
- en: As the man page for PCS states,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如 PCS 手册页所述，
- en: '*Starting resources on a cluster is (almost) always done by pacemaker and not
    directly from PCS. If your resource isn''t starting, it''s usually due to either
    a misconfiguration of the resource (which you debug in the system log), or constraints
    preventing the resource from starting or the resource being disabled. You can
    use `pcs resource debug-start` to test resource configuration, but it should not
    normally be used to start resources in a cluster.*'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*在集群上启动资源几乎总是由 pacemaker 完成，而不是直接由 PCS 执行。如果你的资源没有启动，通常是由于资源配置错误（你可以通过系统日志进行调试）、约束条件阻止资源启动或资源被禁用。你可以使用
    `pcs resource debug-start` 测试资源配置，但通常不应使用它来启动集群中的资源。*'
- en: 'Having said that, when `pacemaker` cannot, for some reason, properly start
    a resource, execute the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，当 `pacemaker` 因某些原因无法正常启动资源时，可以执行以下命令：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will force the specified resource to start on the current node, ignoring
    the cluster recommendations. The result will be printed to the screen (use the
    `--full` flag to obtain more detailed output) and will provide helpful information
    to assist you in troubleshooting the resource and the cluster operation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制在当前节点上启动指定的资源，忽略集群的推荐配置。结果将显示在屏幕上（使用 `--full` 参数可以获取更详细的输出），并提供有助于排除资源和集群操作故障的相关信息。
- en: 'In the following screenshot, the output of `pcs resource debug-start virtual_ip
    --full` is truncated for the sake of brevity:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，`pcs resource debug-start virtual_ip --full` 的输出已被截断，以简化显示：
- en: '![When a resource refuses to start](img/00065.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![当资源拒绝启动](img/00065.jpeg)'
- en: 'From this example, you can begin to glimpse how useful this command can be
    as it provides you with very detailed information, step by step, of the resource
    operation. For example, if the `dbserver` resource refuses to start and returns
    errors even after repeatedly having cleaned it up, run the following command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个示例中，你可以开始领略到这个命令的实用性，因为它会逐步为你提供关于资源操作的非常详细的信息。例如，如果 `dbserver` 资源即使在反复清理后仍然无法启动并返回错误，可以运行以下命令：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With this, you will be able to view—with great detail—the steps that are usually
    performed by the cluster when trying to bring up such a resource. If this process
    fails at some point, you will be provided with a description of what went wrong
    and when, and then you will be better able to fix it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此命令，你将能够非常详细地查看集群通常在尝试启动此类资源时执行的步骤。如果该过程在某个环节失败，你将获得关于出错原因和时间的描述，帮助你更好地修复问题。
- en: Checking the availability of core components
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查核心组件的可用性
- en: Before wrapping up, let's go back to the first example (checking the online
    status of each node) and extend it so that we can also monitor the core components
    of the cluster framework, that is, `pacemaker`, `corosync`, and `pcsd`, as outlined
    earlier in [Chapter 2](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0
    "Chapter 2. Installing Cluster Services and Configuring Network Components"),
    *Installing Cluster Services and Configuring Network Components*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，让我们回到第一个示例（检查每个节点的在线状态），并对其进行扩展，以便我们还能够监控集群框架的核心组件，即`pacemaker`、`corosync`和`pcsd`，这些内容在[第2章](part0018_split_000.html#H5A42-1d2c6d19b9d242db82da724021b51ea0
    "第2章 安装集群服务和配置网络组件")，*安装集群服务和配置网络组件*中已有概述。
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'In order to ensure a successful connection via `ssh` from a node to itself,
    you will need to copy its key to `authorized_keys` Thus, to enable passwordless
    user login for user root, run the following command on both nodes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保通过`ssh`从一个节点成功连接到自身，您需要将其密钥复制到`authorized_keys`中。因此，要为用户root启用无密码登录，请在两个节点上运行以下命令：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the best case scenario, during a graceful failover, you will want to be
    notified whenever one (or more) of those services is stopped. Adding a few lines
    to the script will also check for the status of the corresponding daemons and
    alert you if they''re down:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在最佳情况下，在优雅的故障转移期间，您希望在其中一个（或多个）服务停止时收到通知。通过在脚本中添加几行代码，还可以检查相应守护进程的状态，并在它们停止时发出警报：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As a simple test, stop the cluster (`pcs cluster stop`) on `node02` (`192.168.0.3`),
    run the script from the monitoring host or from any node, and check your mail
    inbox to verify that it is working correctly. In the following screenshot, you
    can see an example of what it should look like:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的测试，请在`node02`（`192.168.0.3`）上停止集群（`pcs cluster stop`），从监控主机或任何节点运行脚本，并检查您的邮件收件箱，以验证它是否正常工作。在以下截图中，您可以看到其应该呈现的样子：
- en: '![Checking the availability of core components](img/00066.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![检查核心组件的可用性](img/00066.jpeg)'
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explained how to monitor, troubleshoot, and fix common
    cluster problems and needs. Not all of these will be undesired or unexpected as
    a sudden system crash. There will be times when you need to bring down the cluster
    and the resources it is running for some planned maintenance or during a power
    outage before your **uninterruptible power supply** (**UPS**) runs out.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们已经解释了如何监控、故障排除和修复常见的集群问题和需求。并非所有问题都会像突然的系统崩溃那样是不期而至或不希望出现的。有时，您需要关闭集群及其运行的资源，以便进行计划维护，或者在断电时，确保**不间断电源**（**UPS**）还没有耗尽。
- en: Because prevention is your best ally in these circumstances, ensure that you
    routinely monitor the health of your cluster. Follow the procedures outlined in
    this chapter so that you don't run into any surprises when real emergencies come
    up. Specifically, under either real or simulated cases, ensure that you back up
    the cluster configuration, stop the cluster on both nodes separately, and then
    and only then, halt the node.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预防是此类情况下最好的盟友，请确保定期监控集群的健康状况。按照本章中概述的程序进行操作，以确保在真实的紧急情况出现时不会遇到任何意外。具体来说，无论是在真实还是模拟的情况下，都必须备份集群配置，分别停止两个节点上的集群，然后才能停用节点。
